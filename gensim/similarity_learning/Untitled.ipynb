{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we will install the dependencies for running the Similarity Learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from gensim.similarity_learning import DRMM_TKS_Model\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to provide data in a format which is understood by the model.\n",
    "The model understands sentences as a list of words. \n",
    "Further, we need to give a :\n",
    " 1. Queries List\n",
    " 2. Candidate Document List\n",
    " 3. Correct Label List\n",
    "\n",
    "1 is a list of list of words\n",
    "2 and 3 is actually a list of list of list of words/ints\n",
    "\n",
    "Example:\n",
    "```\n",
    "queries = [\"When was Abraham Lincoln born ?\".split(), \n",
    "            \"When was the first World War ?\".split()]\n",
    "docs = [\n",
    "\t\t [\"Abraham Lincoln was the president of the United States of America\".split(),\n",
    "\t\t \"He was born in 1809\".split()],\n",
    "\t\t [\"The first world war was bad\".split(),\n",
    "\t\t \"It was fought in 1914\".split(),\n",
    "\t\t \"There were over a million deaths\".split()]\n",
    "       ]\n",
    "labels = [[0,\n",
    "           1],\n",
    "\t\t  [0,\n",
    "           1,\n",
    "           0]\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset : WikiQA\n",
    "\n",
    "The WikiQA corpus is a set of question-answer pairs in which for every query there are several candidate documents of which none, one or more documents might be relevant.\n",
    "Relevance is purely binary, i.e., 1: relavant, 0: not relevant\n",
    "\n",
    "Sample data:\n",
    "```\n",
    "QuestionID\tQuestion\tDocumentID\tDocumentTitle\tSentenceID\tSentence\tLabel\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-0\tA partly submerged glacier cave on Perito Moreno Glacier .\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-1\tThe ice facade is approximately 60 m high\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-2\tIce formations in the Titlis glacier cave\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-3\tA glacier cave is a cave formed within the ice of a glacier .\t1\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-4\tGlacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice.\t0\n",
    "```\n",
    "\n",
    "## Data Preprocessing\n",
    "We need to take the above text and make it into `queries, docs, labels` form\n",
    "We use the below code for that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill the below with wherever you have your WikiQACorpus Folder\n",
    "wikiqa_data_path = os.path.join('data', 'WikiQACorpus', 'WikiQA-train.tsv')\n",
    "\n",
    "\n",
    "def preprocess_sent(sent):\n",
    "    \"\"\"Utility function to lower, strip and tokenize each sentence\n",
    "    \n",
    "    Replace this function if you want to handle preprocessing differently\"\"\"\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", sent.strip().lower()).split()\n",
    "\n",
    "# Defining some consants for .tsv reading\n",
    "QUESTION_ID_INDEX = 0\n",
    "QUESTION_INDEX = 1\n",
    "ANSWER_INDEX = 5\n",
    "LABEL_INDEX = 6\n",
    "\n",
    "with open(wikiqa_data_path, encoding='utf8') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    data_rows = []\n",
    "    for row in tsv_reader:\n",
    "        data_rows.append(row)\n",
    "\n",
    "\n",
    "        \n",
    "document_group = []\n",
    "label_group = []\n",
    "\n",
    "n_relevant_docs = 0\n",
    "n_filtered_docs = 0\n",
    "\n",
    "queries = []\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for i, line in enumerate(data_rows[1:], start=1):\n",
    "    if i < len(data_rows) - 1:  # check if out of bounds might occur\n",
    "        if data_rows[i][QUESTION_ID_INDEX] == data_rows[i + 1][QUESTION_ID_INDEX]:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "        else:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "            if n_relevant_docs > 0:\n",
    "                docs.append(document_group)\n",
    "                labels.append(label_group)\n",
    "                queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "            else:\n",
    "                n_filtered_docs += 1\n",
    "\n",
    "            n_relevant_docs = 0\n",
    "            document_group = []\n",
    "            label_group = []\n",
    "\n",
    "    else:\n",
    "        # If we are on the last line\n",
    "        document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "        label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "        n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "        if n_relevant_docs > 0:\n",
    "            docs.append(document_group)\n",
    "            labels.append(label_group)\n",
    "            queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "        else:\n",
    "            n_filtered_docs += 1\n",
    "            n_relevant_docs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'did', 'hurricane', 'katrina', 'begin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hurricane', 'katrina', 'was', 'the', 'deadliest', 'and', 'most', 'destructive', 'atlantic', 'hurricane', 'of', 'the', '2005', 'atlantic', 'hurricane', 'season'], ['it', 'was', 'the', 'costliest', 'natural', 'disaster', 'as', 'well', 'as', 'one', 'of', 'the', 'five', 'deadliest', 'hurricanes', 'in', 'the', 'history', 'of', 'the', 'united', 'states'], ['among', 'recorded', 'atlantic', 'hurricanes', 'it', 'was', 'the', 'sixth', 'strongest', 'overall'], ['at', 'least', '1', '833', 'people', 'died', 'in', 'the', 'hurricane', 'and', 'subsequent', 'floods', 'making', 'it', 'the', 'deadliest', 'u', 's', 'hurricane', 'since', 'the', '1928', 'okeechobee', 'hurricane', 'total', 'property', 'damage', 'was', 'estimated', 'at', '81', 'billion', '2005', 'usd', 'nearly', 'triple', 'the', 'damage', 'brought', 'by', 'hurricane', 'andrew', 'in', '1992'], ['hurricane', 'katrina', 'formed', 'over', 'the', 'bahamas', 'on', 'august', '23', '2005', 'and', 'crossed', 'southern', 'florida', 'as', 'a', 'moderate', 'category', '1', 'hurricane', 'causing', 'some', 'deaths', 'and', 'flooding', 'there', 'before', 'strengthening', 'rapidly', 'in', 'the', 'gulf', 'of', 'mexico'], ['the', 'hurricane', 'strengthened', 'to', 'a', 'category', '5', 'hurricane', 'over', 'the', 'warm', 'gulf', 'water', 'but', 'weakened', 'before', 'making', 'its', 'second', 'landfall', 'as', 'a', 'category', '3', 'hurricane', 'on', 'the', 'morning', 'of', 'monday', 'august', '29', 'in', 'southeast', 'louisiana'], ['it', 'caused', 'severe', 'destruction', 'along', 'the', 'gulf', 'coast', 'from', 'central', 'florida', 'to', 'texas', 'much', 'of', 'it', 'due', 'to', 'the', 'storm', 'surge'], ['the', 'most', 'significant', 'number', 'of', 'deaths', 'occurred', 'in', 'new', 'orleans', 'louisiana', 'which', 'flooded', 'as', 'the', 'levee', 'system', 'catastrophically', 'failed', 'in', 'many', 'cases', 'hours', 'after', 'the', 'storm', 'had', 'moved', 'inland'], ['eventually', '80', 'of', 'the', 'city', 'and', 'large', 'tracts', 'of', 'neighboring', 'parishes', 'became', 'flooded', 'and', 'the', 'floodwaters', 'lingered', 'for', 'weeks'], ['however', 'the', 'worst', 'property', 'damage', 'occurred', 'in', 'coastal', 'areas', 'such', 'as', 'all', 'mississippi', 'beachfront', 'towns', 'which', 'were', 'flooded', 'over', '90', 'in', 'hours', 'as', 'boats', 'and', 'casino', 'barges', 'rammed', 'buildings', 'pushing', 'cars', 'and', 'houses', 'inland', 'with', 'waters', 'reaching', '6', '12', 'miles', '10', '19', 'km', 'from', 'the', 'beach'], ['the', 'hurricane', 'surge', 'protection', 'failures', 'in', 'new', 'orleans', 'are', 'considered', 'the', 'worst', 'civil', 'engineering', 'disaster', 'in', 'u', 's', 'history', 'and', 'prompted', 'a', 'lawsuit', 'against', 'the', 'u', 's', 'army', 'corps', 'of', 'engineers', 'usace', 'the', 'designers', 'and', 'builders', 'of', 'the', 'levee', 'system', 'as', 'mandated', 'by', 'the', 'flood', 'control', 'act', 'of', '1965'], ['responsibility', 'for', 'the', 'failures', 'and', 'flooding', 'was', 'laid', 'squarely', 'on', 'the', 'army', 'corps', 'in', 'january', '2008', 'by', 'judge', 'stanwood', 'duval', 'u', 's', 'district', 'court', 'but', 'the', 'federal', 'agency', 'could', 'not', 'be', 'held', 'financially', 'liable', 'due', 'to', 'sovereign', 'immunity', 'in', 'the', 'flood', 'control', 'act', 'of', '1928'], ['there', 'was', 'also', 'an', 'investigation', 'of', 'the', 'responses', 'from', 'federal', 'state', 'and', 'local', 'governments', 'resulting', 'in', 'the', 'resignation', 'of', 'federal', 'emergency', 'management', 'agency', 'fema', 'director', 'michael', 'd', 'brown', 'and', 'of', 'new', 'orleans', 'police', 'department', 'nopd', 'superintendent', 'eddie', 'compass'], ['several', 'agencies', 'including', 'the', 'united', 'states', 'coast', 'guard', 'uscg', 'national', 'hurricane', 'center', 'nhc', 'and', 'national', 'weather', 'service', 'nws', 'were', 'commended', 'for', 'their', 'actions'], ['they', 'provided', 'accurate', 'hurricane', 'weather', 'tracking', 'forecasts', 'with', 'sufficient', 'lead', 'time'], ['even', 'the', 'most', 'insistent', 'appeals', 'from', 'national', 'state', 'and', 'local', 'public', 'officials', 'to', 'residents', 'to', 'evacuate', 'before', 'the', 'storm', 'did', 'not', 'warn', 'that', 'the', 'levees', 'could', 'breach', 'and', 'fail']]\n"
     ]
    }
   ],
   "source": [
    "print(docs[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a train-validation split\n",
    "At this point, it would be good to make a train-validation split so we can see how the model performs as it trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries, test_queries = queries[:int(len(queries)*0.8)], queries[int(len(queries)*0.8): ]\n",
    "train_docs, test_docs = docs[:int(len(docs)*0.8)], docs[int(len(docs)*0.8):]\n",
    "train_labels, test_labels = labels[:int(len(labels)*0.8)], labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697 175\n",
      "697 175\n",
      "697 175\n"
     ]
    }
   ],
   "source": [
    "print(len(train_queries), len(test_queries))\n",
    "print(len(train_docs), len(test_docs))\n",
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "If we want to train the model with some pretrained word embeddingd like Glove, we will have to specify the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_path = os.path.join('data', 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to monitor the progress of training of the model.\n",
    "However, we can't rely on the metrics provided by keras as those metrics don't necessarily apply to Information Retrieval problems.\n",
    "\n",
    "We can additionally provide a validation dataset which will be tested after every epoch.\n",
    "\n",
    "Now that we have the preprocessed extracted data, training the model just takes one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:51:51,255 : INFO : Starting Vocab Build\n",
      "2018-06-18 19:51:51,380 : INFO : Vocab Build Complete\n",
      "2018-06-18 19:51:51,381 : INFO : Vocab Size is 17776\n",
      "2018-06-18 19:51:51,382 : INFO : Building embedding index using pretrained word embeddings\n",
      "2018-06-18 19:52:03,115 : INFO : The embeddings_index built from the given file has 400000 words of 50 dimensions\n",
      "2018-06-18 19:52:03,117 : INFO : Embedding Matrix for Embedding Layer has shape (17777, 50) \n",
      "2018-06-18 19:52:03,165 : INFO : There are 590 words not in the embeddings. Setting them to zero\n",
      "2018-06-18 19:52:03,166 : INFO : Adding additional dimensions from the embedding file to embedding matrix\n",
      "2018-06-18 19:52:04,632 : INFO : Normalizing the word embeddings\n",
      "2018-06-18 19:52:05,276 : INFO : Embedding Matrix now has shape (400593, 50)\n",
      "2018-06-18 19:52:05,279 : INFO : Pad word has been set to index 400590\n",
      "2018-06-18 19:52:05,285 : INFO : Embedding index build complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query (InputLayer)              (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "doc (InputLayer)                (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 50)      20029650    query[0][0]                      \n",
      "                                                                 doc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "mm_q_embed_DOT_d_embed (Dot)    (None, 200, 200)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mm_k_topk_mm (Lambda)           (None, 200, 50)      0           mm_q_embed_DOT_d_embed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mm_k_Dense_100_mm_k (Dense)     (None, 200, 100)     5100        mm_k_topk_mm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mm_k_Dense_1_mm_k (Dense)       (None, 200, 1)       101         mm_k_Dense_100_mm_k[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "w_g_Dense_1_q_embed (Dense)     (None, 200, 1)       51          embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mm_k_dropout_Dropout_mm_k (Drop (None, 200, 1)       0           mm_k_Dense_1_mm_k[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "g_Softmax_w_g (Lambda)          (None, 200)          0           w_g_Dense_1_q_embed[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "mm_reshape_Reshape_maxlen_mm_k_ (None, 200)          0           mm_k_dropout_Dropout_mm_k[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "g_Reshape_maxlen_w_g (Reshape)  (None, 200)          0           g_Softmax_w_g[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mean_mm_reshape_DOT_g (Dot)     (None, 1)            0           mm_reshape_Reshape_maxlen_mm_k_dr\n",
      "                                                                 g_Reshape_maxlen_w_g[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "out_Reshape_mean (Reshape)      (None, 1)            0           mean_mm_reshape_DOT_g[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 20,034,902\n",
      "Trainable params: 5,252\n",
      "Non-trainable params: 20,029,650\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "14956/14956 [==============================] - 43s 3ms/step - loss: 0.2443 - acc: 0.5661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:52:55,605 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:52:55,625 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:52:55,644 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:52:55,667 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:52:55,687 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:52:55,712 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5148505514499304\n",
      "nDCG@ 1 :  0.33714285714285713\n",
      "nDCG@ 3 :  0.5147417470374223\n",
      "nDCG@ 5 :  0.5763412968630361\n",
      "nDCG@ 10 :  0.6312150086506307\n",
      "nDCG@ 20 :  0.6441387764470141\n",
      "Epoch 2/10\n",
      "14956/14956 [==============================] - 46s 3ms/step - loss: 0.2431 - acc: 0.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:53:44,085 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:53:44,097 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:53:44,111 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:53:44,124 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:53:44,138 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:53:44,155 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5216076752784827\n",
      "nDCG@ 1 :  0.3314285714285714\n",
      "nDCG@ 3 :  0.5341093278066933\n",
      "nDCG@ 5 :  0.58575913485778\n",
      "nDCG@ 10 :  0.6370480273675955\n",
      "nDCG@ 20 :  0.649876765940878\n",
      "Epoch 3/10\n",
      "14956/14956 [==============================] - 36s 2ms/step - loss: 0.2412 - acc: 0.5776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:54:23,214 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:54:23,228 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:54:23,239 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:54:23,255 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:54:23,267 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:54:23,281 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5316416888839249\n",
      "nDCG@ 1 :  0.35428571428571426\n",
      "nDCG@ 3 :  0.5336843589095192\n",
      "nDCG@ 5 :  0.5971383676102587\n",
      "nDCG@ 10 :  0.6445071782700846\n",
      "nDCG@ 20 :  0.6573150417223009\n",
      "Epoch 4/10\n",
      "14956/14956 [==============================] - 40s 3ms/step - loss: 0.2400 - acc: 0.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:55:06,206 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:06,220 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:06,235 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:06,250 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:06,263 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:06,277 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5350727855650217\n",
      "nDCG@ 1 :  0.36\n",
      "nDCG@ 3 :  0.5364738097247982\n",
      "nDCG@ 5 :  0.5996773969921729\n",
      "nDCG@ 10 :  0.6484333190423246\n",
      "nDCG@ 20 :  0.6601953660352579\n",
      "Epoch 5/10\n",
      "14956/14956 [==============================] - 47s 3ms/step - loss: 0.2398 - acc: 0.5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:55:56,269 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:56,307 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:56,341 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:56,388 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:56,451 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:55:56,487 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5373028281242567\n",
      "nDCG@ 1 :  0.37142857142857144\n",
      "nDCG@ 3 :  0.5370187504788523\n",
      "nDCG@ 5 :  0.5971636097417318\n",
      "nDCG@ 10 :  0.6484826998744052\n",
      "nDCG@ 20 :  0.661812118324339\n",
      "Epoch 6/10\n",
      "14956/14956 [==============================] - 41s 3ms/step - loss: 0.2387 - acc: 0.5889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:56:40,373 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:56:40,384 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:56:40,398 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:56:40,411 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:56:40,424 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:56:40,439 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5471022957373889\n",
      "nDCG@ 1 :  0.38285714285714284\n",
      "nDCG@ 3 :  0.5484473219074237\n",
      "nDCG@ 5 :  0.6066320151337564\n",
      "nDCG@ 10 :  0.6561264864429803\n",
      "nDCG@ 20 :  0.6694559048929141\n",
      "Epoch 7/10\n",
      "14956/14956 [==============================] - 36s 2ms/step - loss: 0.2387 - acc: 0.5877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:57:18,349 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:18,358 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:18,369 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:18,379 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:18,389 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:18,400 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5502088665302951\n",
      "nDCG@ 1 :  0.38857142857142857\n",
      "nDCG@ 3 :  0.5579885069893176\n",
      "nDCG@ 5 :  0.6065795860359091\n",
      "nDCG@ 10 :  0.6611604432569254\n",
      "nDCG@ 20 :  0.6714596617183064\n",
      "Epoch 8/10\n",
      "14956/14956 [==============================] - 37s 2ms/step - loss: 0.2389 - acc: 0.5911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:57:58,372 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:58,392 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:58,411 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:58,424 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:58,434 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:57:58,451 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5525359204287775\n",
      "nDCG@ 1 :  0.3942857142857143\n",
      "nDCG@ 3 :  0.5557239904718314\n",
      "nDCG@ 5 :  0.6102927756720273\n",
      "nDCG@ 10 :  0.6614013697050168\n",
      "nDCG@ 20 :  0.6731490781017055\n",
      "Epoch 9/10\n",
      "14956/14956 [==============================] - 38s 3ms/step - loss: 0.2390 - acc: 0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:58:39,081 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:58:39,098 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:58:39,109 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:58:39,119 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:58:39,131 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:58:39,142 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5443128736343021\n",
      "nDCG@ 1 :  0.38285714285714284\n",
      "nDCG@ 3 :  0.5425974989686475\n",
      "nDCG@ 5 :  0.6019846618312067\n",
      "nDCG@ 10 :  0.6551148717902364\n",
      "nDCG@ 20 :  0.6668608783992421\n",
      "Epoch 10/10\n",
      "14956/14956 [==============================] - 45s 3ms/step - loss: 0.2372 - acc: 0.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-18 19:59:26,760 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:59:26,770 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:59:26,784 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:59:26,801 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:59:26,815 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n",
      "2018-06-18 19:59:26,834 : INFO : Using 175 out of 175 data points which is 0.00%. 0 were skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  0.5404327061827062\n",
      "nDCG@ 1 :  0.37142857142857144\n",
      "nDCG@ 3 :  0.5387366770097259\n",
      "nDCG@ 5 :  0.605006023715385\n",
      "nDCG@ 10 :  0.6521062841469755\n",
      "nDCG@ 20 :  0.663808927216609\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "drmm_tks_model = DRMM_TKS_Model(train_queries, train_docs, train_labels, word_embedding_path=word_embedding_path,\n",
    "                                epochs=10, validation_data=[test_queries, test_docs, test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "\n",
    "The testing of the data can be done on completely unseen data using `model.predict(queries, docs)` where\n",
    "queries: list of list of words\n",
    "docs: list of list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "queries = [\"how are glacier caves formed ?\".split()]\n",
    "docs = [\"A partly submerged glacier cave on Perito Moreno Glacier\".split(),\n",
    "        \"A glacier cave is a cave formed within the ice of a glacier\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45600405]\n",
      " [0.5344676 ]]\n"
     ]
    }
   ],
   "source": [
    "drmm_tks_model.predict(queries, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the correct answer has the higher similarity score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
