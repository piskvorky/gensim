{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we will install the dependencies for running the Similarity Learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from gensim.similarity_learning import DRMM_TKS_Model\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to provide data in a format which is understood by the model.\n",
    "The model understands sentences as a list of words. \n",
    "Further, we need to give a :\n",
    " 1. Queries List\n",
    " 2. Candidate Document List\n",
    " 3. Correct Label List\n",
    "\n",
    "1 is a list of list of words\n",
    "2 and 3 is actually a list of list of list of words/ints\n",
    "\n",
    "Example:\n",
    "```\n",
    "queries = [\"When was Abraham Lincoln born ?\".split(), \n",
    "            \"When was the first World War ?\".split()]\n",
    "docs = [\n",
    "\t\t [\"Abraham Lincoln was the president of the United States of America\".split(),\n",
    "\t\t \"He was born in 1809\".split()],\n",
    "\t\t [\"The first world war was bad\".split(),\n",
    "\t\t \"It was fought in 1914\".split(),\n",
    "\t\t \"There were over a million deaths\".split()]\n",
    "       ]\n",
    "labels = [[0,\n",
    "           1],\n",
    "\t\t  [0,\n",
    "           1,\n",
    "           0]\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset : WikiQA\n",
    "\n",
    "The WikiQA corpus is a set of question-answer pairs in which for every query there are several candidate documents of which none, one or more documents might be relevant.\n",
    "Relevance is purely binary, i.e., 1: relavant, 0: not relevant\n",
    "\n",
    "Sample data:\n",
    "```\n",
    "QuestionID\tQuestion\tDocumentID\tDocumentTitle\tSentenceID\tSentence\tLabel\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-0\tA partly submerged glacier cave on Perito Moreno Glacier .\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-1\tThe ice facade is approximately 60 m high\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-2\tIce formations in the Titlis glacier cave\t0\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-3\tA glacier cave is a cave formed within the ice of a glacier .\t1\n",
    "Q1\thow are glacier caves formed?\tD1\tGlacier cave\tD1-4\tGlacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice.\t0\n",
    "```\n",
    "\n",
    "## Data Preprocessing\n",
    "We need to take the above text and make it into `queries, docs, labels` form\n",
    "We use the below code for that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill the below with wherever you have your WikiQACorpus Folder\n",
    "wikiqa_data_path = os.path.join('data', 'WikiQACorpus', 'WikiQA-train.tsv')\n",
    "\n",
    "\n",
    "def preprocess_sent(sent):\n",
    "    \"\"\"Utility function to lower, strip and tokenize each sentence\n",
    "    \n",
    "    Replace this function if you want to handle preprocessing differently\"\"\"\n",
    "    return re.sub(\"[^a-zA-Z0-9]\", \" \", sent.strip().lower()).split()\n",
    "\n",
    "# Defining some consants for .tsv reading\n",
    "QUESTION_ID_INDEX = 0\n",
    "QUESTION_INDEX = 1\n",
    "ANSWER_INDEX = 5\n",
    "LABEL_INDEX = 6\n",
    "\n",
    "with open(wikiqa_data_path, encoding='utf8') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    data_rows = []\n",
    "    for row in tsv_reader:\n",
    "        data_rows.append(row)\n",
    "\n",
    "\n",
    "        \n",
    "document_group = []\n",
    "label_group = []\n",
    "\n",
    "n_relevant_docs = 0\n",
    "n_filtered_docs = 0\n",
    "\n",
    "queries = []\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for i, line in enumerate(data_rows[1:], start=1):\n",
    "    if i < len(data_rows) - 1:  # check if out of bounds might occur\n",
    "        if data_rows[i][QUESTION_ID_INDEX] == data_rows[i + 1][QUESTION_ID_INDEX]:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "        else:\n",
    "            document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "            label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "\n",
    "            n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "            if n_relevant_docs > 0:\n",
    "                docs.append(document_group)\n",
    "                labels.append(label_group)\n",
    "                queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "            else:\n",
    "                n_filtered_docs += 1\n",
    "\n",
    "            n_relevant_docs = 0\n",
    "            document_group = []\n",
    "            label_group = []\n",
    "\n",
    "    else:\n",
    "        # If we are on the last line\n",
    "        document_group.append(preprocess_sent(data_rows[i][ANSWER_INDEX]))\n",
    "        label_group.append(int(data_rows[i][LABEL_INDEX]))\n",
    "        n_relevant_docs += int(data_rows[i][LABEL_INDEX])\n",
    "\n",
    "        if n_relevant_docs > 0:\n",
    "            docs.append(document_group)\n",
    "            labels.append(label_group)\n",
    "            queries.append(preprocess_sent(data_rows[i][QUESTION_INDEX]))\n",
    "        else:\n",
    "            n_filtered_docs += 1\n",
    "            n_relevant_docs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'did', 'hurricane', 'katrina', 'begin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hurricane', 'katrina', 'was', 'the', 'deadliest', 'and', 'most', 'destructive', 'atlantic', 'hurricane', 'of', 'the', '2005', 'atlantic', 'hurricane', 'season'], ['it', 'was', 'the', 'costliest', 'natural', 'disaster', 'as', 'well', 'as', 'one', 'of', 'the', 'five', 'deadliest', 'hurricanes', 'in', 'the', 'history', 'of', 'the', 'united', 'states'], ['among', 'recorded', 'atlantic', 'hurricanes', 'it', 'was', 'the', 'sixth', 'strongest', 'overall'], ['at', 'least', '1', '833', 'people', 'died', 'in', 'the', 'hurricane', 'and', 'subsequent', 'floods', 'making', 'it', 'the', 'deadliest', 'u', 's', 'hurricane', 'since', 'the', '1928', 'okeechobee', 'hurricane', 'total', 'property', 'damage', 'was', 'estimated', 'at', '81', 'billion', '2005', 'usd', 'nearly', 'triple', 'the', 'damage', 'brought', 'by', 'hurricane', 'andrew', 'in', '1992'], ['hurricane', 'katrina', 'formed', 'over', 'the', 'bahamas', 'on', 'august', '23', '2005', 'and', 'crossed', 'southern', 'florida', 'as', 'a', 'moderate', 'category', '1', 'hurricane', 'causing', 'some', 'deaths', 'and', 'flooding', 'there', 'before', 'strengthening', 'rapidly', 'in', 'the', 'gulf', 'of', 'mexico'], ['the', 'hurricane', 'strengthened', 'to', 'a', 'category', '5', 'hurricane', 'over', 'the', 'warm', 'gulf', 'water', 'but', 'weakened', 'before', 'making', 'its', 'second', 'landfall', 'as', 'a', 'category', '3', 'hurricane', 'on', 'the', 'morning', 'of', 'monday', 'august', '29', 'in', 'southeast', 'louisiana'], ['it', 'caused', 'severe', 'destruction', 'along', 'the', 'gulf', 'coast', 'from', 'central', 'florida', 'to', 'texas', 'much', 'of', 'it', 'due', 'to', 'the', 'storm', 'surge'], ['the', 'most', 'significant', 'number', 'of', 'deaths', 'occurred', 'in', 'new', 'orleans', 'louisiana', 'which', 'flooded', 'as', 'the', 'levee', 'system', 'catastrophically', 'failed', 'in', 'many', 'cases', 'hours', 'after', 'the', 'storm', 'had', 'moved', 'inland'], ['eventually', '80', 'of', 'the', 'city', 'and', 'large', 'tracts', 'of', 'neighboring', 'parishes', 'became', 'flooded', 'and', 'the', 'floodwaters', 'lingered', 'for', 'weeks'], ['however', 'the', 'worst', 'property', 'damage', 'occurred', 'in', 'coastal', 'areas', 'such', 'as', 'all', 'mississippi', 'beachfront', 'towns', 'which', 'were', 'flooded', 'over', '90', 'in', 'hours', 'as', 'boats', 'and', 'casino', 'barges', 'rammed', 'buildings', 'pushing', 'cars', 'and', 'houses', 'inland', 'with', 'waters', 'reaching', '6', '12', 'miles', '10', '19', 'km', 'from', 'the', 'beach'], ['the', 'hurricane', 'surge', 'protection', 'failures', 'in', 'new', 'orleans', 'are', 'considered', 'the', 'worst', 'civil', 'engineering', 'disaster', 'in', 'u', 's', 'history', 'and', 'prompted', 'a', 'lawsuit', 'against', 'the', 'u', 's', 'army', 'corps', 'of', 'engineers', 'usace', 'the', 'designers', 'and', 'builders', 'of', 'the', 'levee', 'system', 'as', 'mandated', 'by', 'the', 'flood', 'control', 'act', 'of', '1965'], ['responsibility', 'for', 'the', 'failures', 'and', 'flooding', 'was', 'laid', 'squarely', 'on', 'the', 'army', 'corps', 'in', 'january', '2008', 'by', 'judge', 'stanwood', 'duval', 'u', 's', 'district', 'court', 'but', 'the', 'federal', 'agency', 'could', 'not', 'be', 'held', 'financially', 'liable', 'due', 'to', 'sovereign', 'immunity', 'in', 'the', 'flood', 'control', 'act', 'of', '1928'], ['there', 'was', 'also', 'an', 'investigation', 'of', 'the', 'responses', 'from', 'federal', 'state', 'and', 'local', 'governments', 'resulting', 'in', 'the', 'resignation', 'of', 'federal', 'emergency', 'management', 'agency', 'fema', 'director', 'michael', 'd', 'brown', 'and', 'of', 'new', 'orleans', 'police', 'department', 'nopd', 'superintendent', 'eddie', 'compass'], ['several', 'agencies', 'including', 'the', 'united', 'states', 'coast', 'guard', 'uscg', 'national', 'hurricane', 'center', 'nhc', 'and', 'national', 'weather', 'service', 'nws', 'were', 'commended', 'for', 'their', 'actions'], ['they', 'provided', 'accurate', 'hurricane', 'weather', 'tracking', 'forecasts', 'with', 'sufficient', 'lead', 'time'], ['even', 'the', 'most', 'insistent', 'appeals', 'from', 'national', 'state', 'and', 'local', 'public', 'officials', 'to', 'residents', 'to', 'evacuate', 'before', 'the', 'storm', 'did', 'not', 'warn', 'that', 'the', 'levees', 'could', 'breach', 'and', 'fail']]\n"
     ]
    }
   ],
   "source": [
    "print(docs[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a train-validation split\n",
    "At this point, it would be good to make a train-validation split so we can see how the model performs as it trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries, test_queries = queries[:int(len(queries)*0.8)], queries[int(len(queries)*0.8): ]\n",
    "train_docs, test_docs = docs[:int(len(docs)*0.8)], docs[int(len(docs)*0.8):]\n",
    "train_labels, test_labels = labels[:int(len(labels)*0.8)], labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697 175\n",
      "697 175\n",
      "697 175\n"
     ]
    }
   ],
   "source": [
    "print(len(train_queries), len(test_queries))\n",
    "print(len(train_docs), len(test_docs))\n",
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "If we want to train the model with some pretrained word embeddingd like Glove, we will have to specify the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_path = os.path.join('data', 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to monitor the progress of training of the model.\n",
    "However, we can't rely on the metrics provided by keras as those metrics don't necessarily apply to Information Retrieval problems.\n",
    "\n",
    "We can additionally provide a validation dataset which will be tested after every epoch.\n",
    "\n",
    "Now that we have the preprocessed extracted data, training the model just takes one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 21:00:24,833 : INFO : Starting Vocab Build\n",
      "2018-06-20 21:00:24,917 : INFO : Vocab Build Complete\n",
      "2018-06-20 21:00:24,918 : INFO : Vocab Size is 17800\n",
      "2018-06-20 21:00:24,919 : INFO : Building embedding index using pretrained word embeddings\n",
      "2018-06-20 21:00:33,808 : INFO : The embeddings_index built from the given file has 400000 words of 50 dimensions\n",
      "2018-06-20 21:00:33,810 : INFO : Embedding Matrix for Embedding Layer has shape (17801, 50) \n",
      "2018-06-20 21:00:33,843 : INFO : There are 594 words not in the embeddings. Setting them to zero\n",
      "2018-06-20 21:00:33,844 : INFO : Adding additional dimensions from the embedding file to embedding matrix\n",
      "2018-06-20 21:00:34,458 : INFO : Normalizing the word embeddings\n",
      "2018-06-20 21:00:35,206 : INFO : Embedding Matrix now has shape (400597, 50)\n",
      "2018-06-20 21:00:35,207 : INFO : Pad word has been set to index 400594\n",
      "2018-06-20 21:00:35,207 : INFO : Embedding index build complete\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text_maxlen: 200 isn't big enough. Error at sentence of length 305. Sentence is ['in', 'his', 'retelling', 'of', 'fairy', 'tales', 'in', 'the', 'scots', 'language', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '0', 'ben', 'schulz', 'player', 'of', 'leeroy', 'jenkins', 'at', 'blizzcon', '2007', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '1', 'leeroy', 'jenkins', 'sometimes', 'misspelled', 'leroy', 'jenkins', 'and', 'often', 'elongated', 'with', 'numerous', 'additional', 'letters', 'is', 'an', 'internet', 'meme', 'named', 'for', 'a', 'player', 'character', 'created', 'by', 'ben', 'schulz', 'in', 'blizzard', 'entertainment', 's', 'mmorpg', 'world', 'of', 'warcraft', '1', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '2', 'the', 'character', 'became', 'popular', 'due', 'to', 'a', 'video', 'of', 'the', 'game', 'that', 'circulated', 'around', 'the', 'internet', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '3', 'the', 'phenomenon', 'has', 'since', 'spread', 'beyond', 'the', 'boundaries', 'of', 'the', 'gaming', 'community', 'into', 'other', 'online', 'and', 'mainstream', 'media', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '0', 'oscar', 'grant', 'was', 'fatally', 'shot', 'by', 'bart', 'police', 'officer', 'johannes', 'mehserle', 'in', 'oakland', 'california', 'united', 'states', 'in', 'the', 'early', 'morning', 'hours', 'of', 'new', 'year', 's', 'day', '2009', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '1', 'responding', 'to', 'reports', 'of', 'a', 'fight', 'on', 'a', 'crowded', 'bay', 'area', 'rapid', 'transit', 'train', 'returning', 'from', 'san', 'francisco', 'bart', 'police', 'officers', 'detained', 'oscar', 'grant', 'and', 'several', 'other', 'passengers', 'on', 'the', 'platform', 'at', 'the', 'fruitvale', 'bart', 'station', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '2', 'officer', 'johannes', 'mehserle', 'and', 'another', 'officer', 'were', 'restraining', 'grant', 'who', 'was', 'prostrate', 'and', 'allegedly', 'resisting', 'arrest', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '3', 'officer', 'mehserle', 'stood', 'and', 'according', 'to', 'witnesses', 'said', 'get', 'back', 'i', 'm', 'gonna', 'tase', 'him']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0232521cd202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m drmm_tks_model = DRMM_TKS_Model(train_queries, train_docs, train_labels, word_embedding_path=word_embedding_path,\n\u001b[0;32m----> 3\u001b[0;31m                                 epochs=10, validation_data=[test_queries, test_docs, test_labels])\n\u001b[0m",
      "\u001b[0;32m~/aneeshj/gensim/gensim/similarity_learning/DRMM_TKS_Model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, queries, docs, labels, word_embedding_path, text_maxlen, keep_full_embedding, normalize_embeddings, epochs, unk_handle_method, validation_data)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pair_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexed_pair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_indexed_pair_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aneeshj/gensim/gensim/similarity_learning/DRMM_TKS_Model.py\u001b[0m in \u001b[0;36mmake_indexed_pair_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_neg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpair_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             indexed_pair_list.append([self.make_indexed(q),\n\u001b[0;32m--> 359\u001b[0;31m                 self.make_indexed(d_pos), self.make_indexed(d_neg)])\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindexed_pair_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aneeshj/gensim/gensim/similarity_learning/DRMM_TKS_Model.py\u001b[0m in \u001b[0;36mmake_indexed\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_sent\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_maxlen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             raise ValueError(\"text_maxlen: %d isn't big enough. Error at sentence of length %d. Sentence is %s\" % \n\u001b[0;32m--> 269\u001b[0;31m             (self.text_maxlen, len(sentence), sentence))\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mindexed_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed_sent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_word_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_maxlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text_maxlen: 200 isn't big enough. Error at sentence of length 305. Sentence is ['in', 'his', 'retelling', 'of', 'fairy', 'tales', 'in', 'the', 'scots', 'language', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '0', 'ben', 'schulz', 'player', 'of', 'leeroy', 'jenkins', 'at', 'blizzcon', '2007', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '1', 'leeroy', 'jenkins', 'sometimes', 'misspelled', 'leroy', 'jenkins', 'and', 'often', 'elongated', 'with', 'numerous', 'additional', 'letters', 'is', 'an', 'internet', 'meme', 'named', 'for', 'a', 'player', 'character', 'created', 'by', 'ben', 'schulz', 'in', 'blizzard', 'entertainment', 's', 'mmorpg', 'world', 'of', 'warcraft', '1', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '2', 'the', 'character', 'became', 'popular', 'due', 'to', 'a', 'video', 'of', 'the', 'game', 'that', 'circulated', 'around', 'the', 'internet', '0', 'q708', 'what', 'does', 'leeroy', 'jenkins', 'mean', 'd690', 'leeroy', 'jenkins', 'd690', '3', 'the', 'phenomenon', 'has', 'since', 'spread', 'beyond', 'the', 'boundaries', 'of', 'the', 'gaming', 'community', 'into', 'other', 'online', 'and', 'mainstream', 'media', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '0', 'oscar', 'grant', 'was', 'fatally', 'shot', 'by', 'bart', 'police', 'officer', 'johannes', 'mehserle', 'in', 'oakland', 'california', 'united', 'states', 'in', 'the', 'early', 'morning', 'hours', 'of', 'new', 'year', 's', 'day', '2009', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '1', 'responding', 'to', 'reports', 'of', 'a', 'fight', 'on', 'a', 'crowded', 'bay', 'area', 'rapid', 'transit', 'train', 'returning', 'from', 'san', 'francisco', 'bart', 'police', 'officers', 'detained', 'oscar', 'grant', 'and', 'several', 'other', 'passengers', 'on', 'the', 'platform', 'at', 'the', 'fruitvale', 'bart', 'station', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '2', 'officer', 'johannes', 'mehserle', 'and', 'another', 'officer', 'were', 'restraining', 'grant', 'who', 'was', 'prostrate', 'and', 'allegedly', 'resisting', 'arrest', '0', 'q709', 'what', 'happened', 'to', 'the', 'officer', 'in', 'bart', 'shooting', 'd691', 'bart', 'police', 'shooting', 'of', 'oscar', 'grant', 'd691', '3', 'officer', 'mehserle', 'stood', 'and', 'according', 'to', 'witnesses', 'said', 'get', 'back', 'i', 'm', 'gonna', 'tase', 'him']"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "drmm_tks_model = DRMM_TKS_Model(train_queries, train_docs, train_labels, word_embedding_path=word_embedding_path,\n",
    "                                epochs=10, validation_data=[test_queries, test_docs, test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "\n",
    "The testing of the data can be done on completely unseen data using `model.predict(queries, docs)` where\n",
    "queries: list of list of words\n",
    "docs: list of list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "queries = [\"how are glacier caves formed ?\".split()]\n",
    "docs = [\"A partly submerged glacier cave on Perito Moreno Glacier\".split(),\n",
    "        \"A glacier cave is a cave formed within the ice of a glacier\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drmm_tks_model.predict(queries, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the correct answer has the higher similarity score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
