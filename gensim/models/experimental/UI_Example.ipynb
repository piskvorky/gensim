{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset\n",
    "A script has been provided to download all the datasets required for running the below examples.\n",
    "It will dowload and unzip the WikiQA Corpus and the Quora Duplicate Questions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python experimental_data/get_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies for running the Similarity Learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2018-07-06 00:34:19,104 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from gensim.models.experimental import DRMM_TKS\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to provide data in a format which is understood by the model.\n",
    "The model understands sentences as a list of words. \n",
    "Further, we need to give a :\n",
    " 1. Queries List\n",
    " 2. Candidate Document List\n",
    " 3. Correct Label List\n",
    "\n",
    "1 is a list of list of words\n",
    "2 and 3 is actually a list of list of list of words/ints\n",
    "\n",
    "Example:\n",
    "```\n",
    "queries = [\"When was Abraham Lincoln born ?\".split(), \n",
    "            \"When was the first World War ?\".split()]\n",
    "docs = [\n",
    "\t\t [\"Abraham Lincoln was the president of the United States of America\".split(),\n",
    "\t\t \"He was born in 1809\".split()],\n",
    "\t\t [\"The first world war was bad\".split(),\n",
    "\t\t \"It was fought in 1914\".split(),\n",
    "\t\t \"There were over a million deaths\".split()]\n",
    "       ]\n",
    "labels = [[0,\n",
    "           1],\n",
    "\t\t  [0,\n",
    "           1,\n",
    "           0]\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset : WikiQA\n",
    "\n",
    "The WikiQA corpus is a set of question-answer pairs in which for every query there are several candidate documents of which none, one or more documents might be relevant.\n",
    "Relevance is purely binary, i.e., 1: relavant, 0: not relevant\n",
    "\n",
    "Sample data:\n",
    "\n",
    "QuestionID | Question | DocumentID | DocumentTitle | SentenceID | Sentence | Label\n",
    "-- | -- | -- | -- | -- | -- | --\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-0 | A partly submerged glacier cave on Perito Moreno Glacier . | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-1 | The ice facade is approximately 60 m high | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-2 | Ice formations in the Titlis glacier cave | 0\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-3 | A glacier cave is a cave formed within the ice of a glacier . | 1\n",
    "Q1 | how are glacier caves formed? | D1 | Glacier cave | D1-4 | Glacier caves are often called ice caves , but this term is properly used to describe bedrock caves that contain year-round ice. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-0 | In physics , circular motion is a movement of an object along the circumference of a circle or rotation along a circular path. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-1 | It can be uniform, with constant angular rate of rotation (and constant speed), or non-uniform with a changing rate of rotation. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-2 | The rotation around a fixed axis of a three-dimensional body involves circular motion of its parts. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-3 | The equations of motion describe the movement of the center of mass of a body. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-4 | Examples of circular motion include: an artificial satellite orbiting the Earth at constant height, a stone which is tied to a rope and is being swung in circles, a car turning through a curve in a race track , an electron moving perpendicular to a uniform magnetic field , and a gear turning inside a mechanism. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-5 | Since the object's velocity vector is constantly changing direction, the moving object is undergoing acceleration by a centripetal force in the direction of the center of rotation. | 0\n",
    "Q2 | How are the directions of the velocity and force vectors related in a circular motion | D2 | Circular motion | D2-6 | Without this acceleration, the object would move in a straight line, according to Newton's laws of motion . | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to take the above text and make it into `queries, docs, labels` form. For this, we will create an iterable object with the below class which will allow the data to be streamed into the model as the need arises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWikiIterable:\n",
    "    \"\"\"\"Yields the next data point in the data set based on the `iter_type`\n",
    "    \n",
    "    Based on `iter_type` the object can yield the following:\n",
    "        'query' : list of str words\n",
    "        'doc' : list of docs\n",
    "                    where a doc is a list of str words\n",
    "        'label' : list of int\n",
    "                  The relevance between adjacent queries and docs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iter_type, fpath):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        iter_type : {'query', 'doc', 'label'}\n",
    "            the type of iterable to be yielded\n",
    "        fpath : str\n",
    "            path to the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # To map the `iter_type` to an index\n",
    "        self.type_translator = {'query': 0, 'doc': 1, 'label': 2}\n",
    "        self.iter_type = iter_type\n",
    "\n",
    "        with open(fpath, encoding='utf8') as tsv_file:\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "            self.data_rows = []\n",
    "            self.data_rows = [row for row in tsv_reader]\n",
    "\n",
    "    def preprocess_sent(self, sent):\n",
    "        \"\"\"Utility function to lower, strip and tokenize each sentence\n",
    "        Replace this function if you want to handle preprocessing differently\"\"\"\n",
    "\n",
    "        return simple_preprocess(sent)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Defining some consants for .tsv reading\n",
    "        # They represent the columns of the respective values\n",
    "        QUESTION_ID_INDEX = 0\n",
    "        QUESTION_INDEX = 1\n",
    "        ANSWER_INDEX = 5\n",
    "        LABEL_INDEX = 6\n",
    "\n",
    "\n",
    "        # The group of documents and labels that belong to one question\n",
    "        document_group = []\n",
    "        label_group = []\n",
    "\n",
    "        # Number of relevant documents per query\n",
    "        n_relevant_docs = 0\n",
    "        # Number of filtered docs (query-doc pairs which have zero relevant docs)\n",
    "        n_filtered_docs = 0\n",
    "\n",
    "        # The data\n",
    "        queries = []\n",
    "        docs = []\n",
    "        labels = []\n",
    "\n",
    "        # The code below goes through the data line by line\n",
    "        # It checks the current document id with the next document id\n",
    "        for i, line in enumerate(self.data_rows[1:], start=1):\n",
    "            if i < len(self.data_rows) - 1:  # check if out of bounds might occur\n",
    "                if self.data_rows[i][QUESTION_ID_INDEX] == self.data_rows[i + 1][QUESTION_ID_INDEX]:\n",
    "                    document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                    label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "                    n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "                else:\n",
    "                    document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                    label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "\n",
    "                    n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "\n",
    "                    if n_relevant_docs > 0:\n",
    "                        docs.append(document_group)\n",
    "                        labels.append(label_group)\n",
    "                        queries.append(self.preprocess_sent(self.data_rows[i][QUESTION_INDEX]))\n",
    "\n",
    "                        yield [queries[-1], document_group, label_group][self.type_translator[self.iter_type]]\n",
    "                    else:\n",
    "                        n_filtered_docs += 1\n",
    "\n",
    "                    n_relevant_docs = 0\n",
    "                    document_group = []\n",
    "                    label_group = []\n",
    "\n",
    "            else:\n",
    "                # If we are on the last line\n",
    "                document_group.append(self.preprocess_sent(self.data_rows[i][ANSWER_INDEX]))\n",
    "                label_group.append(int(self.data_rows[i][LABEL_INDEX]))\n",
    "                n_relevant_docs += int(self.data_rows[i][LABEL_INDEX])\n",
    "\n",
    "                if n_relevant_docs > 0:\n",
    "                    docs.append(document_group)\n",
    "                    labels.append(label_group)\n",
    "                    queries.append(self.preprocess_sent(self.data_rows[i][QUESTION_INDEX]))\n",
    "                    yield [queries[-1], document_group, label_group][self.type_translator[self.iter_type]]\n",
    "                else:\n",
    "                    n_filtered_docs += 1\n",
    "                    n_relevant_docs = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, will use the class to create objects of the training iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_iterable = MyWikiIterable('query', os.path.join( 'experimental_data', 'WikiQACorpus', 'WikiQA-train.tsv'))\n",
    "d_iterable = MyWikiIterable('doc', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-train.tsv'))\n",
    "l_iterable = MyWikiIterable('label', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-train.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also initialize some validation iterables\n",
    "Note: the path has `dev` in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_val_iterable = MyWikiIterable('query', os.path.join( 'experimental_data', 'WikiQACorpus', 'WikiQA-dev.tsv'))\n",
    "d_val_iterable = MyWikiIterable('doc', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-dev.tsv'))\n",
    "l_val_iterable = MyWikiIterable('label', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-dev.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings\n",
    "We also need to get the word embeddings for the training. For this, we will use the Glove Embeddings.\n",
    "Luckily, [gensim-data](https://github.com/RaRe-Technologies/gensim-data) provides an easy interface for it.\n",
    "\n",
    "We will use the [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) object that we for from gensim-data api and pass it as the `word_embedding` parameter in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:34:23,010 : INFO : loading projection weights from /home/aneeshj/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2018-07-06 00:36:07,145 : INFO : loaded (400000, 300) matrix from /home/aneeshj/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "kv_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Now that we have the preprocessed extracted data and word embeddings, training the model just takes one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:36:07,151 : INFO : Starting Vocab Build\n",
      "2018-07-06 00:36:08,602 : INFO : Vocab Build Complete\n",
      "2018-07-06 00:36:08,603 : INFO : Vocab Size is 18814\n",
      "2018-07-06 00:36:08,605 : INFO : Building embedding index using KeyedVector pretrained word embeddings\n",
      "2018-07-06 00:36:08,605 : INFO : The embeddings_index built from the given file has 400000 words of 300 dimensions\n",
      "2018-07-06 00:36:08,606 : INFO : Building the Embedding Matrix for the model's Embedding Layer\n",
      "2018-07-06 00:36:08,836 : INFO : There are 642 words out of 18814 (3.41%) not in the embeddings. Setting them to random\n",
      "2018-07-06 00:36:08,836 : INFO : Adding additional words from the embedding file to embedding matrix\n",
      "2018-07-06 00:36:10,775 : INFO : Normalizing the word embeddings\n",
      "2018-07-06 00:36:59,403 : INFO : Embedding Matrix build complete. It now has shape (400644, 300)\n",
      "2018-07-06 00:37:06,320 : INFO : Pad word has been set to index 400642\n",
      "2018-07-06 00:37:06,815 : INFO : Unknown word has been set to index 400643\n",
      "2018-07-06 00:37:06,901 : INFO : Embedding index build complete\n",
      "2018-07-06 00:37:22,881 : INFO : Input is an iterable amd will be streamed\n",
      "2018-07-06 00:38:24,108 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,203 : INFO : Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "2018-07-06 00:38:24,236 : INFO : ==================================================================================================\n",
      "2018-07-06 00:38:24,342 : INFO : query (InputLayer)              (None, 200)          0                                            \n",
      "2018-07-06 00:38:24,343 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,343 : INFO : doc (InputLayer)                (None, 200)          0                                            \n",
      "2018-07-06 00:38:24,344 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,345 : INFO : embedding_1 (Embedding)         (None, 200, 300)     120193200   query[0][0]                      \n",
      "2018-07-06 00:38:24,346 : INFO :                                                                  doc[0][0]                        \n",
      "2018-07-06 00:38:24,346 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,432 : INFO : dot_1 (Dot)                     (None, 200, 200)     0           embedding_1[0][0]                \n",
      "2018-07-06 00:38:24,455 : INFO :                                                                  embedding_1[1][0]                \n",
      "2018-07-06 00:38:24,456 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,478 : INFO : top_k_layer_1 (TopKLayer)       (None, 200, 20)      0           dot_1[0][0]                      \n",
      "2018-07-06 00:38:24,479 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,496 : INFO : dense_2 (Dense)                 (None, 200, 100)     2100        top_k_layer_1[0][0]              \n",
      "2018-07-06 00:38:24,517 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,518 : INFO : dense_3 (Dense)                 (None, 200, 1)       101         dense_2[0][0]                    \n",
      "2018-07-06 00:38:24,519 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,553 : INFO : dropout_1 (Dropout)             (None, 200, 1)       0           dense_3[0][0]                    \n",
      "2018-07-06 00:38:24,554 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,554 : INFO : dense_1 (Dense)                 (None, 200, 1)       301         embedding_1[0][0]                \n",
      "2018-07-06 00:38:24,555 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,555 : INFO : reshape_2 (Reshape)             (None, 200)          0           dropout_1[0][0]                  \n",
      "2018-07-06 00:38:24,556 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,557 : INFO : reshape_1 (Reshape)             (None, 200)          0           dense_1[0][0]                    \n",
      "2018-07-06 00:38:24,557 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,558 : INFO : dot_2 (Dot)                     (None, 1)            0           reshape_2[0][0]                  \n",
      "2018-07-06 00:38:24,575 : INFO :                                                                  reshape_1[0][0]                  \n",
      "2018-07-06 00:38:24,576 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:24,577 : INFO : reshape_3 (Reshape)             (None, 1)            0           dot_2[0][0]                      \n",
      "2018-07-06 00:38:24,577 : INFO : ==================================================================================================\n",
      "2018-07-06 00:38:24,588 : INFO : Total params: 120,195,702\n",
      "2018-07-06 00:38:24,588 : INFO : Trainable params: 2,502\n",
      "2018-07-06 00:38:24,589 : INFO : Non-trainable params: 120,193,200\n",
      "2018-07-06 00:38:24,590 : INFO : __________________________________________________________________________________________________\n",
      "2018-07-06 00:38:33,655 : INFO : Found 14 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:38:53,729 : INFO : Found 90 unknown words. Set them to unknown word index : 400643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "900/900 [==============================] - 85s 95ms/step - loss: 1.0646 - acc: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:41:17,729 : INFO : MAP: 0.55\n",
      "2018-07-06 00:41:17,735 : INFO : nDCG@1 : 0.38\n",
      "2018-07-06 00:41:17,740 : INFO : nDCG@3 : 0.54\n",
      "2018-07-06 00:41:17,746 : INFO : nDCG@5 : 0.60\n",
      "2018-07-06 00:41:17,751 : INFO : nDCG@10 : 0.66\n",
      "2018-07-06 00:41:17,756 : INFO : nDCG@20 : 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "900/900 [==============================] - 84s 94ms/step - loss: 0.9310 - acc: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:42:46,586 : INFO : MAP: 0.61\n",
      "2018-07-06 00:42:46,592 : INFO : nDCG@1 : 0.46\n",
      "2018-07-06 00:42:46,597 : INFO : nDCG@3 : 0.61\n",
      "2018-07-06 00:42:46,604 : INFO : nDCG@5 : 0.67\n",
      "2018-07-06 00:42:46,616 : INFO : nDCG@10 : 0.71\n",
      "2018-07-06 00:42:46,621 : INFO : nDCG@20 : 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 0.8035 - acc: 0.1486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:44:15,788 : INFO : MAP: 0.62\n",
      "2018-07-06 00:44:15,793 : INFO : nDCG@1 : 0.46\n",
      "2018-07-06 00:44:15,800 : INFO : nDCG@3 : 0.60\n",
      "2018-07-06 00:44:15,809 : INFO : nDCG@5 : 0.67\n",
      "2018-07-06 00:44:15,815 : INFO : nDCG@10 : 0.71\n",
      "2018-07-06 00:44:15,821 : INFO : nDCG@20 : 0.72\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "drmm_tks_model = DRMM_TKS(\n",
    "                    queries=q_iterable, docs=d_iterable, labels=l_iterable, word_embedding=kv_model, epochs=3,\n",
    "                    validation_data=[q_val_iterable, d_val_iterable, l_val_iterable], topk=20\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "\n",
    "The testing of the data can be done on completely unseen data using `model.predict(queries, docs)` where\n",
    "queries: list of list of words\n",
    "docs: list of list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [simple_preprocess(\"how are glacier caves formed\"),\n",
    "           simple_preprocess(\"What is AWS\")]\n",
    "\n",
    "docs = [[simple_preprocess(\"A partly submerged glacier cave on Perito Moreno Glacier\"),\n",
    "        simple_preprocess(\"A glacier cave is a cave formed within the ice of a glacier\")],\n",
    "       [simple_preprocess(\"AWS stands for Amazon Web Services\"),\n",
    "        simple_preprocess(\"AWS was established in 2001\"),\n",
    "        simple_preprocess(\"It is a cloud service\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function returns the similarity between a query-document pair in a list format\n",
    "\n",
    "For example\n",
    "```\n",
    "queries = [q1, q2]\n",
    "docs = [[d1_1, d1_2],\n",
    "        [d2_1, d2_2, d2_3]]\n",
    "\n",
    "model.predict(queries, docs)\n",
    "\n",
    "Output\n",
    "------\n",
    "q1-d1_1 similarity\n",
    "q1-d1_2 similarity\n",
    "q2-d2_1 similarity\n",
    "q2-d2_2 similarity\n",
    "q2-d2_3 similarity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:46:33,249 : INFO : Found 0 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:46:33,283 : INFO : Found 0 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:46:33,778 : INFO : Predictions in the format query, doc, similarity\n",
      "2018-07-06 00:46:33,800 : INFO : ['how', 'are', 'glacier', 'caves', 'formed']\t['partly', 'submerged', 'glacier', 'cave', 'on', 'perito', 'moreno', 'glacier']\t0.75623834\n",
      "2018-07-06 00:46:33,801 : INFO : ['how', 'are', 'glacier', 'caves', 'formed']\t['glacier', 'cave', 'is', 'cave', 'formed', 'within', 'the', 'ice', 'of', 'glacier']\t0.88229656\n",
      "2018-07-06 00:46:33,802 : INFO : ['what', 'is', 'aws']\t['aws', 'stands', 'for', 'amazon', 'web', 'services']\t0.5922452\n",
      "2018-07-06 00:46:33,802 : INFO : ['what', 'is', 'aws']\t['aws', 'was', 'established', 'in']\t0.581025\n",
      "2018-07-06 00:46:33,803 : INFO : ['what', 'is', 'aws']\t['it', 'is', 'cloud', 'service']\t0.65737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.75623834],\n",
       "       [0.88229656],\n",
       "       [0.5922452 ],\n",
       "       [0.581025  ],\n",
       "       [0.65737   ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drmm_tks_model.predict(queries, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the logs and results above, within each query-document group, the correct answer has the highest score\n",
    "\n",
    "For example,\n",
    "In the first group\n",
    "```\n",
    "['how', 'are', 'glacier', 'caves', 'formed'] ['partly', 'submerged', 'glacier', 'cave', 'on', 'perito', 'moreno', 'glacier']\t0.7\n",
    "['how', 'are', 'glacier', 'caves', 'formed'] ['glacier', 'cave', 'is', 'cave', 'formed', 'within', 'the', 'ice', 'of', 'glacier']\t0.8\n",
    "```\n",
    "\n",
    "The correct answer, \"glacier cave is cave ...\" has the higher score as compared to the first answer\n",
    "The same can be seen for the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a test set\n",
    "We can pass a whole dataset and get evaluations based on that. Let's try with the test set of WikiQA Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_test_iterable = MyWikiIterable('query', os.path.join( 'experimental_data', 'WikiQACorpus', 'WikiQA-test.tsv'))\n",
    "d_test_iterable = MyWikiIterable('doc', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-test.tsv'))\n",
    "l_test_iterable = MyWikiIterable('label', os.path.join('experimental_data', 'WikiQACorpus', 'WikiQA-test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:48:00,129 : INFO : Found 21 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:48:00,202 : INFO : Found 253 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:48:09,461 : INFO : MAP: 0.60\n",
      "2018-07-06 00:48:09,523 : INFO : nDCG@1 : 0.47\n",
      "2018-07-06 00:48:09,541 : INFO : nDCG@3 : 0.60\n",
      "2018-07-06 00:48:09,567 : INFO : nDCG@5 : 0.66\n",
      "2018-07-06 00:48:09,591 : INFO : nDCG@10 : 0.70\n",
      "2018-07-06 00:48:09,607 : INFO : nDCG@20 : 0.71\n"
     ]
    }
   ],
   "source": [
    "drmm_tks_model.evaluate(q_test_iterable, d_test_iterable, l_test_iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing DRMM TKS with other models\n",
    "\n",
    "It would be good to get an idea of how our model works against some unsupervised models like word2vec and FastText.\n",
    "For this, we will, given a query-document pair, we will get a vector for the query and document. We can get the similarity between them using the cosine similarity between their vectors.\n",
    "\n",
    "### For word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.experimental import mapk, mean_ndcg\n",
    "\n",
    "def eval_model(queries, docs, labels, model):\n",
    "    long_doc_list = []\n",
    "    long_label_list = []\n",
    "    long_query_list = []\n",
    "    doc_lens = []\n",
    "\n",
    "    def sent2vec(sentence):\n",
    "        vec = np.zeros((model.vector_size))\n",
    "        for word in sentence:\n",
    "            if word in model:\n",
    "                vec += model[word]\n",
    "        return vec/len(sentence)\n",
    "    \n",
    "    for query, doc, label in zip(queries, docs, labels):\n",
    "        i = 0\n",
    "        for d, l in zip(doc, label):\n",
    "            if len(d) == 0 or len(query) == 0:\n",
    "                print(\"skipping query-doc pair due to no words in vocab\")\n",
    "                continue\n",
    "            long_query_list.append(sent2vec(query))\n",
    "            long_doc_list.append(sent2vec(d))\n",
    "            long_label_list.append(l)\n",
    "            i += 1\n",
    "        doc_lens.append(len(doc))\n",
    "\n",
    "    doc_lens = np.array(doc_lens)\n",
    "\n",
    "    predictions = []\n",
    "    for q, d in zip(long_query_list, long_doc_list):\n",
    "        predictions.append(cosine_similarity(q, d))\n",
    "\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    offset = 0\n",
    "\n",
    "    for doc_size in doc_lens:\n",
    "        Y_pred.append(predictions[offset: offset + doc_size])\n",
    "        Y_true.append(long_label_list[offset: offset + doc_size])\n",
    "        offset += doc_size\n",
    "        \n",
    "    print(\"MAP: %.2f\"% mapk(Y_true, Y_pred))\n",
    "    for k in [1, 3, 5, 10, 20]:\n",
    "        print(\"nDCG@%d : %.2f \" % (k, mean_ndcg(Y_true, Y_pred, k=k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping query-doc pair due to no words in vocab\n",
      "skipping query-doc pair due to no words in vocab\n",
      "MAP: 0.58\n",
      "nDCG@1 : 0.43 \n",
      "nDCG@3 : 0.60 \n",
      "nDCG@5 : 0.66 \n",
      "nDCG@10 : 0.70 \n",
      "nDCG@20 : 0.71 \n"
     ]
    }
   ],
   "source": [
    "eval_model(q_test_iterable, d_test_iterable, l_test_iterable, kv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare that with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:49:11,315 : INFO : Found 21 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:49:11,379 : INFO : Found 253 unknown words. Set them to unknown word index : 400643\n",
      "2018-07-06 00:49:21,218 : INFO : MAP: 0.60\n",
      "2018-07-06 00:49:21,229 : INFO : nDCG@1 : 0.47\n",
      "2018-07-06 00:49:21,246 : INFO : nDCG@3 : 0.60\n",
      "2018-07-06 00:49:21,263 : INFO : nDCG@5 : 0.66\n",
      "2018-07-06 00:49:21,274 : INFO : nDCG@10 : 0.70\n",
      "2018-07-06 00:49:21,286 : INFO : nDCG@20 : 0.71\n"
     ]
    }
   ],
   "source": [
    "drmm_tks_model.evaluate(q_test_iterable, d_test_iterable, l_test_iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy isn't any better, it is worse, this is still a Work In Progress and we hope to improve it further soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model\n",
    "The trained model can be saved and loaded from memory for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-06 00:44:16,527 : INFO : saving DRMM_TKS object under drmm_tks_model, separately None\n",
      "2018-07-06 00:44:16,529 : INFO : storing np array 'vectors' to drmm_tks_model.word_embedding.vectors.npy\n",
      "2018-07-06 00:45:09,654 : INFO : storing np array 'embedding_matrix' to drmm_tks_model.embedding_matrix.npy\n",
      "2018-07-06 00:45:18,682 : INFO : not storing attribute model\n",
      "2018-07-06 00:45:18,684 : INFO : not storing attribute _get_pair_list\n",
      "2018-07-06 00:45:18,685 : INFO : not storing attribute _get_full_batch_iter\n",
      "2018-07-06 00:45:18,687 : INFO : not storing attribute queries\n",
      "2018-07-06 00:45:18,688 : INFO : not storing attribute docs\n",
      "2018-07-06 00:45:18,690 : INFO : not storing attribute labels\n",
      "2018-07-06 00:45:18,691 : INFO : not storing attribute pair_list\n",
      "2018-07-06 00:45:36,062 : INFO : saved drmm_tks_model\n"
     ]
    }
   ],
   "source": [
    "drmm_tks_model.save('drmm_tks_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del drmm_tks_model\n",
    "drmm_tks_model = DRMM_TKS.load('drmm_tks_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
