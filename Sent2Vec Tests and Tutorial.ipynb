{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sent2Vec via Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about using sent2vec model in Gensim. Here, we'll learn to work with the sent2vec library for training sentence-embedding models, saving & loading them and performing similarity operations. This notebook also contains a comparison of the gensim implementation with the [original c++ implementation](https://github.com/epfml/sent2vec). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sent2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec delivers numerical representations (features) for short texts or sentences, which can be used as input to any machine learning task later on. Think of it as an unsupervised version of FastText, and an extension of word2vec (CBOW) to sentences. The method uses a simple but efficient unsupervised objective to train distributed representations of sentences. The algorithm outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings, see the [paper](https://arxiv.org/abs/1703.02507) for more details.\n",
    "\n",
    "The sentence embedding is defined as the average of the source word embeddings of its constituent words. This model is furthermore augmented by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.sent2vec import Sent2Vec as s2v\n",
    "from gensim.utils import tokenize\n",
    "import scipy\n",
    "import re\n",
    "from numpy import dot\n",
    "from gensim import matutils\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = data_dir + 'lee_background.cor'\n",
    "lee_data = []\n",
    "with open(lee_train_file) as f1, open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if line not in ['\\n', '\\r\\n']:\n",
    "            line = re.split('\\.|\\?|\\n', line.strip())\n",
    "            for sentence in line:\n",
    "                if len(sentence) > 1:\n",
    "                    sentence = tokenize(sentence)\n",
    "                    lee_data.append(list(sentence))\n",
    "                    f2.write(' '.join(lee_data[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hundreds', u'of', u'people', u'have', u'been', u'forced', u'to', u'vacate', u'their', u'homes', u'in', u'the', u'Southern', u'Highlands', u'of', u'New', u'South', u'Wales', u'as', u'strong', u'winds', u'today', u'pushed', u'a', u'huge', u'bushfire', u'towards', u'the', u'town', u'of', u'Hill', u'Top'] \n",
      "\n",
      "[u'A', u'new', u'blaze', u'near', u'Goulburn', u'south', u'west', u'of', u'Sydney', u'has', u'forced', u'the', u'closure', u'of', u'the', u'Hume', u'Highway'] \n",
      "\n",
      "[u'At', u'about', u'pm', u'AEDT', u'a', u'marked', u'deterioration', u'in', u'the', u'weather', u'as', u'a', u'storm', u'cell', u'moved', u'east', u'across', u'the', u'Blue', u'Mountains', u'forced', u'authorities', u'to', u'make', u'a', u'decision', u'to', u'evacuate', u'people', u'from', u'homes', u'in', u'outlying', u'streets', u'at', u'Hill', u'Top', u'in', u'the', u'New', u'South', u'Wales', u'southern', u'highlands'] \n",
      "\n",
      "[u'An', u'estimated', u'residents', u'have', u'left', u'their', u'homes', u'for', u'nearby', u'Mittagong'] \n",
      "\n",
      "[u'The', u'New', u'South', u'Wales', u'Rural', u'Fire', u'Service', u'says', u'the', u'weather', u'conditions', u'which', u'caused', u'the', u'fire', u'to', u'burn', u'in', u'a', u'finger', u'formation', u'have', u'now', u'eased', u'and', u'about', u'fire', u'units', u'in', u'and', u'around', u'Hill', u'Top', u'are', u'optimistic', u'of', u'defending', u'all', u'properties'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample training data\n",
    "for sentence in lee_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gensim implementation of sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Read 0.060302M words\n",
      "Dictionary created, dictionary size: 1307 ,tokens read: 60302\n",
      "Initializing model...\n",
      "Training...\n",
      "Begin epoch 0 :\n",
      "Progress:  3.96330138304 % lr:  0.192073397234  loss:  3.79220661775\n",
      "Begin epoch 1 :\n",
      "Progress:  7.93016815363 % lr:  0.184139663693  loss:  3.6578121557\n",
      "Begin epoch 2 :\n",
      "Progress:  11.8970349242 % lr:  0.176205930152  loss:  3.55657217233\n",
      "Begin epoch 3 :\n",
      "Progress:  15.8639016948 % lr:  0.16827219661  loss:  3.46168299212\n",
      "Begin epoch 4 :\n",
      "Progress:  19.8307684654 % lr:  0.160338463069  loss:  3.37070538195\n",
      "Begin epoch 5 :\n",
      "Progress:  23.797635236 % lr:  0.152404729528  loss:  3.28155531049\n",
      "Begin epoch 6 :\n",
      "Progress:  27.7645020066 % lr:  0.144470995987  loss:  3.19723932184\n",
      "Begin epoch 7 :\n",
      "Progress:  31.7313687772 % lr:  0.136537262446  loss:  3.11839643823\n",
      "Begin epoch 8 :\n",
      "Progress:  35.6982355477 % lr:  0.128603528905  loss:  3.04488396391\n",
      "Begin epoch 9 :\n",
      "Progress:  39.6651023183 % lr:  0.120669795363  loss:  2.97539395926\n",
      "Begin epoch 10 :\n",
      "Progress:  43.6319690889 % lr:  0.112736061822  loss:  2.90969444642\n",
      "Begin epoch 11 :\n",
      "Progress:  47.5988358595 % lr:  0.104802328281  loss:  2.84838200744\n",
      "Begin epoch 12 :\n",
      "Progress:  51.5657026301 % lr:  0.0968685947398  loss:  2.79084509107\n",
      "Begin epoch 13 :\n",
      "Progress:  55.5325694007 % lr:  0.0889348611986  loss:  2.73706903668\n",
      "Begin epoch 14 :\n",
      "Progress:  59.4994361713 % lr:  0.0810011276575  loss:  2.68632415594\n",
      "Begin epoch 15 :\n",
      "Progress:  63.4663029419 % lr:  0.0730673941163  loss:  2.63823476249\n",
      "Begin epoch 16 :\n",
      "Progress:  67.4331697124 % lr:  0.0651336605751  loss:  2.59364204129\n",
      "Begin epoch 17 :\n",
      "Progress:  71.400036483 % lr:  0.0571999270339  loss:  2.55186653644\n",
      "Begin epoch 18 :\n",
      "Progress:  75.3669032536 % lr:  0.0492661934928  loss:  2.51222365388\n",
      "Begin epoch 19 :\n",
      "Progress:  79.3337700242 % lr:  0.0413324599516  loss:  2.47520869171\n",
      "\n",
      "\n",
      "Total training time: 276.946504831 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model\n",
    "sent2vec_model = s2v(vector_size=100, epochs=20)\n",
    "sent2vec_model.train(lee_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec supports the folllowing parameters:\n",
    "\n",
    " - vector_size: Size of embeddings to be learnt (Default 100)\n",
    " - alpha: Initial learning rate (Default 0.2)\n",
    " - min_count: Ignore words with number of occurrences below this (Default 5)\n",
    " - loss: Training objective. Allowed values: `ns` (Default `ns`)\n",
    " - negative: Number of negative words to sample, for `ns` (Default 10)\n",
    " - epochs: Number of epochs (Default 5)\n",
    " - bucket: Number of hash buckets for vocabulary (Default 2000000)\n",
    " - lr_update_rate: Change the rate of updates for the learning rate (Default 100)\n",
    " - t: Sampling threshold (Default 0.0001)\n",
    " - dropoutk: Number of ngrams dropped when training a sent2vec model (Default 2)\n",
    " - word_ngrams: Max length of word ngram (Default: 2)\n",
    " - min_n: min length of char ngrams (Default 3)\n",
    " - max_n: max length of char ngrams (Default 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68231279,  0.27833666,  0.16755685, -0.42549644,  0.44356017,\n",
       "        0.03805984,  0.11425159, -0.18843327, -0.35050581,  0.37883625,\n",
       "       -0.04714561,  0.12688964,  0.03892192,  0.50464108,  0.41076339,\n",
       "        0.46617015,  0.43055977,  0.63853766,  0.29246627, -0.41670265,\n",
       "       -0.20256535,  0.2954225 , -0.59524875,  0.3225076 ,  0.83679135,\n",
       "       -0.44026166,  0.53004422,  0.36827652,  0.46622952, -0.41337579,\n",
       "        0.23997068,  0.0134885 ,  0.19703447,  0.16905064,  0.40177966,\n",
       "        0.41733581, -0.08684079, -0.07431549,  0.56583327,  0.3994801 ,\n",
       "       -0.19969126,  0.39581204,  0.14362896, -0.75979071,  0.38220697,\n",
       "       -0.08246933,  0.65596725,  1.07621031, -0.971896  ,  0.03194186,\n",
       "        0.25659506,  0.18591891,  0.22224946,  0.02544531,  0.99685255,\n",
       "        0.12317446,  0.53159963,  0.33656909, -0.11080538,  0.44820802,\n",
       "        0.28795333,  0.19592373,  0.26167464,  0.29693983,  0.98503746,\n",
       "        0.20594575, -0.51032157,  0.25811482,  0.39786814,  0.77587788,\n",
       "       -0.13194723, -0.14540405, -0.19255935, -0.14553278, -0.77461305,\n",
       "        0.28586947, -0.69584958,  0.39072327, -0.0436395 ,  0.50911248,\n",
       "        0.02588281, -0.15553748,  0.0806892 ,  1.23171531,  0.78596241,\n",
       "        0.01776074,  0.07815729, -0.17178019, -0.09341821,  0.28480482,\n",
       "        0.4092523 ,  0.24662529,  0.14705368,  0.17165078, -0.09715987,\n",
       "       -0.00455668,  1.13259663, -0.25130135, -0.32098398,  0.08968815])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sentence vector\n",
    "sent2vec_model.sentence_vectors(\"This is an awesome gift.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0819638686729\n",
      "0.792567220458\n"
     ]
    }
   ],
   "source": [
    "# Print cosine similarity between two sentences\n",
    "print sent2vec_model.similarity(\"The sky is blue.\", \"I am going to a party.\")\n",
    "print sent2vec_model.similarity(\"This is an awesome gift.\", \"This present is great.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the load and save methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save trained sent2vec model\n",
    "sent2vec_model.save('sent2vec1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pretrained sent2vec model\n",
    "loaded_model = s2v.load('sent2vec1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised similarity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised evaluation of the the learnt sentence embeddings is performed using the sentence cosine similarity, on the [SICK 2014](http://clic.cimec.unitn.it/composes/sick.html) datasets. These similarity scores are compared to the gold-standard human judgements using [Pearsonâ€™s correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "train_sick = []\n",
    "test_sick = []\n",
    "trial_sick = []\n",
    "with open(\"./SICK/SICK.txt\") as f, open('./train_sick.txt', 'w') as f1, open('./test_sick.txt', 'w') as f2, open('./trial_sick.txt', 'w') as f3:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if tokens[0].isdigit():\n",
    "            if tokens[11] == 'TRAIN':\n",
    "                train_sick.append((tokens[1], tokens[2], tokens[4]))\n",
    "                f1.write(tokens[1] + '\\n' + tokens[2] + '\\n')\n",
    "            elif tokens[11] == 'TEST':\n",
    "                test_sick.append((tokens[1], tokens[2], tokens[4]))\n",
    "                f2.write(tokens[1] + '\\n' + tokens[2] + '\\n')\n",
    "            else:\n",
    "                trial_sick.append((tokens[1], tokens[2], tokens[4]))\n",
    "                f3.write(tokens[1] + '\\n' + tokens[2] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A group of kids is playing in a yard and an old man is standing in the background', 'A group of boys in a yard is playing and a man is standing in the background', '4.5')\n",
      "('A group of children is playing in the house and there is no man standing in the background', 'A group of kids is playing in a yard and an old man is standing in the background', '3.2')\n",
      "('The young boys are playing outdoors and the man is smiling nearby', 'The kids are playing outdoors near a man with a smile', '4.7')\n",
      "('The kids are playing outdoors near a man with a smile', 'A group of kids is playing in a yard and an old man is standing in the background', '3.4')\n",
      "('The young boys are playing outdoors and the man is smiling nearby', 'A group of kids is playing in a yard and an old man is standing in the background', '3.7')\n"
     ]
    }
   ],
   "source": [
    "#Print sample evaluation data\n",
    "# Evaluation data is of the form: (sentence1, sentence2, similarity_score)\n",
    "for example in train_sick[:5]:\n",
    "    print example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate pearson correlation score for gensim implementation for sent2vec\n",
    "def pearson_score_gensim(input_):\n",
    "    input_cosine = []\n",
    "    input_sick = []\n",
    "    for example in input_:\n",
    "        input_cosine.append(loaded_model.similarity(example[0], example[1]))\n",
    "        input_sick.append(float(example[2]))\n",
    "    return scipy.stats.pearsonr(input_cosine, input_sick), input_sick, input_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.374514112341 0.385498607302 0.388873105103\n"
     ]
    }
   ],
   "source": [
    "train_score, train_sick_score, train_cosine = pearson_score_gensim(train_sick)\n",
    "test_score, test_sick_score, test_cosine = pearson_score_gensim(test_sick)\n",
    "trial_score, trial_sick_score, trial_cosine = pearson_score_gensim(trial_sick)\n",
    "print train_score[0], test_score[0], trial_score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with original c++ implementation of sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build and train c++ implementation of sent2vec, use the following commands. This will produce object files for all the classes as well as the main binary sent2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1837\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 27019  lr: 0.000000  loss: 3.113516  eta: 0h0m 511982  eta: 0h0m 3.496721  eta: 0h0m   eta: 0h0m \n",
      "\n",
      "\n",
      "Total training time: 26.651419878 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "! git clone https://github.com/epfml/sent2vec.git\n",
    "% cd sent2vec\n",
    "! make\n",
    "start_time = time.time()\n",
    "! ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 20 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 20 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
    "print \"\\n\\nTotal training time: %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get sentence vectors from trained sent2vec model\n",
    "! ./fasttext print-sentence-vectors my_model.bin < ../train_sick.txt > train_output.txt\n",
    "! ./fasttext print-sentence-vectors my_model.bin < ../test_sick.txt > test_output.txt\n",
    "! ./fasttext print-sentence-vectors my_model.bin < ../trial_sick.txt > trial_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(sent1, sent2):\n",
    "    return dot(matutils.unitvec(sent1), matutils.unitvec(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate pearson correlation score for original c++ implementation for sent2vec\n",
    "def pearson_score_original(filename, input_sick):\n",
    "    input_cosine = []\n",
    "    with open(filename) as f:\n",
    "        input_ = []\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip().split()\n",
    "            input_.append([float(j) for j in line])\n",
    "            if i % 2 != 0:\n",
    "                input_cosine.append(similarity(np.array(input_[i]), np.array(input_[i-1])))\n",
    "    return scipy.stats.pearsonr(input_cosine, input_sick), input_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.310250645609 0.292698006351 0.319031428662\n"
     ]
    }
   ],
   "source": [
    "train_score, train_cosine = pearson_score_original('train_output.txt', train_sick_score)\n",
    "test_score, test_cosine = pearson_score_original('test_output.txt', test_sick_score)\n",
    "trial_score, trial_cosine = pearson_score_original('trial_output.txt', trial_sick_score)\n",
    "print train_score[0], test_score[0], trial_score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| S.No. | Model             | Training Time (in seconds) | Pearson score on SICK train set | Pearson score on SICK test set | Pearson score on SICK trial set |\n",
    "|-------|-------------------|----------------------------|---------------------------------|--------------------------------|---------------------------------|\n",
    "| 1.    | Gensim sent2vec   | 276.94                     | 0.37                            | 0.38                           | 0.38                            |\n",
    "| 2.    | Original sent2vec | 26.65                      | 0.31                            | 0.29                           | 0.31                            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
