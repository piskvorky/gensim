{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sent2Vec via Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about using sent2vec model in Gensim. Here, we'll learn to work with the sent2vec library for training sentence-embedding models, saving & loading them and performing similarity operations. This notebook also contains a comparison of the gensim implementation with the [original c++ implementation](https://github.com/epfml/sent2vec), Gensim's Doc2Vec and Gensim's FastText. All the evaluation scripts used in the notebook can be found [here](https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sent2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec delivers numerical representations (features) for short texts or sentences, which can be used as input to any machine learning task later on. Think of it as an unsupervised version of FastText, and an extension of word2vec (CBOW) to sentences. The method uses a simple but efficient unsupervised objective to train distributed representations of sentences. The algorithm outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings, see the [paper](https://arxiv.org/abs/1703.02507) for more details.\n",
    "\n",
    "The sentence embedding is defined as the average of the source word embeddings of its constituent words. This model is furthermore augmented by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model. All models are trained with the same hyperparameters for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.sent2vec import Sent2Vec as s2v\n",
    "from gensim.models.fasttext import FastText as ft\n",
    "from gensim.utils import tokenize\n",
    "import scipy\n",
    "import re\n",
    "from numpy import dot\n",
    "from gensim import matutils\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import eval_sick\n",
    "import eval_classification\n",
    "import eval_trec\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = data_dir + 'lee_background.cor'\n",
    "lee_data = []\n",
    "with open(lee_train_file) as f1, open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if line not in ['\\n', '\\r\\n']:\n",
    "            line = re.split('\\.|\\?|\\n', line.strip())\n",
    "            for sentence in line:\n",
    "                if len(sentence) > 1:\n",
    "                    sentence = tokenize(sentence)\n",
    "                    lee_data.append(list(sentence))\n",
    "                    f2.write(' '.join(lee_data[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hundreds', u'of', u'people', u'have', u'been', u'forced', u'to', u'vacate', u'their', u'homes', u'in', u'the', u'Southern', u'Highlands', u'of', u'New', u'South', u'Wales', u'as', u'strong', u'winds', u'today', u'pushed', u'a', u'huge', u'bushfire', u'towards', u'the', u'town', u'of', u'Hill', u'Top'] \n",
      "\n",
      "[u'A', u'new', u'blaze', u'near', u'Goulburn', u'south', u'west', u'of', u'Sydney', u'has', u'forced', u'the', u'closure', u'of', u'the', u'Hume', u'Highway'] \n",
      "\n",
      "[u'At', u'about', u'pm', u'AEDT', u'a', u'marked', u'deterioration', u'in', u'the', u'weather', u'as', u'a', u'storm', u'cell', u'moved', u'east', u'across', u'the', u'Blue', u'Mountains', u'forced', u'authorities', u'to', u'make', u'a', u'decision', u'to', u'evacuate', u'people', u'from', u'homes', u'in', u'outlying', u'streets', u'at', u'Hill', u'Top', u'in', u'the', u'New', u'South', u'Wales', u'southern', u'highlands'] \n",
      "\n",
      "[u'An', u'estimated', u'residents', u'have', u'left', u'their', u'homes', u'for', u'nearby', u'Mittagong'] \n",
      "\n",
      "[u'The', u'New', u'South', u'Wales', u'Rural', u'Fire', u'Service', u'says', u'the', u'weather', u'conditions', u'which', u'caused', u'the', u'fire', u'to', u'burn', u'in', u'a', u'finger', u'formation', u'have', u'now', u'eased', u'and', u'about', u'fire', u'units', u'in', u'and', u'around', u'Hill', u'Top', u'are', u'optimistic', u'of', u'defending', u'all', u'properties'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample training data\n",
    "for sentence in lee_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gensim implementation of sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 0.06 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 1307, tokens read: 60302\n",
      "INFO:gensim.models.sent2vec:Training...\n",
      "INFO:gensim.models.sent2vec:Begin epoch 0 :\n",
      "INFO:gensim.models.sent2vec:Progress: 3.96, lr: 0.1921, loss: 3.7907\n",
      "INFO:gensim.models.sent2vec:Begin epoch 1 :\n",
      "INFO:gensim.models.sent2vec:Progress: 7.93, lr: 0.1841, loss: 3.6546\n",
      "INFO:gensim.models.sent2vec:Begin epoch 2 :\n",
      "INFO:gensim.models.sent2vec:Progress: 11.90, lr: 0.1762, loss: 3.5554\n",
      "INFO:gensim.models.sent2vec:Begin epoch 3 :\n",
      "INFO:gensim.models.sent2vec:Progress: 15.86, lr: 0.1683, loss: 3.4607\n",
      "INFO:gensim.models.sent2vec:Begin epoch 4 :\n",
      "INFO:gensim.models.sent2vec:Progress: 19.83, lr: 0.1603, loss: 3.3682\n",
      "INFO:gensim.models.sent2vec:Begin epoch 5 :\n",
      "INFO:gensim.models.sent2vec:Progress: 23.80, lr: 0.1524, loss: 3.2797\n",
      "INFO:gensim.models.sent2vec:Begin epoch 6 :\n",
      "INFO:gensim.models.sent2vec:Progress: 27.76, lr: 0.1445, loss: 3.1946\n",
      "INFO:gensim.models.sent2vec:Begin epoch 7 :\n",
      "INFO:gensim.models.sent2vec:Progress: 31.73, lr: 0.1365, loss: 3.1151\n",
      "INFO:gensim.models.sent2vec:Begin epoch 8 :\n",
      "INFO:gensim.models.sent2vec:Progress: 35.70, lr: 0.1286, loss: 3.0411\n",
      "INFO:gensim.models.sent2vec:Begin epoch 9 :\n",
      "INFO:gensim.models.sent2vec:Progress: 39.67, lr: 0.1207, loss: 2.9710\n",
      "INFO:gensim.models.sent2vec:Begin epoch 10 :\n",
      "INFO:gensim.models.sent2vec:Progress: 43.63, lr: 0.1127, loss: 2.9059\n",
      "INFO:gensim.models.sent2vec:Begin epoch 11 :\n",
      "INFO:gensim.models.sent2vec:Progress: 47.60, lr: 0.1048, loss: 2.8440\n",
      "INFO:gensim.models.sent2vec:Begin epoch 12 :\n",
      "INFO:gensim.models.sent2vec:Progress: 51.57, lr: 0.0969, loss: 2.7861\n",
      "INFO:gensim.models.sent2vec:Begin epoch 13 :\n",
      "INFO:gensim.models.sent2vec:Progress: 55.53, lr: 0.0889, loss: 2.7323\n",
      "INFO:gensim.models.sent2vec:Begin epoch 14 :\n",
      "INFO:gensim.models.sent2vec:Progress: 59.50, lr: 0.0810, loss: 2.6818\n",
      "INFO:gensim.models.sent2vec:Begin epoch 15 :\n",
      "INFO:gensim.models.sent2vec:Progress: 63.47, lr: 0.0731, loss: 2.6347\n",
      "INFO:gensim.models.sent2vec:Begin epoch 16 :\n",
      "INFO:gensim.models.sent2vec:Progress: 67.43, lr: 0.0651, loss: 2.5900\n",
      "INFO:gensim.models.sent2vec:Begin epoch 17 :\n",
      "INFO:gensim.models.sent2vec:Progress: 71.40, lr: 0.0572, loss: 2.5480\n",
      "INFO:gensim.models.sent2vec:Begin epoch 18 :\n",
      "INFO:gensim.models.sent2vec:Progress: 75.37, lr: 0.0493, loss: 2.5089\n",
      "INFO:gensim.models.sent2vec:Begin epoch 19 :\n",
      "INFO:gensim.models.sent2vec:Progress: 79.33, lr: 0.0413, loss: 2.4717\n",
      "INFO:gensim.models.sent2vec:Total training time: 319.090888023 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model\n",
    "sent2vec_model = s2v(lee_data, vector_size=100, epochs=20, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec supports the folllowing parameters:\n",
    "\n",
    " - vector_size: Size of embeddings to be learnt (Default 100)\n",
    " - alpha: Initial learning rate (Default 0.2)\n",
    " - min_count: Ignore words with number of occurrences below this (Default 5)\n",
    " - loss: Training objective. Allowed values: `ns` (Default `ns`)\n",
    " - neg: Number of negative words to sample, for `ns` (Default 10)\n",
    " - epochs: Number of epochs (Default 5)\n",
    " - bucket: Number of hash buckets for vocabulary (Default 2000000)\n",
    " - lr_update_rate: Change the rate of updates for the learning rate (Default 100)\n",
    " - t: Sampling threshold (Default 0.0001)\n",
    " - dropoutk: Number of ngrams dropped when training a sent2vec model (Default 2)\n",
    " - word_ngrams: Max length of word ngram (Default 2)\n",
    " - minn: min length of char ngrams (Default 3)\n",
    " - maxn: max length of char ngrams (Default 6)\n",
    " - seed: random seed for reproducibility reasons (Default 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.47282963e-01,   1.87189350e-01,  -3.21987474e-02,\n",
       "         7.91178968e-02,   3.39226979e-01,  -4.05825705e-01,\n",
       "         8.00900208e-01,   1.35698618e-01,  -4.38281501e-02,\n",
       "         7.56798528e-01,  -3.80137642e-01,  -2.22912740e-01,\n",
       "        -8.74431924e-02,   5.80761992e-02,   4.83582117e-01,\n",
       "         1.01573390e-01,   7.24145461e-01,   2.94534907e-01,\n",
       "         1.56936339e-01,  -1.77839273e-01,  -1.38541675e-01,\n",
       "         2.78566131e-01,  -7.17920556e-01,  -2.89763518e-01,\n",
       "         1.50396665e-01,  -1.23929553e-01,   6.27710213e-01,\n",
       "         2.74299500e-01,   3.58840180e-01,   4.12131903e-02,\n",
       "        -1.07678619e-01,  -1.87951293e-01,  -5.40631978e-01,\n",
       "        -1.87393133e-01,   6.79594841e-01,   8.36914707e-01,\n",
       "         1.57756821e-02,  -6.81864588e-01,   3.33469583e-01,\n",
       "         9.79331323e-01,  -1.11638314e-03,   8.25131676e-01,\n",
       "         8.70815820e-01,  -3.52632190e-01,   2.81376163e-01,\n",
       "         3.82966643e-01,   3.37886963e-01,  -4.67265898e-01,\n",
       "         7.07851315e-01,  -1.03258879e-01,   7.12496253e-01,\n",
       "         2.16417260e-01,   1.98831043e-01,   2.04401469e-01,\n",
       "         3.06833554e-01,   5.52802442e-02,  -4.06280493e-02,\n",
       "        -2.07916741e-01,   3.33660708e-01,   9.26138098e-01,\n",
       "         4.69811145e-01,   9.98826805e-01,   8.88274326e-02,\n",
       "        -4.50999787e-02,   6.71231523e-01,  -2.17487060e-01,\n",
       "        -4.25969387e-01,  -3.33714633e-01,  -1.24998775e-01,\n",
       "        -2.57119644e-01,   4.02696011e-01,   3.66844870e-01,\n",
       "         6.97825902e-02,   3.58792679e-01,   3.37613875e-01,\n",
       "         5.85787878e-01,   4.29306178e-01,  -3.25707705e-01,\n",
       "        -4.94586031e-01,   4.60758259e-01,   4.59632207e-01,\n",
       "        -4.50481765e-02,  -9.22794458e-02,   2.29469164e-01,\n",
       "        -3.29727348e-01,   5.22608589e-01,   2.70233076e-01,\n",
       "         6.44984041e-01,  -6.51028773e-01,   1.63713927e-01,\n",
       "         7.23727747e-01,   5.73119011e-01,   1.22002313e-01,\n",
       "        -3.63567753e-01,  -3.85430169e-01,   1.65674589e-01,\n",
       "         1.22678258e+00,  -4.91405363e-01,  -1.16092820e-01,\n",
       "         3.87987852e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sentence vector\n",
    "sent2vec_model.sentence_vectors(['This', 'is', 'an', 'awesome', 'gift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.116041437211\n",
      "0.783033600606\n"
     ]
    }
   ],
   "source": [
    "# Print cosine similarity between two sentences\n",
    "print sent2vec_model.similarity(['The', 'sky', 'is', 'blue'], ['I', 'am', 'going', 'to', 'a', 'party'])\n",
    "print sent2vec_model.similarity(['This', 'is', 'an', 'awesome', 'gift'], ['This', 'present', 'is', 'great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the load and save methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Sent2Vec object under s2v1, separately None\n",
      "INFO:gensim.utils:storing np array 'wi' to s2v1.wi.npy\n",
      "INFO:gensim.utils:saved s2v1\n"
     ]
    }
   ],
   "source": [
    "# Save trained sent2vec model\n",
    "sent2vec_model.save('s2v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Sent2Vec object from s2v1\n",
      "INFO:gensim.utils:loading wi from s2v1.wi.npy with mmap=None\n",
      "INFO:gensim.utils:loaded s2v1\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained sent2vec model\n",
    "loaded_model = s2v.load('s2v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised similarity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised evaluation of the the learnt sentence embeddings is performed using the sentence cosine similarity, on the [SICK 2014](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools) datasets. These similarity scores are compared to the gold-standard human judgements using [Pearson’s correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs. We use the code provided by [Kiros et al., 2015](https://github.com/ryankiros/skip-thoughts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.488022039971\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.480271787239\n",
      "Test Spearman: 0.489470899414\n",
      "Test MSE: 0.786314446477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.09487504,  3.26262388,  3.3487608 , ...,  3.32659516,\n",
       "        2.54281916,  3.60840255])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(loaded_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Downstream Supervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings are evaluated for various supervised classification tasks. We evaluate classification of movie review sentiment (MR) (Pang & Lee, 2005), subjectivity classification (SUBJ)(Pang & Lee, 2004) and question type classification (TREC) (Voorhees, 2002). To classify, we use the code provided by [(Kiros et al., 2015)](https://github.com/ryankiros/skip-thoughts). Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the [MR and SUBJ](https://www.cs.cornell.edu/people/pabo/movie-review-data/) datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the [TREC dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/), the accuracy is computed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.77100000000000002]\n",
      "[0.77100000000000002, 0.77200000000000002]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751, 0.753]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751, 0.753, 0.77200000000000002]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751, 0.753, 0.77200000000000002, 0.77300000000000002]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751, 0.753, 0.77200000000000002, 0.77300000000000002, 0.753]\n",
      "[0.77100000000000002, 0.77200000000000002, 0.76800000000000002, 0.77900000000000003, 0.751, 0.753, 0.77200000000000002, 0.77300000000000002, 0.753, 0.79300000000000004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.77100000000000002,\n",
       " 0.77200000000000002,\n",
       " 0.76800000000000002,\n",
       " 0.77900000000000003,\n",
       " 0.751,\n",
       " 0.753,\n",
       " 0.77200000000000002,\n",
       " 0.77300000000000002,\n",
       " 0.753,\n",
       " 0.79300000000000004]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=loaded_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.58856607310215558]\n",
      "[0.58856607310215558, 0.56982193064667297]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637, 0.59568480300187621]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637, 0.59568480300187621, 0.58818011257035652]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637, 0.59568480300187621, 0.58818011257035652, 0.59287054409005624]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637, 0.59568480300187621, 0.58818011257035652, 0.59287054409005624, 0.60506566604127576]\n",
      "[0.58856607310215558, 0.56982193064667297, 0.59099437148217637, 0.56378986866791747, 0.59099437148217637, 0.59568480300187621, 0.58818011257035652, 0.59287054409005624, 0.60506566604127576, 0.57035647279549717]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.58856607310215558,\n",
       " 0.56982193064667297,\n",
       " 0.59099437148217637,\n",
       " 0.56378986866791747,\n",
       " 0.59099437148217637,\n",
       " 0.59568480300187621,\n",
       " 0.58818011257035652,\n",
       " 0.59287054409005624,\n",
       " 0.60506566604127576,\n",
       " 0.57035647279549717]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=loaded_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.634\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(model=loaded_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of original c++ implementation of sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build and train c++ implementation of sent2vec, use the following commands. This will produce object files for all the classes as well as the main binary sent2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/sent2vec\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/epfml/sent2vec.git\n",
    "% cd sent2vec\n",
    "! make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1837\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 27315  lr: 0.000000  loss: 3.016871  eta: 0h0m m 0m 0h0m   loss: 3.108133  eta: 0h0m 3.092520  eta: 0h0m h0m \n",
      "\n",
      "\n",
      "Total training time: 25.7511711121 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "start_time = time.time()\n",
    "! ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 20 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 20 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
    "print \"\\n\\nTotal training time: %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.417315994108\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.420326770285\n",
      "Test Spearman: 0.425649516516\n",
      "Test MSE: 0.838276461109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.98017314,  3.09968709,  3.31417498, ...,  3.26684362,\n",
       "        2.37790687,  2.85882165])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.78600000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003, 0.76200000000000001]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.79000000000000004]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.79000000000000004, 0.76900000000000002]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77700000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.79000000000000004, 0.76900000000000002, 0.80100000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78600000000000003,\n",
       " 0.79900000000000004,\n",
       " 0.78500000000000003,\n",
       " 0.77700000000000002,\n",
       " 0.78100000000000003,\n",
       " 0.76200000000000001,\n",
       " 0.78200000000000003,\n",
       " 0.79000000000000004,\n",
       " 0.76900000000000002,\n",
       " 0.80100000000000005]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.56794751640112462]\n",
      "[0.56794751640112462, 0.59700093720712277]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.61819887429643527]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.61819887429643527, 0.61069418386491559]\n",
      "[0.56794751640112462, 0.59700093720712277, 0.56472795497185746, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.61819887429643527, 0.61069418386491559, 0.59662288930581608]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.56794751640112462,\n",
       " 0.59700093720712277,\n",
       " 0.56472795497185746,\n",
       " 0.57786116322701686,\n",
       " 0.58067542213883683,\n",
       " 0.59849906191369606,\n",
       " 0.60506566604127576,\n",
       " 0.61819887429643527,\n",
       " 0.61069418386491559,\n",
       " 0.59662288930581608]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.594\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model1 with PV-DM and sum of context word vectors\n",
    "doc2vec_model1 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model1.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model2 with PV-DBOW and sum of context word vectors\n",
    "doc2vec_model2 = gensim.models.doc2vec.Doc2Vec(dm=0, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model2.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model3 with PV-DM and mean of context word vectors\n",
    "doc2vec_model3 = gensim.models.doc2vec.Doc2Vec(dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model3.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model4 with PV-DBOW and mean of context word vectors\n",
    "doc2vec_model4 = gensim.models.doc2vec.Doc2Vec(dm=0, dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model4.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.53% examples, 451598 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724888 effective words) took 1.6s, 459770 effective words/s\n",
      "CPU times: user 3.73 s, sys: 230 ms, total: 3.96 s\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model1.train(train_corpus, total_examples=doc2vec_model1.corpus_count, epochs=doc2vec_model1.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.98% examples, 579813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724799 effective words) took 1.2s, 586018 effective words/s\n",
      "CPU times: user 3.07 s, sys: 126 ms, total: 3.2 s\n",
      "Wall time: 1.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724799"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model2.train(train_corpus, total_examples=doc2vec_model2.corpus_count, epochs=doc2vec_model2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.78% examples, 262306 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.73% examples, 311389 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724835 effective words) took 2.2s, 326296 effective words/s\n",
      "CPU times: user 3.69 s, sys: 163 ms, total: 3.86 s\n",
      "Wall time: 2.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724835"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model3.train(train_corpus, total_examples=doc2vec_model3.corpus_count, epochs=doc2vec_model3.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.20% examples, 500867 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724340 effective words) took 1.4s, 527263 effective words/s\n",
      "CPU times: user 3.2 s, sys: 132 ms, total: 3.33 s\n",
      "Wall time: 1.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724340"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model4.train(train_corpus, total_examples=doc2vec_model4.corpus_count, epochs=doc2vec_model4.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.286438142989\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.269150921565\n",
      "Test Spearman: 0.262674319422\n",
      "Test MSE: 0.94450382038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.33818458,  3.32924404,  3.22118222, ...,  3.62762797,\n",
       "        3.44393652,  3.15340341])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model1, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.67400000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004, 0.67100000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004, 0.67100000000000004, 0.69199999999999995]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.67400000000000004,\n",
       " 0.67000000000000004,\n",
       " 0.65600000000000003,\n",
       " 0.64000000000000001,\n",
       " 0.66800000000000004,\n",
       " 0.68000000000000005,\n",
       " 0.67900000000000005,\n",
       " 0.66800000000000004,\n",
       " 0.67100000000000004,\n",
       " 0.69199999999999995]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model1, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.5135895032802249]\n",
      "[0.5135895032802249, 0.53045923149015928]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584, 0.5684803001876173]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584, 0.5684803001876173, 0.5478424015009381]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5135895032802249,\n",
       " 0.53045923149015928,\n",
       " 0.54878048780487809,\n",
       " 0.54409005628517826,\n",
       " 0.55816135084427765,\n",
       " 0.5684803001876173,\n",
       " 0.57879924953095685,\n",
       " 0.5412757973733584,\n",
       " 0.5684803001876173,\n",
       " 0.5478424015009381]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model1, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.39693084071\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.332600557652\n",
      "Test Spearman: 0.331574006828\n",
      "Test MSE: 0.905109955103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.57159891,  3.35031843,  3.5568133 , ...,  3.06880136,\n",
       "        3.49490334,  3.19048271])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model2, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.75]\n",
      "[0.75, 0.71699999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997, 0.71199999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997, 0.71199999999999997, 0.73499999999999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.75,\n",
       " 0.71699999999999997,\n",
       " 0.70499999999999996,\n",
       " 0.73999999999999999,\n",
       " 0.72299999999999998,\n",
       " 0.72099999999999997,\n",
       " 0.71599999999999997,\n",
       " 0.71099999999999997,\n",
       " 0.71199999999999997,\n",
       " 0.73499999999999999]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model2, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57638238050609181]\n",
      "[0.57638238050609181, 0.57357075913776945]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732, 0.56566604127579734]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732, 0.56566604127579734, 0.59568480300187621]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57638238050609181,\n",
       " 0.57357075913776945,\n",
       " 0.59474671669793622,\n",
       " 0.55253283302063794,\n",
       " 0.57317073170731703,\n",
       " 0.55628517823639778,\n",
       " 0.59849906191369606,\n",
       " 0.56754221388367732,\n",
       " 0.56566604127579734,\n",
       " 0.59568480300187621]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model2, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.218506672244\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.268180823329\n",
      "Test Spearman: 0.267794536402\n",
      "Test MSE: 0.946454984438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.55381104,  3.65387629,  3.32172886, ...,  3.50422149,\n",
       "        3.73455072,  3.27572114])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model3, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.64700000000000002]\n",
      "[0.64700000000000002, 0.65500000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004, 0.66000000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004, 0.66000000000000003, 0.67800000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.64700000000000002,\n",
       " 0.65500000000000003,\n",
       " 0.623,\n",
       " 0.66400000000000003,\n",
       " 0.65100000000000002,\n",
       " 0.67300000000000004,\n",
       " 0.69399999999999995,\n",
       " 0.66500000000000004,\n",
       " 0.66000000000000003,\n",
       " 0.67800000000000005]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model3, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.50140581068416124]\n",
      "[0.50140581068416124, 0.53701968134957823]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765, 0.53752345215759845]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765, 0.53752345215759845, 0.58724202626641653]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.50140581068416124,\n",
       " 0.53701968134957823,\n",
       " 0.54690431519699811,\n",
       " 0.5478424015009381,\n",
       " 0.57035647279549717,\n",
       " 0.56378986866791747,\n",
       " 0.56754221388367732,\n",
       " 0.55816135084427765,\n",
       " 0.53752345215759845,\n",
       " 0.58724202626641653]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model3, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.409921757155\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.341916171513\n",
      "Test Spearman: 0.340134184451\n",
      "Test MSE: 0.898672297686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.4285679 ,  3.71794175,  2.89571583, ...,  3.20624222,\n",
       "        3.11745406,  2.71925664])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model4, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.751]\n",
      "[0.751, 0.70699999999999996]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997, 0.72099999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997, 0.72099999999999997, 0.73099999999999998]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.751,\n",
       " 0.70699999999999996,\n",
       " 0.71999999999999997,\n",
       " 0.73299999999999998,\n",
       " 0.73899999999999999,\n",
       " 0.69999999999999996,\n",
       " 0.71599999999999997,\n",
       " 0.71399999999999997,\n",
       " 0.72099999999999997,\n",
       " 0.73099999999999998]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model4, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57919400187441428]\n",
      "[0.57919400187441428, 0.56419868791002814]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173, 0.575046904315197]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173, 0.575046904315197, 0.61444652908067543]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57919400187441428,\n",
       " 0.56419868791002814,\n",
       " 0.6031894934333959,\n",
       " 0.57410881801125702,\n",
       " 0.56285178236397748,\n",
       " 0.57129455909943716,\n",
       " 0.59474671669793622,\n",
       " 0.5684803001876173,\n",
       " 0.575046904315197,\n",
       " 0.61444652908067543]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model4, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.398\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model1, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.422\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model2, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.378\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model3, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.404\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model4, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of sentence vectors obtained from averaging FastText word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 10781 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 45 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.fasttext:Total number of ngrams is 17006\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.90% examples, 149 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.75% examples, 280 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.48% examples, 404 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.25% examples, 302 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.15% examples, 349 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.95% examples, 409 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.85% examples, 347 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.72% examples, 367 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.45% examples, 400 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.22% examples, 365 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.13% examples, 379 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.93% examples, 399 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.83% examples, 381 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.68% examples, 384 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.42% examples, 399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.17% examples, 385 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.07% examples, 388 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.85% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.77% examples, 392 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.62% examples, 390 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.37% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.12% examples, 394 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.03% examples, 392 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.82% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.73% examples, 397 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.58% examples, 395 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.33% examples, 398 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.07% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.95% examples, 397 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.73% examples, 397 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.67% examples, 401 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.52% examples, 398 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.27% examples, 395 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.03% examples, 401 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.90% examples, 398 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.72% examples, 395 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.62% examples, 403 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.47% examples, 399 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.20% examples, 394 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.95% examples, 402 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.85% examples, 399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.62% examples, 396 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.57% examples, 403 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.43% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.13% examples, 396 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.77% examples, 400 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.55% examples, 397 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.37% examples, 401 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.07% examples, 397 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.80% examples, 404 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.68% examples, 402 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.48% examples, 398 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.42% examples, 405 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.32% examples, 404 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.00% examples, 398 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.77% examples, 405 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.62% examples, 405 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.45% examples, 399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.25% examples, 406 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.98% examples, 400 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.68% examples, 406 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.55% examples, 407 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.38% examples, 402 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.23% examples, 407 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.18% examples, 409 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.90% examples, 403 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.62% examples, 408 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.48% examples, 411 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.32% examples, 405 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.15% examples, 409 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.07% examples, 412 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.85% examples, 406 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.57% examples, 409 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.40% examples, 414 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.27% examples, 408 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.18% examples, 410 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.82% examples, 408 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.63% examples, 411 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.37% examples, 416 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.23% examples, 410 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.13% examples, 412 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.00% examples, 417 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.78% examples, 411 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.53% examples, 413 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.18% examples, 412 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.10% examples, 414 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.93% examples, 418 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.77% examples, 412 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.57% examples, 414 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.17% examples, 414 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.05% examples, 415 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.87% examples, 419 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.72% examples, 414 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.48% examples, 415 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.13% examples, 415 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.03% examples, 416 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.83% examples, 420 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.68% examples, 416 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.42% examples, 417 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.07% examples, 417 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.98% examples, 418 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.77% examples, 421 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.62% examples, 417 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.37% examples, 418 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.12% examples, 422 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.03% examples, 418 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.95% examples, 419 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.73% examples, 422 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.58% examples, 418 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.32% examples, 418 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.33% examples, 420 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.22% examples, 421 words/s, in_qsize 1, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 100.00% examples, 424 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1197800 raw words (652596 effective words) took 1540.0s, 424 effective words/s\n",
      "\n",
      "\n",
      "Total training time: 1540.17631984 seconds\n"
     ]
    }
   ],
   "source": [
    "lee_data = LineSentence(lee_train_file)\n",
    "fasttext_model = ft(size=100, alpha=0.2, negative=10, max_vocab_size=30000000, seed=42, iter=20)\n",
    "fasttext_model.build_vocab(lee_data)\n",
    "start_time = time.time()\n",
    "fasttext_model.train(lee_data, total_examples=fasttext_model.corpus_count, epochs=fasttext_model.iter)\n",
    "print \"\\n\\nTotal training time: %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving FastText object under ft1, separately None\n",
      "INFO:gensim.utils:not storing attribute syn0norm\n",
      "INFO:gensim.utils:not storing attribute syn0_ngrams_norm\n",
      "INFO:gensim.utils:not storing attribute syn0_vocab_norm\n",
      "INFO:gensim.utils:saved ft1\n"
     ]
    }
   ],
   "source": [
    "fasttext_model.save('ft1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading FastText object from ft1\n",
      "INFO:gensim.utils:loading wv recursively from ft1.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.utils:setting ignored attribute syn0_ngrams_norm to None\n",
      "INFO:gensim.utils:setting ignored attribute syn0_vocab_norm to None\n",
      "INFO:gensim.utils:loaded ft1\n"
     ]
    }
   ],
   "source": [
    "ft_loaded_model = ft.load('ft1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.508652314637\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.502745225838\n",
      "Test Spearman: 0.498571233172\n",
      "Test MSE: 0.762192068723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.79715686,  3.30016953,  3.30776085, ...,  3.46108352,\n",
       "        3.05130163,  2.48509623])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(ft_loaded_model, seed=42, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.82299999999999995]\n",
      "[0.82299999999999995, 0.80500000000000005]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005, 0.78600000000000003]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005, 0.78600000000000003, 0.80500000000000005]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005, 0.78600000000000003, 0.80500000000000005, 0.80800000000000005]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005, 0.78600000000000003, 0.80500000000000005, 0.80800000000000005, 0.78800000000000003]\n",
      "[0.82299999999999995, 0.80500000000000005, 0.79700000000000004, 0.80600000000000005, 0.80800000000000005, 0.78600000000000003, 0.80500000000000005, 0.80800000000000005, 0.78800000000000003, 0.81899999999999995]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.82299999999999995,\n",
       " 0.80500000000000005,\n",
       " 0.79700000000000004,\n",
       " 0.80600000000000005,\n",
       " 0.80800000000000005,\n",
       " 0.78600000000000003,\n",
       " 0.80500000000000005,\n",
       " 0.80800000000000005,\n",
       " 0.78800000000000003,\n",
       " 0.81899999999999995]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=ft_loaded_model, name='SUBJ', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.61480787253983127]\n",
      "[0.61480787253983127, 0.60449859418931584]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638, 0.60694183864915574]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638, 0.60694183864915574, 0.64446529080675419]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638, 0.60694183864915574, 0.64446529080675419, 0.61257035647279545]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638, 0.60694183864915574, 0.64446529080675419, 0.61257035647279545, 0.60412757973733588]\n",
      "[0.61480787253983127, 0.60449859418931584, 0.59193245778611636, 0.61819887429643527, 0.59005628517823638, 0.60694183864915574, 0.64446529080675419, 0.61257035647279545, 0.60412757973733588, 0.60694183864915574]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.61480787253983127,\n",
       " 0.60449859418931584,\n",
       " 0.59193245778611636,\n",
       " 0.61819887429643527,\n",
       " 0.59005628517823638,\n",
       " 0.60694183864915574,\n",
       " 0.64446529080675419,\n",
       " 0.61257035647279545,\n",
       " 0.60412757973733588,\n",
       " 0.60694183864915574]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=ft_loaded_model, name='MR', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(ft_loaded_model, evalcv=False, evaltest=True, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| S.No. | Model Name                                | Training Time (in seconds) | Pearson/Spearman/MSE on SICK | Mean SUBJ | Mean MR | TREC |\n",
    "|-------|-------------------------------------------|----------------------------|------------------------------|-----------|---------|------|\n",
    "| 1.    | Gensim Sent2Vec                           | 319.09                     | 0.48/0.49/0.78               | 0.76      | 0.58    | 0.63 |\n",
    "| 2.    | Original Sent2Vec                         | 25.75                      | 0.42/0.43/0.82               | 0.78      | 0.59    | 0.59 |\n",
    "| 3.    | PV-DM with sum of context word vectors    | 3.57                       | 0.27/0.27/0.94               | 0.66      | 0.55    | 0.37 |\n",
    "| 4.    | PV-DM with mean of context word vectors   | 3.8                        | 0.28/0.28/0.93               | 0.67      | 0.55    | 0.38 |\n",
    "| 5.    | PV-DBOW with sum of context word vector   | 3.06                       | 0.36/0.35/0.88               | 0.73      | 0.57    | 0.42 |\n",
    "| 6.    | PV-DBOW with mean of context word vectors | 2.92                       | 0.34/0.34/0.89               | 0.72      | 0.57    | 0.41 |\n",
    "| 7.    | Mean of gensim fasttext word vectors      | 1540.17                    | 0.49/0.49/0.76               | 0.80      | 0.60    | 0.62 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on sample of Toronto Book Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "toronto_data = []\n",
    "lines = 0\n",
    "with open('./books_in_sentences/books_large_p1.txt') as f1, open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if np.random.random() > 0.5:\n",
    "            if lines >= 100000:\n",
    "                break\n",
    "            lines += 1\n",
    "            if line not in ['\\n', '\\r\\n']:\n",
    "                line = re.split('\\.|\\?|\\n', line.strip())\n",
    "                for sentence in line:\n",
    "                    if len(sentence) > 1:\n",
    "                        sentence = tokenize(sentence)\n",
    "                        toronto_data.append(list(sentence))\n",
    "                        f2.write(' '.join(toronto_data[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'isbn', u'isbn', u'for', u'my', u'family', u'who', u'encouraged', u'me', u'to', u'never', u'stop', u'fighting', u'for', u'my', u'dreams', u'chapter', u'summer', u'vacations', u'supposed', u'to', u'be', u'fun', u'right'] \n",
      "\n",
      "[u'starlings', u'new', u'york', u'is', u'not', u'the', u'place', u'youd', u'expect', u'much', u'to', u'happen'] \n",
      "\n",
      "[u'its', u'a', u'small', u'quiet', u'town', u'the', u'kind', u'where', u'everyone', u'knows', u'your', u'name'] \n",
      "\n",
      "[u'only', u'because', u'everyone', u'felt', u'so', u'safe', u'so', u'comfy'] \n",
      "\n",
      "[u'they', u'dont', u'know', u'the', u'half', u'of', u'it'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample training data\n",
    "for sentence in toronto_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 1.00 M words\n",
      "INFO:gensim.models.sent2vec:Read 1.35 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 11552, tokens read: 1348104\n",
      "INFO:gensim.models.sent2vec:Training...\n",
      "INFO:gensim.models.sent2vec:Begin epoch 0 :\n",
      "INFO:gensim.models.sent2vec:Progress: 19.33, lr: 0.1613, loss: 2.9068\n",
      "INFO:gensim.models.sent2vec:Begin epoch 1 :\n",
      "INFO:gensim.models.sent2vec:Progress: 38.66, lr: 0.1227, loss: 2.7171\n",
      "INFO:gensim.models.sent2vec:Begin epoch 2 :\n",
      "INFO:gensim.models.sent2vec:Progress: 57.98, lr: 0.0840, loss: 2.5908\n",
      "INFO:gensim.models.sent2vec:Begin epoch 3 :\n",
      "INFO:gensim.models.sent2vec:Progress: 77.31, lr: 0.0454, loss: 2.4977\n",
      "INFO:gensim.models.sent2vec:Begin epoch 4 :\n",
      "INFO:gensim.models.sent2vec:Progress: 96.64, lr: 0.0067, loss: 2.4345\n",
      "INFO:gensim.models.sent2vec:Total training time: 1414.88167095 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model on part of the Toronto Book Corpus (100,000 sentences)\n",
    "sent2vec_toronto_model = s2v(toronto_data, vector_size=100, epochs=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Sent2Vec object under s2v2, separately None\n",
      "INFO:gensim.utils:storing np array 'wi' to s2v2.wi.npy\n",
      "INFO:gensim.utils:saved s2v2\n"
     ]
    }
   ],
   "source": [
    "sent2vec_toronto_model.save('s2v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.478225382288\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.527323452228\n",
      "Test Spearman: 0.520549235358\n",
      "Test MSE: 0.739274124711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.92524702,  2.96355812,  3.1027343 , ...,  3.37268152,\n",
       "        2.13288476,  3.2024786 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(sent2vec_toronto_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.572\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(sent2vec_toronto_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.60637300843486408]\n",
      "[0.60637300843486408, 0.60918462980318655]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559, 0.61538461538461542]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559, 0.61538461538461542, 0.62288930581613511]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559, 0.61538461538461542, 0.62288930581613511, 0.61913696060037526]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559, 0.61538461538461542, 0.62288930581613511, 0.61913696060037526, 0.61257035647279545]\n",
      "[0.60637300843486408, 0.60918462980318655, 0.61913696060037526, 0.60694183864915574, 0.61069418386491559, 0.61538461538461542, 0.62288930581613511, 0.61913696060037526, 0.61257035647279545, 0.60694183864915574]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60637300843486408,\n",
       " 0.60918462980318655,\n",
       " 0.61913696060037526,\n",
       " 0.60694183864915574,\n",
       " 0.61069418386491559,\n",
       " 0.61538461538461542,\n",
       " 0.62288930581613511,\n",
       " 0.61913696060037526,\n",
       " 0.61257035647279545,\n",
       " 0.60694183864915574]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=sent2vec_toronto_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.79500000000000004]\n",
      "[0.79500000000000004, 0.80500000000000005]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002, 0.78100000000000003]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002, 0.78100000000000003, 0.77300000000000002]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002, 0.78100000000000003, 0.77300000000000002, 0.80500000000000005]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002, 0.78100000000000003, 0.77300000000000002, 0.80500000000000005, 0.79900000000000004]\n",
      "[0.79500000000000004, 0.80500000000000005, 0.77900000000000003, 0.78700000000000003, 0.77200000000000002, 0.78100000000000003, 0.77300000000000002, 0.80500000000000005, 0.79900000000000004, 0.80900000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.79500000000000004,\n",
       " 0.80500000000000005,\n",
       " 0.77900000000000003,\n",
       " 0.78700000000000003,\n",
       " 0.77200000000000002,\n",
       " 0.78100000000000003,\n",
       " 0.77300000000000002,\n",
       " 0.80500000000000005,\n",
       " 0.79900000000000004,\n",
       " 0.80900000000000005]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=sent2vec_toronto_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/sent2vec\n",
      "Read 1M words\n",
      "Number of words:  12990\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 85591  lr: 0.000000  loss: 2.670269  eta: 0h0m 99612  loss: 7.089671  eta: 0h17m 3.376575  eta: 0h0m m %  words/sec/thread: 29819  lr: 0.158044  loss: 3.205851  eta: 0h0m m %  words/sec/thread: 51649  lr: 0.113969  loss: 2.956678  eta: 0h0m %  words/sec/thread: 53235  lr: 0.110118  loss: 2.931963  eta: 0h0m 55.1%  words/sec/thread: 60925  lr: 0.089842  loss: 2.857768  eta: 0h0m 57.3%  words/sec/thread: 62504  lr: 0.085355  loss: 2.848190  eta: 0h0m 0m   lr: 0.076588  loss: 2.827639  eta: 0h0m 63.6%  words/sec/thread: 66675  lr: 0.072802  loss: 2.817499  eta: 0h0m   eta: 0h0m 0h0m   lr: 0.031771  loss: 2.690434  eta: 0h0m 0m \n",
      "\n",
      "\n",
      "Total training time: 32.5075762272 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "% cd sent2vec\n",
    "start_time = time.time()\n",
    "! ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 5 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 20 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
    "print \"\\n\\nTotal training time: %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.417587833758\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.419756504215\n",
      "Test Spearman: 0.425938719051\n",
      "Test MSE: 0.838645111858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.97449634,  3.08862349,  3.33243425, ...,  3.27892888,\n",
       "        2.38570633,  2.86452046])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.78600000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003, 0.76200000000000001]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.78900000000000003]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.78900000000000003, 0.76900000000000002]\n",
      "[0.78600000000000003, 0.79900000000000004, 0.78500000000000003, 0.77800000000000002, 0.78100000000000003, 0.76200000000000001, 0.78200000000000003, 0.78900000000000003, 0.76900000000000002, 0.80100000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78600000000000003,\n",
       " 0.79900000000000004,\n",
       " 0.78500000000000003,\n",
       " 0.77800000000000002,\n",
       " 0.78100000000000003,\n",
       " 0.76200000000000001,\n",
       " 0.78200000000000003,\n",
       " 0.78900000000000003,\n",
       " 0.76900000000000002,\n",
       " 0.80100000000000005]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.5670103092783505]\n",
      "[0.5670103092783505, 0.59700093720712277]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.62007504690431525]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.62007504690431525, 0.60881801125703561]\n",
      "[0.5670103092783505, 0.59700093720712277, 0.56378986866791747, 0.57786116322701686, 0.58067542213883683, 0.59849906191369606, 0.60506566604127576, 0.62007504690431525, 0.60881801125703561, 0.59756097560975607]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5670103092783505,\n",
       " 0.59700093720712277,\n",
       " 0.56378986866791747,\n",
       " 0.57786116322701686,\n",
       " 0.58067542213883683,\n",
       " 0.59849906191369606,\n",
       " 0.60506566604127576,\n",
       " 0.62007504690431525,\n",
       " 0.60881801125703561,\n",
       " 0.59756097560975607]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.594\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: It is evident that more data = better results as the above model (trained for 5 epochs) achieves similar results to the model trained on the much smaller Lee corpus (trained for 20 epochs)\n",
    "\n",
    "| S.No. | Model             | Training Time (in seconds) | Pearson/Spearman/MSE on SICK | MR   | SUBJ | TREC |\n",
    "|-------|-------------------|----------------------------|------------------------------|------|------|------|\n",
    "| 1.    | Gensim Sent2Vec   | 1414.88                    | 0.52/0.52/0.73               | 0.61 | 0.79 | 0.57 |\n",
    "| 2.    | Original Sent2Vec | 32.5                       | 0.41/0.42/0.83               | 0.59 | 0.78 | 0.59 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
