{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCore Concepts\n=============\n\nThis tutorial introduces Documents, Corpora, Vectors and Models: the basic concepts and terms needed to understand and use gensim.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The core concepts of ``gensim`` are:\n\n1. `core_concepts_document`: some text.\n2. `core_concepts_corpus`: a collection of documents.\n3. `core_concepts_vector`: a mathematically convenient representation of a document.\n4. `core_concepts_model`: an algorithm for transforming vectors from one representation to another.\n\nLet's examine each of these in slightly more detail.\n\n\nDocument\n--------\n\nIn Gensim, a *document* is an object of the `text sequence type <https://docs.python.org/3.7/library/stdtypes.html#text-sequence-type-str>`_ (commonly known as ``str`` in Python 3).\nA document could be anything from a short 140 character tweet, a single\nparagraph (i.e., journal article abstract), a news article, or a book.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "document = \"Human machine interface for lab abc computer applications\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCorpus\n------\n\nA *corpus* is a collection of `core_concepts_document` objects.\nCorpora serve two roles in Gensim:\n\n1. Input for training a `core_concepts_model`.\n   During training, the models use this *training corpus* to look for common\n   themes and topics, initializing their internal model parameters.\n\n   Gensim focuses on *unsupervised* models so that no human intervention,\n   such as costly annotations or tagging documents by hand, is required.\n\n2. Documents to organize.\n   After training, a topic model can be used to extract topics from new\n   documents (documents not seen in the training corpus).\n\n   Such corpora can be indexed for\n   `sphx_glr_auto_examples_core_run_similarity_queries.py`,\n   queried by semantic similarity, clustered etc.\n\nHere is an example corpus.\nIt consists of 9 documents, where each document is a string consisting of a single sentence.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text_corpus = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Important::\n  The above example loads the entire corpus into memory.\n  In practice, corpora may be very large, so loading them into memory may be impossible.\n  Gensim intelligently handles such corpora by *streaming* them one document at a time.\n  See `corpus_streaming_tutorial` for details.\n\nThis is a particularly small example of a corpus for illustration purposes.\nAnother example could be a list of all the plays written by Shakespeare, list\nof all wikipedia articles, or all tweets by a particular person of interest.\n\nAfter collecting our corpus, there are typically a number of preprocessing\nsteps we want to undertake. We'll keep it simple and just remove some\ncommonly used English words (such as 'the') and words that occur only once in\nthe corpus. In the process of doing so, we'll tokenize our data.\nTokenization breaks up the documents into words (in this case using space as\na delimiter).\n\n.. Important::\n  There are better ways to perform preprocessing than just lower-casing and\n  splitting by space.  Effective preprocessing is beyond the scope of this\n  tutorial: if you're interested, check out the\n  :py:func:`gensim.utils.simple_preprocess` function.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a set of frequent words\nstoplist = set('for a of the and to in'.split(' '))\n# Lowercase each document, split it by white space and filter out stopwords\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in text_corpus]\n\n# Count word frequencies\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\n# Only keep words that appear more than once\nprocessed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\npprint.pprint(processed_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before proceeding, we want to associate each word in the corpus with a unique\ninteger ID. We can do this using the :py:class:`gensim.corpora.Dictionary`\nclass.  This dictionary defines the vocabulary of all words that our\nprocessing knows about.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n\ndictionary = corpora.Dictionary(processed_corpus)\nprint(dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because our corpus is small, there are only 12 different tokens in this\n:py:class:`gensim.corpora.Dictionary`. For larger corpuses, dictionaries that\ncontains hundreds of thousands of tokens are quite common.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nVector\n------\n\nTo infer the latent structure in our corpus we need a way to represent\ndocuments that we can manipulate mathematically. One approach is to represent\neach document as a vector of *features*.\nFor example, a single feature may be thought of as a question-answer pair:\n\n1. How many times does the word *splonge* appear in the document? Zero.\n2. How many paragraphs does the document consist of? Two.\n3. How many fonts does the document use? Five.\n\nThe question is usually represented only by its integer id (such as `1`, `2` and `3`).\nThe representation of this document then becomes a series of pairs like ``(1, 0.0), (2, 2.0), (3, 5.0)``.\nThis is known as a *dense vector*, because it contains an explicit answer to each of the above questions.\n\nIf we know all the questions in advance, we may leave them implicit\nand simply represent the document as ``(0, 2, 5)``.\nThis sequence of answers is the **vector** for our document (in this case a 3-dimensional dense vector).\nFor practical purposes, only questions to which the answer is (or\ncan be converted to) a *single floating point number* are allowed in Gensim.\n\nIn practice, vectors often consist of many zero values.\nTo save memory, Gensim omits all vector elements with value 0.0.\nThe above example thus becomes ``(2, 2.0), (3, 5.0)``.\nThis is known as a *sparse vector* or *bag-of-words vector*.\nThe values of all missing features in this sparse representation can be unambiguously resolved to zero, ``0.0``.\n\nAssuming the questions are the same, we can compare the vectors of two different documents to each other.\nFor example, assume we are given two vectors ``(0.0, 2.0, 5.0)`` and ``(0.1, 1.9, 4.9)``.\nBecause the vectors are very similar to each other, we can conclude that the documents corresponding to those vectors are similar, too.\nOf course, the correctness of that conclusion depends on how well we picked the questions in the first place.\n\nAnother approach to represent a document as a vector is the *bag-of-words\nmodel*.\nUnder the bag-of-words model each document is represented by a vector\ncontaining the frequency counts of each word in the dictionary.\nFor example, assume we have a dictionary containing the words\n``['coffee', 'milk', 'sugar', 'spoon']``.\nA document consisting of the string ``\"coffee milk coffee\"`` would then\nbe represented by the vector ``[2, 1, 0, 0]`` where the entries of the vector\nare (in order) the occurrences of \"coffee\", \"milk\", \"sugar\" and \"spoon\" in\nthe document. The length of the vector is the number of entries in the\ndictionary. One of the main properties of the bag-of-words model is that it\ncompletely ignores the order of the tokens in the document that is encoded,\nwhich is where the name bag-of-words comes from.\n\nOur processed corpus has 12 unique words in it, which means that each\ndocument will be represented by a 12-dimensional vector under the\nbag-of-words model. We can use the dictionary to turn tokenized documents\ninto these 12-dimensional vectors. We can see what these IDs correspond to:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint(dictionary.token2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, suppose we wanted to vectorize the phrase \"Human computer\ninteraction\" (note that this phrase was not in our original corpus). We can\ncreate the bag-of-word representation for a document using the ``doc2bow``\nmethod of the dictionary, which returns a sparse representation of the word\ncounts:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_doc = \"Human computer interaction\"\nnew_vec = dictionary.doc2bow(new_doc.lower().split())\nprint(new_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first entry in each tuple corresponds to the ID of the token in the\ndictionary, the second corresponds to the count of this token.\n\nNote that \"interaction\" did not occur in the original corpus and so it was\nnot included in the vectorization. Also note that this vector only contains\nentries for words that actually appeared in the document. Because any given\ndocument will only contain a few words out of the many words in the\ndictionary, words that do not appear in the vectorization are represented as\nimplicitly zero as a space saving measure.\n\nWe can convert our entire original corpus to a list of vectors:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\npprint.pprint(bow_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that while this list lives entirely in memory, in most applications you\nwill want a more scalable solution. Luckily, ``gensim`` allows you to use any\niterator that returns a single document vector at a time. See the\ndocumentation for more details.\n\n.. Important::\n  The distinction between a document and a vector is that the former is text,\n  and the latter is a mathematically convenient representation of the text.\n  Sometimes, people will use the terms interchangeably: for example, given\n  some arbitrary document ``D``, instead of saying \"the vector that\n  corresponds to document ``D``\", they will just say \"the vector ``D``\" or\n  the \"document ``D``\".  This achieves brevity at the cost of ambiguity.\n\n  As long as you remember that documents exist in document space, and that\n  vectors exist in vector space, the above ambiguity is acceptable.\n\n.. Important::\n  Depending on how the representation was obtained, two different documents\n  may have the same vector representations.\n\n\nModel\n-----\n\nNow that we have vectorized our corpus we can begin to transform it using\n*models*. We use model as an abstract term referring to a *transformation* from\none document representation to another. In ``gensim`` documents are\nrepresented as vectors so a model can be thought of as a transformation\nbetween two vector spaces. The model learns the details of this\ntransformation during training, when it reads the training\n`core_concepts_corpus`.\n\nOne simple example of a model is `tf-idf\n<https://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_.  The tf-idf model\ntransforms vectors from the bag-of-words representation to a vector space\nwhere the frequency counts are weighted according to the relative rarity of\neach word in the corpus.\n\nHere's a simple example. Let's initialize the tf-idf model, training it on\nour corpus and transforming the string \"system minors\":\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim import models\n\n# train the model\ntfidf = models.TfidfModel(bow_corpus)\n\n# transform the \"system minors\" string\nwords = \"system minors\".lower().split()\nprint(tfidf[dictionary.doc2bow(words)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``tfidf`` model again returns a list of tuples, where the first entry is\nthe token ID and the second entry is the tf-idf weighting. Note that the ID\ncorresponding to \"system\" (which occurred 4 times in the original corpus) has\nbeen weighted lower than the ID corresponding to \"minors\" (which only\noccurred twice).\n\nYou can save trained models to disk and later load them back, either to\ncontinue training on new training documents or to transform new documents.\n\n``gensim`` offers a number of different models/transformations.\nFor more, see `sphx_glr_auto_examples_core_run_topics_and_transformations.py`.\n\nOnce you've created the model, you can do all sorts of cool stuff with it.\nFor example, to transform the whole corpus via TfIdf and index it, in\npreparation for similarity queries:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim import similarities\n\nindex = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and to query the similarity of our query document ``query_document`` against every document in the corpus:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "query_document = 'system engineering'.split()\nquery_bow = dictionary.doc2bow(query_document)\nsims = index[tfidf[query_bow]]\nprint(list(enumerate(sims)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to read this output?\nDocument 3 has a similarity score of 0.718=72%, document 2 has a similarity score of 42% etc.\nWe can make this slightly more readable by sorting:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n    print(document_number, score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n-------\n\nThe core concepts of ``gensim`` are:\n\n1. `core_concepts_document`: some text.\n2. `core_concepts_corpus`: a collection of documents.\n3. `core_concepts_vector`: a mathematically convenient representation of a document.\n4. `core_concepts_model`: an algorithm for transforming vectors from one representation to another.\n\nWe saw these concepts in action.\nFirst, we started with a corpus of documents.\nNext, we transformed these documents to a vector space representation.\nAfter that, we created a model that transformed our original vector representation to TfIdf.\nFinally, we used our model to calculate the similarity between some query document and all documents in the corpus.\n\nWhat Next?\n----------\n\nThere's still much more to learn about `sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('run_core_concepts.png')\nimgplot = plt.imshow(img)\nplt.axis('off')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}