
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tutorials/run_fasttext.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_fasttext.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_fasttext.py:


FastText Model
==============

Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.

.. GENERATED FROM PYTHON SOURCE LINES 7-11

.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








.. GENERATED FROM PYTHON SOURCE LINES 12-15

Here, we'll learn to work with fastText library for training word-embedding
models, saving & loading them and performing similarity operations & vector
lookups analogous to Word2Vec.

.. GENERATED FROM PYTHON SOURCE LINES 18-45

When to use fastText?
---------------------

The main principle behind `fastText <https://github.com/facebookresearch/fastText>`_ is that the
morphological structure of a word carries important information about the meaning of the word.
Such structure is not taken into account by traditional word embeddings like Word2Vec, which
train a unique word embedding for every individual word.
This is especially significant for morphologically rich languages (German, Turkish) in which a
single word can have a large number of morphological forms, each of which might occur rarely,
thus making it hard to train good word embeddings.


fastText attempts to solve this by treating each word as the aggregation of its subwords.
For the sake of simplicity and language-independence, subwords are taken to be the character ngrams
of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.


According to a detailed comparison of Word2Vec and fastText in
`this notebook <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>`__,
fastText does significantly better on syntactic tasks as compared to the original Word2Vec,
especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText
on semantic tasks though. The differences grow smaller as the size of the training corpus increases.


fastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its
component char-ngrams, provided at least one of the char-ngrams was present in the training data.


.. GENERATED FROM PYTHON SOURCE LINES 49-52

Training models
---------------


.. GENERATED FROM PYTHON SOURCE LINES 56-60

For the following examples, we'll use the Lee Corpus (which you already have if you've installed Gensim) for training our model.




.. GENERATED FROM PYTHON SOURCE LINES 61-82

.. code-block:: default

    from pprint import pprint as print
    from gensim.models.fasttext import FastText
    from gensim.test.utils import datapath

    # Set file names for train and test data
    corpus_file = datapath('lee_background.cor')

    model = FastText(vector_size=100)

    # build the vocabulary
    model.build_vocab(corpus_file=corpus_file)

    # train the model
    model.train(
        corpus_file=corpus_file, epochs=model.epochs,
        total_examples=model.corpus_count, total_words=model.corpus_total_words,
    )

    print(model)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2022-04-15 18:28:44,999 : INFO : adding document #0 to Dictionary<0 unique tokens: []>
    2022-04-15 18:28:45,000 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)
    2022-04-15 18:28:45,011 : INFO : Dictionary lifecycle event {'msg': "built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)", 'datetime': '2022-04-15T18:28:45.000439', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'created'}
    2022-04-15 18:28:45,120 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-04-15T18:28:45.120058', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'created'}
    2022-04-15 18:28:45,120 : INFO : collecting all words and their counts
    2022-04-15 18:28:45,120 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2022-04-15 18:28:45,141 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences
    2022-04-15 18:28:45,141 : INFO : Creating a fresh vocabulary
    2022-04-15 18:28:45,154 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.343567387069847%% of original 10781, drops 9019)', 'datetime': '2022-04-15T18:28:45.154889', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}
    2022-04-15 18:28:45,155 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.94773751878444%% of original 59890, drops 13806)', 'datetime': '2022-04-15T18:28:45.155136', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}
    2022-04-15 18:28:45,179 : INFO : deleting the raw counts dictionary of 10781 items
    2022-04-15 18:28:45,180 : INFO : sample=0.001 downsamples 45 most-common words
    2022-04-15 18:28:45,180 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2022-04-15T18:28:45.180609', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}
    2022-04-15 18:28:45,236 : INFO : estimated required memory for 1762 words, 2000000 buckets and 100 dimensions: 802597824 bytes
    2022-04-15 18:28:45,236 : INFO : resetting layer weights
    2022-04-15 18:28:47,661 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-04-15T18:28:47.661728', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'build_vocab'}
    2022-04-15 18:28:47,662 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-04-15T18:28:47.662150', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'train'}
    2022-04-15 18:28:47,892 : INFO : worker thread finished; awaiting finish of 2 more threads
    2022-04-15 18:28:47,910 : INFO : worker thread finished; awaiting finish of 1 more threads
    2022-04-15 18:28:47,916 : INFO : worker thread finished; awaiting finish of 0 more threads
    2022-04-15 18:28:47,916 : INFO : EPOCH - 1 : training on 60387 raw words (32958 effective words) took 0.2s, 133954 effective words/s
    2022-04-15 18:28:48,125 : INFO : worker thread finished; awaiting finish of 2 more threads
    2022-04-15 18:28:48,153 : INFO : worker thread finished; awaiting finish of 1 more threads
    2022-04-15 18:28:48,157 : INFO : worker thread finished; awaiting finish of 0 more threads
    2022-04-15 18:28:48,157 : INFO : EPOCH - 2 : training on 60387 raw words (32906 effective words) took 0.2s, 140644 effective words/s
    2022-04-15 18:28:48,306 : INFO : worker thread finished; awaiting finish of 2 more threads
    2022-04-15 18:28:48,387 : INFO : worker thread finished; awaiting finish of 1 more threads
    2022-04-15 18:28:48,392 : INFO : worker thread finished; awaiting finish of 0 more threads
    2022-04-15 18:28:48,392 : INFO : EPOCH - 3 : training on 60387 raw words (32863 effective words) took 0.2s, 145877 effective words/s
    2022-04-15 18:28:48,538 : INFO : worker thread finished; awaiting finish of 2 more threads
    2022-04-15 18:28:48,601 : INFO : worker thread finished; awaiting finish of 1 more threads
    2022-04-15 18:28:48,616 : INFO : worker thread finished; awaiting finish of 0 more threads
    2022-04-15 18:28:48,616 : INFO : EPOCH - 4 : training on 60387 raw words (32832 effective words) took 0.2s, 153286 effective words/s
    2022-04-15 18:28:48,761 : INFO : worker thread finished; awaiting finish of 2 more threads
    2022-04-15 18:28:48,818 : INFO : worker thread finished; awaiting finish of 1 more threads
    2022-04-15 18:28:48,823 : INFO : worker thread finished; awaiting finish of 0 more threads
    2022-04-15 18:28:48,824 : INFO : EPOCH - 5 : training on 60387 raw words (32827 effective words) took 0.2s, 165849 effective words/s
    2022-04-15 18:28:48,824 : INFO : FastText lifecycle event {'msg': 'training on 301935 raw words (164386 effective words) took 1.2s, 141437 effective words/s', 'datetime': '2022-04-15T18:28:48.824584', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'train'}
    <gensim.models.fasttext.FastText object at 0x7f16ec7b59a0>




.. GENERATED FROM PYTHON SOURCE LINES 83-86

Training hyperparameters
^^^^^^^^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 90-118

Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:

- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)
- vector_size: Dimensionality of vector embeddings to be learnt (Default 100)
- alpha: Initial learning rate (Default 0.025)
- window: Context window size (Default 5)
- min_count: Ignore words with number of occurrences below this (Default 5)
- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)
- sample: Threshold for downsampling higher-frequency words (Default 0.001)
- negative: Number of negative words to sample, for `ns` (Default 5)
- epochs: Number of epochs (Default 5)
- sorted_vocab: Sort vocab by descending frequency (Default 1)
- threads: Number of threads to use (Default 12)


In addition, fastText has three additional parameters:

- min_n: min length of char ngrams (Default 3)
- max_n: max length of char ngrams (Default 6)
- bucket: number of buckets used for hashing ngrams (Default 2000000)


Parameters ``min_n`` and ``max_n`` control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0, or to be lesser than ``min_n``\ , no character ngrams are used, and the model effectively reduces to Word2Vec.



To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the `Fowler-Noll-Vo hashing function <http://www.isthe.com/chongo/tech/comp/fnv>`_ (FNV-1a variant) is employed.


.. GENERATED FROM PYTHON SOURCE LINES 122-124

**Note:** You can continue to train your model while using Gensim's native implementation of fastText.


.. GENERATED FROM PYTHON SOURCE LINES 128-131

Saving/loading models
---------------------


.. GENERATED FROM PYTHON SOURCE LINES 135-138

Models can be saved and loaded via the ``load`` and ``save`` methods, just like
any other model in Gensim.


.. GENERATED FROM PYTHON SOURCE LINES 139-153

.. code-block:: default



    # Save a model trained via Gensim's fastText implementation to temp.
    import tempfile
    import os
    with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:
        model.save(tmp.name, separately=[])

    # Load back the same model.
    loaded_model = FastText.load(tmp.name)
    print(loaded_model)

    os.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2022-04-15 18:28:49,194 : INFO : FastText lifecycle event {'fname_or_handle': '/tmp/saved_model_gensim-ujg3006p', 'separately': '[]', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-15T18:28:49.193949', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'saving'}
    2022-04-15 18:28:49,195 : INFO : storing np array 'vectors_ngrams' to /tmp/saved_model_gensim-ujg3006p.wv.vectors_ngrams.npy
    2022-04-15 18:28:50,644 : INFO : not storing attribute vectors
    2022-04-15 18:28:50,645 : INFO : not storing attribute buckets_word
    2022-04-15 18:28:50,645 : INFO : not storing attribute cum_table
    2022-04-15 18:28:50,678 : INFO : saved /tmp/saved_model_gensim-ujg3006p
    2022-04-15 18:28:50,679 : INFO : loading FastText object from /tmp/saved_model_gensim-ujg3006p
    2022-04-15 18:28:50,684 : INFO : loading wv recursively from /tmp/saved_model_gensim-ujg3006p.wv.* with mmap=None
    2022-04-15 18:28:50,684 : INFO : loading vectors_ngrams from /tmp/saved_model_gensim-ujg3006p.wv.vectors_ngrams.npy with mmap=None
    2022-04-15 18:28:51,772 : INFO : setting ignored attribute vectors to None
    2022-04-15 18:28:51,772 : INFO : setting ignored attribute buckets_word to None
    2022-04-15 18:28:51,919 : INFO : setting ignored attribute cum_table to None
    2022-04-15 18:28:51,956 : INFO : FastText lifecycle event {'fname': '/tmp/saved_model_gensim-ujg3006p', 'datetime': '2022-04-15T18:28:51.956167', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'loaded'}
    <gensim.models.fasttext.FastText object at 0x7f16ddb4e940>




.. GENERATED FROM PYTHON SOURCE LINES 154-158

The ``save_word2vec_format`` is also available for fastText models, but will
cause all vectors for ngrams to be lost.
As a result, a model loaded in this way will behave as a regular word2vec model.


.. GENERATED FROM PYTHON SOURCE LINES 162-172

Word vector lookup
------------------


All information necessary for looking up fastText words (incl. OOV words) is
contained in its ``model.wv`` attribute.

If you don't need to continue training your model, you can export & save this `.wv`
attribute and discard `model`, to save space and RAM.


.. GENERATED FROM PYTHON SOURCE LINES 173-181

.. code-block:: default

    wv = model.wv
    print(wv)

    #
    # FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.
    #
    print('night' in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <gensim.models.fasttext.FastTextKeyedVectors object at 0x7f16ec82b310>
    True




.. GENERATED FROM PYTHON SOURCE LINES 183-185

.. code-block:: default

    print('nights' in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    False




.. GENERATED FROM PYTHON SOURCE LINES 187-189

.. code-block:: default

    print(wv['night'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    array([-0.22207159,  0.22077355, -0.27423275, -0.12641521,  0.07287006,
            0.4083373 ,  0.21238557,  0.46251252,  0.25193176, -0.20717509,
            0.01104122, -0.16850752, -0.1976137 ,  0.47486067, -0.39905465,
           -0.54326177,  0.20709044, -0.2399467 , -0.43859157, -0.52974105,
           -0.47117457, -0.07892875, -0.39079466, -0.13349132, -0.21139224,
           -0.35893348, -0.73871654, -0.12033081, -0.36359233,  0.2941681 ,
           -0.3433954 ,  0.32726443,  0.82508403, -0.26969045,  0.18187422,
            0.41465673,  0.3357784 , -0.1352851 , -0.4158488 , -0.34467757,
            0.43315125, -0.41209778,  0.02017804, -0.44057542, -0.49704787,
           -0.26325086, -0.13446096,  0.11956058,  0.41598788, -0.00717945,
            0.35173428, -0.41351745,  0.26581395, -0.3975636 , -0.15131967,
           -0.17296672, -0.13397802, -0.15426044,  0.03024485, -0.3538838 ,
           -0.3144943 , -0.44602963, -0.11996318,  0.359241  , -0.15560251,
            0.6724791 ,  0.06538071,  0.05428659,  0.42706105,  0.18553604,
           -0.24318866,  0.35659605,  0.47357708, -0.66754127,  0.35161147,
           -0.14617886,  0.27490965, -0.01956096,  0.0580783 ,  0.41598275,
            0.17644556, -0.5228533 , -0.82049775, -0.14422251, -0.11561928,
           -0.8318165 ,  0.49774447,  0.17220779, -0.0035387 , -0.2084888 ,
            0.01826598,  0.38385594, -0.08003889,  0.06094757, -0.15682602,
            0.607231  , -0.2315485 , -0.34553114, -0.04828577, -0.2248888 ],
          dtype=float32)




.. GENERATED FROM PYTHON SOURCE LINES 191-194

.. code-block:: default

    print(wv['nights'])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    array([-0.19359249,  0.19266072, -0.23831715, -0.10963183,  0.06205885,
            0.35349017,  0.18585438,  0.40367636,  0.21932858, -0.18163101,
            0.01131329, -0.14488953, -0.17280772,  0.4104396 , -0.34801972,
           -0.47257116,  0.17917019, -0.20796512, -0.37961343, -0.4608993 ,
           -0.40641037, -0.06964058, -0.33943632, -0.11738791, -0.18228863,
           -0.31037334, -0.6403033 , -0.10227475, -0.3155495 ,  0.25704917,
           -0.29632765,  0.28378326,  0.71502787, -0.23392071,  0.15810877,
            0.359666  ,  0.29306924, -0.11749208, -0.36147878, -0.29998863,
            0.3754248 , -0.35710186,  0.01701352, -0.3821959 , -0.4325981 ,
           -0.22746918, -0.11383997,  0.10449301,  0.3627854 , -0.00514597,
            0.3071107 , -0.35944855,  0.23158357, -0.34526545, -0.13099143,
           -0.14903384, -0.11835003, -0.13222367,  0.02756706, -0.30474514,
           -0.27212575, -0.38782486, -0.10387871,  0.31152904, -0.13458222,
            0.58513045,  0.05689588,  0.04451419,  0.37070122,  0.16232586,
           -0.2117109 ,  0.30781513,  0.4124873 , -0.5797168 ,  0.3068925 ,
           -0.12592888,  0.23819536, -0.01782229,  0.05022268,  0.36139277,
            0.15376486, -0.45452076, -0.7120637 , -0.12636082, -0.09942135,
           -0.72365934,  0.4322844 ,  0.1497761 , -0.00104655, -0.18171164,
            0.01591134,  0.33218506, -0.07001692,  0.05357721, -0.13666429,
            0.5276215 , -0.20245516, -0.2964478 , -0.04159901, -0.1961598 ],
          dtype=float32)




.. GENERATED FROM PYTHON SOURCE LINES 195-198

Similarity operations
---------------------


.. GENERATED FROM PYTHON SOURCE LINES 202-204

Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**


.. GENERATED FROM PYTHON SOURCE LINES 205-209

.. code-block:: default



    print("nights" in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    False




.. GENERATED FROM PYTHON SOURCE LINES 211-213

.. code-block:: default

    print("night" in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    True




.. GENERATED FROM PYTHON SOURCE LINES 215-217

.. code-block:: default

    print(wv.similarity("night", "nights"))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.9999919




.. GENERATED FROM PYTHON SOURCE LINES 218-220

Syntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. A detailed comparison is provided `here <Word2Vec_FastText_Comparison.ipynb>`_.


.. GENERATED FROM PYTHON SOURCE LINES 224-228

Other similarity operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only

.. GENERATED FROM PYTHON SOURCE LINES 229-231

.. code-block:: default

    print(wv.most_similar("nights"))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('night', 0.9999918937683105),
     ('rights', 0.9999880194664001),
     ('flights', 0.9999876022338867),
     ('overnight', 0.9999872446060181),
     ('fighting', 0.9999854564666748),
     ('fighters', 0.99998539686203),
     ('entered', 0.9999850988388062),
     ('fight', 0.9999850988388062),
     ('fighter', 0.9999849796295166),
     ('eight', 0.9999846816062927)]




.. GENERATED FROM PYTHON SOURCE LINES 233-235

.. code-block:: default

    print(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.9999401




.. GENERATED FROM PYTHON SOURCE LINES 237-239

.. code-block:: default

    print(wv.doesnt_match("breakfast cereal dinner lunch".split()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    'lunch'




.. GENERATED FROM PYTHON SOURCE LINES 241-243

.. code-block:: default

    print(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('capital,', 0.9996517896652222),
     ('find', 0.9996505975723267),
     ('findings', 0.9996435046195984),
     ('field', 0.9996432662010193),
     ('seekers.', 0.999640941619873),
     ('finding', 0.9996404051780701),
     ('abuse', 0.999639093875885),
     ('storm', 0.9996387362480164),
     ('had', 0.9996375441551208),
     ('26-year-old', 0.9996355772018433)]




.. GENERATED FROM PYTHON SOURCE LINES 245-247

.. code-block:: default

    print(wv.evaluate_word_analogies(datapath('questions-words.txt')))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2022-04-15 18:29:00,063 : INFO : Evaluating word analogies for top 300000 words in the model on /home/thomas/Documents/FOSS/gensim-tlouf/gensim/test/test_data/questions-words.txt
    2022-04-15 18:29:00,107 : INFO : family: 0.0% (0/2)
    2022-04-15 18:29:00,141 : INFO : gram3-comparative: 8.3% (1/12)
    2022-04-15 18:29:00,157 : INFO : gram4-superlative: 33.3% (4/12)
    2022-04-15 18:29:00,176 : INFO : gram5-present-participle: 45.0% (9/20)
    2022-04-15 18:29:00,201 : INFO : gram6-nationality-adjective: 30.0% (6/20)
    2022-04-15 18:29:00,225 : INFO : gram7-past-tense: 5.0% (1/20)
    2022-04-15 18:29:00,243 : INFO : gram8-plural: 33.3% (4/12)
    2022-04-15 18:29:00,251 : INFO : Quadruplets with out-of-vocabulary words: 99.5%
    2022-04-15 18:29:00,253 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use "dummy4unknown=True"
    2022-04-15 18:29:00,253 : INFO : Total accuracy: 25.5% (25/98)
    (0.25510204081632654,
     [{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},
      {'correct': [], 'incorrect': [], 'section': 'capital-world'},
      {'correct': [], 'incorrect': [], 'section': 'currency'},
      {'correct': [], 'incorrect': [], 'section': 'city-in-state'},
      {'correct': [],
       'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],
       'section': 'family'},
      {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},
      {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},
      {'correct': [('LONG', 'LONGER', 'GREAT', 'GREATER')],
       'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                     ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                     ('GOOD', 'BETTER', 'LOW', 'LOWER'),
                     ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'LOW', 'LOWER'),
                     ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'LOW', 'LOWER'),
                     ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'LONG', 'LONGER')],
       'section': 'gram3-comparative'},
      {'correct': [('GOOD', 'BEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST')],
       'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                     ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                     ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
                     ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],
       'section': 'gram4-superlative'},
      {'correct': [('GO', 'GOING', 'PLAY', 'PLAYING'),
                   ('GO', 'GOING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
                   ('PLAY', 'PLAYING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                   ('SAY', 'SAYING', 'GO', 'GOING'),
                   ('SAY', 'SAYING', 'PLAY', 'PLAYING')],
       'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),
                     ('GO', 'GOING', 'RUN', 'RUNNING'),
                     ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
                     ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                     ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                     ('RUN', 'RUNNING', 'GO', 'GOING'),
                     ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                     ('SAY', 'SAYING', 'RUN', 'RUNNING')],
       'section': 'gram5-present-participle'},
      {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
                   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
                   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
                   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN')],
       'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                     ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                     ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                     ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                     ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),
                     ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],
       'section': 'gram6-nationality-adjective'},
      {'correct': [('PAYING', 'PAID', 'SAYING', 'SAID')],
       'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),
                     ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                     ('GOING', 'WENT', 'SAYING', 'SAID'),
                     ('GOING', 'WENT', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                     ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                     ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                     ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                     ('SAYING', 'SAID', 'GOING', 'WENT'),
                     ('SAYING', 'SAID', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'GOING', 'WENT'),
                     ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                     ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'SAYING', 'SAID')],
       'section': 'gram7-past-tense'},
      {'correct': [('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
                   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
       'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
                     ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
                     ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                     ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
                     ('MAN', 'MEN', 'CAR', 'CARS')],
       'section': 'gram8-plural'},
      {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},
      {'correct': [('LONG', 'LONGER', 'GREAT', 'GREATER'),
                   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),
                   ('GO', 'GOING', 'PLAY', 'PLAYING'),
                   ('GO', 'GOING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
                   ('PLAY', 'PLAYING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                   ('SAY', 'SAYING', 'GO', 'GOING'),
                   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
                   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
                   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
                   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),
                   ('PAYING', 'PAID', 'SAYING', 'SAID'),
                   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
                   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
       'incorrect': [('HE', 'SHE', 'HIS', 'HER'),
                     ('HIS', 'HER', 'HE', 'SHE'),
                     ('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                     ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                     ('GOOD', 'BETTER', 'LOW', 'LOWER'),
                     ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'LOW', 'LOWER'),
                     ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'LOW', 'LOWER'),
                     ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'LONG', 'LONGER'),
                     ('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                     ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                     ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
                     ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),
                     ('GO', 'GOING', 'LOOK', 'LOOKING'),
                     ('GO', 'GOING', 'RUN', 'RUNNING'),
                     ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
                     ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                     ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                     ('RUN', 'RUNNING', 'GO', 'GOING'),
                     ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                     ('SAY', 'SAYING', 'RUN', 'RUNNING'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                     ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                     ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                     ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                     ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),
                     ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),
                     ('GOING', 'WENT', 'PAYING', 'PAID'),
                     ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                     ('GOING', 'WENT', 'SAYING', 'SAID'),
                     ('GOING', 'WENT', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                     ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                     ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                     ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                     ('SAYING', 'SAID', 'GOING', 'WENT'),
                     ('SAYING', 'SAID', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'GOING', 'WENT'),
                     ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                     ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'SAYING', 'SAID'),
                     ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
                     ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
                     ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                     ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
                     ('MAN', 'MEN', 'CAR', 'CARS')],
       'section': 'Total accuracy'}])




.. GENERATED FROM PYTHON SOURCE LINES 248-254

Word Movers distance
^^^^^^^^^^^^^^^^^^^^

You'll need the optional ``POT`` library for this section, ``pip install POT``.

Let's start with two sentences:

.. GENERATED FROM PYTHON SOURCE LINES 254-258

.. code-block:: default

    sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
    sentence_president = 'The president greets the press in Chicago'.lower().split()









.. GENERATED FROM PYTHON SOURCE LINES 259-261

Remove their stopwords.


.. GENERATED FROM PYTHON SOURCE LINES 261-265

.. code-block:: default

    from gensim.parsing.preprocessing import STOPWORDS
    sentence_obama = [w for w in sentence_obama if w not in STOPWORDS]
    sentence_president = [w for w in sentence_president if w not in STOPWORDS]








.. GENERATED FROM PYTHON SOURCE LINES 266-267

Compute the Word Movers Distance between the two sentences.

.. GENERATED FROM PYTHON SOURCE LINES 267-270

.. code-block:: default

    distance = wv.wmdistance(sentence_obama, sentence_president)
    print(f"Word Movers Distance is {distance} (lower means closer)")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2022-04-15 18:29:01,199 : INFO : adding document #0 to Dictionary<0 unique tokens: []>
    2022-04-15 18:29:01,200 : INFO : built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)
    2022-04-15 18:29:01,200 : INFO : Dictionary lifecycle event {'msg': "built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)", 'datetime': '2022-04-15T18:29:01.200324', 'gensim': '4.1.3.dev0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]', 'platform': 'Linux-5.13.0-39-generic-x86_64-with-glibc2.29', 'event': 'created'}
    'Word Movers Distance is 0.015939769681979948 (lower means closer)'




.. GENERATED FROM PYTHON SOURCE LINES 271-273

That's all! You've made it to the end of this tutorial.


.. GENERATED FROM PYTHON SOURCE LINES 273-278

.. code-block:: default

    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('fasttext-logo-color-web.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image-sg:: /auto_examples/tutorials/images/sphx_glr_run_fasttext_001.png
   :alt: run fasttext
   :srcset: /auto_examples/tutorials/images/sphx_glr_run_fasttext_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  17.984 seconds)

**Estimated memory usage:**  1616 MB


.. _sphx_glr_download_auto_examples_tutorials_run_fasttext.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_fasttext.py <run_fasttext.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_fasttext.ipynb <run_fasttext.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
