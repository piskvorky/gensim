
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tutorials/run_wmd.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_wmd.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_wmd.py:


Word Mover's Distance
=====================

Demonstrates using Gensim's implemenation of the WMD.

.. GENERATED FROM PYTHON SOURCE LINES 10-35

Word Mover's Distance (WMD) is a promising new tool in machine learning that
allows us to submit a query and return the most relevant documents. This
tutorial introduces WMD and shows how you can compute the WMD distance
between two documents using ``wmdistance``.

WMD Basics
----------

WMD enables us to assess the "distance" between two documents in a meaningful
way even when they have no words in common. It uses `word2vec
<http://rare-technologies.com/word2vec-tutorial/>`_ [4] vector embeddings of
words. It been shown to outperform many of the state-of-the-art methods in
k-nearest neighbors classification [3].

WMD is illustrated below for two very similar sentences (illustration taken
from `Vlad Niculae's blog
<http://vene.ro/blog/word-movers-distance-in-python.html>`_). The sentences
have no words in common, but by matching the relevant words, WMD is able to
accurately measure the (dis)similarity between the two sentences. The method
also uses the bag-of-words representation of the documents (simply put, the
word's frequencies in the documents), noted as $d$ in the figure below. The
intuition behind the method is that we find the minimum "traveling distance"
between documents, in other words the most efficient way to "move" the
distribution of document 1 to the distribution of document 2.


.. GENERATED FROM PYTHON SOURCE LINES 35-44

.. code-block:: default


    # Image from https://vene.ro/images/wmd-obama.png
    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('wmd-obama.png')
    imgplot = plt.imshow(img)
    plt.axis('off')
    plt.show()




.. image-sg:: /auto_examples/tutorials/images/sphx_glr_run_wmd_001.png
   :alt: run wmd
   :srcset: /auto_examples/tutorials/images/sphx_glr_run_wmd_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 45-68

This method was introduced in the article "From Word Embeddings To Document
Distances" by Matt Kusner et al. (\ `link to PDF
<http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf>`_\ ). It is inspired
by the "Earth Mover's Distance", and employs a solver of the "transportation
problem".

In this tutorial, we will learn how to use Gensim's WMD functionality, which
consists of the ``wmdistance`` method for distance computation, and the
``WmdSimilarity`` class for corpus based similarity queries.

.. Important::
   If you use Gensim's WMD functionality, please consider citing [1] and [2].

Computing the Word Mover's Distance
-----------------------------------

To use WMD, you need some existing word embeddings.
You could train your own Word2Vec model, but that is beyond the scope of this tutorial
(check out :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py` if you're interested).
For this tutorial, we'll be using an existing Word2Vec model.

Let's take some sentences to compute the distance between.


.. GENERATED FROM PYTHON SOURCE LINES 68-76

.. code-block:: default


    # Initialize logging.
    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    sentence_obama = 'Obama speaks to the media in Illinois'
    sentence_president = 'The president greets the press in Chicago'








.. GENERATED FROM PYTHON SOURCE LINES 77-81

These sentences have very similar content, and as such the WMD should be low.
Before we compute the WMD, we want to remove stopwords ("the", "to", etc.),
as these do not contribute a lot to the information in the sentences.


.. GENERATED FROM PYTHON SOURCE LINES 81-94

.. code-block:: default


    # Import and download stopwords from NLTK.
    from nltk.corpus import stopwords
    from nltk import download
    download('stopwords')  # Download stopwords list.
    stop_words = stopwords.words('english')

    def preprocess(sentence):
        return [w for w in sentence.lower().split() if w not in stop_words]

    sentence_obama = preprocess(sentence_obama)
    sentence_president = preprocess(sentence_president)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [nltk_data] Downloading package stopwords to /home/thomas/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!




.. GENERATED FROM PYTHON SOURCE LINES 95-101

Now, as mentioned earlier, we will be using some downloaded pre-trained
embeddings. We load these into a Gensim Word2Vec model class.

.. Important::
  The embeddings we have chosen here require a lot of memory.


.. GENERATED FROM PYTHON SOURCE LINES 101-104

.. code-block:: default

    import gensim.downloader as api
    model = api.load('word2vec-google-news-300')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    2022-10-23 11:18:41,292 : INFO : loading projection weights from /home/thomas/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
    2022-10-23 11:19:12,793 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/thomas/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-10-23T11:19:12.755440', 'gensim': '4.2.1.dev0', 'python': '3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-76051900-generic-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}




.. GENERATED FROM PYTHON SOURCE LINES 105-107

So let's compute WMD using the ``wmdistance`` method.


.. GENERATED FROM PYTHON SOURCE LINES 107-110

.. code-block:: default

    distance = model.wmdistance(sentence_obama, sentence_president)
    print('distance = %.4f' % distance)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    2022-10-23 11:19:12,860 : INFO : adding document #0 to Dictionary<0 unique tokens: []>
    2022-10-23 11:19:12,861 : INFO : built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)
    2022-10-23 11:19:12,861 : INFO : Dictionary lifecycle event {'msg': "built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)", 'datetime': '2022-10-23T11:19:12.861331', 'gensim': '4.2.1.dev0', 'python': '3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-76051900-generic-x86_64-with-glibc2.35', 'event': 'created'}
    distance = 1.0175




.. GENERATED FROM PYTHON SOURCE LINES 111-113

Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.


.. GENERATED FROM PYTHON SOURCE LINES 113-117

.. code-block:: default

    sentence_orange = preprocess('Oranges are my favorite fruit')
    distance = model.wmdistance(sentence_obama, sentence_orange)
    print('distance = %.4f' % distance)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    2022-10-23 11:19:15,303 : INFO : adding document #0 to Dictionary<0 unique tokens: []>
    2022-10-23 11:19:15,304 : INFO : built Dictionary<7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...> from 2 documents (total 7 corpus positions)
    2022-10-23 11:19:15,304 : INFO : Dictionary lifecycle event {'msg': "built Dictionary<7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...> from 2 documents (total 7 corpus positions)", 'datetime': '2022-10-23T11:19:15.304338', 'gensim': '4.2.1.dev0', 'python': '3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-76051900-generic-x86_64-with-glibc2.35', 'event': 'created'}
    distance = 1.3664




.. GENERATED FROM PYTHON SOURCE LINES 118-125

References
----------

1. Rémi Flamary et al. *POT: Python Optimal Transport*, 2021.
2. Matt Kusner et al. *From Embeddings To Document Distances*, 2015.
3. Tomáš Mikolov et al. *Efficient Estimation of Word Representations in Vector Space*, 2013.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  36.418 seconds)

**Estimated memory usage:**  7551 MB


.. _sphx_glr_download_auto_examples_tutorials_run_wmd.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: run_wmd.py <run_wmd.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: run_wmd.ipynb <run_wmd.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
