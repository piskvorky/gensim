
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tutorials/run_ensemblelda.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_ensemblelda.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_ensemblelda.py:


Ensemble LDA
============

Introduces Gensim's EnsembleLda model

.. GENERATED FROM PYTHON SOURCE LINES 8-12

.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








.. GENERATED FROM PYTHON SOURCE LINES 13-18

This tutorial will explain how to use the EnsembleLDA model class.

EnsembleLda is a method of finding and generating stable topics from the results of multiple topic models,
it can be used to remove topics from your results that are noise and are not reproducible.


.. GENERATED FROM PYTHON SOURCE LINES 20-27

Corpus
------
We will use the gensim downloader api to get a small corpus for training our ensemble.

The preprocessing is similar to :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py`,
so it won't be explained again in detail.


.. GENERATED FROM PYTHON SOURCE LINES 27-39

.. code-block:: default


    import gensim.downloader as api
    from gensim.corpora import Dictionary
    from nltk.stem.wordnet import WordNetLemmatizer

    lemmatizer = WordNetLemmatizer()
    docs = api.load('text8')
    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]
    dictionary = Dictionary(docs)
    dictionary.filter_extremes(no_below=20, no_above=0.5)
    corpus = [dictionary.doc2bow(doc) for doc in docs]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-01-24 11:02:47,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2021-01-24 11:02:52,618 : INFO : built Dictionary(238542 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1701 documents (total 17005207 corpus positions)
    2021-01-24 11:02:52,798 : INFO : discarding 218466 tokens: [('a', 1701), ('ability', 934), ('able', 1202), ('about', 1687), ('above', 1327), ('abstention', 13), ('accepted', 945), ('according', 1468), ('account', 1113), ('act', 1312)]...
    2021-01-24 11:02:52,798 : INFO : keeping 20076 tokens which were in no less than 20 and no more than 850 (=50.0%) documents
    2021-01-24 11:02:52,873 : INFO : resulting dictionary: Dictionary(20076 unique tokens: ['abacus', 'abnormal', 'abolished', 'abolition', 'absence']...)




.. GENERATED FROM PYTHON SOURCE LINES 40-48

Training
--------

Training the ensemble works very similar to training a single model,

You can use any model that is based on LdaModel, such as LdaMulticore, to train the Ensemble.
In experiments, LdaMulticore showed better results.


.. GENERATED FROM PYTHON SOURCE LINES 48-52

.. code-block:: default


    from gensim.models import LdaModel
    topic_model_class = LdaModel








.. GENERATED FROM PYTHON SOURCE LINES 53-56

Any arbitrary number of models can be used, but it should be a multiple of your workers so that the
load can be distributed properly. In this example, 4 processes will train 8 models each.


.. GENERATED FROM PYTHON SOURCE LINES 56-60

.. code-block:: default


    ensemble_workers = 4
    num_models = 8








.. GENERATED FROM PYTHON SOURCE LINES 61-64

After training all the models, some distance computations are required which can take quite some
time as well. You can speed this up by using workers for that as well.


.. GENERATED FROM PYTHON SOURCE LINES 64-67

.. code-block:: default


    distance_workers = 4








.. GENERATED FROM PYTHON SOURCE LINES 68-70

All other parameters that are unknown to EnsembleLda are forwarded to each LDA Model, such as


.. GENERATED FROM PYTHON SOURCE LINES 70-73

.. code-block:: default

    num_topics = 20
    passes = 2








.. GENERATED FROM PYTHON SOURCE LINES 74-79

Now start the training

Since 20 topics were trained on each of the 8 models, we expect there to be 160 different topics.
The number of stable topics which are clustered from all those topics is smaller.


.. GENERATED FROM PYTHON SOURCE LINES 79-95

.. code-block:: default


    from gensim.models import EnsembleLda
    ensemble = EnsembleLda(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        passes=passes,
        num_models=num_models,
        topic_model_class=LdaModel,
        ensemble_workers=ensemble_workers,
        distance_workers=distance_workers
    )

    print(len(ensemble.ttda))
    print(len(ensemble.get_topics()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-01-24 11:02:56,264 : INFO : generating 8 topic models...
    2021-01-24 11:07:21,883 : INFO : generating a 160 x 160 asymmetric distance matrix...
    2021-01-24 11:07:22,745 : INFO : fitting the clustering model, using 4 for min_samples
    2021-01-24 11:07:22,773 : INFO : generating stable topics, using 3 for min_cores
    2021-01-24 11:07:22,773 : INFO : found 30 clusters
    2021-01-24 11:07:22,776 : INFO : found 2 stable topics
    2021-01-24 11:07:22,776 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-01-24 11:07:22,923 : INFO : using symmetric alpha at 0.5
    2021-01-24 11:07:22,923 : INFO : using symmetric eta at 0.5
    2021-01-24 11:07:22,925 : INFO : using serial LDA version on this node
    2021-01-24 11:07:22,927 : INFO : running online (multi-pass) LDA training, 2 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-01-24 11:07:22,927 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    160
    2




.. GENERATED FROM PYTHON SOURCE LINES 96-109

Tuning
------

Different from LdaModel, the number of resulting topics varies greatly depending on the clustering parameters.

You can provide those in the ``recluster()`` function or the ``EnsembleLda`` constructor.

Play around until you get as many topics as you desire, which however may reduce their quality.
If your ensemble doesn't have enough topics to begin with, you should make sure to make it large enough.

Having an epsilon that is smaller than the smallest distance doesn't make sense.
Make sure to chose one that is within the range of values in ``asymmetric_distance_matrix``.


.. GENERATED FROM PYTHON SOURCE LINES 109-119

.. code-block:: default


    import numpy as np
    shape = ensemble.asymmetric_distance_matrix.shape
    without_diagonal = ensemble.asymmetric_distance_matrix[~np.eye(shape[0],dtype=bool)].reshape(shape[0],-1)
    print(without_diagonal.min(), without_diagonal.mean(), without_diagonal.max())

    ensemble.recluster(eps=0.09, min_samples=2, min_cores=2)

    print(len(ensemble.get_topics()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.06984649987340175 0.12851575757379297 0.31934422319450084
    2021-01-24 11:07:23,142 : INFO : fitting the clustering model
    2021-01-24 11:07:23,154 : INFO : generating stable topics
    2021-01-24 11:07:23,154 : INFO : found 22 clusters
    2021-01-24 11:07:23,157 : INFO : found 2 stable topics
    2021-01-24 11:07:23,157 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-01-24 11:07:23,159 : INFO : using symmetric alpha at 0.5
    2021-01-24 11:07:23,159 : INFO : using symmetric eta at 0.5
    2021-01-24 11:07:23,161 : INFO : using serial LDA version on this node
    2021-01-24 11:07:23,163 : INFO : running online (multi-pass) LDA training, 2 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-01-24 11:07:23,163 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2




.. GENERATED FROM PYTHON SOURCE LINES 120-131

Increasing the Size
-------------------

If you have some models lying around that were trained on a corpus based on the same dictionary,
they are compatible and you can add them to the ensemble.

By setting num_models of the EnsembleLda constructor to 0 you can also create an ensemble that is
entirely made out of your existing topic models with the following method.

Afterwards the number and quality of stable topics might be different depending on your added topics and parameters.


.. GENERATED FROM PYTHON SOURCE LINES 131-156

.. code-block:: default


    from gensim.models import LdaMulticore

    model1 = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=9,
        passes=4,
    )

    model2 = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=11,
        passes=2,
    )

    # add_model supports various types of input, check out its docstring
    ensemble.add_model(model1)
    ensemble.add_model(model2)

    ensemble.recluster()

    print(len(ensemble.ttda))
    print(len(ensemble.get_topics()))




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-01-24 11:07:23,511 : INFO : using symmetric alpha at 0.1111111111111111
    2021-01-24 11:07:23,512 : INFO : using symmetric eta at 0.1111111111111111
    2021-01-24 11:07:23,515 : INFO : using serial LDA version on this node
    2021-01-24 11:07:23,526 : INFO : running online LDA training, 9 topics, 4 passes over the supplied corpus of 1701 documents, updating every 22000 documents, evaluating every ~1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-01-24 11:07:23,526 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-01-24 11:07:23,572 : INFO : training LDA model using 11 processes
    2021-01-24 11:07:23,721 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-01-24 11:07:28,538 : INFO : topic #1 (0.111): 0.001*"minister" + 0.001*"soviet" + 0.001*"election" + 0.001*"israel" + 0.001*"jewish" + 0.001*"jew" + 0.001*"chinese" + 0.001*"emperor" + 0.001*"actor" + 0.001*"km"
    2021-01-24 11:07:28,538 : INFO : topic #4 (0.111): 0.001*"economy" + 0.001*"female" + 0.001*"minister" + 0.001*"km" + 0.001*"election" + 0.001*"est" + 0.001*"jewish" + 0.001*"japanese" + 0.001*"animal" + 0.001*"band"
    2021-01-24 11:07:28,539 : INFO : topic #7 (0.111): 0.001*"election" + 0.001*"minister" + 0.001*"km" + 0.001*"cell" + 0.001*"software" + 0.001*"energy" + 0.001*"actor" + 0.001*"band" + 0.001*"bc" + 0.001*"japanese"
    2021-01-24 11:07:28,539 : INFO : topic #0 (0.111): 0.002*"soviet" + 0.001*"league" + 0.001*"km" + 0.001*"actor" + 0.001*"territory" + 0.001*"album" + 0.001*"emperor" + 0.001*"season" + 0.001*"jewish" + 0.001*"female"
    2021-01-24 11:07:28,539 : INFO : topic #6 (0.111): 0.001*"actor" + 0.001*"india" + 0.001*"software" + 0.001*"band" + 0.001*"bc" + 0.001*"novel" + 0.001*"km" + 0.001*"album" + 0.001*"emperor" + 0.001*"y"
    2021-01-24 11:07:28,539 : INFO : topic diff=1.009234, rho=1.000000
    2021-01-24 11:07:38,853 : INFO : -9.257 per-word bound, 612.0 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:07:38,853 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-01-24 11:07:43,986 : INFO : topic #5 (0.111): 0.001*"chinese" + 0.001*"y" + 0.001*"blue" + 0.001*"spanish" + 0.001*"user" + 0.001*"china" + 0.001*"actor" + 0.001*"canadian" + 0.001*"color" + 0.001*"emperor"
    2021-01-24 11:07:43,986 : INFO : topic #4 (0.111): 0.002*"est" + 0.002*"economy" + 0.002*"km" + 0.002*"election" + 0.001*"minister" + 0.001*"female" + 0.001*"male" + 0.001*"prime" + 0.001*"israel" + 0.001*"bank"
    2021-01-24 11:07:43,986 : INFO : topic #7 (0.111): 0.001*"cell" + 0.001*"energy" + 0.001*"color" + 0.001*"map" + 0.001*"horse" + 0.001*"ship" + 0.001*"bc" + 0.001*"election" + 0.001*"card" + 0.001*"specie"
    2021-01-24 11:07:43,987 : INFO : topic #1 (0.111): 0.001*"jewish" + 0.001*"jew" + 0.001*"israel" + 0.001*"aircraft" + 0.001*"minister" + 0.001*"comic" + 0.001*"soviet" + 0.001*"actor" + 0.001*"irish" + 0.001*"election"
    2021-01-24 11:07:43,987 : INFO : topic #6 (0.111): 0.001*"actor" + 0.001*"software" + 0.001*"india" + 0.001*"band" + 0.001*"bc" + 0.001*"novel" + 0.001*"y" + 0.001*"jewish" + 0.001*"album" + 0.001*"emperor"
    2021-01-24 11:07:43,987 : INFO : topic diff=0.143780, rho=0.592297
    2021-01-24 11:07:54,251 : INFO : -9.190 per-word bound, 583.9 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:07:54,252 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-01-24 11:07:58,980 : INFO : topic #6 (0.111): 0.002*"actor" + 0.001*"software" + 0.001*"india" + 0.001*"novel" + 0.001*"bc" + 0.001*"band" + 0.001*"hebrew" + 0.001*"jewish" + 0.001*"y" + 0.001*"japanese"
    2021-01-24 11:07:58,980 : INFO : topic #5 (0.111): 0.002*"y" + 0.001*"chinese" + 0.001*"blue" + 0.001*"user" + 0.001*"china" + 0.001*"z" + 0.001*"color" + 0.001*"server" + 0.001*"canadian" + 0.001*"spanish"
    2021-01-24 11:07:58,980 : INFO : topic #4 (0.111): 0.003*"est" + 0.003*"km" + 0.003*"economy" + 0.002*"election" + 0.002*"minister" + 0.002*"male" + 0.002*"territory" + 0.002*"israel" + 0.002*"female" + 0.002*"prime"
    2021-01-24 11:07:58,980 : INFO : topic #2 (0.111): 0.002*"cell" + 0.001*"import" + 0.001*"machine" + 0.001*"software" + 0.001*"user" + 0.001*"league" + 0.001*"memory" + 0.001*"bit" + 0.001*"apple" + 0.001*"aircraft"
    2021-01-24 11:07:58,981 : INFO : topic #8 (0.111): 0.002*"emperor" + 0.001*"chinese" + 0.001*"copyright" + 0.001*"minister" + 0.001*"mary" + 0.001*"software" + 0.001*"y" + 0.001*"animal" + 0.001*"actor" + 0.001*"japanese"
    2021-01-24 11:07:58,981 : INFO : topic diff=0.178240, rho=0.509614
    2021-01-24 11:08:09,249 : INFO : -9.125 per-word bound, 558.3 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:08:09,249 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-01-24 11:08:13,965 : INFO : topic #6 (0.111): 0.002*"actor" + 0.001*"hebrew" + 0.001*"novel" + 0.001*"jewish" + 0.001*"software" + 0.001*"bc" + 0.001*"singer" + 0.001*"india" + 0.001*"italian" + 0.001*"la"
    2021-01-24 11:08:13,965 : INFO : topic #7 (0.111): 0.002*"energy" + 0.002*"cell" + 0.001*"horse" + 0.001*"atom" + 0.001*"color" + 0.001*"specie" + 0.001*"chemical" + 0.001*"acid" + 0.001*"animal" + 0.001*"map"
    2021-01-24 11:08:13,965 : INFO : topic #8 (0.111): 0.003*"emperor" + 0.002*"chinese" + 0.002*"copyright" + 0.001*"mary" + 0.001*"software" + 0.001*"japanese" + 0.001*"minister" + 0.001*"y" + 0.001*"bc" + 0.001*"animal"
    2021-01-24 11:08:13,965 : INFO : topic #4 (0.111): 0.003*"est" + 0.003*"km" + 0.003*"election" + 0.003*"economy" + 0.003*"minister" + 0.002*"territory" + 0.002*"male" + 0.002*"israel" + 0.002*"prime" + 0.002*"female"
    2021-01-24 11:08:13,966 : INFO : topic #3 (0.111): 0.003*"album" + 0.003*"band" + 0.001*"bass" + 0.001*"alexander" + 0.001*"love" + 0.001*"ball" + 0.001*"fan" + 0.001*"philosophy" + 0.001*"artist" + 0.001*"me"
    2021-01-24 11:08:13,966 : INFO : topic diff=0.153747, rho=0.454053
    2021-01-24 11:08:24,231 : INFO : -9.087 per-word bound, 543.7 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:08:24,310 : INFO : using symmetric alpha at 0.09090909090909091
    2021-01-24 11:08:24,310 : INFO : using symmetric eta at 0.09090909090909091
    2021-01-24 11:08:24,312 : INFO : using serial LDA version on this node
    2021-01-24 11:08:24,325 : INFO : running online (multi-pass) LDA training, 11 topics, 2 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-01-24 11:08:24,325 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-01-24 11:08:36,120 : INFO : -10.499 per-word bound, 1447.4 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:08:36,120 : INFO : PROGRESS: pass 0, at document #1701/1701
    2021-01-24 11:08:40,292 : INFO : topic #7 (0.091): 0.001*"band" + 0.001*"minister" + 0.001*"album" + 0.001*"km" + 0.001*"cell" + 0.001*"female" + 0.001*"lake" + 0.001*"election" + 0.001*"africa" + 0.001*"chinese"
    2021-01-24 11:08:40,292 : INFO : topic #5 (0.091): 0.001*"election" + 0.001*"japanese" + 0.001*"cell" + 0.001*"energy" + 0.001*"km" + 0.001*"economy" + 0.001*"actor" + 0.001*"ship" + 0.001*"county" + 0.001*"emperor"
    2021-01-24 11:08:40,293 : INFO : topic #6 (0.091): 0.001*"jewish" + 0.001*"actor" + 0.001*"energy" + 0.001*"cell" + 0.001*"la" + 0.001*"y" + 0.001*"russian" + 0.001*"china" + 0.001*"league" + 0.001*"soviet"
    2021-01-24 11:08:40,293 : INFO : topic #4 (0.091): 0.001*"chinese" + 0.001*"season" + 0.001*"emperor" + 0.001*"ball" + 0.001*"novel" + 0.001*"color" + 0.001*"league" + 0.001*"spanish" + 0.001*"lake" + 0.001*"energy"
    2021-01-24 11:08:40,293 : INFO : topic #8 (0.091): 0.001*"emperor" + 0.001*"km" + 0.001*"band" + 0.001*"software" + 0.001*"actor" + 0.001*"album" + 0.001*"russian" + 0.001*"economy" + 0.001*"male" + 0.001*"minister"
    2021-01-24 11:08:40,294 : INFO : topic diff=1.023860, rho=1.000000
    2021-01-24 11:08:53,644 : INFO : -9.277 per-word bound, 620.5 perplexity estimate based on a held-out corpus of 1701 documents with 4692704 words
    2021-01-24 11:08:53,644 : INFO : PROGRESS: pass 1, at document #1701/1701
    2021-01-24 11:08:57,030 : INFO : topic #3 (0.091): 0.002*"actor" + 0.001*"album" + 0.001*"energy" + 0.001*"india" + 0.001*"software" + 0.001*"love" + 0.001*"singer" + 0.001*"actress" + 0.001*"y" + 0.001*"canadian"
    2021-01-24 11:08:57,030 : INFO : topic #5 (0.091): 0.001*"aircraft" + 0.001*"emperor" + 0.001*"plant" + 0.001*"ireland" + 0.001*"irish" + 0.001*"county" + 0.001*"actor" + 0.001*"henry" + 0.001*"cell" + 0.001*"japanese"
    2021-01-24 11:08:57,030 : INFO : topic #4 (0.091): 0.002*"ball" + 0.002*"season" + 0.001*"chinese" + 0.001*"league" + 0.001*"color" + 0.001*"emperor" + 0.001*"novel" + 0.001*"nfl" + 0.001*"lake" + 0.001*"mary"
    2021-01-24 11:08:57,031 : INFO : topic #0 (0.091): 0.002*"soviet" + 0.002*"election" + 0.001*"japanese" + 0.001*"chinese" + 0.001*"league" + 0.001*"cell" + 0.001*"km" + 0.001*"japan" + 0.001*"card" + 0.001*"minister"
    2021-01-24 11:08:57,031 : INFO : topic #7 (0.091): 0.002*"band" + 0.001*"album" + 0.001*"lake" + 0.001*"bass" + 0.001*"kong" + 0.001*"minister" + 0.001*"cell" + 0.001*"chinese" + 0.001*"africa" + 0.001*"liberal"
    2021-01-24 11:08:57,031 : INFO : topic diff=0.167986, rho=0.577350
    2021-01-24 11:08:57,032 : INFO : ensemble contains 9 models and 160 topics now
    2021-01-24 11:08:57,038 : INFO : ensemble contains 10 models and 169 topics now
    2021-01-24 11:08:57,045 : INFO : asymmetric distance matrix is outdated due to add_model
    2021-01-24 11:08:57,045 : INFO : generating a 180 x 180 asymmetric distance matrix...
    2021-01-24 11:08:58,136 : INFO : fitting the clustering model, using 5 for min_samples
    2021-01-24 11:08:58,170 : INFO : generating stable topics, using 3 for min_cores
    2021-01-24 11:08:58,171 : INFO : found 26 clusters
    2021-01-24 11:08:58,175 : INFO : found 1 stable topics
    2021-01-24 11:08:58,175 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-01-24 11:08:58,176 : INFO : using symmetric alpha at 1.0
    2021-01-24 11:08:58,176 : INFO : using symmetric eta at 1.0
    2021-01-24 11:08:58,180 : INFO : using serial LDA version on this node
    2021-01-24 11:08:58,183 : INFO : running online (multi-pass) LDA training, 1 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-01-24 11:08:58,183 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    180
    1





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 6 minutes  48.976 seconds)

**Estimated memory usage:**  1614 MB


.. _sphx_glr_download_auto_examples_tutorials_run_ensemblelda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_ensemblelda.py <run_ensemblelda.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_ensemblelda.ipynb <run_ensemblelda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
