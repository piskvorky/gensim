.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_word2vec.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_word2vec.py:


Word2Vec Model
==============

Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








In case you missed the buzz, Word2Vec is a widely used algorithm based on neural
networks, commonly referred to as "deep learning" (though word2vec itself is rather shallow).
Using large amounts of unannotated plain text, word2vec learns relationships
between words automatically. The output are vectors, one vector per word,
with remarkable linear relationships that allow us to do things like:

* vec("king") - vec("man") + vec("woman") =~ vec("queen")
* vec("Montreal Canadiens") – vec("Montreal") + vec("Toronto") =~ vec("Toronto Maple Leafs").

Word2vec is very useful in `automatic text tagging
<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\ , recommender
systems and machine translation.

This tutorial:

#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words
#. Shows off a demo of ``Word2Vec`` using a pre-trained model
#. Demonstrates training a new model from your own data
#. Demonstrates loading and saving models
#. Introduces several training parameters and demonstrates their effect
#. Discusses memory requirements
#. Visualizes Word2Vec embeddings by applying dimensionality reduction

Review: Bag-of-words
--------------------

.. Note:: Feel free to skip these review sections if you're already familiar with the models.

You may be familiar with the `bag-of-words model
<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the
:ref:`core_concepts_vector` section.
This model transforms each document to a fixed-length vector of integers.
For example, given the sentences:

- ``John likes to watch movies. Mary likes movies too.``
- ``John also likes to watch football games. Mary hates football.``

The model outputs the vectors:

- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``
- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``

Each vector has 10 elements, where each element counts the number of times a
particular word occurred in the document.
The order of elements is arbitrary.
In the example above, the order of the elements corresponds to the words:
``["John", "likes", "to", "watch", "movies", "Mary", "too", "also", "football", "games", "hates"]``.

Bag-of-words models are surprisingly effective, but have several weaknesses.

First, they lose all information about word order: "John likes Mary" and
"Mary likes John" correspond to identical vectors. There is a solution: bag
of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__
models consider word phrases of length n to represent documents as
fixed-length vectors to capture local word order but suffer from data
sparsity and high dimensionality.

Second, the model does not attempt to learn the meaning of the underlying
words, and as a consequence, the distance between vectors doesn't always
reflect the difference in meaning.  The ``Word2Vec`` model addresses this
second problem.

Introducing: the ``Word2Vec`` Model
-----------------------------------

``Word2Vec`` is a more recent model that embeds words in a lower-dimensional
vector space using a shallow neural network. The result is a set of
word-vectors where vectors close together in vector space have similar
meanings based on context, and word-vectors distant to each other have
differing meanings. For example, ``strong`` and ``powerful`` would be close
together and ``strong`` and ``Paris`` would be relatively far.

The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`
class implements them both:

1. Skip-grams (SG)
2. Continuous-bag-of-words (CBOW)

.. Important::
  Don't let the implementation details below scare you.
  They're advanced material: if it's too much, then move on to the next section.

The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__
model, for example, takes in pairs (word1, word2) generated by moving a
window across text data, and trains a 1-hidden-layer neural network based on
the synthetic task of given an input word, giving us a predicted probability
distribution of nearby words to the input. A virtual `one-hot
<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words
goes through a 'projection layer' to the hidden layer; these projection
weights are later interpreted as the word embeddings. So if the hidden layer
has 300 neurons, this network will give us 300-dimensional word embeddings.

Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It
is also a 1-hidden-layer neural network. The synthetic training task now uses
the average of multiple input context words, rather than a single word as in
skip-gram, to predict the center word. Again, the projection weights that
turn one-hot words into averageable vectors, of the same width as the hidden
layer, are interpreted as the word embeddings.


Word2Vec Demo
-------------

To see what ``Word2Vec`` can do, let's download a pre-trained model and play
around with it. We will fetch the Word2Vec model trained on part of the
Google News dataset, covering approximately 3 million words and phrases. Such
a model can take hours to train, but since it's already available,
downloading and loading it with Gensim takes minutes.

.. Important::
  The model is approximately 2GB, so you'll need a decent network connection
  to proceed.  Otherwise, skip ahead to the "Training Your Own Model" section
  below.

You may also check out an `online word2vec demo
<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try
this vector algebra for yourself. That demo runs ``word2vec`` on the
**entire** Google News dataset, of **about 100 billion words**.



.. code-block:: default

    import gensim.downloader as api
    wv = api.load('word2vec-google-news-300')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:00:40,474 : INFO : loading projection weights from /Users/kofola3/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz
    2020-09-30 17:01:46,484 : INFO : loaded (3000000, 300) matrix from /Users/kofola3/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz




A common operation is to retrieve the vocabulary of a model. That is trivial:


.. code-block:: default

    for index, word in enumerate(wv.index_to_key):
        if index == 10:
            break
        print(f"word #{index}/{len(wv.index_to_key)} is {word}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    word #0/3000000 is </s>
    word #1/3000000 is in
    word #2/3000000 is for
    word #3/3000000 is that
    word #4/3000000 is is
    word #5/3000000 is on
    word #6/3000000 is ##
    word #7/3000000 is The
    word #8/3000000 is with
    word #9/3000000 is said




We can easily obtain vectors for terms the model is familiar with:



.. code-block:: default

    vec_king = wv['king']








Unfortunately, the model is unable to infer vectors for unfamiliar words.
This is one limitation of Word2Vec: if this limitation matters to you, check
out the FastText model.



.. code-block:: default

    try:
        vec_cameroon = wv['cameroon']
    except KeyError:
        print("The word 'cameroon' does not appear in this model")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    The word 'cameroon' does not appear in this model




Moving on, ``Word2Vec`` supports several word similarity tasks out of the
box.  You can see how the similarity intuitively decreases as the words get
less and less similar.



.. code-block:: default

    pairs = [
        ('car', 'minivan'),   # a minivan is a kind of car
        ('car', 'bicycle'),   # still a wheeled vehicle
        ('car', 'airplane'),  # ok, no wheels, but still a vehicle
        ('car', 'cereal'),    # ... and so on
        ('car', 'communism'),
    ]
    for w1, w2 in pairs:
        print('%r\t%r\t%.2f' % (w1, w2, wv.similarity(w1, w2)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    'car'   'minivan'       0.69
    'car'   'bicycle'       0.54
    'car'   'airplane'      0.42
    'car'   'cereal'        0.14
    'car'   'communism'     0.06




Print the 5 most similar words to "car" or "minivan"


.. code-block:: default

    print(wv.most_similar(positive=['car', 'minivan'], topn=5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]




Which of the below does not belong in the sequence?


.. code-block:: default

    print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    car




Training Your Own Model
-----------------------

To start, you'll need some data for training the model. For the following
examples, we'll use the `Lee Evaluation Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_
(which you `already have
<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_
if you've installed Gensim).

This corpus is small enough to fit entirely in memory, but we'll implement a
memory-friendly iterator that reads it line-by-line to demonstrate how you
would handle a larger corpus.



.. code-block:: default


    from gensim.test.utils import datapath
    from gensim import utils

    class MyCorpus(object):
        """An interator that yields sentences (lists of str)."""

        def __iter__(self):
            corpus_path = datapath('lee_background.cor')
            for line in open(corpus_path):
                # assume there's one document per line, tokens separated by whitespace
                yield utils.simple_preprocess(line)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:00,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2020-09-30 17:02:00,366 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)




If we wanted to do any custom preprocessing, e.g. decode a non-standard
encoding, lowercase, remove numbers, extract named entities... All of this can
be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to
know. All that is required is that the input yields one sentence (list of
utf8 words) after another.

Let's go ahead and train a model on our corpus.  Don't worry about the
training parameters much for now, we'll revisit them later.



.. code-block:: default

    import gensim.models

    sentences = MyCorpus()
    model = gensim.models.Word2Vec(sentences=sentences)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:00,550 : INFO : collecting all words and their counts
    2020-09-30 17:02:00,551 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:00,657 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences
    2020-09-30 17:02:00,657 : INFO : Loading a fresh vocabulary
    2020-09-30 17:02:00,668 : INFO : effective_min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)
    2020-09-30 17:02:00,668 : INFO : effective_min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)
    2020-09-30 17:02:00,683 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 17:02:00,741 : INFO : sample=0.001 downsamples 51 most-common words
    2020-09-30 17:02:00,741 : INFO : downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)
    2020-09-30 17:02:00,769 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes
    2020-09-30 17:02:00,770 : INFO : resetting layer weights
    2020-09-30 17:02:00,875 : INFO : training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:00,993 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:00,994 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:00,999 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:00,999 : INFO : EPOCH - 1 : training on 58152 raw words (35967 effective words) took 0.1s, 305737 effective words/s
    2020-09-30 17:02:01,099 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:01,103 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:01,106 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:01,106 : INFO : EPOCH - 2 : training on 58152 raw words (35955 effective words) took 0.1s, 343839 effective words/s
    2020-09-30 17:02:01,210 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:01,218 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:01,220 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:01,220 : INFO : EPOCH - 3 : training on 58152 raw words (35878 effective words) took 0.1s, 316674 effective words/s
    2020-09-30 17:02:01,326 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:01,333 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:01,336 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:01,336 : INFO : EPOCH - 4 : training on 58152 raw words (35809 effective words) took 0.1s, 312256 effective words/s
    2020-09-30 17:02:01,434 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:01,438 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:01,441 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:01,441 : INFO : EPOCH - 5 : training on 58152 raw words (35998 effective words) took 0.1s, 344237 effective words/s
    2020-09-30 17:02:01,441 : INFO : training on a 290760 raw words (179607 effective words) took 0.6s, 317010 effective words/s




Once we have our model, we can use it in the same way as in the demo above.

The main part of the model is ``model.wv``\ , where "wv" stands for "word vectors".



.. code-block:: default

    vec_king = model.wv['king']








Retrieving the vocabulary works the same way:


.. code-block:: default

    for index, word in enumerate(wv.index_to_key):
        if index == 10:
            break
        print(f"word #{index}/{len(wv.index_to_key)} is {word}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    word #0/3000000 is </s>
    word #1/3000000 is in
    word #2/3000000 is for
    word #3/3000000 is that
    word #4/3000000 is is
    word #5/3000000 is on
    word #6/3000000 is ##
    word #7/3000000 is The
    word #8/3000000 is with
    word #9/3000000 is said




Storing and loading models
--------------------------

You'll notice that training non-trivial models can take time.  Once you've
trained your model and it works as expected, you can save it to disk.  That
way, you don't have to spend time training it all over again later.

You can store/load models using the standard gensim methods:



.. code-block:: default

    import tempfile

    with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:
        temporary_filepath = tmp.name
        model.save(temporary_filepath)
        #
        # The model is now safely stored in the filepath.
        # You can copy it to other machines, share it with others, etc.
        #
        # To load a saved model:
        #
        new_model = gensim.models.Word2Vec.load(temporary_filepath)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:01,737 : INFO : saving Word2Vec object under /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d, separately None
    2020-09-30 17:02:01,740 : INFO : not storing attribute cum_table
    2020-09-30 17:02:01,785 : INFO : saved /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d
    2020-09-30 17:02:01,786 : INFO : loading Word2Vec object from /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d
    2020-09-30 17:02:01,801 : INFO : loading wv recursively from /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d.wv.* with mmap=None
    2020-09-30 17:02:01,801 : INFO : setting ignored attribute cum_table to None
    2020-09-30 17:02:01,821 : INFO : loaded /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d




which uses pickle internally, optionally ``mmap``\ ‘ing the model’s internal
large NumPy matrices into virtual memory directly from disk files, for
inter-process memory sharing.

In addition, you can load models created by the original C tool, both using
its text and binary formats::

  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)
  # using gzipped/bz2 input works too, no need to unzip
  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)


Training Parameters
-------------------

``Word2Vec`` accepts several parameters that affect both training speed and quality.

min_count
---------

``min_count`` is for pruning the internal dictionary. Words that appear only
once or twice in a billion-word corpus are probably uninteresting typos and
garbage. In addition, there’s not enough data to make any meaningful training
on those words, so it’s best to ignore them:

default value of min_count=5


.. code-block:: default

    model = gensim.models.Word2Vec(sentences, min_count=10)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:01,918 : INFO : collecting all words and their counts
    2020-09-30 17:02:01,921 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:02,011 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences
    2020-09-30 17:02:02,011 : INFO : Loading a fresh vocabulary
    2020-09-30 17:02:02,018 : INFO : effective_min_count=10 retains 889 unique words (12% of original 6981, drops 6092)
    2020-09-30 17:02:02,018 : INFO : effective_min_count=10 leaves 43776 word corpus (75% of original 58152, drops 14376)
    2020-09-30 17:02:02,028 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 17:02:02,029 : INFO : sample=0.001 downsamples 55 most-common words
    2020-09-30 17:02:02,029 : INFO : downsampling leaves estimated 29691 word corpus (67.8% of prior 43776)
    2020-09-30 17:02:02,041 : INFO : estimated required memory for 889 words and 100 dimensions: 1155700 bytes
    2020-09-30 17:02:02,041 : INFO : resetting layer weights
    2020-09-30 17:02:02,083 : INFO : training model with 3 workers on 889 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:02,184 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,190 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,192 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,192 : INFO : EPOCH - 1 : training on 58152 raw words (29629 effective words) took 0.1s, 276020 effective words/s
    2020-09-30 17:02:02,287 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,292 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,295 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,295 : INFO : EPOCH - 2 : training on 58152 raw words (29624 effective words) took 0.1s, 290768 effective words/s
    2020-09-30 17:02:02,394 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,397 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,400 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,400 : INFO : EPOCH - 3 : training on 58152 raw words (29769 effective words) took 0.1s, 286475 effective words/s
    2020-09-30 17:02:02,496 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,499 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,501 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,502 : INFO : EPOCH - 4 : training on 58152 raw words (29578 effective words) took 0.1s, 293835 effective words/s
    2020-09-30 17:02:02,598 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,601 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,604 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,604 : INFO : EPOCH - 5 : training on 58152 raw words (29707 effective words) took 0.1s, 292782 effective words/s
    2020-09-30 17:02:02,604 : INFO : training on a 290760 raw words (148307 effective words) took 0.5s, 284858 effective words/s




vector_size
-----------

``vector_size`` is the number of dimensions (N) of the N-dimensional space that
gensim Word2Vec maps the words onto.

Bigger size values require more training data, but can lead to better (more
accurate) models. Reasonable values are in the tens to hundreds.



.. code-block:: default


    # The default value of vector_size is 100.
    model = gensim.models.Word2Vec(sentences, vector_size=200)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:02,626 : INFO : collecting all words and their counts
    2020-09-30 17:02:02,628 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:02,722 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences
    2020-09-30 17:02:02,722 : INFO : Loading a fresh vocabulary
    2020-09-30 17:02:02,734 : INFO : effective_min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)
    2020-09-30 17:02:02,734 : INFO : effective_min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)
    2020-09-30 17:02:02,748 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 17:02:02,748 : INFO : sample=0.001 downsamples 51 most-common words
    2020-09-30 17:02:02,748 : INFO : downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)
    2020-09-30 17:02:02,770 : INFO : estimated required memory for 1750 words and 200 dimensions: 3675000 bytes
    2020-09-30 17:02:02,770 : INFO : resetting layer weights
    2020-09-30 17:02:02,864 : INFO : training model with 3 workers on 1750 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:02,973 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:02,979 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:02,982 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:02,982 : INFO : EPOCH - 1 : training on 58152 raw words (35994 effective words) took 0.1s, 307729 effective words/s
    2020-09-30 17:02:03,087 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,093 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,097 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,097 : INFO : EPOCH - 2 : training on 58152 raw words (35944 effective words) took 0.1s, 317636 effective words/s
    2020-09-30 17:02:03,202 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,208 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,212 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,212 : INFO : EPOCH - 3 : training on 58152 raw words (36007 effective words) took 0.1s, 314282 effective words/s
    2020-09-30 17:02:03,320 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,327 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,330 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,330 : INFO : EPOCH - 4 : training on 58152 raw words (35992 effective words) took 0.1s, 307219 effective words/s
    2020-09-30 17:02:03,436 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,442 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,445 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,446 : INFO : EPOCH - 5 : training on 58152 raw words (36003 effective words) took 0.1s, 314793 effective words/s
    2020-09-30 17:02:03,446 : INFO : training on a 290760 raw words (179940 effective words) took 0.6s, 309327 effective words/s




workers
-------

``workers`` , the last of the major parameters (full list `here
<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)
is for training parallelization, to speed up training:



.. code-block:: default


    # default value of workers=3 (tutorial says 1...)
    model = gensim.models.Word2Vec(sentences, workers=4)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:03,470 : INFO : collecting all words and their counts
    2020-09-30 17:02:03,472 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:03,571 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences
    2020-09-30 17:02:03,571 : INFO : Loading a fresh vocabulary
    2020-09-30 17:02:03,582 : INFO : effective_min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)
    2020-09-30 17:02:03,582 : INFO : effective_min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)
    2020-09-30 17:02:03,595 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 17:02:03,595 : INFO : sample=0.001 downsamples 51 most-common words
    2020-09-30 17:02:03,595 : INFO : downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)
    2020-09-30 17:02:03,616 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes
    2020-09-30 17:02:03,616 : INFO : resetting layer weights
    2020-09-30 17:02:03,704 : INFO : training model with 4 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:03,809 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 17:02:03,810 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,810 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,815 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,816 : INFO : EPOCH - 1 : training on 58152 raw words (35953 effective words) took 0.1s, 326539 effective words/s
    2020-09-30 17:02:03,912 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 17:02:03,913 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:03,915 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:03,920 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:03,920 : INFO : EPOCH - 2 : training on 58152 raw words (35895 effective words) took 0.1s, 348415 effective words/s
    2020-09-30 17:02:04,017 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 17:02:04,018 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,021 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,024 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,024 : INFO : EPOCH - 3 : training on 58152 raw words (35907 effective words) took 0.1s, 347822 effective words/s
    2020-09-30 17:02:04,127 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 17:02:04,127 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,128 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,134 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,134 : INFO : EPOCH - 4 : training on 58152 raw words (35909 effective words) took 0.1s, 333947 effective words/s
    2020-09-30 17:02:04,232 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 17:02:04,232 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,233 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,238 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,238 : INFO : EPOCH - 5 : training on 58152 raw words (35957 effective words) took 0.1s, 347693 effective words/s
    2020-09-30 17:02:04,238 : INFO : training on a 290760 raw words (179621 effective words) took 0.5s, 335988 effective words/s




The ``workers`` parameter only has an effect if you have `Cython
<http://cython.org/>`_ installed. Without Cython, you’ll only be able to use
one core because of the `GIL
<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``
training will be `miserably slow
<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\ ).


Memory
------

At its core, ``word2vec`` model parameters are stored as matrices (NumPy
arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)
times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).

Three such matrices are held in RAM (work is underway to reduce that number
to two, or even one). So if your input contains 100,000 unique words, and you
asked for layer ``vector_size=200``\ , the model will require approx.
``100,000*200*4*3 bytes = ~229MB``.

There’s a little extra memory needed for storing the vocabulary tree (100,000 words would
take a few megabytes), but unless your words are extremely loooong strings, memory
footprint will be dominated by the three matrices above.


Evaluating
----------

``Word2Vec`` training is an unsupervised task, there’s no good way to
objectively evaluate the result. Evaluation depends on your end application.

Google has released their testing set of about 20,000 syntactic and semantic
test examples, following the “A is to B as C is to D” task. It is provided in
the 'datasets' folder.

For example a syntactic analogy of comparative type is ``bad:worse;good:?``.
There are total of 9 types of syntactic comparisons in the dataset like
plural nouns and nouns of opposite meaning.

The semantic questions contain five types of semantic analogies, such as
capital cities (``Paris:France;Tokyo:?``) or family members
(``brother:sister;dad:?``).


Gensim supports the same evaluation set, in exactly the same format:



.. code-block:: default

    model.wv.evaluate_word_analogies(datapath('questions-words.txt'))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:04,350 : INFO : Evaluating word analogies for top 300000 words in the model on /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/questions-words.txt
    2020-09-30 17:02:04,358 : INFO : capital-common-countries: 0.0% (0/6)
    2020-09-30 17:02:04,376 : INFO : capital-world: 0.0% (0/2)
    2020-09-30 17:02:04,392 : INFO : family: 0.0% (0/6)
    2020-09-30 17:02:04,409 : INFO : gram3-comparative: 0.0% (0/20)
    2020-09-30 17:02:04,416 : INFO : gram4-superlative: 0.0% (0/12)
    2020-09-30 17:02:04,423 : INFO : gram5-present-participle: 0.0% (0/20)
    2020-09-30 17:02:04,435 : INFO : gram6-nationality-adjective: 0.0% (0/30)
    2020-09-30 17:02:04,445 : INFO : gram7-past-tense: 0.0% (0/20)
    2020-09-30 17:02:04,457 : INFO : gram8-plural: 3.3% (1/30)
    2020-09-30 17:02:04,462 : INFO : Quadruplets with out-of-vocabulary words: 99.3%
    2020-09-30 17:02:04,465 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use "dummy4unknown=True"
    2020-09-30 17:02:04,465 : INFO : Total accuracy: 0.7% (1/146)

    (0.00684931506849315, [{'section': 'capital-common-countries', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]}, {'section': 'capital-world', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]}, {'section': 'currency', 'correct': [], 'incorrect': []}, {'section': 'city-in-state', 'correct': [], 'incorrect': []}, {'section': 'family', 'correct': [], 'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER')]}, {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []}, {'section': 'gram2-opposite', 'correct': [], 'incorrect': []}, {'section': 'gram3-comparative', 'correct': [], 'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER')]}, {'section': 'gram4-superlative', 'correct': [], 'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]}, {'section': 'gram5-present-participle', 'correct': [], 'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING')]}, {'section': 'gram6-nationality-adjective', 'correct': [], 'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]}, {'section': 'gram7-past-tense', 'correct': [], 'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID')]}, {'section': 'gram8-plural', 'correct': [('CAR', 'CARS', 'BUILDING', 'BUILDINGS')], 'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}, {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []}, {'section': 'Total accuracy', 'correct': [('CAR', 'CARS', 'BUILDING', 'BUILDINGS')], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER'), ('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER'), ('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'), ('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING'), ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'), ('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID'), ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])



This ``evaluate_word_analogies`` method takes an `optional parameter
<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_
``restrict_vocab`` which limits which test examples are to be considered.


In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.

By default it uses an academic dataset WS-353 but one can create a dataset
specific to your business based on it. It contains word pairs together with
human-assigned similarity judgments. It measures the relatedness or
co-occurrence of two words. For example, 'coast' and 'shore' are very similar
as they appear in the same context. At the same time 'clothes' and 'closet'
are less similar because they are related but not interchangeable.



.. code-block:: default

    model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:04,681 : INFO : Pearson correlation coefficient against /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/wordsim353.tsv: 0.1072
    2020-09-30 17:02:04,682 : INFO : Spearman rank-order correlation coefficient against /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/wordsim353.tsv: 0.0977
    2020-09-30 17:02:04,682 : INFO : Pairs with unknown words ratio: 83.0%

    ((0.10718629411012633, 0.41498744701424156), SpearmanrResult(correlation=0.09773516803468056, pvalue=0.4575366217424267), 83.0028328611898)



.. Important::
  Good performance on Google's or WS-353 test set doesn’t mean word2vec will
  work well in your application, or vice versa. It’s always best to evaluate
  directly on your intended task. For an example of how to use word2vec in a
  classifier pipeline, see this `tutorial
  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.


Online training / Resuming training
-----------------------------------

Advanced users can load a model and continue training it with more sentences
and `new vocabulary words <online_w2v_tutorial.ipynb>`_:



.. code-block:: default

    model = gensim.models.Word2Vec.load(temporary_filepath)
    more_sentences = [
        ['Advanced', 'users', 'can', 'load', 'a', 'model',
         'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],
    ]
    model.build_vocab(more_sentences, update=True)
    model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)

    # cleaning up temporary file
    import os
    os.remove(temporary_filepath)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:04,775 : INFO : loading Word2Vec object from /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d
    2020-09-30 17:02:04,788 : INFO : loading wv recursively from /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d.wv.* with mmap=None
    2020-09-30 17:02:04,789 : INFO : setting ignored attribute cum_table to None
    2020-09-30 17:02:04,809 : INFO : loaded /var/folders/w0/f7blghz9277068cnyyd3nd200000gn/T/gensim-model-36yeu47d
    2020-09-30 17:02:04,809 : INFO : collecting all words and their counts
    2020-09-30 17:02:04,809 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:04,810 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences
    2020-09-30 17:02:04,810 : INFO : Updating model with new vocabulary
    2020-09-30 17:02:04,819 : INFO : New added 0 unique words (0% of original 13) and increased the count of 0 pre-existing words (0% of original 13)
    2020-09-30 17:02:04,819 : INFO : deleting the raw counts dictionary of 13 items
    2020-09-30 17:02:04,819 : INFO : sample=0.001 downsamples 0 most-common words
    2020-09-30 17:02:04,819 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)
    2020-09-30 17:02:04,838 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes
    2020-09-30 17:02:04,838 : INFO : updating layer weights
    2020-09-30 17:02:04,839 : WARNING : Effective 'alpha' higher than previous training cycles
    2020-09-30 17:02:04,839 : INFO : training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:04,842 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,843 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,843 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,843 : INFO : EPOCH - 1 : training on 13 raw words (6 effective words) took 0.0s, 5326 effective words/s
    2020-09-30 17:02:04,844 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,845 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,845 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,845 : INFO : EPOCH - 2 : training on 13 raw words (5 effective words) took 0.0s, 6975 effective words/s
    2020-09-30 17:02:04,846 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,846 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,846 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,846 : INFO : EPOCH - 3 : training on 13 raw words (5 effective words) took 0.0s, 8539 effective words/s
    2020-09-30 17:02:04,847 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,847 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,847 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,848 : INFO : EPOCH - 4 : training on 13 raw words (6 effective words) took 0.0s, 11100 effective words/s
    2020-09-30 17:02:04,848 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:04,849 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:04,849 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:04,849 : INFO : EPOCH - 5 : training on 13 raw words (5 effective words) took 0.0s, 9718 effective words/s
    2020-09-30 17:02:04,849 : INFO : training on a 65 raw words (27 effective words) took 0.0s, 2900 effective words/s




You may need to tweak the ``total_words`` parameter to ``train()``,
depending on what learning rate decay you want to simulate.

Note that it’s not possible to resume training with models generated by the C
tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for
querying/similarity, but information vital for training (the vocab tree) is
missing there.


Training Loss Computation
-------------------------

The parameter ``compute_loss`` can be used to toggle computation of loss
while training the Word2Vec model. The computed loss is stored in the model
attribute ``running_training_loss`` and can be retrieved using the function
``get_latest_training_loss`` as follows :



.. code-block:: default


    # instantiating and training the Word2Vec model
    model_with_loss = gensim.models.Word2Vec(
        sentences,
        min_count=1,
        compute_loss=True,
        hs=0,
        sg=1,
        seed=42,
    )

    # getting the training loss value
    training_loss = model_with_loss.get_latest_training_loss()
    print(training_loss)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 17:02:05,025 : INFO : collecting all words and their counts
    2020-09-30 17:02:05,027 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
    2020-09-30 17:02:05,112 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences
    2020-09-30 17:02:05,112 : INFO : Loading a fresh vocabulary
    2020-09-30 17:02:05,152 : INFO : effective_min_count=1 retains 6981 unique words (100% of original 6981, drops 0)
    2020-09-30 17:02:05,152 : INFO : effective_min_count=1 leaves 58152 word corpus (100% of original 58152, drops 0)
    2020-09-30 17:02:05,207 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 17:02:05,207 : INFO : sample=0.001 downsamples 43 most-common words
    2020-09-30 17:02:05,207 : INFO : downsampling leaves estimated 45723 word corpus (78.6% of prior 58152)
    2020-09-30 17:02:05,294 : INFO : estimated required memory for 6981 words and 100 dimensions: 9075300 bytes
    2020-09-30 17:02:05,294 : INFO : resetting layer weights
    2020-09-30 17:02:05,651 : INFO : training model with 3 workers on 6981 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 17:02:05,800 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:05,839 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:05,841 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:05,841 : INFO : EPOCH - 1 : training on 58152 raw words (45692 effective words) took 0.2s, 242729 effective words/s
    2020-09-30 17:02:06,028 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:06,032 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:06,037 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:06,037 : INFO : EPOCH - 2 : training on 58152 raw words (45778 effective words) took 0.2s, 234367 effective words/s
    2020-09-30 17:02:06,218 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:06,222 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:06,225 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:06,225 : INFO : EPOCH - 3 : training on 58152 raw words (45684 effective words) took 0.2s, 244363 effective words/s
    2020-09-30 17:02:06,400 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:06,407 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:06,409 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:06,409 : INFO : EPOCH - 4 : training on 58152 raw words (45651 effective words) took 0.2s, 249862 effective words/s
    2020-09-30 17:02:06,558 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 17:02:06,597 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 17:02:06,600 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 17:02:06,600 : INFO : EPOCH - 5 : training on 58152 raw words (45745 effective words) took 0.2s, 240328 effective words/s
    2020-09-30 17:02:06,600 : INFO : training on a 290760 raw words (228550 effective words) took 0.9s, 240759 effective words/s
    1365568.125




Benchmarks
----------

Let's run some benchmarks to see effect of the training loss computation code
on training time.

We'll use the following data for the benchmarks:

#. Lee Background corpus: included in gensim's test data
#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the
   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.



.. code-block:: default


    import io
    import os

    import gensim.models.word2vec
    import gensim.downloader as api
    import smart_open


    def head(path, size):
        with smart_open.open(path) as fin:
            return io.StringIO(fin.read(size))


    def generate_input_data():
        lee_path = datapath('lee_background.cor')
        ls = gensim.models.word2vec.LineSentence(lee_path)
        ls.name = '25kB'
        yield ls

        text8_path = api.load('text8').fn
        labels = ('1MB', '10MB', '50MB', '100MB')
        sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)
        for l, s in zip(labels, sizes):
            ls = gensim.models.word2vec.LineSentence(head(text8_path, s))
            ls.name = l
            yield ls


    input_data = list(generate_input_data())








We now compare the training time taken for different combinations of input
data and model training parameters like ``hs`` and ``sg``.

For each combination, we repeat the test several times to obtain the mean and
standard deviation of the test duration.



.. code-block:: default


    # Temporarily reduce logging verbosity
    logging.root.level = logging.ERROR

    import time
    import numpy as np
    import pandas as pd

    train_time_values = []
    seed_val = 42
    sg_values = [0, 1]
    hs_values = [0, 1]

    fast = True
    if fast:
        input_data_subset = input_data[:3]
    else:
        input_data_subset = input_data


    for data in input_data_subset:
        for sg_val in sg_values:
            for hs_val in hs_values:
                for loss_flag in [True, False]:
                    time_taken_list = []
                    for i in range(3):
                        start_time = time.time()
                        w2v_model = gensim.models.Word2Vec(
                            data,
                            compute_loss=loss_flag,
                            sg=sg_val,
                            hs=hs_val,
                            seed=seed_val,
                        )
                        time_taken_list.append(time.time() - start_time)

                    time_taken_list = np.array(time_taken_list)
                    time_mean = np.mean(time_taken_list)
                    time_std = np.std(time_taken_list)

                    model_result = {
                        'train_data': data.name,
                        'compute_loss': loss_flag,
                        'sg': sg_val,
                        'hs': hs_val,
                        'train_time_mean': time_mean,
                        'train_time_std': time_std,
                    }
                    print("Word2vec model #%i: %s" % (len(train_time_values), model_result))
                    train_time_values.append(model_result)

    train_times_table = pd.DataFrame(train_time_values)
    train_times_table = train_times_table.sort_values(
        by=['train_data', 'sg', 'hs', 'compute_loss'],
        ascending=[False, False, True, False],
    )
    print(train_times_table)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Word2vec model #0: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.3307774066925049, 'train_time_std': 0.00578659163388716}
    Word2vec model #1: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.3314487934112549, 'train_time_std': 0.004201913501261655}
    Word2vec model #2: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.5213752587636312, 'train_time_std': 0.008047867089155704}
    Word2vec model #3: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.5293020407358805, 'train_time_std': 0.005368254032954145}
    Word2vec model #4: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 0.571751594543457, 'train_time_std': 0.001023259266794945}
    Word2vec model #5: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 0.5736987590789795, 'train_time_std': 0.00740075638673385}
    Word2vec model #6: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 1.1089734236399333, 'train_time_std': 0.029923990619945186}
    Word2vec model #7: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 1.2068419456481934, 'train_time_std': 0.006783016321594606}
    Word2vec model #8: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.9139569600423177, 'train_time_std': 0.04541121423444599}
    Word2vec model #9: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.9152584075927734, 'train_time_std': 0.05191135337399049}
    Word2vec model #10: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 1.6703286170959473, 'train_time_std': 0.11292966925292192}
    Word2vec model #11: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 1.583152135213216, 'train_time_std': 0.04577290669842482}
    Word2vec model #12: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 1.811710516611735, 'train_time_std': 0.01081321887556254}
    Word2vec model #13: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 1.8143157164255779, 'train_time_std': 0.026406013455100835}
    Word2vec model #14: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 3.5845812956492105, 'train_time_std': 0.08968344917541199}
    Word2vec model #15: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 3.6167975266774497, 'train_time_std': 0.14609390508721276}
    Word2vec model #16: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 8.021462361017862, 'train_time_std': 0.21593094159548987}
    Word2vec model #17: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 7.931290070215861, 'train_time_std': 0.25084118769867136}
    Word2vec model #18: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 15.51533571879069, 'train_time_std': 0.8857355166766315}
    Word2vec model #19: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 15.930208921432495, 'train_time_std': 0.6417048653898146}
    Word2vec model #20: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 21.687038342158, 'train_time_std': 0.3261330075856754}
    Word2vec model #21: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 21.280882279078167, 'train_time_std': 0.12885843584913614}
    Word2vec model #22: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 43.11969208717346, 'train_time_std': 0.8133788671881127}
    Word2vec model #23: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 40.59294398625692, 'train_time_std': 0.47622639550838375}
        compute_loss  hs  sg train_data  train_time_mean  train_time_std
    4           True   0   1       25kB         0.571752        0.001023
    5          False   0   1       25kB         0.573699        0.007401
    6           True   1   1       25kB         1.108973        0.029924
    7          False   1   1       25kB         1.206842        0.006783
    0           True   0   0       25kB         0.330777        0.005787
    1          False   0   0       25kB         0.331449        0.004202
    2           True   1   0       25kB         0.521375        0.008048
    3          False   1   0       25kB         0.529302        0.005368
    12          True   0   1        1MB         1.811711        0.010813
    13         False   0   1        1MB         1.814316        0.026406
    14          True   1   1        1MB         3.584581        0.089683
    15         False   1   1        1MB         3.616798        0.146094
    8           True   0   0        1MB         0.913957        0.045411
    9          False   0   0        1MB         0.915258        0.051911
    10          True   1   0        1MB         1.670329        0.112930
    11         False   1   0        1MB         1.583152        0.045773
    20          True   0   1       10MB        21.687038        0.326133
    21         False   0   1       10MB        21.280882        0.128858
    22          True   1   1       10MB        43.119692        0.813379
    23         False   1   1       10MB        40.592944        0.476226
    16          True   0   0       10MB         8.021462        0.215931
    17         False   0   0       10MB         7.931290        0.250841
    18          True   1   0       10MB        15.515336        0.885736
    19         False   1   0       10MB        15.930209        0.641705




Visualising Word Embeddings
---------------------------

The word embeddings made by the model can be visualised by reducing
dimensionality of the words to 2 dimensions using tSNE.

Visualisations can be used to notice semantic and syntactic trends in the data.

Example:

* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by
* Syntactic: words like run, running or cut, cutting lie close together.

Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.

.. Important::
  The model used for the visualisation is trained on a small corpus. Thus
  some of the relations might not be so clear.



.. code-block:: default


    from sklearn.decomposition import IncrementalPCA    # inital reduction
    from sklearn.manifold import TSNE                   # final reduction
    import numpy as np                                  # array handling


    def reduce_dimensions(model):
        num_dimensions = 2  # final num dimensions (2D, 3D, etc)

        # extract the words & their vectors, as numpy arrays
        vectors = np.asarray(model.wv.vectors)
        labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings

        # reduce using t-SNE
        tsne = TSNE(n_components=num_dimensions, random_state=0)
        vectors = tsne.fit_transform(vectors)

        x_vals = [v[0] for v in vectors]
        y_vals = [v[1] for v in vectors]
        return x_vals, y_vals, labels


    x_vals, y_vals, labels = reduce_dimensions(model)

    def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):
        from plotly.offline import init_notebook_mode, iplot, plot
        import plotly.graph_objs as go

        trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)
        data = [trace]

        if plot_in_notebook:
            init_notebook_mode(connected=True)
            iplot(data, filename='word-embedding-plot')
        else:
            plot(data, filename='word-embedding-plot.html')


    def plot_with_matplotlib(x_vals, y_vals, labels):
        import matplotlib.pyplot as plt
        import random

        random.seed(0)

        plt.figure(figsize=(12, 12))
        plt.scatter(x_vals, y_vals)

        #
        # Label randomly subsampled 25 data points
        #
        indices = list(range(len(labels)))
        selected_indices = random.sample(indices, 25)
        for i in selected_indices:
            plt.annotate(labels[i], (x_vals[i], y_vals[i]))

    try:
        get_ipython()
    except Exception:
        plot_function = plot_with_matplotlib
    else:
        plot_function = plot_with_plotly

    plot_function(x_vals, y_vals, labels)




.. image:: /auto_examples/tutorials/images/sphx_glr_run_word2vec_001.png
    :alt: run word2vec
    :class: sphx-glr-single-img





Conclusion
----------

In this tutorial we learned how to train word2vec models on your custom data
and also how to evaluate it. Hope that you too will find this popular tool
useful in your Machine Learning tasks!

Links
-----

- API docs: :py:mod:`gensim.models.word2vec`
- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 11 minutes  46.634 seconds)

**Estimated memory usage:**  6399 MB


.. _sphx_glr_download_auto_examples_tutorials_run_word2vec.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_word2vec.py <run_word2vec.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_word2vec.ipynb <run_word2vec.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
