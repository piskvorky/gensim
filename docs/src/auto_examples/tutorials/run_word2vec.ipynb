{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nWord2Vec Model\n==============\n\nIntroduces Gensim's Word2Vec model and demonstrates its use on the Lee Corpus.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you missed the buzz, word2vec is a widely featured as a member of the\n\u201cnew wave\u201d of machine learning algorithms based on neural networks, commonly\nreferred to as \"deep learning\" (though word2vec itself is rather shallow).\nUsing large amounts of unannotated plain text, word2vec learns relationships\nbetween words automatically. The output are vectors, one vector per word,\nwith remarkable linear relationships that allow us to do things like:\n\n* vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n* vec(\"Montreal Canadiens\") \u2013 vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\").\n\nWord2vec is very useful in `automatic text tagging\n<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\\ , recommender\nsystems and machine translation.\n\nThis tutorial:\n\n#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n#. Shows off a demo of ``Word2Vec`` using a pre-trained model\n#. Demonstrates training a new model from your own data\n#. Demonstrates loading and saving models\n#. Introduces several training parameters and demonstrates their effect\n#. Discusses memory requirements\n#. Visualizes Word2Vec embeddings by applying dimensionality reduction\n\nReview: Bag-of-words\n--------------------\n\n.. Note:: Feel free to skip these review sections if you're already familiar with the models.\n\nYou may be familiar with the `bag-of-words model\n<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n`core_concepts_vector` section.\nThis model transforms each document to a fixed-length vector of integers.\nFor example, given the sentences:\n\n- ``John likes to watch movies. Mary likes movies too.``\n- ``John also likes to watch football games. Mary hates football.``\n\nThe model outputs the vectors:\n\n- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n\nEach vector has 10 elements, where each element counts the number of times a\nparticular word occurred in the document.\nThe order of elements is arbitrary.\nIn the example above, the order of the elements corresponds to the words:\n``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n\nBag-of-words models are surprisingly effective, but have several weaknesses.\n\nFirst, they lose all information about word order: \"John likes Mary\" and\n\"Mary likes John\" correspond to identical vectors. There is a solution: bag\nof `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\nmodels consider word phrases of length n to represent documents as\nfixed-length vectors to capture local word order but suffer from data\nsparsity and high dimensionality.\n\nSecond, the model does not attempt to learn the meaning of the underlying\nwords, and as a consequence, the distance between vectors doesn't always\nreflect the difference in meaning.  The ``Word2Vec`` model addresses this\nsecond problem.\n\nIntroducing: the ``Word2Vec`` Model\n-----------------------------------\n\n``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\nvector space using a shallow neural network. The result is a set of\nword-vectors where vectors close together in vector space have similar\nmeanings based on context, and word-vectors distant to each other have\ndiffering meanings. For example, ``strong`` and ``powerful`` would be close\ntogether and ``strong`` and ``Paris`` would be relatively far.\n\nThe are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\nclass implements them both:\n\n1. Skip-grams (SG)\n2. Continuous-bag-of-words (CBOW)\n\n.. Important::\n  Don't let the implementation details below scare you.\n  They're advanced material: if it's too much, then move on to the next section.\n\nThe `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__\nmodel, for example, takes in pairs (word1, word2) generated by moving a\nwindow across text data, and trains a 1-hidden-layer neural network based on\nthe synthetic task of given an input word, giving us a predicted probability\ndistribution of nearby words to the input. A virtual `one-hot\n<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words\ngoes through a 'projection layer' to the hidden layer; these projection\nweights are later interpreted as the word embeddings. So if the hidden layer\nhas 300 neurons, this network will give us 300-dimensional word embeddings.\n\nContinuous-bag-of-words Word2vec is very similar to the skip-gram model. It\nis also a 1-hidden-layer neural network. The synthetic training task now uses\nthe average of multiple input context words, rather than a single word as in\nskip-gram, to predict the center word. Again, the projection weights that\nturn one-hot words into averageable vectors, of the same width as the hidden\nlayer, are interpreted as the word embeddings.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec Demo\n-------------\n\nTo see what ``Word2Vec`` can do, let's download a pre-trained model and play\naround with it. We will fetch the Word2Vec model trained on part of the\nGoogle News dataset, covering approximately 3 million words and phrases. Such\na model can take hours to train, but since it's already available,\ndownloading and loading it with Gensim takes minutes.\n\n.. Important::\n  The model is approximately 2GB, so you'll need a decent network connection\n  to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n  below.\n\nYou may also check out an `online word2vec demo\n<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try\nthis vector algebra for yourself. That demo runs ``word2vec`` on the\n**entire** Google News dataset, of **about 100 billion words**.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\nwv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A common operation is to retrieve the vocabulary of a model.  That is trivial:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, word in enumerate(wv.vocab):\n    if i == 10:\n        break\n    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily obtain vectors for terms the model is familiar with:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vec_king = wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, the model is unable to infer vectors for unfamiliar words.\nThis is one limitation of Word2Vec: if this limitation matters to you, check\nout the FastText model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    vec_cameroon = wv['cameroon']\nexcept KeyError:\n    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving on, ``Word2Vec`` supports several word similarity tasks out of the\nbox.  You can see how the similarity intuitively decreases as the words get\nless and less similar.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pairs = [\n    ('car', 'minivan'),   # a minivan is a kind of car\n    ('car', 'bicycle'),   # still a wheeled vehicle\n    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n    ('car', 'cereal'),    # ... and so on\n    ('car', 'communism'),\n]\nfor w1, w2 in pairs:\n    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the 5 most similar words to \"car\" or \"minivan\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which of the below does not belong in the sequence?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Your Own Model\n-----------------------\n\nTo start, you'll need some data for training the model.  For the following\nexamples, we'll use the `Lee Corpus\n<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_\n(which you already have if you've installed gensim).\n\nThis corpus is small enough to fit entirely in memory, but we'll implement a\nmemory-friendly iterator that reads it line-by-line to demonstrate how you\nwould handle a larger corpus.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\nfrom gensim import utils\n\nclass MyCorpus(object):\n    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n\n    def __iter__(self):\n        corpus_path = datapath('lee_background.cor')\n        for line in open(corpus_path):\n            # assume there's one document per line, tokens separated by whitespace\n            yield utils.simple_preprocess(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wanted to do any custom preprocessing, e.g. decode a non-standard\nencoding, lowercase, remove numbers, extract named entities... All of this can\nbe done inside the ``MyCorpus`` iterator and ``word2vec`` doesn\u2019t need to\nknow. All that is required is that the input yields one sentence (list of\nutf8 words) after another.\n\nLet's go ahead and train a model on our corpus.  Don't worry about the\ntraining parameters much for now, we'll revisit them later.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gensim.models\n\nsentences = MyCorpus()\nmodel = gensim.models.Word2Vec(sentences=sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have our model, we can use it in the same way as in the demo above.\n\nThe main part of the model is ``model.wv``\\ , where \"wv\" stands for \"word vectors\".\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vec_king = model.wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieving the vocabulary works the same way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, word in enumerate(model.wv.vocab):\n    if i == 10:\n        break\n    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing and loading models\n--------------------------\n\nYou'll notice that training non-trivial models can take time.  Once you've\ntrained your model and it works as expected, you can save it to disk.  That\nway, you don't have to spend time training it all over again later.\n\nYou can store/load models using the standard gensim methods:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile\n\nwith tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n    temporary_filepath = tmp.name\n    model.save(temporary_filepath)\n    #\n    # The model is now safely stored in the filepath.\n    # You can copy it to other machines, share it with others, etc.\n    #\n    # To load a saved model:\n    #\n    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "which uses pickle internally, optionally ``mmap``\\ \u2018ing the model\u2019s internal\nlarge NumPy matrices into virtual memory directly from disk files, for\ninter-process memory sharing.\n\nIn addition, you can load models created by the original C tool, both using\nits text and binary formats::\n\n  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n  # using gzipped/bz2 input works too, no need to unzip\n  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Parameters\n-------------------\n\n``Word2Vec`` accepts several parameters that affect both training speed and quality.\n\nmin_count\n---------\n\n``min_count`` is for pruning the internal dictionary. Words that appear only\nonce or twice in a billion-word corpus are probably uninteresting typos and\ngarbage. In addition, there\u2019s not enough data to make any meaningful training\non those words, so it\u2019s best to ignore them:\n\ndefault value of min_count=5\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec(sentences, min_count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "size\n----\n\n``size`` is the number of dimensions (N) of the N-dimensional space that\ngensim Word2Vec maps the words onto.\n\nBigger size values require more training data, but can lead to better (more\naccurate) models. Reasonable values are in the tens to hundreds.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# default value of size=100\nmodel = gensim.models.Word2Vec(sentences, size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "workers\n-------\n\n``workers`` , the last of the major parameters (full list `here\n<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)\nis for training parallelization, to speed up training:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# default value of workers=3 (tutorial says 1...)\nmodel = gensim.models.Word2Vec(sentences, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``workers`` parameter only has an effect if you have `Cython\n<http://cython.org/>`_ installed. Without Cython, you\u2019ll only be able to use\none core because of the `GIL\n<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``\ntraining will be `miserably slow\n<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\\ ).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory\n------\n\nAt its core, ``word2vec`` model parameters are stored as matrices (NumPy\narrays). Each array is **#vocabulary** (controlled by min_count parameter)\ntimes **#size** (size parameter) of floats (single precision aka 4 bytes).\n\nThree such matrices are held in RAM (work is underway to reduce that number\nto two, or even one). So if your input contains 100,000 unique words, and you\nasked for layer ``size=200``\\ , the model will require approx.\n``100,000*200*4*3 bytes = ~229MB``.\n\nThere\u2019s a little extra memory needed for storing the vocabulary tree (100,000 words would take a few megabytes), but unless your words are extremely loooong strings, memory footprint will be dominated by the three matrices above.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating\n----------\n\n``Word2Vec`` training is an unsupervised task, there\u2019s no good way to\nobjectively evaluate the result. Evaluation depends on your end application.\n\nGoogle has released their testing set of about 20,000 syntactic and semantic\ntest examples, following the \u201cA is to B as C is to D\u201d task. It is provided in\nthe 'datasets' folder.\n\nFor example a syntactic analogy of comparative type is bad:worse;good:?.\nThere are total of 9 types of syntactic comparisons in the dataset like\nplural nouns and nouns of opposite meaning.\n\nThe semantic questions contain five types of semantic analogies, such as\ncapital cities (Paris:France;Tokyo:?) or family members\n(brother:sister;dad:?).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gensim supports the same evaluation set, in exactly the same format:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.accuracy('./datasets/questions-words.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``accuracy`` takes an `optional parameter\n<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.accuracy>`_\n``restrict_vocab`` which limits which test examples are to be considered.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n\nBy default it uses an academic dataset WS-353 but one can create a dataset\nspecific to your business based on it. It contains word pairs together with\nhuman-assigned similarity judgments. It measures the relatedness or\nco-occurrence of two words. For example, 'coast' and 'shore' are very similar\nas they appear in the same context. At the same time 'clothes' and 'closet'\nare less similar because they are related but not interchangeable.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.evaluate_word_pairs(datapath('wordsim353.tsv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Important::\n  Good performance on Google's or WS-353 test set doesn\u2019t mean word2vec will\n  work well in your application, or vice versa. It\u2019s always best to evaluate\n  directly on your intended task. For an example of how to use word2vec in a\n  classifier pipeline, see this `tutorial\n  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Online training / Resuming training\n-----------------------------------\n\nAdvanced users can load a model and continue training it with more sentences\nand `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load(temporary_filepath)\nmore_sentences = [\n    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n]\nmodel.build_vocab(more_sentences, update=True)\nmodel.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n\n# cleaning up temporary file\nimport os\nos.remove(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may need to tweak the ``total_words`` parameter to ``train()``,\ndepending on what learning rate decay you want to simulate.\n\nNote that it\u2019s not possible to resume training with models generated by the C\ntool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\nquerying/similarity, but information vital for training (the vocab tree) is\nmissing there.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loss Computation\n-------------------------\n\nThe parameter ``compute_loss`` can be used to toggle computation of loss\nwhile training the Word2Vec model. The computed loss is stored in the model\nattribute ``running_training_loss`` and can be retrieved using the function\n``get_latest_training_loss`` as follows :\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# instantiating and training the Word2Vec model\nmodel_with_loss = gensim.models.Word2Vec(\n    sentences,\n    min_count=1,\n    compute_loss=True,\n    hs=0,\n    sg=1,\n    seed=42\n)\n\n# getting the training loss value\ntraining_loss = model_with_loss.get_latest_training_loss()\nprint(training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks\n----------\n\nLet's run some benchmarks to see effect of the training loss computation code\non training time.\n\nWe'll use the following data for the benchmarks:\n\n#. Lee Background corpus: included in gensim's test data\n#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the\n   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import io\nimport os\n\nimport gensim.models.word2vec\nimport gensim.downloader as api\nimport smart_open\n\n\ndef head(path, size):\n    with smart_open.open(path) as fin:\n        return io.StringIO(fin.read(size))\n\n\ndef generate_input_data():\n    lee_path = datapath('lee_background.cor')\n    ls = gensim.models.word2vec.LineSentence(lee_path)\n    ls.name = '25kB'\n    yield ls\n\n    text8_path = api.load('text8').fn\n    labels = ('1MB', '10MB', '50MB', '100MB')\n    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n    for l, s in zip(labels, sizes):\n        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n        ls.name = l\n        yield ls\n\n\ninput_data = list(generate_input_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compare the training time taken for different combinations of input\ndata and model training parameters like ``hs`` and ``sg``.\n\nFor each combination, we repeat the test several times to obtain the mean and\nstandard deviation of the test duration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Temporarily reduce logging verbosity\nlogging.root.level = logging.ERROR\n\nimport time\nimport numpy as np\nimport pandas as pd\n\ntrain_time_values = []\nseed_val = 42\nsg_values = [0, 1]\nhs_values = [0, 1]\n\nfast = True\nif fast:\n    input_data_subset = input_data[:3]\nelse:\n    input_data_subset = input_data\n\n\nfor data in input_data_subset:\n    for sg_val in sg_values:\n        for hs_val in hs_values:\n            for loss_flag in [True, False]:\n                time_taken_list = []\n                for i in range(3):\n                    start_time = time.time()\n                    w2v_model = gensim.models.Word2Vec(\n                        data,\n                        compute_loss=loss_flag,\n                        sg=sg_val,\n                        hs=hs_val,\n                        seed=seed_val,\n                    )\n                    time_taken_list.append(time.time() - start_time)\n\n                time_taken_list = np.array(time_taken_list)\n                time_mean = np.mean(time_taken_list)\n                time_std = np.std(time_taken_list)\n\n                model_result = {\n                    'train_data': data.name,\n                    'compute_loss': loss_flag,\n                    'sg': sg_val,\n                    'hs': hs_val,\n                    'train_time_mean': time_mean,\n                    'train_time_std': time_std,\n                }\n                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n                train_time_values.append(model_result)\n\ntrain_times_table = pd.DataFrame(train_time_values)\ntrain_times_table = train_times_table.sort_values(\n    by=['train_data', 'sg', 'hs', 'compute_loss'],\n    ascending=[False, False, True, False],\n)\nprint(train_times_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding Word2Vec \"model to dict\" method to production pipeline\n-------------------------------------------------------------\n\nSuppose, we still want more performance improvement in production.\n\nOne good way is to cache all the similar words in a dictionary.\n\nSo that next time when we get the similar query word, we'll search it first in the dict.\n\nAnd if it's a hit then we will show the result directly from the dictionary.\n\notherwise we will query the word and then cache it so that it doesn't miss next time.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# re-enable logging\nlogging.root.level = logging.INFO\n\nmost_similars_precalc = {word : model.wv.most_similar(word) for word in model.wv.index2word}\nfor i, (key, value) in enumerate(most_similars_precalc.items()):\n    if i == 3:\n        break\n    print(key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparison with and without caching\n-----------------------------------\n\nfor time being lets take 4 words randomly\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nwords = ['voted', 'few', 'their', 'around']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Without caching\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start = time.time()\nfor word in words:\n    result = model.wv.most_similar(word)\n    print(result)\nend = time.time()\nprint(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now with caching\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start = time.time()\nfor word in words:\n    if 'voted' in most_similars_precalc:\n        result = most_similars_precalc[word]\n        print(result)\n    else:\n        result = model.wv.most_similar(word)\n        most_similars_precalc[word] = result\n        print(result)\n\nend = time.time()\nprint(end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly you can see the improvement but this difference will be even larger\nwhen we take more words in the consideration.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualising the Word Embeddings\n-------------------------------\n\nThe word embeddings made by the model can be visualised by reducing\ndimensionality of the words to 2 dimensions using tSNE.\n\nVisualisations can be used to notice semantic and syntactic trends in the data.\n\nExample:\n\n* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n* Syntactic: words like run, running or cut, cutting lie close together.\n\nVector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n\n.. Important::\n  The model used for the visualisation is trained on a small corpus. Thus\n  some of the relations might not be so clear.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\nfrom sklearn.manifold import TSNE                   # final reduction\nimport numpy as np                                  # array handling\n\n\ndef reduce_dimensions(model):\n    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n\n    vectors = [] # positions in vector space\n    labels = [] # keep track of words to label our data again later\n    for word in model.wv.vocab:\n        vectors.append(model.wv[word])\n        labels.append(word)\n\n    # convert both lists into numpy vectors for reduction\n    vectors = np.asarray(vectors)\n    labels = np.asarray(labels)\n\n    # reduce using t-SNE\n    vectors = np.asarray(vectors)\n    tsne = TSNE(n_components=num_dimensions, random_state=0)\n    vectors = tsne.fit_transform(vectors)\n\n    x_vals = [v[0] for v in vectors]\n    y_vals = [v[1] for v in vectors]\n    return x_vals, y_vals, labels\n\n\nx_vals, y_vals, labels = reduce_dimensions(model)\n\ndef plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n    from plotly.offline import init_notebook_mode, iplot, plot\n    import plotly.graph_objs as go\n\n    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n    data = [trace]\n\n    if plot_in_notebook:\n        init_notebook_mode(connected=True)\n        iplot(data, filename='word-embedding-plot')\n    else:\n        plot(data, filename='word-embedding-plot.html')\n\n\ndef plot_with_matplotlib(x_vals, y_vals, labels):\n    import matplotlib.pyplot as plt\n    import random\n\n    random.seed(0)\n\n    plt.figure(figsize=(12, 12))\n    plt.scatter(x_vals, y_vals)\n\n    #\n    # Label randomly subsampled 25 data points\n    #\n    indices = list(range(len(labels)))\n    selected_indices = random.sample(indices, 25)\n    for i in selected_indices:\n        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n\ntry:\n    get_ipython()\nexcept Exception:\n    plot_function = plot_with_matplotlib\nelse:\n    plot_function = plot_with_plotly\n\nplot_function(x_vals, y_vals, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nIn this tutorial we learned how to train word2vec models on your custom data\nand also how to evaluate it. Hope that you too will find this popular tool\nuseful in your Machine Learning tasks!\n\nLinks\n-----\n\n- API docs: :py:mod:`gensim.models.word2vec`\n- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}