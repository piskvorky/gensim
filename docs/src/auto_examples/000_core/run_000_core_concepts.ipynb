{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nCore Concepts\n=============\n\nThis tutorial introduces the basic concepts and terms needed to understand and use ``gensim`` and provides a simple usage example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At a very high-level, ``gensim`` is a tool for discovering the semantic\nstructure of documents by examining the patterns of words (or higher-level\nstructures such as entire sentences or documents). ``gensim`` accomplishes\nthis by taking a *corpus*\\ , a collection of text documents, and producing a\n*vector* representation of the text in the corpus. The vector representation\ncan then be used to train a *model*\\ , which is an algorithms to create\ndifferent representations of the data, which are usually more semantic. These\nthree concepts are key to understanding how ``gensim`` works so let's take a\nmoment to explain what each of them means. At the same time, we'll work\nthrough a simple example that illustrates each of them.\n\nCorpus\n^^^^^^\n\nA *corpus* is a collection of digital documents. This collection is the input\nto ``gensim`` from which it will infer the structure of the documents, their\ntopics, etc. The latent structure inferred from the corpus can later be used\nto assign topics to new documents which were not present in the training\ncorpus. For this reason, we also refer to this collection as the *training\ncorpus*. No human intervention (such as tagging the documents by hand) is\nrequired - the topic classification is `unsupervised\n<https://en.wikipedia.org/wiki/Unsupervised_learning>`_.\n\nFor our corpus, we'll use a list of 9 strings, each consisting of only a single sentence.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_corpus = [\n    \"Human machine interface for lab abc computer applications\",\n    \"A survey of user opinion of computer system response time\",\n    \"The EPS user interface management system\",\n    \"System and human system engineering testing of EPS\",\n    \"Relation of user perceived response time to error measurement\",\n    \"The generation of random binary unordered trees\",\n    \"The intersection graph of paths in trees\",\n    \"Graph minors IV Widths of trees and well quasi ordering\",\n    \"Graph minors A survey\",\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a particularly small example of a corpus for illustration purposes.\nAnother example could be a list of all the plays written by Shakespeare, list\nof all wikipedia articles, or all tweets by a particular person of interest.\n\nAfter collecting our corpus, there are typically a number of preprocessing\nsteps we want to undertake. We'll keep it simple and just remove some\ncommonly used English words (such as 'the') and words that occur only once in\nthe corpus. In the process of doing so, we'll [tokenise][1] our data.\nTokenization breaks up the documents into words (in this case using space as\na delimiter).\n\nCreate a set of frequent words\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stoplist = set('for a of the and to in'.split(' '))\n# Lowercase each document, split it by white space and filter out stopwords\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in raw_corpus]\n\n# Count word frequencies\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\n# Only keep words that appear more than once\nprocessed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\nprocessed_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before proceeding, we want to associate each word in the corpus with a unique\ninteger ID. We can do this using the ``gensim.corpora.Dictionary`` class.\nThis dictionary defines the vocabulary of all words that our processing knows\nabout.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n\ndictionary = corpora.Dictionary(processed_corpus)\nprint(dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because our corpus is small, there are only 12 different tokens in this\n``Dictionary``. For larger corpuses, dictionaries that contains hundreds of\nthousands of tokens are quite common.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vector\n^^^^^^\n\nTo infer the latent structure in our corpus we need a way to represent\ndocuments that we can manipulate mathematically. One approach is to represent\neach document as a vector. There are various approaches for creating a vector\nrepresentation of a document but a simple example is the *bag-of-words\nmodel*. Under the bag-of-words model each document is represented by a vector\ncontaining the frequency counts of each word in the dictionary. For example,\ngiven a dictionary containing the words ``['coffee', 'milk', 'sugar',\n'spoon']`` a document consisting of the string ``\"coffee milk coffee\"`` could\nbe represented by the vector ``[2, 1, 0, 0]`` where the entries of the vector\nare (in order) the occurrences of \"coffee\", \"milk\", \"sugar\" and \"spoon\" in\nthe document. The length of the vector is the number of entries in the\ndictionary. One of the main properties of the bag-of-words model is that it\ncompletely ignores the order of the tokens in the document that is encoded,\nwhich is where the name bag-of-words comes from.\n\nOur processed corpus has 12 unique words in it, which means that each\ndocument will be represented by a 12-dimensional vector under the\nbag-of-words model. We can use the dictionary to turn tokenized documents\ninto these 12-dimensional vectors. We can see what these IDs correspond to:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(dictionary.token2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, suppose we wanted to vectorize the phrase \"Human computer\ninteraction\" (note that this phrase was not in our original corpus). We can\ncreate the bag-of-word representation for a document using the ``doc2bow``\nmethod of the dictionary, which returns a sparse representation of the word\ncounts:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_doc = \"Human computer interaction\"\nnew_vec = dictionary.doc2bow(new_doc.lower().split())\nnew_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first entry in each tuple corresponds to the ID of the token in the\ndictionary, the second corresponds to the count of this token.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that \"interaction\" did not occur in the original corpus and so it was\nnot included in the vectorization. Also note that this vector only contains\nentries for words that actually appeared in the document. Because any given\ndocument will only contain a few words out of the many words in the\ndictionary, words that do not appear in the vectorization are represented as\nimplicitly zero as a space saving measure.\n\nWe can convert our entire original corpus to a list of vectors:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\nbow_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that while this list lives entirely in memory, in most applications you\nwill want a more scalable solution. Luckily, ``gensim`` allows you to use any\niterator that returns a single document vector at a time. See the\ndocumentation for more details.\n\nModel\n^^^^^\n\nNow that we have vectorized our corpus we can begin to transform it using\n*models*. We use model as an abstract term referring to a transformation from\none document representation to another. In ``gensim`` documents are\nrepresented as vectors so a model can be thought of as a transformation\nbetween two vector spaces. The details of this transformation are learned\nfrom the training corpus.\n\nOne simple example of a model is `tf-idf\n<https://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_.  The tf-idf model\ntransforms vectors from the bag-of-words representation to a vector space\nwhere the frequency counts are weighted according to the relative rarity of\neach word in the corpus.\n\nHere's a simple example. Let's initialize the tf-idf model, training it on\nour corpus and transforming the string \"system minors\":\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim import models\n# train the model\ntfidf = models.TfidfModel(bow_corpus)\n# transform the \"system minors\" string\ntfidf[dictionary.doc2bow(\"system minors\".lower().split())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``tfidf`` model again returns a list of tuples, where the first entry is\nthe token ID and the second entry is the tf-idf weighting. Note that the ID\ncorresponding to \"system\" (which occurred 4 times in the original corpus) has\nbeen weighted lower than the ID corresponding to \"minors\" (which only\noccurred twice).\n\n``gensim`` offers a number of different models/transformations. See\n`Transformations and Topics <Topics_and_Transformations.ipynb>`_ for details.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}