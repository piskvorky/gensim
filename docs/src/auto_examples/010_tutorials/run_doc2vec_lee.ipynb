{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nDoc2Vec Model\n=============\n\nIntroduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Doc2Vec is an NLP tool for representing documents as a vector and is a\ngeneralizing of the Word2Vec method.  This tutorial will serve as an\nintroduction to Doc2Vec and present ways to train and assess a Doc2Vec model.\n\nThis tutorial will take you through the following steps:\n\n1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n4. Assess the model\n5. Test the model on the test corpus\n\nGetting Started\n---------------\n\nTo get going, we'll need to have a set of documents to train our doc2vec\nmodel. In theory, a document could be anything from a short 140 character\ntweet, a single paragraph (i.e., journal article abstract), a news article,\nor a book. In NLP parlance a collection or set of documents is often referred\nto as a **corpus**. \n\nFor this tutorial, we'll be training our model using the `Lee Background\nCorpus\n<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\nincluded in gensim. This corpus contains 314 documents selected from the\nAustralian Broadcasting Corporation\u2019s news mail service, which provides text\ne-mails of headline stories and covers a number of broad topics.\n\nAnd we'll test our model by eye using the much shorter `Lee Corpus\n<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\nwhich contains 50 documents.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport gensim\n# Set file names for train and test data\ntest_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\nlee_train_file = test_data_dir + os.sep + 'lee_background.cor'\nlee_test_file = test_data_dir + os.sep + 'lee.cor'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a Function to Read and Preprocess Text\n---------------------------------------------\n\nBelow, we define a function to open the train/test file (with latin\nencoding), read the file line-by-line, pre-process each line using a simple\ngensim pre-processing tool (i.e., tokenize text into individual words, remove\npunctuation, set to lowercase, etc), and return a list of words. Note that,\nfor a given file (aka corpus), each continuous line constitutes a single\ndocument and the length of each line (i.e., document) can vary. Also, to\ntrain the model, we'll need to associate a tag/number with each document of\nthe training corpus. In our case, the tag is simply the zero-based line\nnumber.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import smart_open\n\ndef read_corpus(fname, tokens_only=False):\n    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n        for i, line in enumerate(f):\n            if tokens_only:\n                yield gensim.utils.simple_preprocess(line)\n            else:\n                # For training data, add tags\n                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n\ntrain_corpus = list(read_corpus(lee_train_file))\ntest_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the training corpus\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(train_corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the testing corpus looks like this:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(test_corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the testing corpus is just a list of lists and does not contain\nany tags.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the Model\n------------------\n\nNow, we'll instantiate a Doc2Vec model with a vector size with 50 words and\niterating over the training corpus 40 times. We set the minimum word count to\n2 in order to discard words with very few occurrences. (Without a variety of\nrepresentative examples, retaining such infrequent words can often make a\nmodel worse!) Typical iteration counts in published 'Paragraph Vectors'\nresults, using 10s-of-thousands to millions of docs, are 10-20. More\niterations take more time and eventually reach a point of diminishing\nreturns.\n\nHowever, this is a very very small dataset (300 documents) with shortish\ndocuments (a few hundred words). Adding training passes can sometimes help\nwith such small datasets.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a vocabulary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essentially, the vocabulary is a dictionary (accessible via\n``model.wv.vocab``\\ ) of all of the unique words extracted from the training\ncorpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\ncounts for the word ``penalty``\\ ).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, train the model on the corpus.\nIf the BLAS library is being used, this should take no more than 3 seconds.\nIf the BLAS library is not being used, this should take no more than 2\nminutes, so use BLAS if you value your time.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can use the trained model to infer a vector for any piece of text\nby passing a list of words to the ``model.infer_vector`` function. This\nvector can then be compared with other vectors via cosine similarity.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\nprint(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that ``infer_vector()`` does *not* take a string, but rather a list of\nstring tokens, which should have already been tokenized the same way as the\n``words`` property of original training document objects. \n\nAlso note that because the underlying training/inference algorithms are an\niterative approximation problem that makes use of internal randomization,\nrepeated inferences of the same text will return slightly different vectors.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assessing Model\n---------------\n\nTo assess our new model, we'll first infer new vectors for each document of\nthe training corpus, compare the inferred vectors with the training corpus,\nand then returning the rank of the document based on self-similarity.\nBasically, we're pretending as if the training corpus is some new unseen data\nand then seeing how they compare with the trained model. The expectation is\nthat we've likely overfit our model (i.e., all of the ranks will be less than\n2) and so we should be able to find similar documents very easily.\nAdditionally, we'll keep track of the second ranks for a comparison of less\nsimilar documents. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ranks = []\nsecond_ranks = []\nfor doc_id in range(len(train_corpus)):\n    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n    rank = [docid for docid, sim in sims].index(doc_id)\n    ranks.append(rank)\n    \n    second_ranks.append(sims[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's count how each document ranks with respect to the training corpus \n\nNB. Results vary between runs due to random seeding and very small corpus\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import collections\n\ncounter = collections.Counter(ranks)\nprint(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basically, greater than 95% of the inferred documents are found to be most\nsimilar to itself and about 5% of the time it is mistakenly most similar to\nanother document. the checking of an inferred-vector against a\ntraining-vector is a sort of 'sanity check' as to whether the model is\nbehaving in a usefully consistent manner, though not a real 'accuracy' value.\n\nThis is great and not entirely surprising. We can take a look at an example:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Document ({}): \u00ab{}\u00bb\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: \u00ab%s\u00bb\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice above that the most similar document (usually the same text) is has a\nsimilarity score approaching 1.0. However, the similarity score for the\nsecond-ranked documents should be significantly lower (assuming the documents\nare in fact different) and the reasoning becomes obvious when we examine the\ntext itself.\n\nWe can run the next cell repeatedly to see a sampling other target-document\ncomparisons. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Pick a random document from the corpus and infer a vector from the model\nimport random\ndoc_id = random.randint(0, len(train_corpus) - 1)\n\n# Compare and print the second-most-similar document\nprint('Train Document ({}): \u00ab{}\u00bb\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\nsim_id = second_ranks[doc_id]\nprint('Similar Document {}: \u00ab{}\u00bb\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the Model\n-----------------\n\nUsing the same approach above, we'll infer the vector for a randomly chosen\ntest document, and compare the document to our model by eye.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Pick a random document from the test corpus and infer a vector from the model\ndoc_id = random.randint(0, len(test_corpus) - 1)\ninferred_vector = model.infer_vector(test_corpus[doc_id])\nsims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n\n# Compare and print the most/median/least similar documents from the train corpus\nprint('Test Document ({}): \u00ab{}\u00bb\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\nprint(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\nfor label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n    print(u'%s %s: \u00ab%s\u00bb\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nLet's review what we've seen in this tutorial:\n\n1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n4. Assess the model\n5. Test the model on the test corpus\n\nThat's it! Doc2Vec is a great way to explore relationships between documents.\n\nAdditional Resources\n--------------------\n\nIf you'd like to know more about the subject matter of this tutorial, check out the links below.\n\n* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}