{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nText Summarization\n==================\n\nDemonstrates summarizing text by extracting the most important sentences from it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This module automatically summarizes the given text, by extracting one or\nmore important sentences from the text. In a similar way, it can also extract\nkeywords. This tutorial will teach you to use this summarization module via\nsome examples. First, we will try a small example, then we will try two\nlarger ones, and then we will review the performance of the summarizer in\nterms of speed.\n\nThis summarizer is based on the , from an `\"TextRank\" algorithm by Mihalcea\net al <http://web.eecs.umich.edu/%7Emihalcea/papers/mihalcea.emnlp04.pdf>`_.\nThis algorithm was later improved upon by `Barrios et al.\n<https://raw.githubusercontent.com/summanlp/docs/master/articulo/articulo-en.pdf>`_,\nby introducing something called a \"BM25 ranking function\". \n\n.. important::\n    Gensim's summarization only works for English for now, because the text\n    is pre-processed so that stopwords are removed and the words are stemmed,\n    and these processes are language-dependent.\n\nSmall example\n-------------\n\nFirst of all, we import the :py:func:`gensim.summarization.summarize` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint as print\nfrom gensim.summarization import summarize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will try summarizing a small toy example; later we will use a larger piece of text. In reality, the text is too small, but it suffices as an illustrative example.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text = (\n    \"Thomas A. Anderson is a man living two lives. By day he is an \"\n    \"average computer programmer and by night a hacker known as \"\n    \"Neo. Neo has always questioned his reality, but the truth is \"\n    \"far beyond his imagination. Neo finds himself targeted by the \"\n    \"police when he is contacted by Morpheus, a legendary computer \"\n    \"hacker branded a terrorist by the government. Morpheus awakens \"\n    \"Neo to the real world, a ravaged wasteland where most of \"\n    \"humanity have been captured by a race of machines that live \"\n    \"off of the humans' body heat and electrochemical energy and \"\n    \"who imprison their minds within an artificial reality known as \"\n    \"the Matrix. As a rebel against the machines, Neo must return to \"\n    \"the Matrix and confront the agents: super-powerful computer \"\n    \"programs devoted to snuffing out Neo and the entire human \"\n    \"rebellion. \"\n)\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To summarize this text, we pass the **raw string data** as input to the\nfunction \"summarize\", and it will return a summary.\n\nNote: make sure that the string does not contain any newlines where the line\nbreaks in a sentence. A sentence with a newline in it (i.e. a carriage\nreturn, \"\\n\") will be treated as two sentences.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(summarize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the \"split\" option if you want a list of strings instead of a single string.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(summarize(text, split=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can adjust how much text the summarizer outputs via the \"ratio\" parameter\nor the \"word_count\" parameter. Using the \"ratio\" parameter, you specify what\nfraction of sentences in the original text should be returned as output.\nBelow we specify that we want 50% of the original text (the default is 20%).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(summarize(text, ratio=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the \"word_count\" parameter, we specify the maximum amount of words we\nwant in the summary. Below we have specified that we want no more than 50\nwords.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(summarize(text, word_count=50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned earlier, this module also supports **keyword** extraction.\nKeyword extraction works in the same way as summary generation (i.e. sentence\nextraction), in that the algorithm tries to find words that are important or\nseem representative of the entire text. They keywords are not always single\nwords; in the case of multi-word keywords, they are typically all nouns.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.summarization import keywords\nprint(keywords(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Larger example\n--------------\n\nLet us try an example with a larger piece of text. We will be using a\nsynopsis of the movie \"The Matrix\", which we have taken from `this\n<http://www.imdb.com/title/tt0133093/synopsis?ref_=ttpl_pl_syn>`_ IMDb page.\n\nIn the code below, we read the text file directly from a web-page using\n\"requests\". Then we produce a summary and some keywords.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import requests\n\ntext = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text\nprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, the summary\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(summarize(text, ratio=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, the keywords:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(keywords(text, ratio=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you know this movie, you see that this summary is actually quite good. We\nalso see that some of the most important characters (Neo, Morpheus, Trinity)\nwere extracted as keywords.\n\nAnother example\n---------------\n\nLet's try an example similar to the one above. This time, we will use the IMDb synopsis\n`The Big Lebowski <http://www.imdb.com/title/tt0118715/synopsis?ref_=tt_stry_pl>`_.\n\nAgain, we download the text and produce a summary and some keywords.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "text = requests.get('http://rare-technologies.com/the_big_lebowski_synopsis.txt').text\nprint(text)\nprint(summarize(text, ratio=0.01))\nprint(keywords(text, ratio=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time around, the summary is not of high quality, as it does not tell us\nmuch about the movie. In a way, this might not be the algorithms fault,\nrather this text simply doesn't contain one or two sentences that capture the\nessence of the text as in \"The Matrix\" synopsis.\n\nThe keywords, however, managed to find some of the main characters.\n\nPerformance\n-----------\n\nWe will test how the speed of the summarizer scales with the size of the\ndataset. These tests were run on an Intel Core i5 4210U CPU @ 1.70 GHz x 4\nprocessor. Note that the summarizer does **not** support multithreading\n(parallel processing).\n\nThe tests were run on the book \"Honest Abe\" by Alonzo Rothschild. Download\nthe book in plain-text `here <http://www.gutenberg.org/ebooks/49679>`__.\n\nIn the **plot below** , we see the running times together with the sizes of\nthe datasets. To create datasets of different sizes, we have simply taken\nprefixes of text; in other words we take the first **n** characters of the\nbook. The algorithm seems to be **quadratic in time** , so one needs to be\ncareful before plugging a large dataset into the summarizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('summarization_tutorial_plot.png')\nimgplot = plt.imshow(img)\nplt.axis('off')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Text-content dependent running times\n------------------------------------\n\nThe running time is not only dependent on the size of the dataset. For\nexample, summarizing \"The Matrix\" synopsis (about 36,000 characters) takes\nabout 3.1 seconds, while summarizing 35,000 characters of this book takes\nabout 8.5 seconds. So the former is **more than twice as fast**.\n\nOne reason for this difference in running times is the data structure that is\nused. The algorithm represents the data using a graph, where vertices (nodes)\nare sentences, and then constructs weighted edges between the vertices that\nrepresent how the sentences relate to each other. This means that every piece\nof text will have a different graph, thus making the running times different.\nThe size of this data structure is **quadratic in the worst case** (the worst\ncase is when each vertex has an edge to every other vertex).\n\nAnother possible reason for the difference in running times is that the\nproblems converge at different rates, meaning that the error drops slower for\nsome datasets than for others.\n\nMontemurro and Zanette's entropy based keyword extraction algorithm\n-------------------------------------------------------------------\n\n`This paper <https://arxiv.org/abs/0907.1558>`__ describes a technique to\nidentify words that play a significant role in the large-scale structure of a\ntext. These typically correspond to the major themes of the text. The text is\ndivided into blocks of ~1000 words, and the entropy of each word's\ndistribution amongst the blocks is caclulated and compared with the expected\nentropy if the word were distributed randomly.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import requests\nfrom gensim.summarization import mz_keywords\n\ntext=requests.get(\"http://www.gutenberg.org/files/49679/49679-0.txt\").text\nprint(mz_keywords(text,scores=True,threshold=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the algorithm weights the entropy by the overall frequency of the\nword in the document. We can remove this weighting by setting weighted=False\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(mz_keywords(text,scores=True,weighted=False,threshold=1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When this option is used, it is possible to calculate a threshold\nautomatically from the number of blocks\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(mz_keywords(text,scores=True,weighted=False,threshold=\"auto\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The complexity of the algorithm is **O**\\ (\\ *Nw*\\ ), where *N* is the number\nof words in the document and *w* is the number of unique words.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}