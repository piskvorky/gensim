{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nHow to Classify News Articles into Topics\n=========================================\n\nDemonstrates classification of the Lee Corpus using a variety of topic models (LSI, HDP, LDA, etc).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "News article classification is performed on a huge scale by news agencies all\nover the world. We will be looking into how topic modeling can be used to\naccurately classify news articles into different categories such as sports,\ntechnology, politics etc.\n\nThis guide demonstrates training a topic model which can come up with topics\nthat can easily be interpreted by us. On top of assigning a topic to an\narbitrary document, this model can also discover hidden structure in the\ncorpus.\n\nWe will be using the Lee corpus which is a shortened version of the `Lee\nBackground Corpus\n<http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF>`_. The\nshortened version consists of 300 documents selected from the Australian\nBroadcasting Corporation's news mail service. It consists of texts of\nheadline stories from around the year 2000-2001.\n\nWe will examine the following models:\n\n- LSI (Latent Semantic Indexing)\n- HDP (Hierarchical Dirichlet Process)\n- LDA (Latent Dirichlet Allocation)\n- LDA (tweaked with topic coherence to find optimal number of topics) and\n- LDA as LSI with the help of topic coherence metrics\n\nAll of these models are in gensim and can be used easily.  We will start by\ntraining our models on the data, and then compare them against each other.\nFor LDA, we will also use the # topic coherence metrics based on `Exploring\nthe Space of Topic Coherence Measures\n<http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf>`_ by Roder\net al.\n\nAccompanying slides can be found `here <https://speakerdeck.com/dsquareindia/pycon-delhi-lightening>`_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport re\nimport operator\nimport matplotlib.pyplot as plt\nimport warnings\nimport gensim\nimport numpy as np\nwarnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n\nimport nltk\nnltk.download('stopwords') # Let's make sure the 'stopword' package is downloaded & updated\nnltk.download('wordnet') # Let's also download wordnet, which will be used for lemmatization\n\nfrom pprint import pprint\nfrom smart_open import open\n\ntest_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\nlee_train_file = test_data_dir + os.sep + 'lee_background.cor'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysing our corpus.\n\n\n   - The first document talks about a bushfire that had occured in New South Wales.\n   - The second talks about conflict between India and Pakistan in Kashmir.\n   - The third talks about road accidents in the New South Wales area.\n   - The fourth one talks about Argentina's economic and political crisis during that time.\n   - The last one talks about the use of drugs by midwives in a Sydney hospital.\n\nOur final topic model should be giving us keywords which we can easily\ninterpret and make a small summary out of. Without this the topic model\ncannot be of much practical use.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(lee_train_file, 'rb') as f:\n    for n, l in enumerate(f):\n        if n < 5:\n            print(l[:100])\n\ndef build_texts(fname):\n    \"\"\"\n    Function to build tokenized texts from file\n    \n    Parameters:\n    ----------\n    fname: File to be read\n    \n    Returns:\n    -------\n    yields preprocessed line\n    \"\"\"\n    with open(fname, 'rb') as f:\n        for line in f:\n            yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n\ntrain_texts = list(build_texts(lee_train_file))\nprint(len(train_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing our data. Remember: Garbage In Garbage Out\n--------------------------------------------------------\n\nThis is the single most important step in setting up a good topic modeling\nsystem. If the preprocessing is not good, the algorithm can't do much since\nwe would be feeding it a lot of noise. In this tutorial, we will be filtering\nout the noise using the following steps in this order for each line:\n\n#. Stopword removal using NLTK's english stopwords dataset.\n#. Bigram collocation detection (frequently co-occuring tokens) using\n   gensim's `Phrases <https://radimrehurek.com/gensim/models/phrases.html>`_.\n   This is our first attempt to find some hidden structure in the corpus. You\n   can even try trigram collocation detection.\n#. Lemmatization (using :py:func:`gensim.utils.lemmatize`) to\n   only keep the nouns. Lemmatization is generally better than stemming in the\n   case of topic modeling since the words after lemmatization still remain\n   understable. However, generally stemming might be preferred if the data is\n   being fed into a vectorizer and isn't intended to be viewed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bigram = gensim.models.Phrases(train_texts)  # for bigram collocation detection\n\nbigram[['new', 'york', 'example']]\n\nfrom gensim.utils import lemmatize\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words('english'))  # nltk stopwords list\n\ndef process_texts(texts):\n    \"\"\"\n    Function to process texts. Following are the steps we take:\n    \n    1. Stopword Removal.\n    2. Collocation detection.\n    3. Lemmatization (not stem since stemming can reduce the interpretability).\n    \n    Parameters:\n    ----------\n    texts: Tokenized texts.\n    \n    Returns:\n    -------\n    texts: Pre-processed tokenized texts.\n    \"\"\"\n    texts = [[word for word in line if word not in stops] for line in texts]\n    texts = [bigram[line] for line in texts]\n    \n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n\n    texts = [[word for word in lemmatizer.lemmatize(' '.join(line), pos='v').split()] for line in texts]\n    return texts\n\ntrain_texts = process_texts(train_texts)\nprint(train_texts[5:6][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalising our dictionary and corpus\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\ndictionary = Dictionary(train_texts)\ncorpus = [dictionary.doc2bow(text) for text in train_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Topic modeling with LSI\n-----------------------\n\nThis is a useful topic modeling algorithm in that it can rank topics by\nitself. Thus it outputs topics in a ranked order. However it does require a\n``num_topics`` parameter (set to 200 by default) to determine the number of\nlatent dimensions after the SVD.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.models import LsiModel\nlsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)\nfor t in lsimodel.show_topics(num_topics=5):\n    print(t)\n\nlsitopics = lsimodel.show_topics(formatted=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Topic modeling with `HDP <http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>`_\n-----------------------------------------------------------------------------------------------------\n\nAn HDP model is fully unsupervised. It can also determine the ideal number of\ntopics it needs through posterior inference.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.models import HdpModel\nhdpmodel = HdpModel(corpus=corpus, id2word=dictionary)\nfor t in hdpmodel.show_topics():\n    print(t)\n\nhdptopics = hdpmodel.show_topics(formatted=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Topic modeling using `LDA <https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf>`_\n----------------------------------------------------------------------------------------------------\n\nThis is one the most popular topic modeling algorithms today. It is a\ngenerative model in that it assumes each document is a mixture of topics and\nin turn, each topic is a mixture of words. To understand it better you can\nwatch `this <https://www.youtube.com/watch?v=DDq3OVp9dNA>`_ lecture by David\nBlei. Let's choose 10 topics to initialize this.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\nldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)\nldatopics = ldamodel.show_topics(formatted=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pyLDAvis is a great way to visualize an LDA model. To summarize in short, the\narea of the circles represent the prevelance of the topic. The length of the\nbars on the right represent the membership of a term in a particular topic.\nFor more, see `LDAVis: A method for visualizing and interpreting topics\n<http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf>`_.\n\nUnfortunately, the visualization only works inside a Jupyter notebook.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import pyLDAvis.gensim\n    get_ipython()\nexcept Exception:\n    pass\nelse:\n    pyLDAvis.enable_notebook()\n    pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Determining the optimal number of topics\n----------------------------------------\n\n**Introduction to topic coherence**\\ :\n\n.. role:: raw-html-m2r(raw)\n   :format: html\n\n:raw-html-m2r:`<img src=\"https://rare-technologies.com/wp-content/uploads/2016/06/pipeline.png\">`\n\nTopic coherence in essence measures the human interpretability of a topic\nmodel. Traditionally `perplexity has been used\n<http://qpleple.com/perplexity-to-evaluate-topic-models/>`_ to evaluate topic\nmodels however this does not correlate with human annotations at times. Topic\ncoherence is another way to evaluate topic models with a much higher\nguarantee on human interpretability. Thus this can be used to compare\ndifferent topic models among many other use-cases. Here's a short blog I\nwrote explaining topic coherence:\n\n`What is topic coherence? <https://rare-technologies.com/what-is-topic-coherence/>`_\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel\n\n\ndef evaluate_graph(dictionary, corpus, texts, limit):\n    \"\"\"\n    Function to display num_topics - LDA graph using c_v coherence\n    \n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    limit : topic limit\n    \n    Returns:\n    -------\n    lm_list : List of LDA topic models\n    c_v : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    c_v = []\n    lm_list = []\n    for num_topics in range(1, limit):\n        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n        lm_list.append(lm)\n        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n        c_v.append(cm.get_coherence())\n        \n    # Show graph\n    x = range(1, limit)\n    plt.plot(x, c_v)\n    plt.xlabel(\"num_topics\")\n    plt.ylabel(\"Coherence score\")\n    plt.legend((\"c_v\"), loc='best')\n    plt.show()\n    \n    return lm_list, c_v\n\nlmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=train_texts, limit=10)\n# pyLDAvis.gensim.prepare(lmlist[2], corpus, dictionary)\nlmtopics = lmlist[5].show_topics(formatted=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LDA as LSI\n----------\n\nOne of the problem with LDA is that if we train it on a large number of\ntopics, the topics get \"lost\" among the numbers. Let us see if we can dig out\nthe best topics from the best LDA model we can produce. The function below\ncan be used to control the quality of the LDA model we produce.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ret_top_model():\n    \"\"\"\n    Since LDAmodel is a probabilistic model, it comes up different topics each time we run it. To control the\n    quality of the topic model we produce, we can see what the interpretability of the best topic is and keep\n    evaluating the topic model until this threshold is crossed. \n    \n    Returns:\n    -------\n    lm: Final evaluated topic model\n    top_topics: ranked topics in decreasing order. List of tuples\n    \"\"\"\n    top_topics = [(0, 0)]\n    while top_topics[0][1] < 0.97:\n        lm = LdaModel(corpus=corpus, id2word=dictionary)\n        coherence_values = {}\n        for n, topic in lm.show_topics(num_topics=-1, formatted=False):\n            topic = [word for word, _ in topic]\n            cm = CoherenceModel(topics=[topic], texts=train_texts, dictionary=dictionary, window_size=10)\n            coherence_values[n] = cm.get_coherence()\n        top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)\n    return lm, top_topics\n\n#\n# This part is broken: the confidence never reaches 0.97.\n# It also takes a prohibitively long time to run.  Disable it for now.\n# Use the regular LDA model instead, to keep the rest of this script working.\n#\n# lm, top_topics = ret_top_model()\n# print(top_topics[:5])\nlm, top_topics = ldamodel, ldatopics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference\n---------\n\nWe can clearly see below that the first topic is about **cinema**\\ , second is about **email malware**\\ , third is about the land which was given back to the **Larrakia aboriginal community of Australia** in 2000. Then there's one about **Australian cricket**. LDA as LSI has worked wonderfully in finding out the best topics from within LDA.\n\npprint([lm.show_topic(topicid) for topicid, c_v in top_topics[:10]])\nlda_lsi_topics = [[word for word, prob in lm.show_topic(topicid)] for topicid, c_v in top_topics]\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating all the topic models\n-------------------------------\n\nAny topic model which can come up with topic terms can be plugged into the\ncoherence pipeline. You can even plug in an `NMF topic model\n<http://derekgreene.com/nmf-topic/>`_ created with scikit-learn.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lsitopics = [[word for word, prob in topic] for topicid, topic in lsitopics]\nhdptopics = [[word for word, prob in topic] for topicid, topic in hdptopics]\nldatopics = [[word for word, prob in topic] for topicid, topic in ldatopics]\nlmtopics = [[word for word, prob in topic] for topicid, topic in lmtopics]\n\ndef create_coherence_model(topics):\n    return CoherenceModel(\n        topics=topics,\n        texts=train_texts,\n        dictionary=dictionary,\n        window_size=10\n    ).get_coherence()\n\nlsi_coherence = create_coherence_model(lsitopics[:10])\nhdp_coherence = create_coherence_model(hdptopics[:10])\nlda_coherence = create_coherence_model(ldatopics)\nlm_coherence = create_coherence_model(lmtopics)\n# lda_lsi_coherence = create_coherence_model(lda_lsi_topics[:10])\n\ndef evaluate_bar_graph(coherences, indices):\n    \"\"\"\n    Function to plot bar graph.\n    \n    coherences: list of coherence values\n    indices: Indices to be used to mark bars. Length of this and coherences should be equal.\n    \"\"\"\n    assert len(coherences) == len(indices)\n    n = len(coherences)\n    x = np.arange(n)\n    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n    plt.xlabel('Models')\n    plt.ylabel('Coherence Value')\n\nvalues = [lsi_coherence, hdp_coherence, lda_coherence, lm_coherence] #, lda_lsi_coherence]\nlabels = ['LSI', 'HDP', 'LDA', 'LDA_Mod'] #, 'LDA_LSI']\nevaluate_bar_graph(values, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Customizing the topic coherence measure\n---------------------------------------\n\nTill now we only used the ``c_v`` coherence measure. There are others such as\n``u_mass``\\ , ``c_uci``\\ , ``c_npmi``. All of these calculate coherence in a\ndifferent way. ``c_v`` is found to be most in line with human ratings but can\nbe much slower than ``u_mass`` since it uses a sliding window over the texts.\n\nMaking your own coherence measure\n---------------------------------\n\nLet's modify ``c_uci`` to use ``s_one_pre`` instead of ``s_one_one`` segmentation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.topic_coherence import (\n    segmentation, probability_estimation, direct_confirmation_measure,\n    indirect_confirmation_measure, aggregation\n)\nfrom gensim.matutils import argsort\nfrom collections import namedtuple\n\nmake_pipeline = namedtuple('Coherence_Measure', 'seg, prob, conf, aggr')\n\nmeasure = make_pipeline(segmentation.s_one_one,\n                        probability_estimation.p_boolean_sliding_window,\n                        direct_confirmation_measure.log_ratio_measure,\n                        aggregation.arithmetic_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get topics out of the topic model:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "topics = []\nfor topic in lm.state.get_lambda():\n    bestn = argsort(topic, topn=10, reverse=True)\ntopics.append(bestn)\n\nfor t in topics:\n    print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**\\ : Segmentation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "segmented_topics = measure.seg(topics)\n\nfor t in segmented_topics:\n    print(t)\n\n#\n# Unfortunately, the stuff below doesn't work, either :(\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2**\\ : Probability estimation\n\nSince this is a window-based coherence measure we will perform window based prob estimation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    per_topic_postings, num_windows = measure.prob(\n        texts=train_texts, segmented_topics=segmented_topics,\n        dictionary=dictionary, window_size=2,\n    )\nexcept Exception:\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3**\\ : Confirmation Measure\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    confirmed_measures = measure.conf(segmented_topics, per_topic_postings, num_windows, normalize=False)\nexcept Exception:\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4**\\ : Aggregation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    print(measure.aggr(confirmed_measures))\nexcept Exception:\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How this topic model can be used further\n========================================\n\nThe best topic model here can be used as a standalone for news article classification. However a topic model can also be used as a dimensionality reduction algorithm to feed into a classifier. A good topic model should be able to extract the signal from the noise efficiently, hence improving the performance of the classifier.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}