

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Similarity Queries &mdash; gensim</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.7.7',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="gensim" href="index.html" />
    <link rel="up" title="Tutorials" href="tutorial.html" />
    <link rel="next" title="Experiments on the English Wikipedia" href="wiki.html" />
    <link rel="prev" title="Topics and Transformations" href="tut2.html" />
<!-- twitter search widget
    <script type="text/javascript" src="_static/widget.js"></script>
-->
<meta property="og:title" content="#gensim" />
<meta property="og:description" content="Efficient topic modelling in Python" />

  </head>
  <body>
<!--
<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="index.html"><img src="_static/logo.png" border="0" alt="gensim logo"/></a>
</div>
--!>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="wiki.html" title="Experiments on the English Wikipedia"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tut2.html" title="Topics and Transformations"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Gensim home</a>|&nbsp;</li>
        <li><a href="apiref.html">API reference</a>|&nbsp;</li>
       <li><a href="tutorial.html">Tutorials</a> &raquo;</li>

          <li><a href="tutorial.html" accesskey="U">Tutorials</a> &raquo;</li> 
      </ul>
    </div>


    
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Similarity Queries</a><ul>
<li><a class="reference internal" href="#similarity-interface">Similarity interface</a><ul>
<li><a class="reference internal" href="#initializing-query-structures">Initializing query structures</a></li>
<li><a class="reference internal" href="#performing-queries">Performing queries</a></li>
</ul>
</li>
<li><a class="reference internal" href="#where-next">Where next?</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="tut2.html"
                        title="previous chapter">Topics and Transformations</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="wiki.html"
                        title="next chapter">Experiments on the English Wikipedia</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
    



    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="similarity-queries">
<span id="tut3"></span><h1>Similarity Queries<a class="headerlink" href="#similarity-queries" title="Permalink to this headline">¶</a></h1>
<p>Don&#8217;t forget to set</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logging</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span> <span class="c"># will suppress DEBUG level events</span>
</pre></div>
</div>
<p>if you want to see logging events.</p>
<div class="section" id="similarity-interface">
<h2>Similarity interface<a class="headerlink" href="#similarity-interface" title="Permalink to this headline">¶</a></h2>
<p>In the previous tutorials on <a class="reference internal" href="tut1.html"><em>Corpora and Vector Spaces</em></a> and <a class="reference internal" href="tut2.html"><em>Topics and Transformations</em></a>, we covered what it means
to create a corpus in the Vector Space Model and how to transform it between different
vector spaces. A common reason for such a charade is that we want to determine
<strong>similarity between pairs of documents</strong>, or the <strong>similarity between a specific document
and a set of other documents</strong> (such as a user query vs. indexed documents).</p>
<p>To show how this can be done in gensim, let us consider the same corpus as in the
previous examples (which really originally comes from Deerwester et al.&#8217;s
<a class="reference external" href="http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf">&#8220;Indexing by Latent Semantic Analysis&#8221;</a>
seminal 1990 article):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">similarities</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;/tmp/deerwester.dict&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">MmCorpus</span><span class="p">(</span><span class="s">&#39;/tmp/deerwester.mm&#39;</span><span class="p">)</span> <span class="c"># comes from the first tutorial, &quot;From strings to vectors&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">corpus</span>
<span class="go">MmCorpus(9 documents, 12 features, 28 non-zero entries)</span>
</pre></div>
</div>
<p>To follow Deerwester&#8217;s example, we first use this tiny corpus to define a 2-dimensional
LSI space:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lsi</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LsiModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="o">.</span><span class="n">id2word</span><span class="p">,</span> <span class="n">numTopics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Now suppose a user typed in the query <cite>&#8220;Human computer interaction&#8221;</cite>. We would
like to sort our nine corpus documents in decreasing order of relevance to this query.
Unlike modern search engines, here we only concentrate on a single aspect of possible
similarities&#8212;on apparent semantic relatedness of their texts (words). No hyperlinks,
no random-walk static ranks, just a semantic extension over the boolean keyword match:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s">&quot;Human computer interaction&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec_bow</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec_lsi</span> <span class="o">=</span> <span class="n">lsi</span><span class="p">[</span><span class="n">vec_bow</span><span class="p">]</span> <span class="c"># convert the query to LSI space</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">vec_lsi</span>
<span class="go">[(0, -0.461821), (1, 0.070028)]</span>
</pre></div>
</div>
<p>In addition, we will be considering <a class="reference external" href="http://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>
to determine the similarity of two vectors. Cosine similarity is a standard measure
in Vector Space Modeling, but wherever the vectors represent probability distributions,
<a class="reference external" href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence">different similarity measures</a>
may be more appropriate.</p>
<div class="section" id="initializing-query-structures">
<h3>Initializing query structures<a class="headerlink" href="#initializing-query-structures" title="Permalink to this headline">¶</a></h3>
<p>To prepare for similarity queries, we need to enter all documents which we want
to compare against subsequent queries. In our case, they are the same nine documents
used for training LSI, converted to 2-D LSA space. But that&#8217;s only incidental, we
might also be indexing a different corpus altogether.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">similarities</span><span class="o">.</span><span class="n">MatrixSimilarity</span><span class="p">(</span><span class="n">lsi</span><span class="p">[</span><span class="n">corpus</span><span class="p">])</span> <span class="c"># transform corpus to LSI space and index it</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The class <tt class="xref py py-class docutils literal"><span class="pre">similarities.MatrixSimilarity</span></tt> is only appropriate when the whole
set of vectors fits into memory. For example, a corpus of one million documents
would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.
Without 2GB of free RAM, you would need to use the <tt class="xref py py-class docutils literal"><span class="pre">similarities.Similarity</span></tt> class.
This class operates in constant memory, in a streaming (and more gensim-like)
fashion, but is also much slower than <tt class="xref py py-class docutils literal"><span class="pre">similarities.MatrixSimilarity</span></tt>, which uses
fast level-2 <a class="reference external" href="http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS routines</a>
to determine similarities.</p>
</div>
<p>Index persistency is handled via the standard <tt class="xref py py-func docutils literal"><span class="pre">save()</span></tt> and <tt class="xref py py-func docutils literal"><span class="pre">load()</span></tt> functions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">index</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;/tmp/deerwester.index&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">similarities</span><span class="o">.</span><span class="n">MatrixSimilarity</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;/tmp/deerwester.index&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="performing-queries">
<h3>Performing queries<a class="headerlink" href="#performing-queries" title="Permalink to this headline">¶</a></h3>
<p>To obtain similarities of our query document against the nine indexed documents:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sims</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">vec_lsi</span><span class="p">]</span> <span class="c"># perform a similarity query against the corpus</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sims</span><span class="p">))</span> <span class="c"># print (document_number, document_similarity) 2-tuples</span>
<span class="go">[(0, 0.99809301), (1, 0.93748635), (2, 0.99844527), (3, 0.9865886), (4, 0.90755945),</span>
<span class="go">(5, -0.12416792), (6, -0.1063926), (7, -0.098794639), (8, 0.05004178)]</span>
</pre></div>
</div>
<p>Cosine measure returns similarities in the range <cite>&lt;-1, 1&gt;</cite> (the greater, the more similar),
so that the first document has a score of 0.99809301 etc.</p>
<p>With some standard Python magic we sort these similarities into descending
order, and obtain the final answer to the query <cite>&#8220;Human computer interaction&#8221;</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sims</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sims</span><span class="p">),</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="o">-</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">sims</span> <span class="c"># print sorted (document number, similarity score) 2-tuples</span>
<span class="go">[(2, 0.99844527), # The EPS user interface management system</span>
<span class="go">(0, 0.99809301), # Human machine interface for lab abc computer applications</span>
<span class="go">(3, 0.9865886), # System and human system engineering testing of EPS</span>
<span class="go">(1, 0.93748635), # A survey of user opinion of computer system response time</span>
<span class="go">(4, 0.90755945), # Relation of user perceived response time to error measurement</span>
<span class="go">(8, 0.050041795), # Graph minors A survey</span>
<span class="go">(7, -0.098794639), # Graph minors IV Widths of trees and well quasi ordering</span>
<span class="go">(6, -0.1063926), # The intersection graph of paths in trees</span>
<span class="go">(5, -0.12416792)] # The generation of random binary unordered trees</span>
</pre></div>
</div>
<p>(I added the original documents in their &#8220;string form&#8221; to the output comments, to
improve clarity.)</p>
<p>The thing to note here is that documents no. 2 (<tt class="docutils literal"><span class="pre">&quot;The</span> <span class="pre">EPS</span> <span class="pre">user</span> <span class="pre">interface</span> <span class="pre">management</span> <span class="pre">system&quot;</span></tt>)
and 4 (<tt class="docutils literal"><span class="pre">&quot;Relation</span> <span class="pre">of</span> <span class="pre">user</span> <span class="pre">perceived</span> <span class="pre">response</span> <span class="pre">time</span> <span class="pre">to</span> <span class="pre">error</span> <span class="pre">measurement&quot;</span></tt>) would never be returned by
a standard boolean fulltext search, because they do not share any common words with <tt class="docutils literal"><span class="pre">&quot;Human</span>
<span class="pre">computer</span> <span class="pre">interaction&quot;</span></tt>. However, after applying LSI, we can observe that both of
them received quite high similarity scores (no. 2 is actually the most similar!),
which corresponds better to our intuition of
them sharing a &#8220;computer-human&#8221; related topic with the query. In fact, this semantic
generalization is the reason why we apply transformations and do topic modelling
in the first place.</p>
</div>
</div>
<div class="section" id="where-next">
<h2>Where next?<a class="headerlink" href="#where-next" title="Permalink to this headline">¶</a></h2>
<p>Congratulations, you have finished the tutorials &#8211; now you know how gensim works :-)
To delve into more details, you can browse through the <a class="reference internal" href="apiref.html"><em>API documentation</em></a> or
perhaps check out <a class="reference internal" href="distributed.html"><em>distributed computing</em></a> in <cite>gensim</cite>.</p>
<p>Please remember that gensim is an experimental package, aimed at the NLP research community.
This means that:</p>
<ul class="simple">
<li>there certainly are parts that could be implemented more efficiently (in C, for example), and there may also be bugs in the code</li>
<li>your <strong>feedback is most welcome</strong> and appreciated, be it in code and idea contributions, bug reports or just user stories.</li>
</ul>
<p>Gensim has no ambition to become a production-level tool, with robust failure handling
and error recoveries. Its main goal is to help NLP newcomers try out popular algorithms
and to facilitate prototyping of new algorithms for NLP researchers.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
        
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="wiki.html" title="Experiments on the English Wikipedia"
             >next</a> |</li>
        <li class="right" >
          <a href="tut2.html" title="Topics and Transformations"
             >previous</a> |</li>
        <li><a href="index.html">Gensim home</a>|&nbsp;</li>
        <li><a href="apiref.html">API reference</a>|&nbsp;</li>
       <li><a href="tutorial.html">Tutorials</a> &raquo;</li>

          <li><a href="tutorial.html" >Tutorials</a> &raquo;</li> 
      </ul>
    </div>
    
    

    <div class="footer">
        &copy; Copyright 2011, Radim Řehůřek &lt;radimrehurek(at)seznam.cz&gt;.
      Last updated on Mar 15, 2011.
    </div>
  </body>
</html>