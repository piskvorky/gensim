{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from imp import reload\n",
    "from pprint import pprint\n",
    "import os, shutil, re, random, logging, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.matutils import sparse2full, hellinger\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.models import atmodel\n",
    "from gensim.models import ldamodel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure logging.\n",
    "\n",
    "log_dir = '../../../log_files/log.log'  # On my own machine.\n",
    "#log_dir = '../../../../log_files/log.log'  # On Hetzner\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=log_dir, mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../../../../data/stackexchange/cooking/'\n",
    "input_fname = data_folder + 'Posts.xml'\n",
    "output_fname = '/tmp/cooking_docs.txt'\n",
    "tree = ET.parse(input_fname)\n",
    "root = tree.getroot()\n",
    "num_docs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in dataset: 54566\n"
     ]
    }
   ],
   "source": [
    "post_ids = []\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    post_ids.append(int(item.get('Id')))\n",
    "\n",
    "print('Number of posts in dataset:', len(post_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_generator(root, num_docs=None):\n",
    "    '''\n",
    "    This generator parses the XML data, do some preliminary\n",
    "    pre-processing and yields the documents.\n",
    "    \n",
    "    '''\n",
    "    num_posts = 0\n",
    "    for post_id in post_ids:\n",
    "        post_text = ''\n",
    "        for i, item in enumerate(root.iter()):\n",
    "            if i == 0:\n",
    "                # This is the <posts> XML element.\n",
    "                continue\n",
    "            elif int(item.get('Id')) == post_id:\n",
    "                # This is the post.\n",
    "                post_text += item.get('Body')\n",
    "            elif item.get('ParentId') is not None and int(item.get('ParentId')) == post_id:\n",
    "                # This is an answer to the post.\n",
    "                post_text += item.get('Body')\n",
    "            else:\n",
    "                # Neither post \"post_id\" or answer to it.\n",
    "                continue\n",
    "\n",
    "            # Remove any HTML tags, such as <p>.\n",
    "            post_text = re.sub('<[^<]+?>', '', post_text)\n",
    "\n",
    "            # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "            post_text = re.sub('\\s', ' ', post_text)\n",
    "\n",
    "        if num_docs is not None and num_posts >= num_docs:\n",
    "            break\n",
    "            \n",
    "        num_posts += 1\n",
    "        \n",
    "        yield post_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the default SpaCy NLP pipeline to process the documents in parallel.\n",
    "# Then use the output of the pipeline to transform the text.\n",
    "# Write the resulting text to a file.\n",
    "entity_freq = {}\n",
    "postid = 0\n",
    "with open(output_fname, 'w') as fid:\n",
    "    for doc in nlp.pipe(doc_generator(root, num_docs=num_docs), n_threads=4):\n",
    "        # Process post text.\n",
    "        \n",
    "        # NOTE: the doc_generator is probably the bottleneck here.\n",
    "        \n",
    "        ents = doc.ents  # Named entities.\n",
    "\n",
    "        # Keep only words (no numbers, no punctuation).\n",
    "        # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "        #doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        \n",
    "        # Remove stopwords and punctuation, and lemmatized tokens.\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        #tokens = [str(token) for token in doc if not token.is_stop and not token.is_punct]\n",
    "        #tokens.extend([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "        \n",
    "        # Add named entities, but only if they are a compound of more than word.\n",
    "        #doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "        \n",
    "        #for entity in ents:\n",
    "        #    if entity_freq.get(entity):\n",
    "        #        entity_freq[entity] += 1\n",
    "        #    else:\n",
    "        #        entity_freq[entity] = 1\n",
    "        \n",
    "        # Write the doc to file.\n",
    "        fid.write(' '.join(tokens) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tags of each post.\n",
    "postid = 0\n",
    "postid2tagname = dict()\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    if item.get('Tags') is None:\n",
    "        # There are many posts with no tags.\n",
    "        continue\n",
    "    \n",
    "    if num_docs is not None and postid >= num_docs:\n",
    "        break\n",
    "\n",
    "    tags = item.get('Tags')\n",
    "    tags = re.findall('<(.+?)>', tags)\n",
    "    # NOTE: consider using a tag that is common for all posts, and/or\n",
    "    # a tag that is only for this particular post. \n",
    "    # NOTE: also consider including posts with no tags, and tag them with\n",
    "    # post ID or \"SUPER_TAG\", maybe both, maybe an extra \"NO_TAG\" tag.\n",
    "    #tags.append('SUPER_TAG')\n",
    "    tags.append('POST_ID' + str(postid))\n",
    "    postid2tagname[postid] = tags\n",
    "    for tag in tags:\n",
    "        tag_set.add(tag)\n",
    "\n",
    "    postid += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** these names aren't great, \"doc_generator\" and \"docs_generator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_generator(fname):\n",
    "    '''\n",
    "    This generator reads the processed text one line\n",
    "    at a time and yields documents (lists of words).\n",
    "    \n",
    "    '''\n",
    "    with open(fname, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()  # Remove newline (\"\\n\").\n",
    "            doc = line.split(' ')  # Split line text into words.\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "docs = docs_generator(output_fname)\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "docs = docs_generator(output_fname)\n",
    "with open(output_fname + '.tmp', 'w') as fid:\n",
    "    for doc in docs:\n",
    "        for token in bigram[doc]:\n",
    "            if '_' in token:\n",
    "                doc.append(token)\n",
    "        fid.write(' '.join(doc) + '\\n')\n",
    "\n",
    "\n",
    "shutil.copyfile(output_fname + '.tmp', output_fname)\n",
    "os.remove(output_fname + '.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "docs = docs_generator(output_fname)\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "# Disregarding stop words, this dataset has a very high number of low frequency words.\n",
    "max_freq = 0.5\n",
    "min_count = 5\n",
    "dictionary.filter_extremes(no_below=min_count, no_above=max_freq)\n",
    "\n",
    "dict0 = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "docs = docs_generator(output_fname)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "num_docs = len(corpus)  # In case num_docs was set to None.\n",
    "\n",
    "# Serialize the corpus.\n",
    "#MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "#corpus = MmCorpus('/tmp/corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagname2postid = atmodel.construct_author2doc(corpus, postid2tagname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: how to do this with MmCorpus.\n",
    "\n",
    "train_corpus = corpus[100:]\n",
    "test_corpus = corpus[:100]\n",
    "train_postid2tagname = {i: postid2tagname[j] for i, j in enumerate(range(100, num_docs))}\n",
    "test_postid2tagname = {i: postid2tagname[j] for i, j in enumerate(range(100))}\n",
    "\n",
    "train_tagname2postid = atmodel.construct_author2doc(train_corpus, train_postid2tagname)\n",
    "test_tagname2postid = atmodel.construct_author2doc(test_corpus, test_postid2tagname)\n",
    "\n",
    "train_tag_set = set()\n",
    "for d, tags in train_postid2tagname.items():\n",
    "    for tag in tags:\n",
    "        train_tag_set.add(tag)\n",
    "\n",
    "docs = docs_generator(output_fname)\n",
    "test_docs = []\n",
    "for d, doc in enumerate(docs):\n",
    "    if d > 100:\n",
    "        break\n",
    "    test_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_doc(doc):\n",
    "    for token in doc:\n",
    "        if token in dictionary.values():\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#docs = list(docs_generator(output_fname))\n",
    "#display_doc(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensionality:\n",
      "Number of authors: 1344 (1462 in total)\n",
      "Number of unique tokens: 2220\n",
      "Number of documents: 900\n"
     ]
    }
   ],
   "source": [
    "print('Train data dimensionality:')\n",
    "print('Number of authors: %d (%d in total)' % (len(train_tag_set), len(tag_set)))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = [1,2,3]\n",
    "B = {1:1, 2:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new = False\n",
    "for a in A:\n",
    "    if not B.get(a):\n",
    "        new = True\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(A).intersection(B)) < len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('model.pickle', 'wb') as fid:\n",
    "#    pickle.dump(model, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(atmodel)\n",
    "AuthorTopicModel = atmodel.AuthorTopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.02640308098\n",
      "-7.46027696685\n",
      "-7.32860734589\n",
      "-7.19242777313\n",
      "-7.0608323278\n",
      "-6.94086803102\n",
      "-6.83188425744\n",
      "-6.73251887124\n",
      "-6.64267204902\n",
      "-6.56241965265\n",
      "CPU times: user 18.4 s, sys: 10.8 s, total: 29.2 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "%time model = AuthorTopicModel(corpus=train_corpus, num_topics=num_topics, id2word=dictionary.id2token, \\\n",
    "                author2doc=None, doc2author=train_postid2tagname, var_lambda=None,  \\\n",
    "                chunksize=1000, passes=10, update_every=1, \\\n",
    "                alpha='symmetric', eta='symmetric', decay=0.5, offset=1.0, \\\n",
    "                eval_every=1, iterations=1, gamma_threshold=1e-10, \\\n",
    "                minimum_probability=0.01, random_state=0, ns_conf={},\\\n",
    "                serialized=False, serialization_path='/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.4469052387\n"
     ]
    }
   ],
   "source": [
    "# Compute the per-word bound.\n",
    "# Number of words in corpus.\n",
    "corpus_words = sum(cnt for document in train_corpus for _, cnt in document)\n",
    "\n",
    "# Compute bound and divide by number of words.\n",
    "perwordbound = model.bound(train_corpus, author2doc=train_tagname2postid, \\\n",
    "                           doc2author=train_postid2tagname) / corpus_words\n",
    "print(perwordbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.remove('/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,\n",
       "  '0.046*\"chicken\" + 0.038*\"skin\" + 0.034*\"fish\" + 0.032*\"breast\" + 0.023*\"hour\" + 0.014*\"juice\" + 0.012*\"freeze\" + 0.012*\"rub\" + 0.012*\"eat\" + 0.012*\"sear\"'),\n",
       " (13,\n",
       "  '0.019*\"crack\" + 0.018*\"paper\" + 0.017*\"container\" + 0.017*\"smell\" + 0.016*\"powder\" + 0.016*\"towel\" + 0.015*\"bacon\" + 0.015*\"fruit\" + 0.014*\"coconut\" + 0.013*\"line\"'),\n",
       " (10,\n",
       "  '0.117*\"garlic\" + 0.047*\"clove\" + 0.036*\"pizza\" + 0.021*\"slice\" + 0.021*\"peel\" + 0.021*\"crush\" + 0.019*\"skin\" + 0.015*\"knife\" + 0.014*\"break\" + 0.014*\"press\"'),\n",
       " (6,\n",
       "  '0.061*\"iron\" + 0.055*\"cast\" + 0.055*\"cast_iron\" + 0.025*\"cheese\" + 0.017*\"seasoning\" + 0.017*\"chocolate\" + 0.016*\"melt\" + 0.016*\"fry\" + 0.016*\"pot\" + 0.015*\"apart\"'),\n",
       " (2,\n",
       "  '0.042*\"cake\" + 0.037*\"bake\" + 0.030*\"measure\" + 0.029*\"baking\" + 0.027*\"pop\" + 0.026*\"layer\" + 0.024*\"double\" + 0.021*\"test\" + 0.021*\"center\" + 0.020*\"bubble\"'),\n",
       " (4,\n",
       "  '0.147*\"knife\" + 0.044*\"steel\" + 0.032*\"blade\" + 0.032*\"cheese\" + 0.030*\"grate\" + 0.028*\"sharpen\" + 0.020*\"edge\" + 0.018*\"french\" + 0.018*\"thumb\" + 0.016*\"set\"'),\n",
       " (19,\n",
       "  '0.058*\"vegetable\" + 0.048*\"gas\" + 0.041*\"electric\" + 0.030*\"bag\" + 0.027*\"leaf\" + 0.024*\"freeze\" + 0.021*\"herb\" + 0.021*\"colour\" + 0.020*\"fridge\" + 0.020*\"green\"'),\n",
       " (11,\n",
       "  '0.020*\"book\" + 0.012*\"egg\" + 0.011*\"eat\" + 0.009*\"peel\" + 0.009*\"technique\" + 0.008*\"beef\" + 0.008*\"home\" + 0.007*\"freeze\" + 0.007*\"cold\" + 0.006*\"second\"'),\n",
       " (0,\n",
       "  '0.126*\"egg\" + 0.123*\"flour\" + 0.040*\"bread\" + 0.029*\"dough\" + 0.029*\"gluten\" + 0.020*\"cake\" + 0.017*\"protein\" + 0.015*\"shell\" + 0.015*\"soft\" + 0.014*\"type\"'),\n",
       " (5,\n",
       "  '0.059*\"gas\" + 0.056*\"onion\" + 0.026*\"freezer\" + 0.021*\"release\" + 0.021*\"sell\" + 0.020*\"item\" + 0.019*\"stove\" + 0.019*\"bottle\" + 0.016*\"month\" + 0.015*\"bag\"')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kitchen\n",
      "#Docs: 3\n",
      "[(15, 0.989037136854092)]\n",
      "\n",
      "seafood\n",
      "#Docs: 2\n",
      "[(12, 0.9561081217189048)]\n",
      "\n",
      "vegan\n",
      "#Docs: 4\n",
      "[(8, 0.93230970897091492)]\n",
      "\n",
      "pan\n",
      "#Docs: 2\n",
      "[(17, 0.97213802935976956)]\n",
      "\n",
      "tartare\n",
      "#Docs: 1\n",
      "[(0, 0.049999997552185323),\n",
      " (1, 0.049999997498422474),\n",
      " (2, 0.05000000872233331),\n",
      " (3, 0.050000001886338089),\n",
      " (4, 0.04999999499943094),\n",
      " (5, 0.049999997405850212),\n",
      " (6, 0.049999995267154818),\n",
      " (7, 0.049999997092325013),\n",
      " (8, 0.050000004100872561),\n",
      " (9, 0.049999997159923648),\n",
      " (10, 0.050000007295847075),\n",
      " (11, 0.050000002003677026),\n",
      " (12, 0.050000000746224099),\n",
      " (13, 0.049999997126121382),\n",
      " (14, 0.050000002752668145),\n",
      " (15, 0.050000004326004793),\n",
      " (16, 0.04999999896722946),\n",
      " (17, 0.050000000602347314),\n",
      " (18, 0.049999994878697052),\n",
      " (19, 0.049999999616347358)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tag = random.choice(list(train_tagname2postid.keys()))\n",
    "    while tag[:7] == 'POST_ID':\n",
    "        tag = random.choice(list(train_tagname2postid.keys()))\n",
    "    print('\\n%s' % tag)\n",
    "    print('#Docs:', len(model.author2doc[tag]))\n",
    "    pprint(model.get_author_topics(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cake', 0.041798281064494305),\n",
       " ('bake', 0.036799056345563914),\n",
       " ('measure', 0.029646700246982956),\n",
       " ('baking', 0.029190879027789197),\n",
       " ('pop', 0.026582234678151219),\n",
       " ('layer', 0.026354052842399108),\n",
       " ('double', 0.024050735528188368),\n",
       " ('test', 0.020978781252733003),\n",
       " ('center', 0.020774919563634326),\n",
       " ('bubble', 0.019779515912893848)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baking\n",
      "#Docs: 74\n",
      "[(14, 0.99944123182339661)]\n",
      "\n",
      "eggs\n",
      "#Docs: 38\n",
      "[(15, 0.78797064392893468), (19, 0.21131357754063784)]\n",
      "\n",
      "pasta\n",
      "#Docs: 19\n",
      "[(14, 0.99793584780062705)]\n",
      "\n",
      "herbs\n",
      "#Docs: 13\n",
      "[(4, 0.99214435647682309)]\n",
      "\n",
      "beef\n",
      "#Docs: 15\n",
      "[(3, 0.9980177727998798)]\n",
      "\n",
      "salmon\n",
      "#Docs: 6\n",
      "[(11, 0.75261915362288345), (19, 0.23653737575723341)]\n"
     ]
    }
   ],
   "source": [
    "tag = 'baking'\n",
    "print('%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'eggs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'pasta'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'herbs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'beef'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'salmon'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Hellinger distance:\n",
    "\n",
    "$$\n",
    "H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^K (\\sqrt{p_i} - \\sqrt{q_i})^2}\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are both topic distributions for two different tags. We define the similarity as\n",
    "$$\n",
    "S(p, q) = \\frac{1}{1 + H(p, q)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vec1, vec2):\n",
    "    dist = hellinger(sparse2full(vec1, num_topics), sparse2full(vec2, num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec, tag_vecs):\n",
    "    sims = [similarity(vec, vec2) for vec2 in tag_vecs]\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_vecs = [model.get_author_topics(tag, minimum_probability=0.0) for tag in train_tag_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2tag = dict(zip(range(len(train_tag_set)), list(train_tag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>ingredient-selection</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nutrient-composition</td>\n",
       "      <td>0.753888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>smoking</td>\n",
       "      <td>0.748792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>celery</td>\n",
       "      <td>0.743314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>sauteing</td>\n",
       "      <td>0.722020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>carrots</td>\n",
       "      <td>0.701454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>stir-fry</td>\n",
       "      <td>0.698036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>cookware</td>\n",
       "      <td>0.693311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>asian-cuisine</td>\n",
       "      <td>0.683168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>smell</td>\n",
       "      <td>0.663680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tag     Score\n",
       "158  ingredient-selection  1.000000\n",
       "22   nutrient-composition  0.753888\n",
       "236               smoking  0.748792\n",
       "218                celery  0.743314\n",
       "240              sauteing  0.722020\n",
       "39                carrots  0.701454\n",
       "279              stir-fry  0.698036\n",
       "429              cookware  0.693311\n",
       "172         asian-cuisine  0.683168\n",
       "391                 smell  0.663680"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = random.choice(list(train_tagname2postid.keys()))\n",
    "while tag[:7] == 'POST_ID':\n",
    "    tag = random.choice(list(train_tagname2postid.keys()))\n",
    "sims = get_sims(model.get_author_topics(tag, minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>beef</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>brownies</td>\n",
       "      <td>0.983048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>apples</td>\n",
       "      <td>0.947878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>professional</td>\n",
       "      <td>0.944643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>meatballs</td>\n",
       "      <td>0.925438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>candy</td>\n",
       "      <td>0.912623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>ribs</td>\n",
       "      <td>0.906636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gelling-agents</td>\n",
       "      <td>0.905063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>texture</td>\n",
       "      <td>0.904730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>liver</td>\n",
       "      <td>0.900559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tag     Score\n",
       "274            beef  1.000000\n",
       "189        brownies  0.983048\n",
       "70           apples  0.947878\n",
       "290    professional  0.944643\n",
       "401       meatballs  0.925438\n",
       "316           candy  0.912623\n",
       "185            ribs  0.906636\n",
       "26   gelling-agents  0.905063\n",
       "140         texture  0.904730\n",
       "406           liver  0.900559"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(model.get_author_topics('beef', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>baking</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>meat</td>\n",
       "      <td>0.992634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>steak</td>\n",
       "      <td>0.991649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>sugar</td>\n",
       "      <td>0.987753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>oil</td>\n",
       "      <td>0.984446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>pasta</td>\n",
       "      <td>0.980935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>italian-cuisine</td>\n",
       "      <td>0.979905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>slow-cooking</td>\n",
       "      <td>0.974038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>tomatoes</td>\n",
       "      <td>0.970183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>food-preservation</td>\n",
       "      <td>0.969655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tag     Score\n",
       "212             baking  1.000000\n",
       "76                meat  0.992634\n",
       "412              steak  0.991649\n",
       "437              sugar  0.987753\n",
       "418                oil  0.984446\n",
       "331              pasta  0.980935\n",
       "18     italian-cuisine  0.979905\n",
       "148       slow-cooking  0.974038\n",
       "74            tomatoes  0.970183\n",
       "136  food-preservation  0.969655"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(model.get_author_topics('baking', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>salmon</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>yeast</td>\n",
       "      <td>0.980547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>sous-vide</td>\n",
       "      <td>0.966504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>vegetarian</td>\n",
       "      <td>0.965828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>cleaning</td>\n",
       "      <td>0.951663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.949653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>food-science</td>\n",
       "      <td>0.945865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>tzatziki</td>\n",
       "      <td>0.945838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>flavor</td>\n",
       "      <td>0.945496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bread</td>\n",
       "      <td>0.941570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tag     Score\n",
       "367        salmon  1.000000\n",
       "417         yeast  0.980547\n",
       "200     sous-vide  0.966504\n",
       "286    vegetarian  0.965828\n",
       "428      cleaning  0.951663\n",
       "284     chocolate  0.949653\n",
       "98   food-science  0.945865\n",
       "294      tzatziki  0.945838\n",
       "420        flavor  0.945496\n",
       "12          bread  0.941570"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(model.get_author_topics('salmon', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the tag of a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=None, num_topics=num_topics, id2word=dictionary.id2token)\n",
    "lda.state.sstats = model.state.sstats\n",
    "lda.iterations = 100  # Make sure training converges on document when calling lda[doc]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post tags:\n",
      " ['oven', 'cooking-time', 'bacon']\n",
      "Post body:\n",
      " [\"'ve\", 'heard', 'people', 'cooking', 'bacon', 'oven', 'laying', 'strips', 'cookie', 'sheet', 'method', 'long', 'cook', 'bacon', 'temperature', 'place', 'bacon', 'cold', 'oven', 'turn', 'oven', '400F.', '', '', 'takes', '15', '20', 'minutes', 'slightly', 'crisp', 'bacon', '', '', '', \"'ve\", 'cooked', 'aluminum', 'foil', '350째F', '~175째C', '20', 'minutes', 'Flipping', 'half', 'way', 'point', 'prefer', 'crispier', '25', 'minutes', 'cookie', 'sheet', 'Use', 'high', 'temp', '375F+', '10', '20', 'minutes', 'depending', 'desired', 'crispness', '', '', 'easier', 'cleaning', 'cookie', 'sheet', 'line', 'aluminum', 'foil', '', '', 'let', 'grease', 'drain', 'corrugate', 'foil', 'far', 'prefer', 'remember', 'tear', 'sheet', 'roll', \"'ll\", 'need', '~2x', 'foil', 'area', 'use', 'tinfoil', 'non', 'stick', 'kind', 'works', 'old', 'baking', 'sheet', 'added', 'boost', 'sprinkle', 'dark', 'brown', 'sugar', 'coursely', 'ground', 'pepper', 'Cook', '350', '400', '10', 'min', 'need', 'flip', 'watch', 'end', \"n't\", 'burn', 'family', 'guests', 'ca', \"n't\", 'candied', 'bacon', 'set', 'oven', '400F', 'line', 'half', 'sheet', 'pan', 'aluminum', 'foil', 'place', 'cooling', 'rack', 'inside', 'pan', 'bacon', 'cooling', 'rack', '', '', 'takes', '20', '30', 'minutes', 'reach', 'point', 'like', 'want', 'stop', 'earlier', '', '', 'good', 'blend', 'brown', 'sugar', 'pecans', 'pecans', 'mixed', 'sugar', 'sprinkle', 'mixture', 'bacon', 'half', 'way', 'cooking', 'bake', 'cooling', 'rack', 'drain', 'excess', 'fat', 'cookie', 'sheet', 'lined', 'foil', 'easy', 'clean', '', '', 'start', 'cool', 'oven', '400', 'degrees', '20', 'minutes', 'crisp', 'turkey', 'bacon', 'special', 'pan', 'second', 'holes', 'allows', 'excess', 'fat', 'drain', 'away', 'middle', 'lower', 'oven', 'broiler', '500F', 'short', 'answer', 'throw', 'oven', 'sure', \"n't\", 'stick', '', '', 'use', 'aluminum', 'foil', 'silpat', 'accomplish', 'non', 'stick', '', '', 'temperatures', 'lot', 'depends', 'want', 'pork', 'remember', 'meat', 'cooks', 'beautifully', 'low', 'temperature', 'fries', 'nicely', 'salt', 'cured', '', '', 'want', 'delectably', 'smooth', 'soft', 'bacon', 'melts', 'mouth', 'try', '225', 'hours', 'like', 'slow', 'cooking', 'pork', 'shoulder', '', '', '', '300', 'quickly', 'cook', 'point', \"'re\", 'aiming', 'crispy', 'bacon', 'hotter', 'heat', 'faster', 'greater', 'danger', \"'re\", 'going', 'burn', 'usually', '375', 'family', 'takes', 'roughly', '20', 'minutes', '', '', 'Use', 'baking', 'tray', 'decent', 'lip', 'stop', 'fat', 'running', 'away', \"n't\", 'use', 'foil', 'extra', '', '', 'Lightly', 'rub', 'required', 'tray', 'surface', 'area', 'Olive', 'oil', '', '', 'Roll', 'piece', 'bacon', 'tight', 'tube', 'slight', 'larger', 'thumb', 'place', 'tray', 'use', 'pencil', 'sized', 'dowel', '', '', 'Balance', 'roll', 'hold', 'place', 'Use', 'toothpick', 'similar', 'hold', 'rolls', 'ends', '', '', 'Grill', 'medium', 'medium', 'high', '20', '30', 'minutes', '', '', 'gets', 'burnt', 'easy', 'handle', 'cooked', '', '', 'nice', 'tidy', 'look', 'plate', '', '', 'got', 'burnt', 'cleanup', 'simple', 'soak', 'tray', '', '', '50', 'slices', 'bacon', 'tray', 'place', 'bacon', 'cooling', 'rack', 'set', 'inside', 'baking', 'pan', 'meat', 'grease', '', '', 'Generally', 'speaking', 'lower', 'temperatures', 'result', 'bitter', 'compounds', 'forming', 'higher', 'temperatures', \"'ll\", 'wait', 'longer', 'delicious', 'delicious', 'bacon', \"'re\", 'baking', 'bacon', 'temperature', 'dish', 'requires', 'fine', '325', '425', '450', '', '', 'bacon', 'looks', 'feels', 'delicious', 'judge', 'color', 'like', 'crispy', 'dark', 'burnt', 'little', 'soft', 'cools', 'little', 'cooked', 'pound', 'bacon', 'yesterday', 'Blt', 'Bites', 'cherry', 'tomato', 'halves', '', '', 'knowing', 'better', 'spread', 'single', 'layer', 'rimmed', 'baking', 'pan', '', '', 'cooked', 'half', 'hour', '300', 'PRE=HEATED', '', '', 'perfect', '', '', 'spattering', '', '', 'Easy', 'clean', '', '', 'like', 'crisper', 'cook', 'longer', '', '', 'smells', 'best', 'method', 'use', 'parchment', 'paper', 'bacon', 'rack', 'fit', 'pan', 'hold', 'bacon', 'dripped', 'fat', 'baked', 'convection', 'oven', 'need', 'rotate', 'flip', '', '', '20', '30', 'minutes', 'depending', 'thickness', 'sliced', 'bacon', 'works', 'best', 'cookie', 'backing', 'trays', 'stacked', 'buy', 'non', 'stick', 'identical', 'cookie', 'trays', 'lay', \"'s\", 'baking', 'parchment', 'lay', 'bacon', 'flat', 'sheet', 'parchment', 'bacon', 'stack', 'second', 'tray', 'apply', 'good', 'pressure', 'sure', 'bacon', 'flat', '', '', 'tray', 'parchment', 'bacon', 'parchment', 'tray', 'clear', '', '', 'bake', 'high', 'temperature', '15', 'minutes', 'color', 'like', 'raise', 'tray', 'check', 'careful', 'grease', 'splashes', 'baking', 'absorb', 'excess', 'fat', 'laying', 'paper', '', '', 'guarantee', 'bacon', 'strips', 'cooking', 'book', 'picture', 'perfect', 'crispy', 'delicious', 'use', 'silicone', 'mats', 'instead', 'tinfoil', 'parchment', 'paper', 'easier', 'cleanup', 'have', 'hear', 'people', 'cook', 'bacon', 'oven', 'lay', 'strip', 'cookie', 'sheet', 'method', 'long', 'cook', 'bacon', 'temperature', 'place', 'bacon', 'cold', 'oven', 'turn', 'oven', '400f.', '', '', 'take', '15', '20', 'minute', 'slightly', 'crisp', 'bacon', '', '', '', 'have', 'cook', 'aluminum', 'foil', '350째f', '~175째c', '20', 'minute', 'flip', 'half', 'way', 'point', 'prefer', 'crispy', '25', 'minute', 'cookie', 'sheet', 'use', 'high', 'temp', '375f+', '10', '20', 'minute', 'depend', 'desire', 'crispness', '', '', 'easy', 'cleaning', 'cookie', 'sheet', 'line', 'aluminum', 'foil', '', '', 'let', 'grease', 'drain', 'corrugate', 'foil', 'far', 'prefer', 'remember', 'tear', 'sheet', 'roll', 'will', 'need', '~2x', 'foil', 'area', 'use', 'tinfoil', 'non', 'stick', 'kind', 'works', 'old', 'baking', 'sheet', 'add', 'boost', 'sprinkle', 'dark', 'brown', 'sugar', 'coursely', 'ground', 'pepper', 'cook', '350', '400', '10', 'min', 'need', 'flip', 'watch', 'end', 'not', 'burn', 'family', 'guest', 'can', 'not', 'candy', 'bacon', 'set', 'oven', '400f', 'line', 'half', 'sheet', 'pan', 'aluminum', 'foil', 'place', 'cooling', 'rack', 'inside', 'pan', 'bacon', 'cooling', 'rack', '', '', 'take', '20', '30', 'minute', 'reach', 'point', 'like', 'want', 'stop', 'earlier', '', '', 'good', 'blend', 'brown', 'sugar', 'pecan', 'pecan', 'mix', 'sugar', 'sprinkle', 'mixture', 'bacon', 'half', 'way', 'cooking', 'bake', 'cool', 'rack', 'drain', 'excess', 'fat', 'cookie', 'sheet', 'line', 'foil', 'easy', 'clean', '', '', 'start', 'cool', 'oven', '400', 'degree', '20', 'minute', 'crisp', 'turkey', 'bacon', 'special', 'pan', 'second', 'hole', 'allow', 'excess', 'fat', 'drain', 'away', 'middle', 'low', 'oven', 'broiler', '500f', 'short', 'answer', 'throw', 'oven', 'sure', 'not', 'stick', '', '', 'use', 'aluminum', 'foil', 'silpat', 'accomplish', 'non', 'stick', '', '', 'temperature', 'lot', 'depend', 'want', 'pork', 'remember', 'meat', 'cook', 'beautifully', 'low', 'temperature', 'fry', 'nicely', 'salt', 'cure', '', '', 'want', 'delectably', 'smooth', 'soft', 'bacon', 'melt', 'mouth', 'try', '225', 'hour', 'like', 'slow', 'cooking', 'pork', 'shoulder', '', '', '', '300', 'quickly', 'cook', 'point', \"'re\", 'aim', 'crispy', 'bacon', 'hot', 'heat', 'fast', 'great', 'danger', \"'re\", 'go', 'burn', 'usually', '375', 'family', 'take', 'roughly', '20', 'minute', '', '', 'use', 'baking', 'tray', 'decent', 'lip', 'stop', 'fat', 'run', 'away', 'not', 'use', 'foil', 'extra', '', '', 'lightly', 'rub', 'require', 'tray', 'surface', 'area', 'olive', 'oil', '', '', 'roll', 'piece', 'bacon', 'tight', 'tube', 'slight', 'large', 'thumb', 'place', 'tray', 'use', 'pencil', 'size', 'dowel', '', '', 'balance', 'roll', 'hold', 'place', 'use', 'toothpick', 'similar', 'hold', 'roll', 'end', '', '', 'grill', 'medium', 'medium', 'high', '20', '30', 'minute', '', '', 'get', 'burn', 'easy', 'handle', 'cook', '', '', 'nice', 'tidy', 'look', 'plate', '', '', 'get', 'burn', 'cleanup', 'simple', 'soak', 'tray', '', '', '50', 'slice', 'bacon', 'tray', 'place', 'bacon', 'cool', 'rack', 'set', 'inside', 'baking', 'pan', 'meat', 'grease', '', '', 'generally', 'speak', 'low', 'temperature', 'result', 'bitter', 'compound', 'form', 'high', 'temperature', 'will', 'wait', 'longer', 'delicious', 'delicious', 'bacon', \"'re\", 'bake', 'bacon', 'temperature', 'dish', 'require', 'fine', '325', '425', '450', '', '', 'bacon', 'look', 'feel', 'delicious', 'judge', 'color', 'like', 'crispy', 'dark', 'burn', 'little', 'soft', 'cool', 'little', 'cook', 'pound', 'bacon', 'yesterday', 'blt', 'bite', 'cherry', 'tomato', 'half', '', '', 'know', 'good', 'spread', 'single', 'layer', 'rimmed', 'baking', 'pan', '', '', 'cook', 'half', 'hour', '300', 'pre=heated', '', '', 'perfect', '', '', 'spattering', '', '', 'easy', 'clean', '', '', 'like', 'crisp', 'cook', 'longer', '', '', 'smell', 'good', 'method', 'use', 'parchment', 'paper', 'bacon', 'rack', 'fit', 'pan', 'hold', 'bacon', 'drip', 'fat', 'bake', 'convection', 'oven', 'need', 'rotate', 'flip', '', '', '20', '30', 'minute', 'depend', 'thickness', 'slice', 'bacon', 'work', 'best', 'cookie', 'backing', 'tray', 'stack', 'buy', 'non', 'stick', 'identical', 'cookie', 'tray', 'lay', \"'s\", 'baking', 'parchment', 'lie', 'bacon', 'flat', 'sheet', 'parchment', 'bacon', 'stack', 'second', 'tray', 'apply', 'good', 'pressure', 'sure', 'bacon', 'flat', '', '', 'tray', 'parchment', 'bacon', 'parchment', 'tray', 'clear', '', '', 'bake', 'high', 'temperature', '15', 'minute', 'color', 'like', 'raise', 'tray', 'check', 'careful', 'grease', 'splash', 'bake', 'absorb', 'excess', 'fat', 'lay', 'paper', '', '', 'guarantee', 'bacon', 'strip', 'cook', 'book', 'picture', 'perfect', 'crispy', 'delicious', 'use', 'silicone', 'mat', 'instead', 'tinfoil', 'parchment', 'paper', 'easy', 'cleanup', \"'ve_heard\", 'cookie_sheet', 'aluminum_foil', 'cookie_sheet', 'cookie_sheet', 'aluminum_foil', 'non_stick', 'baking_sheet', 'brown_sugar', \"ca_n't\", 'aluminum_foil', '20_30', 'brown_sugar', 'cookie_sheet', 'aluminum_foil', 'non_stick', \"'re_going\", 'surface_area', '20_30', 'parchment_paper', '20_30', 'non_stick', 'high_temperature', '15_minutes', 'parchment_paper', 'have_hear', 'cookie_sheet', 'aluminum_foil', 'cookie_sheet', 'cookie_sheet', 'aluminum_foil', 'non_stick', 'baking_sheet', 'brown_sugar', 'can_not', 'aluminum_foil', '20_30', 'brown_sugar', 'cookie_sheet', 'aluminum_foil', 'non_stick', \"'re_go\", 'surface_area', 'olive_oil', '20_30', 'high_temperature', 'parchment_paper', '20_30', 'non_stick', 'high_temperature', '15_minute', 'parchment_paper']\n"
     ]
    }
   ],
   "source": [
    "postid = 1\n",
    "doc = test_corpus[postid]\n",
    "print('Post tags:\\n', test_postid2tagname[postid])\n",
    "print('Post body:\\n', test_docs[postid])\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in train_tag_set:\n",
    "        print('Tag \"', tag, '\" not in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>juice</td>\n",
       "      <td>0.654250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>baking-soda</td>\n",
       "      <td>0.646869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>soymilk</td>\n",
       "      <td>0.645439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tenderizing</td>\n",
       "      <td>0.641415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>quinoa</td>\n",
       "      <td>0.640860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>stainless-steel</td>\n",
       "      <td>0.639342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>jicama</td>\n",
       "      <td>0.638536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>reheating</td>\n",
       "      <td>0.636994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>cherries</td>\n",
       "      <td>0.635620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>pan</td>\n",
       "      <td>0.628226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Tag     Score\n",
       "257            juice  0.654250\n",
       "246      baking-soda  0.646869\n",
       "352          soymilk  0.645439\n",
       "27       tenderizing  0.641415\n",
       "239           quinoa  0.640860\n",
       "309  stainless-steel  0.639342\n",
       "381           jicama  0.638536\n",
       "191        reheating  0.636994\n",
       "328         cherries  0.635620\n",
       "266              pan  0.628226"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(lda.get_document_topics(doc, minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>catering</td>\n",
       "      <td>0.681661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>melting-chocolate</td>\n",
       "      <td>0.832565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tag     Score\n",
       "75            catering  0.681661\n",
       "289  melting-chocolate  0.832565"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_threshold = 0.6\n",
    "pred = sims_df.loc[sims_df.Score > pred_threshold]\n",
    "pred_tags = list(pred.Tag)\n",
    "pred_prob = list(pred.Score)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for tag in pred_tags:\n",
    "    if tag in test_postid2tagname[postid]:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in pred_tags:\n",
    "        fn += 1\n",
    "        \n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "if precision + recall == 0:\n",
    "    f1_score = 0.0\n",
    "else:\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print('F1 score: ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(train_tag_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baking</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>aging</td>\n",
       "      <td>0.994557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>crepe</td>\n",
       "      <td>0.987024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ganache</td>\n",
       "      <td>0.983886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>sauce</td>\n",
       "      <td>0.980548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>puree</td>\n",
       "      <td>0.978956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>roast-beef</td>\n",
       "      <td>0.975619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>roasting</td>\n",
       "      <td>0.971207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>taffy</td>\n",
       "      <td>0.960213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>rye</td>\n",
       "      <td>0.959389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tag     Score\n",
       "0        baking  1.000000\n",
       "88        aging  0.994557\n",
       "24        crepe  0.987024\n",
       "96      ganache  0.983886\n",
       "386       sauce  0.980548\n",
       "38        puree  0.978956\n",
       "191  roast-beef  0.975619\n",
       "243    roasting  0.971207\n",
       "233       taffy  0.960213\n",
       "418         rye  0.959389"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get similarities to some tag.\n",
    "tag_name = 'baking'\n",
    "sims = index[model[tag_name]]\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 0 ns, total: 41.6 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%time lda = LdaModel(corpus, num_topics=10, id2word=dictionary.id2token, iterations=1, \\\n",
    "                     passes=100, eval_every=0, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"sauce\" + 0.031*\"like\" + 0.026*\"taste\" + 0.021*\"eat\" + 0.017*\"rice\" + 0.014*\"dish\" + 0.014*\"ingredient\" + 0.013*\"love\" + 0.012*\"look\" + 0.012*\"tip\"'),\n",
       " (1,\n",
       "  '0.042*\"egg\" + 0.027*\"milk\" + 0.025*\"difference\" + 0.020*\"use\" + 0.017*\"flavor\" + 0.014*\"day\" + 0.014*\"white\" + 0.014*\"question\" + 0.013*\"bit\" + 0.012*\"wine\"'),\n",
       " (2,\n",
       "  '0.036*\"meat\" + 0.025*\"chicken\" + 0.023*\"cook\" + 0.020*\"pan\" + 0.019*\"cut\" + 0.015*\"like\" + 0.015*\"use\" + 0.014*\"hear\" + 0.013*\"food\" + 0.012*\"stick\"'),\n",
       " (3,\n",
       "  '0.064*\"recipe\" + 0.032*\"use\" + 0.022*\"bread\" + 0.020*\"substitute\" + 0.019*\"bake\" + 0.017*\"baking\" + 0.016*\"cake\" + 0.014*\"chocolate\" + 0.012*\"problem\" + 0.011*\"dough\"'),\n",
       " (4,\n",
       "  '0.036*\"way\" + 0.029*\"good\" + 0.023*\"temperature\" + 0.021*\"cheese\" + 0.019*\"time\" + 0.018*\"cook\" + 0.016*\"long\" + 0.014*\"beef\" + 0.014*\"vegetable\" + 0.013*\"use\"'),\n",
       " (5,\n",
       "  '0.028*\"oil\" + 0.022*\"oven\" + 0.020*\"use\" + 0.017*\"knife\" + 0.016*\"good\" + 0.013*\"prepare\" + 0.013*\"wonder\" + 0.011*\"skin\" + 0.011*\"cook\" + 0.011*\"place\"'),\n",
       " (6,\n",
       "  '0.033*\"add\" + 0.031*\"salt\" + 0.021*\"garlic\" + 0.017*\"water\" + 0.015*\"thing\" + 0.012*\"pot\" + 0.012*\"hard\" + 0.012*\"try\" + 0.012*\"chop\" + 0.011*\"point\"'),\n",
       " (7,\n",
       "  '0.052*\"use\" + 0.021*\"butter\" + 0.018*\"boil\" + 0.015*\"pasta\" + 0.015*\"flour\" + 0.014*\"type\" + 0.013*\"cook\" + 0.013*\"good\" + 0.012*\"high\" + 0.011*\"look\"'),\n",
       " (8,\n",
       "  '0.022*\"cream\" + 0.019*\"cook\" + 0.015*\"mix\" + 0.014*\"freeze\" + 0.013*\"half\" + 0.012*\"time\" + 0.012*\"store\" + 0.012*\"dry\" + 0.011*\"hour\" + 0.011*\"try\"'),\n",
       " (9,\n",
       "  '0.033*\"uk\" + 0.021*\"pepper\" + 0.019*\"sugar\" + 0.016*\"au\" + 0.013*\"tomato\" + 0.011*\"potato\" + 0.010*\"ground\" + 0.010*\"bean\" + 0.009*\"ca\" + 0.008*\"hot\"')]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = ET.parse(input_fname)\n",
    "postid2tagname = dict()\n",
    "postid = 0\n",
    "posts = []\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if item.get('Tags') is not None:\n",
    "        tags = item.get('Tags')\n",
    "        tags = re.findall('<(.+?)>', tags)\n",
    "        # NOTE: consider using a tag that is common for all posts, or\n",
    "        # a tag that is only for this particular post.\n",
    "        #tags.append('SUPER_TAG')\n",
    "        #tags.append('POST_ID' + str(postid))\n",
    "        postid2tagname[postid] = tags\n",
    "        posts.append(item.get('Body'))\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        postid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_posts = len(posts)\n",
    "docs = []\n",
    "for post in posts[:]:\n",
    "    # Remove any HTML tags, such as <p>.\n",
    "    text = re.sub('<[^<]+?>', '', post)\n",
    "    doc = nlp(text)\n",
    "    ents = doc.ents  # Named entities.\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in STOPWORDS]\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "#max_freq = 0.5\n",
    "#min_wordcount = 20\n",
    "#dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "dict0 = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Serialize the corpus.\n",
    "#MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "#corpus = MmCorpus('/tmp/corpus.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking VB tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensionality:\n",
      "Number of authors: 237\n",
      "Number of unique tokens: 434\n",
      "Number of documents: 100\n"
     ]
    }
   ],
   "source": [
    "print('Train data dimensionality:')\n",
    "print('Number of authors: %d' % len(tagname2postid))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atfilename = '/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/temp/blocking_vb_tests/atnonblocking.py'\n",
    "with open(atfilename) as f:\n",
    "    code = compile(f.read(), atfilename, 'exec')\n",
    "    exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = AtNonBlocking(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                      author2doc=tagname2postid, doc2author=postid2tagname, threshold=1e-10, \\\n",
    "                      iterations=10, alpha='symmetric', eta='symmetric', minimum_probability=0.01, \\\n",
    "                      eval_every=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atfilename = '/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/temp/blocking_vb_tests/atblocking.py'\n",
    "with open(atfilename) as f:\n",
    "    code = compile(f.read(), atfilename, 'exec')\n",
    "    exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = AtBlocking(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                      author2doc=tagname2postid, doc2author=postid2tagname, threshold=1e-10, \\\n",
    "                      iterations=10, alpha='symmetric', eta='symmetric', minimum_probability=0.01, \\\n",
    "                      eval_every=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.91"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_per_doc = [len(docs) for docs in postid2tagname.values()]\n",
    "sum(tags_per_doc) / len(tags_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2016-12-30 16:08:40,106 - gensim.models.atmodel - WARNING - no author id mapping provided; initializing from corpus, assuming identity\n",
    "    2016-12-30 16:08:40,107 - gensim.models.atmodel - INFO - Starting inference. Training on 100 documents.\n",
    "    2016-12-30 16:08:40,586 - gensim.models.atmodel - INFO - perwordbound: -8.431e+00.\n",
    "    2016-12-30 16:08:44,144 - gensim.models.atmodel - INFO - perwordbound: -7.543e+00.\n",
    "    2016-12-30 16:08:47,576 - gensim.models.atmodel - INFO - perwordbound: -7.514e+00.\n",
    "    2016-12-30 16:08:51,052 - gensim.models.atmodel - INFO - perwordbound: -7.446e+00.\n",
    "    2016-12-30 16:08:54,481 - gensim.models.atmodel - INFO - perwordbound: -7.273e+00.\n",
    "    2016-12-30 16:08:58,230 - gensim.models.atmodel - INFO - perwordbound: -7.000e+00.\n",
    "    2016-12-30 16:09:01,808 - gensim.models.atmodel - INFO - perwordbound: -6.699e+00.\n",
    "    2016-12-30 16:09:05,283 - gensim.models.atmodel - INFO - perwordbound: -6.388e+00.\n",
    "    2016-12-30 16:09:08,763 - gensim.models.atmodel - INFO - perwordbound: -6.083e+00.\n",
    "    2016-12-30 16:09:12,133 - gensim.models.atmodel - INFO - perwordbound: -5.810e+00.\n",
    "    2016-12-30 16:09:15,503 - gensim.models.atmodel - INFO - perwordbound: -5.593e+00.\n",
    "    2016-12-30 16:09:15,563 - gensim.models.atmodel - INFO - Vocabulary consists of 434 words.\n",
    "    2016-12-30 16:09:15,563 - gensim.models.atmodel - INFO - Number of authors: 237.\n",
    "    2016-12-30 16:09:15,563 - gensim.models.atmodel - WARNING - no author id mapping provided; initializing from corpus, assuming identity\n",
    "    2016-12-30 16:09:15,564 - gensim.models.atmodel - INFO - Starting inference. Training on 100 documents.\n",
    "    2016-12-30 16:09:15,980 - gensim.models.atmodel - INFO - perwordbound: -8.431e+00.\n",
    "    2016-12-30 16:09:20,729 - gensim.models.atmodel - INFO - perwordbound: -7.542e+00.\n",
    "    2016-12-30 16:09:25,065 - gensim.models.atmodel - INFO - perwordbound: -7.510e+00.\n",
    "    2016-12-30 16:09:29,330 - gensim.models.atmodel - INFO - perwordbound: -7.433e+00.\n",
    "    2016-12-30 16:09:33,614 - gensim.models.atmodel - INFO - perwordbound: -7.235e+00.\n",
    "    2016-12-30 16:09:37,889 - gensim.models.atmodel - INFO - perwordbound: -6.922e+00.\n",
    "    2016-12-30 16:09:42,393 - gensim.models.atmodel - INFO - perwordbound: -6.570e+00.\n",
    "    2016-12-30 16:09:46,664 - gensim.models.atmodel - INFO - perwordbound: -6.218e+00.\n",
    "    2016-12-30 16:09:51,217 - gensim.models.atmodel - INFO - perwordbound: -5.890e+00.\n",
    "    2016-12-30 16:09:55,439 - gensim.models.atmodel - INFO - perwordbound: -5.617e+00.\n",
    "    2016-12-30 16:09:59,700 - gensim.models.atmodel - INFO - perwordbound: -5.414e+00.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
