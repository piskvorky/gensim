{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from imp import reload\n",
    "from pprint import pprint\n",
    "import os, shutil, re, random, logging, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.matutils import sparse2full, hellinger\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.models import atmodel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure logging.\n",
    "\n",
    "log_dir = '../../../log_files/log.log'  # On my own machine.\n",
    "#log_dir = '../../../../log_files/log.log'  # On Hetzner\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=log_dir, mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../../../../data/stackexchange/cooking/'\n",
    "input_fname = data_folder + 'Posts.xml'\n",
    "output_fname = '/tmp/cooking_docs.txt'\n",
    "tree = ET.parse(input_fname)\n",
    "root = tree.getroot()\n",
    "num_docs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in dataset: 54566\n"
     ]
    }
   ],
   "source": [
    "post_ids = []\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    post_ids.append(int(item.get('Id')))\n",
    "\n",
    "print('Number of posts in dataset:', len(post_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_generator(root, num_docs=None):\n",
    "    '''\n",
    "    This generator parses the XML data, do some preliminary\n",
    "    pre-processing and yields the documents.\n",
    "    \n",
    "    '''\n",
    "    num_posts = 0\n",
    "    for post_id in post_ids:\n",
    "        post_text = ''\n",
    "        for i, item in enumerate(root.iter()):\n",
    "            if i == 0:\n",
    "                # This is the <posts> XML element.\n",
    "                continue\n",
    "            elif int(item.get('Id')) == post_id:\n",
    "                # This is the post.\n",
    "                post_text += item.get('Body')\n",
    "            elif item.get('ParentId') is not None and int(item.get('ParentId')) == post_id:\n",
    "                # This is an answer to the post.\n",
    "                post_text += item.get('Body')\n",
    "            else:\n",
    "                # Neither post \"post_id\" or answer to it.\n",
    "                continue\n",
    "\n",
    "            # Remove any HTML tags, such as <p>.\n",
    "            post_text = re.sub('<[^<]+?>', '', post_text)\n",
    "\n",
    "            # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "            post_text = re.sub('\\s', ' ', post_text)\n",
    "\n",
    "        if num_docs is not None and num_posts >= num_docs:\n",
    "            break\n",
    "            \n",
    "        num_posts += 1\n",
    "        \n",
    "        yield post_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal double-acting baking powder makes CO2 (thus giving a rising effect) in two ways: when it gets wet, and when it is heated.  Baking soda only makes CO2 when it gets wet.  From Wikipedia:     The acid in a baking powder can be   either fast-acting or slow-acting.[6]   A fast-acting acid reacts in a wet   mixture with baking soda at room   temperature, and a slow-acting acid   will not react until heated in an   oven. Baking powders that contain both   fast- and slow-acting acids are double   acting; those that contain only one   acid are single acting. By providing a   second rise in the oven, double-acting   baking powders increase the   reliability of baked goods by   rendering the time elapsed between   mixing and baking less critical, and   this is the type most widely available   to consumers today.to consumers today.    See: http://en.wikipedia.org/wiki/Baking_powder  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the default SpaCy NLP pipeline to process the documents in parallel.\n",
    "# Then use the output of the pipeline to transform the text.\n",
    "# Write the resulting text to a file.\n",
    "postid = 0\n",
    "with open(output_fname, 'w') as fid:\n",
    "    for doc in nlp.pipe(doc_generator(root, num_docs=num_docs), n_threads=4):\n",
    "        # Process post text.\n",
    "        \n",
    "        # NOTE: the doc_generator is probably the bottleneck here.\n",
    "        \n",
    "        ents = doc.ents  # Named entities.\n",
    "\n",
    "        # Keep only words (no numbers, no punctuation).\n",
    "        # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "        #doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        doc = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        \n",
    "        # Remove common words from a stopword list.\n",
    "        #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "        # Add named entities, but only if they are a compound of more than word.\n",
    "        doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "        \n",
    "        # Write the doc to file.\n",
    "        fid.write(' '.join(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tags of each post.\n",
    "postid = 0\n",
    "postid2tagname = dict()\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    if item.get('Tags') is None:\n",
    "        # There are many posts with no tags.\n",
    "        continue\n",
    "    \n",
    "    if num_docs is not None and postid >= num_docs:\n",
    "        break\n",
    "\n",
    "    tags = item.get('Tags')\n",
    "    tags = re.findall('<(.+?)>', tags)\n",
    "    # NOTE: consider using a tag that is common for all posts, and/or\n",
    "    # a tag that is only for this particular post. \n",
    "    # NOTE: also consider including posts with no tags, and tag them with\n",
    "    # post ID or \"SUPER_TAG\", maybe both, maybe an extra \"NO_TAG\" tag.\n",
    "    #tags.append('SUPER_TAG')\n",
    "    #tags.append('POST_ID' + str(postid))\n",
    "    postid2tagname[postid] = tags\n",
    "    for tag in tags:\n",
    "        tag_set.add(tag)\n",
    "\n",
    "    postid += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** these names aren't great, \"doc_generator\" and \"docs_generator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_generator(fname):\n",
    "    '''\n",
    "    This generator reads the processed text one line\n",
    "    at a time and yields documents (lists of words).\n",
    "    \n",
    "    '''\n",
    "    with open(fname, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()  # Remove newline (\"\\n\").\n",
    "            doc = line.split(' ')  # Split line text into words.\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "docs = docs_generator(output_fname)\n",
    "bigram = Phrases(docs, min_count=1)\n",
    "docs = docs_generator(output_fname)\n",
    "with open(output_fname + '.tmp', 'w') as fid:\n",
    "    for doc in docs:\n",
    "        for token in bigram[doc]:\n",
    "            if '_' in token:\n",
    "                doc.append(token)\n",
    "        fid.write(' '.join(doc) + '\\n')\n",
    "\n",
    "\n",
    "shutil.copyfile(output_fname + '.tmp', output_fname)\n",
    "os.remove(output_fname + '.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "docs = docs_generator(output_fname)\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "# Disregarding stop words, this dataset has a very high number of low frequency words.\n",
    "max_freq = 1.0  # No filtering.\n",
    "min_count = 5\n",
    "dictionary.filter_extremes(no_below=min_count, no_above=max_freq)\n",
    "\n",
    "dict0 = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "docs = docs_generator(output_fname)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "num_docs = len(corpus)  # In case num_docs was set to None.\n",
    "\n",
    "# Serialize the corpus.\n",
    "#MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "#corpus = MmCorpus('/tmp/corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagname2postid = atmodel.construct_author2doc(corpus, postid2tagname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: how to do this with MmCorpus.\n",
    "\n",
    "train_corpus = corpus[100:]\n",
    "test_corpus = corpus[:100]\n",
    "train_postid2tagname = {i: postid2tagname[j] for i, j in enumerate(range(100, num_docs))}\n",
    "test_postid2tagname = {i: postid2tagname[j] for i, j in enumerate(range(100))}\n",
    "\n",
    "train_tagname2postid = atmodel.construct_author2doc(train_corpus, train_postid2tagname)\n",
    "test_tagname2postid = atmodel.construct_author2doc(test_corpus, test_postid2tagname)\n",
    "\n",
    "train_tag_set = set()\n",
    "for d, tags in train_postid2tagname.items():\n",
    "    for tag in tags:\n",
    "        train_tag_set.add(tag)\n",
    "\n",
    "docs = docs_generator(output_fname)\n",
    "test_docs = []\n",
    "for d, doc in enumerate(docs):\n",
    "    if d > 100:\n",
    "        break\n",
    "    test_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensionality:\n",
      "Number of authors: 444 (462 in total)\n",
      "Number of unique tokens: 2764\n",
      "Number of documents: 900\n"
     ]
    }
   ],
   "source": [
    "print('Train data dimensionality:')\n",
    "print('Number of authors: %d (%d in total)' % (len(train_tag_set), len(tag_set)))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('big_model.pickle', 'wb') as fid:\n",
    "#    pickle.dump(model, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# big_model_save = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = big_model_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(atmodel)\n",
    "AuthorTopicModel = atmodel.AuthorTopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1min 4s, total: 2min 41s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "%time model = AuthorTopicModel(corpus=train_corpus, num_topics=num_topics, id2word=dictionary.id2token, \\\n",
    "                author2doc=None, doc2author=train_postid2tagname, var_lambda=None,  \\\n",
    "                chunksize=1000, passes=100, update_every=1, \\\n",
    "                alpha='auto', eta='auto', decay=0.5, offset=1.0, \\\n",
    "                eval_every=0, iterations=1, gamma_threshold=1e-10, \\\n",
    "                minimum_probability=0.01, random_state=0, ns_conf={},\\\n",
    "                serialized=False, serialization_path='/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.4469052387\n"
     ]
    }
   ],
   "source": [
    "# Compute the per-word bound.\n",
    "# Number of words in corpus.\n",
    "corpus_words = sum(cnt for document in train_corpus for _, cnt in document)\n",
    "\n",
    "# Compute bound and divide by number of words.\n",
    "perwordbound = model.bound(train_corpus, author2doc=train_tagname2postid, \\\n",
    "                           doc2author=train_postid2tagname) / corpus_words\n",
    "print(perwordbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.remove('/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14,\n",
       "  '0.005*\"pumpkin\" + 0.004*\"knife\" + 0.003*\"rice\" + 0.003*\"pie\" + 0.003*\"bake\" + 0.003*\"egg\" + 0.003*\"flour\" + 0.003*\"cheese\" + 0.003*\"white\" + 0.003*\"type\"'),\n",
       " (8,\n",
       "  '0.006*\"rib\" + 0.005*\"knife\" + 0.004*\"rice\" + 0.004*\"stir\" + 0.004*\"fry\" + 0.003*\"style\" + 0.003*\"marinade\" + 0.003*\"steam\" + 0.003*\"flour\" + 0.003*\"chinese\"'),\n",
       " (0,\n",
       "  '0.005*\"knife\" + 0.004*\"fruit\" + 0.003*\"bean\" + 0.003*\"ethylene\" + 0.003*\"rice\" + 0.003*\"stop\" + 0.003*\"flour\" + 0.003*\"ripening\" + 0.003*\"bread\" + 0.003*\"egg\"'),\n",
       " (13,\n",
       "  '0.021*\"vinegar\" + 0.009*\"wine\" + 0.008*\"rice\" + 0.005*\"white\" + 0.005*\"balsamic\" + 0.004*\"cake\" + 0.004*\"rice_wine\" + 0.004*\"cider\" + 0.004*\"knife\" + 0.004*\"cider_vinegar\"'),\n",
       " (10,\n",
       "  '0.056*\"steak\" + 0.014*\"grill\" + 0.011*\"medium\" + 0.008*\"rest\" + 0.005*\"flip\" + 0.005*\"rare\" + 0.005*\"pepper\" + 0.005*\"thickness\" + 0.005*\"sear\" + 0.005*\"outside\"'),\n",
       " (16,\n",
       "  '0.009*\"hour\" + 0.008*\"onion\" + 0.008*\"rib\" + 0.007*\"half\" + 0.006*\"slice\" + 0.006*\"baking\" + 0.006*\"gas\" + 0.005*\"rice\" + 0.005*\"knife\" + 0.005*\"rest\"'),\n",
       " (4,\n",
       "  '0.019*\"pasta\" + 0.016*\"garlic\" + 0.010*\"fat\" + 0.009*\"cheese\" + 0.007*\"roast\" + 0.007*\"chocolate\" + 0.007*\"melt\" + 0.005*\"bean\" + 0.005*\"yeast\" + 0.005*\"bowl\"'),\n",
       " (5,\n",
       "  '0.015*\"uk\" + 0.014*\"bean\" + 0.010*\"soak\" + 0.007*\"gas\" + 0.007*\"lemon\" + 0.006*\"ice\" + 0.006*\"fruit\" + 0.006*\"juice\" + 0.005*\"board\" + 0.005*\"skin\"'),\n",
       " (3,\n",
       "  '0.013*\"cheese\" + 0.012*\"potato\" + 0.010*\"vegetable\" + 0.007*\"cream\" + 0.007*\"grate\" + 0.006*\"pepper\" + 0.005*\"spice\" + 0.005*\"bake\" + 0.005*\"flour\" + 0.004*\"grater\"'),\n",
       " (11,\n",
       "  '0.010*\"egg\" + 0.006*\"book\" + 0.005*\"garlic\" + 0.005*\"milk\" + 0.005*\"fry\" + 0.004*\"cheese\" + 0.004*\"skin\" + 0.004*\"hour\" + 0.004*\"freeze\" + 0.003*\"chicken\"')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bag', 0.010500341319300025),\n",
       " ('herb', 0.0095810207676818686),\n",
       " ('ice', 0.0075708166957713617),\n",
       " ('leaf', 0.0072657253245815654),\n",
       " ('basil', 0.00665525668017454),\n",
       " ('skin', 0.0061031745693093629),\n",
       " ('paper', 0.0058831482760993087),\n",
       " ('freeze', 0.0058423073262144501),\n",
       " ('cold', 0.0057310478119483865),\n",
       " ('fish', 0.0056977872937214744)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baking\n",
      "#Docs: 74\n",
      "[(3, 0.99969861353423572)]\n",
      "\n",
      "eggs\n",
      "#Docs: 38\n",
      "[(10, 0.99962213699351399)]\n",
      "\n",
      "pasta\n",
      "#Docs: 19\n",
      "[(8, 0.99891468476715684)]\n",
      "\n",
      "herbs\n",
      "#Docs: 13\n",
      "[(7, 0.99709305343567833)]\n",
      "\n",
      "beef\n",
      "#Docs: 15\n",
      "[(12, 0.99879414856070836)]\n",
      "\n",
      "salmon\n",
      "#Docs: 6\n",
      "[(18, 0.99213976423946681)]\n"
     ]
    }
   ],
   "source": [
    "tag = 'baking'\n",
    "print('%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'eggs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'pasta'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'herbs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'beef'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'salmon'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Hellinger distance:\n",
    "\n",
    "$$\n",
    "H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^K (\\sqrt{p_i} - \\sqrt{q_i})^2}\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are both topic distributions for two different tags. We define the similarity as\n",
    "$$\n",
    "S(p, q) = \\frac{1}{1 + H(p, q)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vec1, vec2):\n",
    "    dist = hellinger(sparse2full(vec1, num_topics), sparse2full(vec2, num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec, tag_vecs):\n",
    "    sims = [similarity(vec, vec2) for vec2 in tag_vecs]\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_vecs = [model.get_author_topics(tag, minimum_probability=0.0) for tag in train_tag_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2tag = dict(zip(range(len(train_tag_set)), list(train_tag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>baking</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>food-safety</td>\n",
       "      <td>0.965386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>poaching</td>\n",
       "      <td>0.916794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>bread</td>\n",
       "      <td>0.881186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>chili-peppers</td>\n",
       "      <td>0.866564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>allium</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>dry-aging</td>\n",
       "      <td>0.855882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>please-remove-this-tag</td>\n",
       "      <td>0.843243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>dairy-free</td>\n",
       "      <td>0.840160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>fats</td>\n",
       "      <td>0.816782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Tag     Score\n",
       "231                  baking  1.000000\n",
       "290             food-safety  0.965386\n",
       "278                poaching  0.916794\n",
       "82                    bread  0.881186\n",
       "260           chili-peppers  0.866564\n",
       "85                   allium  0.859600\n",
       "173               dry-aging  0.855882\n",
       "240  please-remove-this-tag  0.843243\n",
       "212              dairy-free  0.840160\n",
       "196                    fats  0.816782"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(model.get_author_topics('baking', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Docs: 1\n"
     ]
    }
   ],
   "source": [
    "print('#Docs:', len(model.author2doc['baking-powder']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>baking-powder</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>stews</td>\n",
       "      <td>0.998336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>juice</td>\n",
       "      <td>0.997528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>creme-brulee</td>\n",
       "      <td>0.995559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>elderberries</td>\n",
       "      <td>0.995318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>measuring-scales</td>\n",
       "      <td>0.995110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>sponge-cake</td>\n",
       "      <td>0.992812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>asparagus</td>\n",
       "      <td>0.992152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>salad-dressing</td>\n",
       "      <td>0.991331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.989778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tag     Score\n",
       "202     baking-powder  1.000000\n",
       "120             stews  0.998336\n",
       "257             juice  0.997528\n",
       "378      creme-brulee  0.995559\n",
       "225      elderberries  0.995318\n",
       "154  measuring-scales  0.995110\n",
       "394       sponge-cake  0.992812\n",
       "339         asparagus  0.992152\n",
       "360    salad-dressing  0.991331\n",
       "54               fire  0.989778"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(model.get_author_topics('baking-powder', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the tag of a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=None, num_topics=num_topics, id2word=dictionary.id2token)\n",
    "lda.state.sstats = model.state.sstats\n",
    "lda.iterations = 1000  # Make sure training converges on document when calling lda[doc]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post tags:\n",
      " ['eggs']\n",
      "Post body:\n",
      " ['use', 'brown', 'extra', 'large', 'egg', 'can', 'honestly', 'habit', 'point', 'distinct', 'advantage', 'disadvantage', 'like', 'flavor', 'shelf', 'life', 'egg', 'nutrition', 'center', 'faq', 'page', 'entry', 'topic', 'basically', 'color', 'egg', 'affect', 'egg', 'flavor', 'nutritional', 'value', 'simply', 'depend', 'particular', 'breed', 'chicken', 'lay', 'egg', 'white', 'egg', 'white', 'hen', 'brown', 'egg', 'brown', 'hen', 'worth', 'note', 'enc', 'point', 'generally', 'brown', 'hen', 'large', 'require', 'feed', 'egg', 'slightly', 'high', 'price', 'difference', 'notice', 'free', 'range', 'egg', 'instead', 'factory', 'farm', 'egg', 'slight', 'yolk', 'color', 'difference', 'think', 'slightly', 'good', 'flavor', 'come', 'range', 'color', 'include', 'green', 'brown', 'white', 'difference', 'white', 'brown', 'egg', 'purely', 'cosmetic', 'nutritional', 'taste', 'difference', 'brown', 'egg', 'imo', 'look', 'cooler', 'cosmetic', 'base', 'breed', 'chicken', 'grow', 'south', 'american', 'araca√±a', 'chicken', 'able', 'fly', 'hen', 'lay', 'pale', 'yellow', 'green', 'egg', 'kind', 'like', 'pre', 'color', 'easter', 'egg', 'South', 'American', 'habit_point', 'shelf_life', 'egg_nutrition', 'center_faq', 'page_entry', 'topic_basically', 'nutritional_value', 'depend_particular', 'breed_chicken', 'egg_white', 'egg_white', 'hen_brown', 'brown_hen', 'worth_note', 'enc_point', 'brown_hen', 'require_feed', 'high_price', 'difference_notice', 'free_range', 'instead_factory', 'farm_egg', 'slight_yolk', 'color_difference', 'range_color', 'include_green', 'egg_imo', 'look_cooler', 'breed_chicken']\n"
     ]
    }
   ],
   "source": [
    "postid = 2\n",
    "doc = test_corpus[postid]\n",
    "print('Post tags:\\n', test_postid2tagname[postid])\n",
    "print('Post body:\\n', test_docs[postid])\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in train_tag_set:\n",
    "        print('Tag \"', tag, '\" not in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>beverages</td>\n",
       "      <td>0.973676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cream</td>\n",
       "      <td>0.969973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>fresh</td>\n",
       "      <td>0.961778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>honey</td>\n",
       "      <td>0.960810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>raw</td>\n",
       "      <td>0.959857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>cheesecake</td>\n",
       "      <td>0.959098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>classification</td>\n",
       "      <td>0.956090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>resources</td>\n",
       "      <td>0.955922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>maple-syrup</td>\n",
       "      <td>0.954864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>shopping</td>\n",
       "      <td>0.953665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tag     Score\n",
       "312       beverages  0.973676\n",
       "3             cream  0.969973\n",
       "105           fresh  0.961778\n",
       "374           honey  0.960810\n",
       "369             raw  0.959857\n",
       "423      cheesecake  0.959098\n",
       "373  classification  0.956090\n",
       "145       resources  0.955922\n",
       "216     maple-syrup  0.954864\n",
       "68         shopping  0.953665"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = get_sims(lda.get_document_topics(doc, minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>catering</td>\n",
       "      <td>0.681661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>melting-chocolate</td>\n",
       "      <td>0.832565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tag     Score\n",
       "75            catering  0.681661\n",
       "289  melting-chocolate  0.832565"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_threshold = 0.6\n",
    "pred = sims_df.loc[sims_df.Score > pred_threshold]\n",
    "pred_tags = list(pred.Tag)\n",
    "pred_prob = list(pred.Score)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for tag in pred_tags:\n",
    "    if tag in test_postid2tagname[postid]:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in pred_tags:\n",
    "        fn += 1\n",
    "        \n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "if precision + recall == 0:\n",
    "    f1_score = 0.0\n",
    "else:\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print('F1 score: ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(train_tag_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baking</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>aging</td>\n",
       "      <td>0.994557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>crepe</td>\n",
       "      <td>0.987024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ganache</td>\n",
       "      <td>0.983886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>sauce</td>\n",
       "      <td>0.980548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>puree</td>\n",
       "      <td>0.978956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>roast-beef</td>\n",
       "      <td>0.975619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>roasting</td>\n",
       "      <td>0.971207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>taffy</td>\n",
       "      <td>0.960213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>rye</td>\n",
       "      <td>0.959389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tag     Score\n",
       "0        baking  1.000000\n",
       "88        aging  0.994557\n",
       "24        crepe  0.987024\n",
       "96      ganache  0.983886\n",
       "386       sauce  0.980548\n",
       "38        puree  0.978956\n",
       "191  roast-beef  0.975619\n",
       "243    roasting  0.971207\n",
       "233       taffy  0.960213\n",
       "418         rye  0.959389"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get similarities to some tag.\n",
    "tag_name = 'baking'\n",
    "sims = index[model[tag_name]]\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 0 ns, total: 41.6 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%time lda = LdaModel(corpus, num_topics=10, id2word=dictionary.id2token, iterations=1, \\\n",
    "                     passes=100, eval_every=0, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"sauce\" + 0.031*\"like\" + 0.026*\"taste\" + 0.021*\"eat\" + 0.017*\"rice\" + 0.014*\"dish\" + 0.014*\"ingredient\" + 0.013*\"love\" + 0.012*\"look\" + 0.012*\"tip\"'),\n",
       " (1,\n",
       "  '0.042*\"egg\" + 0.027*\"milk\" + 0.025*\"difference\" + 0.020*\"use\" + 0.017*\"flavor\" + 0.014*\"day\" + 0.014*\"white\" + 0.014*\"question\" + 0.013*\"bit\" + 0.012*\"wine\"'),\n",
       " (2,\n",
       "  '0.036*\"meat\" + 0.025*\"chicken\" + 0.023*\"cook\" + 0.020*\"pan\" + 0.019*\"cut\" + 0.015*\"like\" + 0.015*\"use\" + 0.014*\"hear\" + 0.013*\"food\" + 0.012*\"stick\"'),\n",
       " (3,\n",
       "  '0.064*\"recipe\" + 0.032*\"use\" + 0.022*\"bread\" + 0.020*\"substitute\" + 0.019*\"bake\" + 0.017*\"baking\" + 0.016*\"cake\" + 0.014*\"chocolate\" + 0.012*\"problem\" + 0.011*\"dough\"'),\n",
       " (4,\n",
       "  '0.036*\"way\" + 0.029*\"good\" + 0.023*\"temperature\" + 0.021*\"cheese\" + 0.019*\"time\" + 0.018*\"cook\" + 0.016*\"long\" + 0.014*\"beef\" + 0.014*\"vegetable\" + 0.013*\"use\"'),\n",
       " (5,\n",
       "  '0.028*\"oil\" + 0.022*\"oven\" + 0.020*\"use\" + 0.017*\"knife\" + 0.016*\"good\" + 0.013*\"prepare\" + 0.013*\"wonder\" + 0.011*\"skin\" + 0.011*\"cook\" + 0.011*\"place\"'),\n",
       " (6,\n",
       "  '0.033*\"add\" + 0.031*\"salt\" + 0.021*\"garlic\" + 0.017*\"water\" + 0.015*\"thing\" + 0.012*\"pot\" + 0.012*\"hard\" + 0.012*\"try\" + 0.012*\"chop\" + 0.011*\"point\"'),\n",
       " (7,\n",
       "  '0.052*\"use\" + 0.021*\"butter\" + 0.018*\"boil\" + 0.015*\"pasta\" + 0.015*\"flour\" + 0.014*\"type\" + 0.013*\"cook\" + 0.013*\"good\" + 0.012*\"high\" + 0.011*\"look\"'),\n",
       " (8,\n",
       "  '0.022*\"cream\" + 0.019*\"cook\" + 0.015*\"mix\" + 0.014*\"freeze\" + 0.013*\"half\" + 0.012*\"time\" + 0.012*\"store\" + 0.012*\"dry\" + 0.011*\"hour\" + 0.011*\"try\"'),\n",
       " (9,\n",
       "  '0.033*\"uk\" + 0.021*\"pepper\" + 0.019*\"sugar\" + 0.016*\"au\" + 0.013*\"tomato\" + 0.011*\"potato\" + 0.010*\"ground\" + 0.010*\"bean\" + 0.009*\"ca\" + 0.008*\"hot\"')]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = ET.parse(input_fname)\n",
    "postid2tagname = dict()\n",
    "postid = 0\n",
    "posts = []\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if item.get('Tags') is not None:\n",
    "        tags = item.get('Tags')\n",
    "        tags = re.findall('<(.+?)>', tags)\n",
    "        # NOTE: consider using a tag that is common for all posts, or\n",
    "        # a tag that is only for this particular post.\n",
    "        #tags.append('SUPER_TAG')\n",
    "        #tags.append('POST_ID' + str(postid))\n",
    "        postid2tagname[postid] = tags\n",
    "        posts.append(item.get('Body'))\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        postid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_posts = len(posts)\n",
    "docs = []\n",
    "for post in posts[:]:\n",
    "    # Remove any HTML tags, such as <p>.\n",
    "    text = re.sub('<[^<]+?>', '', post)\n",
    "    doc = nlp(text)\n",
    "    ents = doc.ents  # Named entities.\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in STOPWORDS]\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "#max_freq = 0.5\n",
    "#min_wordcount = 20\n",
    "#dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "dict0 = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Serialize the corpus.\n",
    "#MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "#corpus = MmCorpus('/tmp/corpus.mm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
