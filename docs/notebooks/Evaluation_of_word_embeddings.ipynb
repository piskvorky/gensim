{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da5ed4a",
   "metadata": {},
   "source": [
    "# How to evaluate word embedding models\n",
    "\n",
    "The existing state-of-the-art approaches for word embedding evaluation can be divided into two major classes: _intrinsic_ vs _extrinsic_ evaluation.\n",
    "\n",
    "_Extrinsic metrics_ perform the evaluation by using embeddings as features for specific downstream tasks. For instance, question deduplication, Part-of-Speech (POS) Tagging, Language Modelling, and Named Entity Recognition (NER).  As a drawback, extrinsic metrics are:\n",
    "1. computationally heavy;\n",
    "2. have high complexity of creating gold standard datasets for downstream tasks;\n",
    "3.  have a lack of performance consistency over different tasks.\n",
    "\n",
    "_Intrinsic metrics_ evaluate the quality of a vector model per se, independently from specific downstream tasks. They measure syntactic or semantic relationships between words directly, typically using a gold benchmark of semantic similarity between pair of words. \n",
    "The gold benchmark can be obtained directly getting human judgements or using automated semantic similarity measures.\n",
    "\n",
    "Some of the limitations of human annotated benchmarks are:\n",
    "1.  suffering from word sense ambiguity(faced by a human tester) and subjectivity;\n",
    "2.  facing difficulties in finding significant differences between models due to the small size and low inter-annotator agreement of existing datasets;\n",
    "3.  need for constructing judgement datasets for each language and each domain, given that word meanings can change a lot across different domains.\n",
    "Moreover, the update and the upscale of handcrafted resources like the similarity human scores are costly, time expensive, and error-prone. \n",
    "\n",
    "A well-known __intrinsic evaluation method__ is Semantic Relatedness (or Similarity) (see [Baroni et al.](https://aclanthology.org/P14-1023.pdf), [Schnabel et al.](https://aclanthology.org/D15-1036.pdf)\n",
    "), which evaluates the performance of a vector model by the correlation between the cosine similarity between pairs of word vectors and the semantic relatedness (or similarity) between them in the gold benchmark.\n",
    "\n",
    "## Intrinsic evaluations\n",
    "\n",
    "Gensim provides the possibility to evaluate word embedding models through two of the most used intrinsic evaluation tasks: semantic similarity and word analogy.\n",
    "\n",
    "You can evaluate your models on those tasks using `evaluate_word_pairs(pairs)` and `evaluate_word_analogies(analogies)`, specifying the benchmark you want to use as the first argument of the function.\n",
    "\n",
    "### How to evaluate word embedding models with taxonomy‐based semantic similarity measures\n",
    "\n",
    "The evaluation based on taxonomy‐based semantic similarity measures is another type of intrinsic evaluation, and it exploit the information encoded in an existing taxonomy to build a benchmark for the evaluation of word embeddings.\n",
    "\n",
    "To perform this type of evaluation you can use the benchmark HSS4570.tsv (you can find it at test/test_data/HSS4570.tsv) through the Gensim function `evaluate_word_pairs()`, which returns:\n",
    "\n",
    "* the Pearson correlation coefficient with 2-tailed p-value;\n",
    "\n",
    "* the Spearman rank-order correlation coefficient between the similarities from the dataset and the similarities produced by the model itself, with 2-tailed p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b765dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "path_to_model = 'models/ft_vectors_0_10_50_5.txt'\n",
    "model = KeyedVectors.load_word2vec_format(path_to_model, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df874c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.42422276164431727, 4.739561944305236e-193),\n",
       " SpearmanrResult(correlation=0.4326345056769725, pvalue=1.508555569557493e-201),\n",
       " 3.063457330415755)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pkgutil\n",
    "similarities = model.evaluate_word_pairs('benchmark/HSS4570.tsv', delimiter='\\t')\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad743a6",
   "metadata": {},
   "source": [
    "To perform this evaluation you can also use the library `TaxoSS` to create your benchmark using the taxonomy of your choice that better suits the topic of your work, to select the word embedding model that best encodes the taxonomic relationships between the concepts in the taxonomy.\n",
    "\n",
    "You can compute the semantic similarity in the following way:\n",
    "\n",
    "1. create a list of pairs of words for comparison, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7e24d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'dog'], ['bird', 'fish'], ['mammal', 'vertebrate']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = ['cat', 'bird', 'mammal']\n",
    "w2 = ['dog', 'fish', 'vertebrate']\n",
    "words = [[x, y] for x, y in zip(w1, w2)]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec3fd9",
   "metadata": {},
   "source": [
    "2. for each pair compute the similarity between the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fd5026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5578320203297156, 1.4267224907237086, 1.4852139609136645]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TaxoSS.functions import semantic_similarity\n",
    "hss = []\n",
    "for w in words:\n",
    "    hss.append(semantic_similarity(w[0], w[1], 'hss'))\n",
    "hss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f68bf",
   "metadata": {},
   "source": [
    "3.  create your benchmark as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15c13f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>hss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>2.557832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>fish</td>\n",
       "      <td>1.426722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mammal</td>\n",
       "      <td>vertebrate</td>\n",
       "      <td>1.485214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1       word2       hss\n",
       "0     cat         dog  2.557832\n",
       "1    bird        fish  1.426722\n",
       "2  mammal  vertebrate  1.485214"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "benchmark = pd.DataFrame({'word1':[x[0] for x in words], 'word2':[x[1] for x in words], 'hss':hss})\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05b2d2",
   "metadata": {},
   "source": [
    "The function `semantic_similarity(word1, word2, kind, ic)` has these options for the argument `kind`:\n",
    "\n",
    "* *hss* -> HSS (_default_)\n",
    "* *wup* -> WUP\n",
    "* *lcs* -> LC\n",
    "* *path_sim* -> Shortest Path\n",
    "* *resnik* -> Resnik\n",
    "* *jcn* -> Jiang-Conrath\n",
    "* *lin* -> Lin\n",
    "* *seco* -> Seco\n",
    "\n",
    "You can choose the one you prefer, and to have more information about them see https://link.springer.com/article/10.1007/s12559-021-09987-7.\n",
    "\n",
    "Now you can evaluate your word embedding model using the benchmark you just created:\n",
    "\n",
    "1. load your word embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd02c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "path_to_model = 'models/ft_vectors_0_10_50_5.txt'\n",
    "model = KeyedVectors.load_word2vec_format(path_to_model, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d39ad",
   "metadata": {},
   "source": [
    "2. compute the cosine similarity for each pair of words in your benchmark in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ed31ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>hss</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>2.557832</td>\n",
       "      <td>0.878396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>fish</td>\n",
       "      <td>1.426722</td>\n",
       "      <td>0.709674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mammal</td>\n",
       "      <td>vertebrate</td>\n",
       "      <td>1.485214</td>\n",
       "      <td>0.784841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1       word2       hss  cosine_similarity\n",
       "0     cat         dog  2.557832           0.878396\n",
       "1    bird        fish  1.426722           0.709674\n",
       "2  mammal  vertebrate  1.485214           0.784841"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = [model.similarity(x, y) for x, y in zip(benchmark.word1, benchmark.word2)]\n",
    "benchmark['cosine_similarity'] = cos_sim\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3042410f",
   "metadata": {},
   "source": [
    "3. compute the Spearman (or Pearson) correlation between `benchmark['hss']` and `benchmark['cosine_similarity']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127ceb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9151906835681539, 0.2640797648114273)\n",
      "SpearmanrResult(correlation=1.0, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "print(scipy.stats.pearsonr(benchmark['hss'], benchmark['cosine_similarity']))\n",
    "print(scipy.stats.spearmanr(benchmark['hss'], benchmark['cosine_similarity']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxoss",
   "language": "python",
   "name": "taxoss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
