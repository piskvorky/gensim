{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Visdom\n",
    "\n",
    "Install it with:\n",
    "\n",
    "`pip install visdom`\n",
    "\n",
    "Start the server:\n",
    "\n",
    "`python -m visdom.server`\n",
    "\n",
    "Visdom now can be accessed at http://localhost:8097 in the browser.\n",
    "\n",
    "To monitor the training progress of LDA live in Visdom browser, you can set `viz=True` in the LDA function call.\n",
    "\n",
    "`model = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, passes=50, viz=True)`\n",
    "\n",
    "When the model is set for training, you can open http://localhost:8097 to see the training progress.\n",
    "\n",
    "\n",
    "## LDA Training Visualization\n",
    "\n",
    "There are four types of graphs which are plotted for LDA:\n",
    "\n",
    "**Coherence**\n",
    "\n",
    "Coherence is a measure used to evaluate topic models. A good model will generate coherent topics, i.e., topics         with high topic coherence scores. Good topics are topics that can be described by a short label based on the topic     terms they spit out. \n",
    "\n",
    "<img src=\"Coherence.gif\">\n",
    "\n",
    "Now, this graph along with the others explained below, can be used to decide if it's time to stop the training. We     can see if the value stops changing after some epochs and that we are able to get the highest possible coherence       of our model.  \n",
    "\n",
    "\n",
    "**Perplexity**\n",
    "\n",
    "Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. In LDA, topics are described by a probability distribution over vocabulary words. So, perplexity can be used to compare probabilistic models like LDA.\n",
    "\n",
    "<img src=\"Perplexity.gif\">\n",
    "\n",
    "For a good model, perplexity should be as low as possible.\n",
    "\n",
    "\n",
    "**Topic Difference**\n",
    "\n",
    "Topic Diff calculates the distance between two LDA models. This distance is calculated based on the topics, by either using their probability distribution over vocabulary words (kullback_leibler, hellinger) or by simply using the common vocabulary words between the topics from both model.\n",
    "\n",
    "<img src=\"Diff.gif\">\n",
    "\n",
    "In the heatmap, X-axis define the Epoch no. and Y-axis define the distance between the identical topic from consecutive epochs. For ex. a particular cell in the heatmap with values (x=3, y=5, z=0.4) represent the distance(=0.4) between the topic 5 from 3rd epoch and topic 5 from 2nd epoch. With increasing epochs, the distance between the identical topics should decrease.\n",
    "  \n",
    "  \n",
    "**Convergence**\n",
    "\n",
    "Convergence is the sum of the difference between all the identical topics from two consecutive epochs. It is basically the sum of column values in the heatmap above.\n",
    "\n",
    "<img src=\"Convergence.gif\">\n",
    "\n",
    "The model is said to be converged when the convergence value stops descending with increasing epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
