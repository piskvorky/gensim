{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Poincare Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how well Poincare embeddings perform on the tasks detailed in the [original paper](https://arxiv.org/pdf/1705.08039.pdf) about the embeddings.\n",
    "\n",
    "The following two external, open-source implementations are used - \n",
    "1. [C++](https://github.com/TatsuyaShirakawa/poincare-embedding)\n",
    "2. [Numpy](https://github.com/nishnik/poincare_embeddings)\n",
    "\n",
    "This is the list of tasks - \n",
    "1. WordNet reconstruction\n",
    "2. WordNet link prediction\n",
    "3. Link prediction in collaboration networks (evaluation incomplete)\n",
    "4. Lexical entailment on HyperLex\n",
    "\n",
    "A more detailed explanation of the tasks and the evaluation methodology is present in the individual evaluation subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "The following section performs the following - \n",
    "1. Imports required python libraries and downloads the wordnet data\n",
    "2. Clones the repositories containing the C++ and Numpy implementations of the Poincare embeddings\n",
    "3. Applies patches containing minor changes to the implementations.\n",
    "4. Compiles the C++ sources to create a binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jayant/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict, OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "import click\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.poincare import PoincareModel, PoincareRelations, \\\n",
    "    ReconstructionEvaluation, LinkPredictionEvaluation, \\\n",
    "    LexicalEntailmentEvaluation, PoincareEmbedding\n",
    "from gensim.utils import check_output\n",
    "import nltk\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "from pygtrie import Trie\n",
    "from scipy.spatial.distance import euclidean, pdist\n",
    "from scipy.stats import spearmanr\n",
    "from smart_open import smart_open\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all the above libraries are part of the gensim dependencies, so they might need to be installed separately. These requirements are listed in the poincare [requirements.txt](poincare/requirements.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please set the variable `parent_directory` below to change the directory to which the repositories are cloned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to `False` to not remove and re-download repos for external implementations\n",
    "force_setup = False\n",
    "\n",
    "# The poincare datasets, models and source code for external models are downloaded to this directory\n",
    "parent_directory = os.path.join(current_directory, 'poincare')\n",
    "! mkdir -p {parent_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jayant/projects/gensim/docs/notebooks/poincare\n",
      "Cloning into 'poincare-np-embedding'...\n",
      "remote: Counting objects: 20, done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 20 (delta 2), reused 20 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (20/20), done.\n",
      "Checking connectivity... done.\n",
      "Cloning into 'poincare-cpp-embedding'...\n",
      "remote: Counting objects: 96, done.\u001b[K\n",
      "remote: Total 96 (delta 0), reused 0 (delta 0), pack-reused 96\u001b[K\n",
      "Unpacking objects: 100% (96/96), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "% cd {parent_directory}\n",
    "\n",
    "# Clone repos\n",
    "np_repo_name = 'poincare-np-embedding'\n",
    "if force_setup and os.path.exists(np_repo_name):\n",
    "    ! rm -rf {np_repo_name}\n",
    "clone_np_repo = not os.path.exists(np_repo_name)\n",
    "if clone_np_repo:\n",
    "    ! git clone https://github.com/nishnik/poincare_embeddings.git {np_repo_name}\n",
    "\n",
    "cpp_repo_name = 'poincare-cpp-embedding'\n",
    "if force_setup and os.path.exists(cpp_repo_name):\n",
    "    ! rm -rf {cpp_repo_name}\n",
    "clone_cpp_repo = not os.path.exists(cpp_repo_name)\n",
    "if clone_cpp_repo:\n",
    "    ! git clone https://github.com/TatsuyaShirakawa/poincare-embedding.git {cpp_repo_name}\n",
    "\n",
    "patches_applied = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply patches\n",
    "if clone_cpp_repo and not patches_applied:\n",
    "    % cd {cpp_repo_name}\n",
    "    ! git apply ../poincare_burn_in_eps.patch\n",
    "\n",
    "if clone_np_repo and not patches_applied:\n",
    "    % cd ../{np_repo_name}\n",
    "    ! git apply ../poincare_numpy.patch\n",
    "    \n",
    "patches_applied = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jayant/projects/gensim/docs/notebooks/poincare/poincare-cpp-embedding\n",
      "/home/jayant/projects/gensim/docs/notebooks/poincare/poincare-cpp-embedding/work\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/jayant/projects/gensim/docs/notebooks/poincare/poincare-cpp-embedding/work\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable poincare_embedding\u001b[0m\n",
      "[100%] Built target poincare_embedding\n",
      "/home/jayant/projects/gensim/docs/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Compile the code for the external c++ implementation into a binary\n",
    "% cd {parent_directory}/{cpp_repo_name}\n",
    "! mkdir -p work\n",
    "% cd work\n",
    "! cmake ..\n",
    "! make\n",
    "% cd {current_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to install an updated version of `cmake` to be able to compile the source code. Please make sure that the binary `poincare_embedding` has been created before proceeding by verifying the above cell does not raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_binary_path = os.path.join(parent_directory, cpp_repo_name, 'work', 'poincare_embedding')\n",
    "assert(os.path.exists(cpp_binary_path)), 'Binary file doesnt exist at %s' % cpp_binary_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "### 2.1 Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These directories are auto created in the current directory for storing poincare datasets and models\n",
    "data_directory = os.path.join(parent_directory, 'data')\n",
    "models_directory = os.path.join(parent_directory, 'models')\n",
    "\n",
    "# Create directories\n",
    "! mkdir -p {data_directory}\n",
    "! mkdir -p {models_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82115 nouns\n",
      "743241 hypernyms\n"
     ]
    }
   ],
   "source": [
    "# Prepare the WordNet data\n",
    "wordnet_file = os.path.join(data_directory, 'wordnet_noun_hypernyms.tsv')\n",
    "! python {parent_directory}/{cpp_repo_name}/scripts/create_wordnet_noun_hierarchy.py {wordnet_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-26 14:09:50--  http://people.ds.cam.ac.uk/iv250/paper/hyperlex/hyperlex-data.zip\n",
      "Resolving people.ds.cam.ac.uk (people.ds.cam.ac.uk)... 131.111.3.47\n",
      "Connecting to people.ds.cam.ac.uk (people.ds.cam.ac.uk)|131.111.3.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 183900 (180K) [application/zip]\n",
      "Saving to: ‘/home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex-data.zip’\n",
      "\n",
      "/home/jayant/projec 100%[===================>] 179.59K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2017-10-26 14:09:50 (2.23 MB/s) - ‘/home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex-data.zip’ saved [183900/183900]\n",
      "\n",
      "Archive:  /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex-data.zip\n",
      "   creating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/nouns-verbs/\n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/nouns-verbs/hyperlex-verbs.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/nouns-verbs/hyperlex-nouns.txt  \n",
      "   creating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/\n",
      "   creating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/random/\n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/random/hyperlex_training_all_random.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/random/hyperlex_test_all_random.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/random/hyperlex_dev_all_random.txt  \n",
      "   creating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/lexical/\n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/lexical/hyperlex_dev_all_lexical.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/lexical/hyperlex_test_all_lexical.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/splits/lexical/hyperlex_training_all_lexical.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/hyperlex-all.txt  \n",
      "  inflating: /home/jayant/projects/gensim/docs/notebooks/poincare/data/hyperlex/README.txt  \n"
     ]
    }
   ],
   "source": [
    "# Prepare the HyperLex data\n",
    "hyperlex_url = \"http://people.ds.cam.ac.uk/iv250/paper/hyperlex/hyperlex-data.zip\"\n",
    "! wget {hyperlex_url} -O {data_directory}/hyperlex-data.zip\n",
    "if os.path.exists(os.path.join(data_directory, 'hyperlex')):\n",
    "    ! rm -r {data_directory}/hyperlex\n",
    "! unzip {data_directory}/hyperlex-data.zip -d {data_directory}/hyperlex/\n",
    "hyperlex_file = os.path.join(data_directory, 'hyperlex', 'nouns-verbs', 'hyperlex-nouns.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training [C++ embeddings](https://github.com/TatsuyaShirakawa/poincare-embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cpp_model(\n",
    "    binary_path, data_file, output_file, dim, epochs, neg,\n",
    "    num_threads, epsilon, burn_in, seed=0):\n",
    "    \"\"\"Train a poincare embedding using the c++ implementation\n",
    "    \n",
    "    Args:\n",
    "        binary_path (str): Path to the compiled c++ implementation binary\n",
    "        data_file (str): Path to tsv file containing relation pairs\n",
    "        output_file (str): Path to output file containing model\n",
    "        dim (int): Number of dimensions of the trained model\n",
    "        epochs (int): Number of epochs to use\n",
    "        neg (int): Number of negative samples to use\n",
    "        num_threads (int): Number of threads to use for training the model\n",
    "        epsilon (float): Constant used for clipping below a norm of one\n",
    "        burn_in (int): Number of epochs to use for burn-in init (0 means no burn-in)\n",
    "    \n",
    "    Notes: \n",
    "        If `output_file` already exists, skips training\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print('File %s exists, skipping' % output_file)\n",
    "        return\n",
    "    args = {\n",
    "        'dim': dim,\n",
    "        'max_epoch': epochs,\n",
    "        'neg_size': neg,\n",
    "        'num_thread': num_threads,\n",
    "        'epsilon': epsilon,\n",
    "        'burn_in': burn_in,\n",
    "        'learning_rate_init': 0.1,\n",
    "        'learning_rate_final': 0.0001,\n",
    "    }\n",
    "    cmd = [binary_path, data_file, output_file]\n",
    "    for option, value in args.items():\n",
    "        cmd.append(\"--%s\" % option)\n",
    "        cmd.append(str(value))\n",
    "    \n",
    "    return check_output(args=cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes = [5, 10, 20, 50, 100, 200]\n",
    "default_params = {\n",
    "    'neg': 20,\n",
    "    'epochs': 50,\n",
    "    'threads': 8,\n",
    "    'eps': 1e-6,\n",
    "    'burn_in': 0,\n",
    "}\n",
    "\n",
    "non_default_params = {\n",
    "    'neg': [10],\n",
    "    'epochs': [100, 200],\n",
    "    'threads': [1],\n",
    "    'eps': [1e-5],\n",
    "    'burn_in': [5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cpp_model_name_from_params(params, prefix):\n",
    "    name = ['%s_%s' % (key, params[key]) for key in sorted(params.keys())]\n",
    "    return '%s_%s' % (prefix, '_'.join(name))\n",
    "\n",
    "\n",
    "def train_model_with_params(params, train_file, model_sizes, prefix, implementation):\n",
    "    \"\"\"Trains models with given params for multiple model sizes using the given implementation\n",
    "    \n",
    "    Args:\n",
    "        params (dict): parameters to train the model with\n",
    "        train_file (str): Path to tsv file containing relation pairs\n",
    "        model_sizes (list): list of dimension sizes (integer) to train the model with\n",
    "        prefix (str): prefix to use for the saved model filenames\n",
    "        implementation (str): whether to use the numpy or c++ implementation,\n",
    "                              allowed values: 'numpy', 'c++'\n",
    "   \n",
    "   Returns:\n",
    "        tuple (model_name, model_files)\n",
    "        model_files is a dict of (size, filename) pairs\n",
    "        Example: ('cpp_model_epochs_50', {5: 'models/cpp_model_epochs_50_dim_5'})\n",
    "    \"\"\"\n",
    "    files = {}\n",
    "    if implementation == 'c++':\n",
    "        model_name = cpp_model_name_from_params(params, prefix)\n",
    "    elif implementation == 'numpy':\n",
    "        model_name = np_model_name_from_params(params, prefix)\n",
    "    else:\n",
    "        raise ValueError('Given implementation %s not found' % implementation)\n",
    "    for model_size in model_sizes:\n",
    "        output_file_name = '%s_dim_%d' % (model_name, model_size)\n",
    "        output_file = os.path.join(models_directory, output_file_name)\n",
    "        print('Training model %s of size %d' % (model_name, model_size))\n",
    "        if implementation == 'c++':\n",
    "            out = train_cpp_model(\n",
    "                cpp_binary_path, train_file, output_file, model_size,\n",
    "                params['epochs'], params['neg'], params['threads'],\n",
    "                params['eps'], params['burn_in'], seed=0)\n",
    "        elif implementation == 'numpy':\n",
    "            train_external_numpy_model(\n",
    "                python_script_path, train_file, output_file, model_size,\n",
    "                params['epochs'], params['neg'], seed=0)\n",
    "        else:\n",
    "            raise ValueError('Given implementation %s not found' % implementation)\n",
    "        files[model_size] = output_file\n",
    "    return (model_name, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_files['c++'] = {}\n",
    "# Train c++ models with default params\n",
    "model_name, files = train_model_with_params(default_params, wordnet_file, model_sizes, 'cpp_model', 'c++')\n",
    "model_files['c++'][model_name] = {}\n",
    "for dim, filepath in files.items():\n",
    "    model_files['c++'][model_name][dim] = filepath\n",
    "# Train c++ models with non-default params\n",
    "for param, values in non_default_params.items():\n",
    "    params = default_params.copy()\n",
    "    for value in values:\n",
    "        params[param] = value\n",
    "        model_name, files = train_model_with_params(params, wordnet_file, model_sizes, 'cpp_model', 'c++')\n",
    "        model_files['c++'][model_name] = {}\n",
    "        for dim, filepath in files.items():\n",
    "            model_files['c++'][model_name][dim] = filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training [numpy embeddings](https://github.com/nishnik/poincare_embeddings) (non-gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_path = os.path.join(parent_directory, np_repo_name, 'poincare.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_model_name_from_params(params, prefix):\n",
    "    param_keys = ['neg', 'epochs']\n",
    "    name = ['%s_%s' % (key, params[key]) for key in sorted(param_keys)]\n",
    "    return '%s_%s' % (prefix, '_'.join(name))\n",
    "\n",
    "def train_external_numpy_model(\n",
    "    script_path, data_file, output_file, dim, epochs, neg, seed=0):\n",
    "    \"\"\"Train a poincare embedding using an external numpy implementation\n",
    "    \n",
    "    Args:\n",
    "        script_path (str): Path to the Python training script\n",
    "        data_file (str): Path to tsv file containing relation pairs\n",
    "        output_file (str): Path to output file containing model\n",
    "        dim (int): Number of dimensions of the trained model\n",
    "        epochs (int): Number of epochs to use\n",
    "        neg (int): Number of negative samples to use\n",
    "    \n",
    "    Notes: \n",
    "        If `output_file` already exists, skips training\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print('File %s exists, skipping' % output_file)\n",
    "        return\n",
    "    args = {\n",
    "        'input-file': data_file,\n",
    "        'output-file': output_file,\n",
    "        'dimensions': dim,\n",
    "        'epochs': epochs,\n",
    "        'learning-rate': 0.01,\n",
    "        'num-negative': neg,\n",
    "    }\n",
    "    cmd = ['python', script_path]\n",
    "    for option, value in args.items():\n",
    "        cmd.append(\"--%s\" % option)\n",
    "        cmd.append(str(value))\n",
    "    \n",
    "    return check_output(args=cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files['numpy'] = {}\n",
    "# Train models with default params\n",
    "model_name, files = train_model_with_params(default_params, wordnet_file, model_sizes, 'np_model', 'numpy')\n",
    "model_files['numpy'][model_name] = {}\n",
    "for dim, filepath in files.items():\n",
    "    model_files['numpy'][model_name][dim] = filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cpp_embedding_to_kv(input_file, output_file, encoding='utf8'):\n",
    "    \"\"\"Given a C++ embedding tsv filepath, converts it to a KeyedVector-supported file\"\"\"\n",
    "    with smart_open(input_file, 'rb') as f:\n",
    "        lines = [line.decode(encoding) for line in f]\n",
    "    if not len(lines):\n",
    "         raise ValueError(\"file is empty\")\n",
    "    first_line = lines[0]\n",
    "    parts = first_line.rstrip().split(\"\\t\")\n",
    "    model_size = len(parts) - 1\n",
    "    vocab_size = len(lines)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('%d %d\\n' % (vocab_size, model_size))\n",
    "        for line in lines:\n",
    "            f.write(line.replace('\\t', ' '))\n",
    "\n",
    "def transform_numpy_embedding_to_kv(input_file, output_file, encoding='utf8'):\n",
    "    \"\"\"Given a numpy poincare embedding pkl filepath, converts it to a KeyedVector-supported file\"\"\"\n",
    "    np_embeddings = pickle.load(open(input_file, 'rb'))\n",
    "    random_embedding = np_embeddings[list(np_embeddings.keys())[0]]\n",
    "    \n",
    "    model_size = random_embedding.shape[0]\n",
    "    vocab_size = len(np_embeddings)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('%d %d\\n' % (vocab_size, model_size))\n",
    "        for key, vector in np_embeddings.items():\n",
    "            vector_string = ' '.join('%.6f' % value for value in vector)\n",
    "            f.write('%s %s\\n' % (key, vector_string))\n",
    "\n",
    "def load_poincare_cpp(input_filename):\n",
    "    \"\"\"Load embedding trained via C++ Poincare model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to tsv file containing embedding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PoincareEmbedding instance.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors_filename = input_filename + '.kv'\n",
    "    transform_cpp_embedding_to_kv(input_filename, keyed_vectors_filename)\n",
    "    keyed_vectors = KeyedVectors.load_word2vec_format(keyed_vectors_filename)\n",
    "    os.unlink(keyed_vectors_filename)\n",
    "    return PoincareEmbedding(keyed_vectors)\n",
    "\n",
    "@classmethod\n",
    "def load_poincare_numpy(input_filename):\n",
    "    \"\"\"Load embedding trained via Python numpy Poincare model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to pkl file containing embedding.\n",
    "\n",
    "    Returns:\n",
    "        PoincareEmbedding instance.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors_filename = input_filename + '.kv'\n",
    "    transform_numpy_embedding_to_kv(input_filename, keyed_vectors_filename)\n",
    "    keyed_vectors = PoincareKeyedVectors.load_word2vec_format(keyed_vectors_filename)\n",
    "    os.unlink(keyed_vectors_filename)\n",
    "    return PoincareEmbedding(keyed_vectors)\n",
    "\n",
    "def load_poincare_gensim(cls, input_filename):\n",
    "    \"\"\"Load embedding trained via Gensim PoincareModel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to model file.\n",
    "\n",
    "    Returns:\n",
    "        PoincareEmbedding instance.\n",
    "\n",
    "    \"\"\"\n",
    "    model = PoincareModel.load(input_filename)\n",
    "    return PoincareEmbedding(model.kv)\n",
    "\n",
    "def load_model(implementation, model_file):\n",
    "    \"\"\"Convenience function over functions to load models from different implementations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    implementation : str\n",
    "        Implementation used to create model file ('c++'/'numpy'/'gensim').\n",
    "    model_file : str\n",
    "        Path to model file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    PoincareEmbedding instance\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Raises ValueError in case of invalid value for `implementation`\n",
    "\n",
    "    \"\"\"\n",
    "    if implementation == 'c++':\n",
    "        return PoincareEmbedding.load_poincare_cpp(model_file)\n",
    "    elif implementation == 'numpy':\n",
    "        return PoincareEmbedding.load_poincare_numpy(model_file)\n",
    "    elif implementation == 'gensim':\n",
    "        return PoincareEmbedding.load_poincare_gensim(model_file)\n",
    "    else:\n",
    "        raise ValueError('Invalid implementation %s' % implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(task_name, results):\n",
    "    \"\"\"Display evaluation results of multiple embeddings on a single task in a tabular format\n",
    "    \n",
    "    Args:\n",
    "        task_name (str): name the task being evaluated\n",
    "        results (dict): mapping between embeddings and corresponding results\n",
    "    \n",
    "    \"\"\"\n",
    "    data = PrettyTable()\n",
    "    data.field_names = [\"Model Description\", \"Metric\"] + [str(dim) for dim in sorted(model_sizes)]\n",
    "    for model_name, model_results in results.items():\n",
    "        metrics = [metric for metric in model_results.keys()]\n",
    "        dims = sorted([dim for dim in model_results[metrics[0]].keys()])\n",
    "        row = [model_name, '\\n'.join(metrics) + '\\n']\n",
    "        for dim in dims:\n",
    "            scores = ['%.2f' % model_results[metric][dim] for metric in metrics]\n",
    "            row.append('\\n'.join(scores))\n",
    "        data.add_row(row)\n",
    "    data.align = 'r'\n",
    "    data_cols = data.get_string().split('\\n')[0].split('+')[1:-1]\n",
    "    col_lengths = [len(col) for col in data_cols]\n",
    "    header_col_1_length = col_lengths[0] + col_lengths[1] - 1\n",
    "    header_col_2_length = sum(col_lengths[2:]) + len(col_lengths[2:-1]) - 2\n",
    "    \n",
    "    header_col_2_content = \"Model Dimensions\"\n",
    "    header_col_2_left_margin = (header_col_2_length - len(header_col_2_content)) // 2\n",
    "    header_col_2_right_margin = header_col_2_length - len(header_col_2_content) - header_col_2_left_margin\n",
    "    header_col_2_string = \"%s%s%s\" % (\n",
    "        \" \" * header_col_2_left_margin, header_col_2_content, \" \" * header_col_2_right_margin)\n",
    "    header = PrettyTable()\n",
    "    header.field_names = [\" \" * header_col_1_length, header_col_2_string]\n",
    "    header_lines = header.get_string(start=0, end=0).split(\"\\n\")[:2]\n",
    "    print('Results for %s task' % task_name)\n",
    "    print(\"\\n\".join(header_lines))\n",
    "    print(data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 WordNet reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_results = {}\n",
    "metrics = ['mean_rank', 'MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for implementation, models in model_files.items():\n",
    "    for model_name, files in models.items():\n",
    "        reconstruction_results[model_name] = OrderedDict()\n",
    "        for metric in metrics:\n",
    "            reconstruction_results[model_name][metric] = {}\n",
    "        for model_size, model_file in files.items():\n",
    "            print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "            embedding = load_model(implementation, model_file)\n",
    "            eval_instance = ReconstructionEvaluation(wordnet_file, embedding)\n",
    "            eval_result = eval_instance.evaluate(max_n=1000)\n",
    "            for metric in metrics:\n",
    "                reconstruction_results[model_name][metric][model_size] = eval_result[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for WordNet Reconstruction task\n",
      "+-----------------------------------------------------------------------+---------------------------------------------------------+\n",
      "|                                                                       |                    Model Dimensions                     |\n",
      "+-----------------------------------------------------------+-----------+---------+---------+---------+---------+--------+--------+\n",
      "|                                         Model Description |    Metric |       5 |      10 |      20 |      50 |    100 |    200 |\n",
      "+-----------------------------------------------------------+-----------+---------+---------+---------+---------+--------+--------+\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |  265.72 |  116.94 |   90.81 |   59.47 |  55.14 |  54.31 |\n",
      "|                                                           |       MAP |    0.28 |    0.41 |    0.49 |    0.56 |   0.58 |   0.59 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "| cpp_model_burn_in_0_epochs_100_eps_1e-06_neg_20_threads_8 | mean_rank |  228.70 |  112.40 |   81.00 |   64.81 |  57.15 |  70.87 |\n",
      "|                                                           |       MAP |    0.31 |    0.42 |    0.49 |    0.53 |   0.56 |   0.54 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_10_threads_8 | mean_rank |  280.17 |  129.46 |   92.06 |   80.41 |  71.42 |  69.30 |\n",
      "|                                                           |       MAP |    0.27 |    0.40 |    0.49 |    0.53 |   0.56 |   0.56 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_1 | mean_rank |  308.69 |  101.92 |   73.20 |   56.17 |  52.97 |  51.37 |\n",
      "|                                                           |       MAP |    0.27 |    0.43 |    0.53 |    0.61 |   0.64 |   0.65 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "|  cpp_model_burn_in_5_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |  227.69 |  175.64 |  171.60 |  144.47 | 146.84 | 139.28 |\n",
      "|                                                           |       MAP |    0.27 |    0.33 |    0.35 |    0.37 |   0.37 |   0.38 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-05_neg_20_threads_8 | mean_rank |  245.48 |  104.66 |   88.33 |   65.40 |  74.88 |  55.66 |\n",
      "|                                                           |       MAP |    0.28 |    0.42 |    0.50 |    0.56 |   0.56 |   0.57 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "| cpp_model_burn_in_0_epochs_200_eps_1e-06_neg_20_threads_8 | mean_rank |  191.69 |   97.65 |   72.07 |   55.48 |  46.76 |  49.62 |\n",
      "|                                                           |       MAP |    0.34 |    0.43 |    0.51 |    0.57 |   0.59 |   0.59 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "|                                 np_model_epochs_50_neg_20 | mean_rank | 9396.18 | 5651.94 | 3523.95 | 1053.29 | 505.56 | 364.93 |\n",
      "|                                                           |       MAP |    0.14 |    0.16 |    0.19 |    0.25 |   0.30 |   0.35 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "| cpp_model_burn_in_10_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |  252.86 |  195.73 |  182.57 |  165.33 | 157.37 | 155.78 |\n",
      "|                                                           |       MAP |    0.26 |    0.32 |    0.34 |    0.36 |   0.36 |   0.36 |\n",
      "|                                                           |           |         |         |         |         |        |        |\n",
      "+-----------------------------------------------------------+-----------+---------+---------+---------+---------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "display_results('WordNet Reconstruction', reconstruction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 WordNet link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_file, test_ratio=0.1):\n",
    "    \"\"\"Creates train and test files from given data file, returns train/test file names\n",
    "    \n",
    "    Args:\n",
    "        data_file (str): path to data file for which train/test split is to be created\n",
    "        test_ratio (float): fraction of lines to be used for test data\n",
    "    \n",
    "    Returns\n",
    "        (train_file, test_file): tuple of strings with train file and test file paths\n",
    "    \"\"\"\n",
    "    train_filename = data_file + '.train'\n",
    "    test_filename = data_file + '.test'\n",
    "    if os.path.exists(train_filename) and os.path.exists(test_filename):\n",
    "        print('Train and test files already exist, skipping')\n",
    "        return (train_filename, test_filename)\n",
    "    root_nodes, leaf_nodes = get_root_and_leaf_nodes(data_file)\n",
    "    test_line_candidates = []\n",
    "    line_count = 0\n",
    "    all_nodes = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            node_1, node_2 = line.split()\n",
    "            all_nodes.update([node_1, node_2])\n",
    "            if (\n",
    "                    node_1 not in leaf_nodes\n",
    "                    and node_2 not in leaf_nodes\n",
    "                    and node_1 not in root_nodes\n",
    "                    and node_2 not in root_nodes\n",
    "                    and node_1 != node_2\n",
    "                ):\n",
    "                test_line_candidates.append(i)\n",
    "            line_count += 1\n",
    "\n",
    "    num_test_lines = int(test_ratio * line_count)\n",
    "    if num_test_lines > len(test_line_candidates):\n",
    "        raise ValueError('Not enough candidate relations for test set')\n",
    "    print('Choosing %d test lines from %d candidates' % (num_test_lines, len(test_line_candidates)))\n",
    "    test_line_indices = set(random.sample(test_line_candidates, num_test_lines))\n",
    "    train_line_indices = set(l for l in range(line_count) if l not in test_line_indices)\n",
    "    \n",
    "    train_set_nodes = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        train_file = open(train_filename, 'wb')\n",
    "        test_file = open(test_filename, 'wb')\n",
    "        for i, line in enumerate(f):\n",
    "            if i in train_line_indices:\n",
    "                train_set_nodes.update(line.split())\n",
    "                train_file.write(line)\n",
    "            elif i in test_line_indices:\n",
    "                test_file.write(line)\n",
    "            else:\n",
    "                raise AssertionError('Line %d not present in either train or test line indices' % i)\n",
    "        train_file.close()\n",
    "        test_file.close()\n",
    "    assert len(train_set_nodes) == len(all_nodes), 'Not all nodes from dataset present in train set relations'\n",
    "    return (train_filename, test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_and_leaf_nodes(data_file):\n",
    "    \"\"\"Return keys of root and leaf nodes from a file with transitive closure relations\n",
    "    \n",
    "    Args:\n",
    "        data_file(str): file path containing transitive closure relations\n",
    "    \n",
    "    Returns:\n",
    "        (root_nodes, leaf_nodes) - tuple containing keys of root and leaf nodes\n",
    "    \"\"\"\n",
    "    root_candidates = set()\n",
    "    leaf_candidates = set()\n",
    "    with open(data_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            nodes = line.split()\n",
    "            root_candidates.update(nodes)\n",
    "            leaf_candidates.update(nodes)\n",
    "    \n",
    "    with open(data_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            node_1, node_2 = line.split()\n",
    "            if node_1 == node_2:\n",
    "                continue\n",
    "            leaf_candidates.discard(node_1)\n",
    "            root_candidates.discard(node_2)\n",
    "    \n",
    "    return (leaf_candidates, root_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test files already exist, skipping\n"
     ]
    }
   ],
   "source": [
    "wordnet_train_file, wordnet_test_file = train_test_split(wordnet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models for link prediction\n",
    "lp_model_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_model_files['c++'] = {}\n",
    "# Train c++ models with default params\n",
    "model_name, files = train_model_with_params(default_params, wordnet_train_file, model_sizes, 'cpp_lp_model', 'c++')\n",
    "lp_model_files['c++'][model_name] = {}\n",
    "for dim, filepath in files.items():\n",
    "    lp_model_files['c++'][model_name][dim] = filepath\n",
    "# Train c++ models with non-default params\n",
    "for param, values in non_default_params.items():\n",
    "    params = default_params.copy()\n",
    "    for value in values:\n",
    "        params[param] = value\n",
    "        model_name, files = train_model_with_params(params, wordnet_train_file, model_sizes, 'cpp_lp_model', 'c++')\n",
    "        lp_model_files['c++'][model_name] = {}\n",
    "        for dim, filepath in files.items():\n",
    "            lp_model_files['c++'][model_name][dim] = filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_model_files['numpy'] = {}\n",
    "# Train numpy models with default params\n",
    "model_name, files = train_model_with_params(default_params, wordnet_train_file, model_sizes, 'np_lp_model', 'numpy')\n",
    "lp_model_files['numpy'][model_name] = {}\n",
    "for dim, filepath in files.items():\n",
    "    lp_model_files['numpy'][model_name][dim] = filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_results = {}\n",
    "metrics = ['mean_rank', 'MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for implementation, models in lp_model_files.items():\n",
    "    for model_name, files in models.items():\n",
    "        lp_results[model_name] = OrderedDict()\n",
    "        for metric in metrics:\n",
    "            lp_results[model_name][metric] = {}\n",
    "        for model_size, model_file in files.items():\n",
    "            print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "            embedding = load_model(implementation, model_file)\n",
    "            eval_instance = LinkPredictionEvaluation(wordnet_train_file, wordnet_test_file, embedding)\n",
    "            eval_result = eval_instance.evaluate(max_n=1000)\n",
    "            for metric in metrics:\n",
    "                lp_results[model_name][metric][model_size] = eval_result[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for WordNet Link Prediction task\n",
      "+--------------------------------------------------------------------------+-----------------------------------------------------------+\n",
      "|                                                                          |                     Model Dimensions                      |\n",
      "+--------------------------------------------------------------+-----------+----------+---------+---------+---------+---------+--------+\n",
      "|                                            Model Description |    Metric |        5 |      10 |      20 |      50 |     100 |    200 |\n",
      "+--------------------------------------------------------------+-----------+----------+---------+---------+---------+---------+--------+\n",
      "|  cpp_lp_model_burn_in_0_epochs_50_eps_1e-06_neg_10_threads_8 | mean_rank |   230.34 |  123.24 |   75.62 |   65.97 |   55.33 |  56.89 |\n",
      "|                                                              |       MAP |     0.14 |    0.22 |    0.28 |    0.31 |    0.33 |   0.34 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "|  cpp_lp_model_burn_in_5_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |   224.50 |  199.29 |  184.29 |  178.89 |  155.52 | 157.39 |\n",
      "|                                                              |       MAP |     0.09 |    0.14 |    0.15 |    0.16 |    0.17 |   0.18 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "|  cpp_lp_model_burn_in_0_epochs_50_eps_1e-05_neg_20_threads_8 | mean_rank |   182.03 |  107.04 |   63.29 |   72.67 |   73.64 |  60.35 |\n",
      "|                                                              |       MAP |     0.16 |    0.25 |    0.31 |    0.34 |    0.36 |   0.37 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "| cpp_lp_model_burn_in_0_epochs_200_eps_1e-06_neg_20_threads_8 | mean_rank |   218.26 |   99.09 |   60.50 |   52.24 |   60.81 |  69.13 |\n",
      "|                                                              |       MAP |     0.15 |    0.24 |    0.31 |    0.35 |    0.36 |   0.36 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "|  cpp_lp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |   687.48 |  281.88 |   72.95 |   57.37 |   52.56 |  61.42 |\n",
      "|                                                              |       MAP |     0.12 |    0.15 |    0.31 |    0.35 |    0.36 |   0.36 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "|  cpp_lp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_1 | mean_rank |   232.82 |   98.78 |   53.79 |   49.79 |   46.52 |  47.03 |\n",
      "|                                                              |       MAP |     0.15 |    0.25 |    0.33 |    0.39 |    0.40 |   0.41 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "| cpp_lp_model_burn_in_0_epochs_100_eps_1e-06_neg_20_threads_8 | mean_rank |   190.93 |  116.63 |   68.68 |   43.67 |   55.01 |  66.02 |\n",
      "|                                                              |       MAP |     0.17 |    0.24 |    0.29 |    0.36 |    0.37 |   0.35 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "| cpp_lp_model_burn_in_10_epochs_50_eps_1e-06_neg_20_threads_8 | mean_rank |   236.31 |  214.85 |  193.30 |  180.27 |  169.00 | 163.22 |\n",
      "|                                                              |       MAP |     0.10 |    0.13 |    0.14 |    0.15 |    0.16 |   0.16 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "|                                 np_lp_model_epochs_50_neg_20 | mean_rank | 14443.26 | 7432.58 | 4299.32 | 2617.49 | 1398.49 | 842.81 |\n",
      "|                                                              |       MAP |     0.00 |    0.02 |    0.03 |    0.07 |    0.08 |   0.13 |\n",
      "|                                                              |           |          |         |         |         |         |        |\n",
      "+--------------------------------------------------------------+-----------+----------+---------+---------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "display_results('WordNet Link Prediction', lp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 HyperLex Lexical Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_results = {}\n",
    "eval_instance = LexicalEntailmentEvaluation(hyperlex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for implementation, models in model_files.items():\n",
    "    for model_name, files in models.items():\n",
    "        entailment_results[model_name] = OrderedDict()\n",
    "        entailment_results[model_name]['spearman'] = {}\n",
    "        for model_size, model_file in files.items():\n",
    "            print('Evaluating model %s of size %d' % (model_name, model_size))\n",
    "            embedding = load_model(implementation, model_file)\n",
    "            entailment_results[model_name]['spearman'][model_size] = eval_instance.evaluate_spearman(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Lexical Entailment (HyperLex) task\n",
      "+----------------------------------------------------------------------+-----------------------------------------+\n",
      "|                                                                      |            Model Dimensions             |\n",
      "+-----------------------------------------------------------+----------+------+------+------+------+------+------+\n",
      "|                                         Model Description |   Metric |    5 |   10 |   20 |   50 |  100 |  200 |\n",
      "+-----------------------------------------------------------+----------+------+------+------+------+------+------+\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_8 | spearman | 0.46 | 0.45 | 0.48 | 0.45 | 0.47 | 0.46 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "| cpp_model_burn_in_0_epochs_100_eps_1e-06_neg_20_threads_8 | spearman | 0.46 | 0.45 | 0.46 | 0.47 | 0.46 | 0.47 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_10_threads_8 | spearman | 0.44 | 0.44 | 0.45 | 0.44 | 0.45 | 0.45 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-06_neg_20_threads_1 | spearman | 0.46 | 0.48 | 0.47 | 0.47 | 0.48 | 0.47 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "|  cpp_model_burn_in_5_epochs_50_eps_1e-06_neg_20_threads_8 | spearman | 0.43 | 0.44 | 0.44 | 0.43 | 0.45 | 0.45 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "|  cpp_model_burn_in_0_epochs_50_eps_1e-05_neg_20_threads_8 | spearman | 0.45 | 0.46 | 0.46 | 0.47 | 0.47 | 0.46 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "| cpp_model_burn_in_0_epochs_200_eps_1e-06_neg_20_threads_8 | spearman | 0.46 | 0.47 | 0.46 | 0.46 | 0.46 | 0.47 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "|                                 np_model_epochs_50_neg_20 | spearman | 0.15 | 0.20 | 0.21 | 0.21 | 0.25 | 0.27 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "| cpp_model_burn_in_10_epochs_50_eps_1e-06_neg_20_threads_8 | spearman | 0.43 | 0.43 | 0.44 | 0.45 | 0.44 | 0.45 |\n",
      "|                                                           |          |      |      |      |      |      |      |\n",
      "+-----------------------------------------------------------+----------+------+------+------+------+------+------+\n"
     ]
    }
   ],
   "source": [
    "display_results('Lexical Entailment (HyperLex)', entailment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Link Prediction for collaboration networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - quite tricky, since the loss function used for training the model on this network is different\n",
    "# Will require changes to how gradients are calculated in C++ code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
