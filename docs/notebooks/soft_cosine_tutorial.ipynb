{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar documents with Word2Vec and Soft Cosine Measure \n",
    "\n",
    "Soft Cosine Measure (SCM) [1, 4] is a promising new tool in machine learning that allows us to submit a query and return the most relevant documents. In **part 1**, we will show how you can compute SCM between two documents using the `inner_product` method. In **part 2**, we will use `SoftCosineSimilarity` to retrieve documents most similar to a query and compare the performance against other similarity measures.\n",
    "\n",
    "First, however, we go through the basics of what Soft Cosine Measure is.\n",
    "\n",
    "## Soft Cosine Measure basics\n",
    "\n",
    "Soft Cosine Measure (SCM) is a method that allows us to assess the similarity between two documents in a meaningful way, even when they have no words in common. It uses a measure of similarity between words, which can be derived [2] using [word2vec][] [3] vector embeddings of words. It has been shown to outperform many of the state-of-the-art methods in the semantic text similarity task in the context of community question answering [2].\n",
    "\n",
    "[word2vec]: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "SCM is illustrated below for two very similar sentences. The sentences have no words in common, but by modeling synonymy, SCM is able to accurately measure the similarity between the two sentences. The method also uses the bag-of-words vector representation of the documents (simply put, the word's frequencies in the documents). The intution behind the method is that we compute standard cosine similarity assuming that the document vectors are expressed in a non-orthogonal basis, where the angle between two basis vectors is derived from the angle between the word2vec embeddings of the corresponding words.\n",
    "\n",
    "![Soft Cosine Measure](soft_cosine_tutorial.png)\n",
    "\n",
    "This method was perhaps first introduced in the article “Soft Measure and Soft Cosine Measure: Measure of Features in Vector Space Model” by Grigori Sidorov, Alexander Gelbukh, Helena Gomez-Adorno, and David Pinto ([link to PDF](http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a7.pdf)).\n",
    "\n",
    "In this tutorial, we will learn how to use Gensim's SCM functionality, which consists of the `inner_product` method for one-off computation, and the `SoftCosineSimilarity` class for corpus-based similarity queries.\n",
    "\n",
    "> **Note**:\n",
    ">\n",
    "> If you use this software, please consider citing [1] and [2].\n",
    ">\n",
    "\n",
    "## Running this notebook\n",
    "You can download this [Jupyter notebook](http://jupyter.org/), and run it on your own computer, provided you have installed the `gensim`, `jupyter`, `sklearn`, `pyemd`, and `wmd` Python packages.\n",
    "\n",
    "The notebook was run on an Ubuntu machine with an Intel core i7-6700HQ CPU 3.10GHz (4 cores) and 16 GB memory. Assuming all resources required by the notebook have already been downloaded, running the entire notebook on this machine takes about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Computing the Soft Cosine Measure\n",
    "\n",
    "To use SCM, we need some word embeddings first of all. You could train a [word2vec][] (see tutorial [here](http://rare-technologies.com/word2vec-tutorial/)) model on some corpus, but we will use pre-trained word2vec embeddings.\n",
    "\n",
    "[word2vec]: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "Let's create some sentences to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "sentence_orange = 'Having a tough time finding an orange juice press machine?'.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two sentences have very similar content, and as such the SCM should be large. Before we compute the SCM, we want to remove stopwords (\"the\", \"to\", etc.), as these do not contribute a lot to the information in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/misha/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "2019-06-17 10:46:35,031 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-06-17 10:46:35,036 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 10:46:35,037 : INFO : built Dictionary(14 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...) from 3 documents (total 15 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  # Download stopwords list.\n",
    "\n",
    "# Remove stopwords.\n",
    "stop_words = stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stop_words]\n",
    "sentence_president = [w for w in sentence_president if w not in stop_words]\n",
    "sentence_orange = [w for w in sentence_orange if w not in stop_words]\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "from gensim import corpora\n",
    "documents = [sentence_obama, sentence_president, sentence_orange]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sentence_obama = dictionary.doc2bow(sentence_obama)\n",
    "sentence_president = dictionary.doc2bow(sentence_president)\n",
    "sentence_orange = dictionary.doc2bow(sentence_orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we mentioned earlier, we will be using some downloaded pre-trained embeddings. Note that the embeddings we have chosen here require a lot of memory. We will use the embeddings to construct a term similarity matrix that will be used by the `inner_product` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:46:35,428 : INFO : loading projection weights from /home/misha/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz\n",
      "2019-06-17 10:46:35,429 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-17 10:47:07,511 : INFO : loaded (400000, 50) matrix from /home/misha/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz\n",
      "2019-06-17 10:47:07,512 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fa0c2347d68>\n",
      "2019-06-17 10:47:07,512 : INFO : iterating over columns in dictionary order\n",
      "2019-06-17 10:47:07,514 : INFO : PROGRESS: at 7.14% columns (1 / 14, 7.142857% density, 7.142857% projected density)\n",
      "2019-06-17 10:47:07,516 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 10:47:08,055 : INFO : constructed a sparse term similarity matrix with 11.224490% density\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 s, sys: 1.27 s, total: 31.7 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim.downloader as api\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "similarity_index = WordEmbeddingSimilarityIndex(w2v_model)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute SCM using the `inner_product` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity = 0.3790\n"
     ]
    }
   ],
   "source": [
    "similarity = similarity_matrix.inner_product(sentence_obama, sentence_president, normalized=True)\n",
    "print('similarity = %.4f' % similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing with two completely unrelated sentences. Notice that the similarity is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity = 0.1108\n"
     ]
    }
   ],
   "source": [
    "similarity = similarity_matrix.inner_product(sentence_obama, sentence_orange, normalized=True)\n",
    "print('similarity = %.4f' % similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Similarity queries using `SoftCosineSimilarity`\n",
    "You can use SCM to get the most similar documents to a query, using the `SoftCosineSimilarity` class. Its interface is similar to what is described in the [Similarity Queries](https://radimrehurek.com/gensim/tut3.html) Gensim tutorial.\n",
    "\n",
    "### Qatar Living unannotated dataset\n",
    "Contestants solving the community question answering task in the [SemEval 2016][semeval16] and [2017][semeval17] competitions had an unannotated dataset of 189,941 questions and 1,894,456 comments from the [Qatar Living][ql] discussion forums. As our first step, we will use the same dataset to build a corpus.\n",
    "\n",
    "[semeval16]: http://alt.qcri.org/semeval2016/task3/\n",
    "[semeval17]: http://alt.qcri.org/semeval2017/task3/\n",
    "[ql]: http://www.qatarliving.com/forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/misha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2019-06-17 10:47:08,404 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "CPU times: user 2min 46s, sys: 2.34 s, total: 2min 48s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from itertools import chain\n",
    "import json\n",
    "from re import sub\n",
    "from os.path import isfile\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "\n",
    "download(\"stopwords\")  # Download stopwords list.\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
    "\n",
    "corpus = list(chain(*[\n",
    "    chain(\n",
    "        [preprocess(thread[\"RelQuestion\"][\"RelQSubject\"]), preprocess(thread[\"RelQuestion\"][\"RelQBody\"])],\n",
    "        [preprocess(relcomment[\"RelCText\"]) for relcomment in thread[\"RelComments\"]])\n",
    "    for thread in api.load(\"semeval-2016-2017-task3-subtaskA-unannotated\")]))\n",
    "\n",
    "print(\"Number of documents: %d\" % len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the corpus we have just build, we will now construct a [dictionary][], a [TF-IDF model][tfidf], a [word2vec model][word2vec], and a term similarity matrix.\n",
    "\n",
    "[dictionary]: https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "[tfidf]: https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "[word2vec]: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:50:00,986 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 10:50:01,293 : INFO : adding document #10000 to Dictionary(20088 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:01,574 : INFO : adding document #20000 to Dictionary(29692 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:01,932 : INFO : adding document #30000 to Dictionary(37971 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:02,293 : INFO : adding document #40000 to Dictionary(43930 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:02,551 : INFO : adding document #50000 to Dictionary(49340 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:02,817 : INFO : adding document #60000 to Dictionary(54734 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:03,084 : INFO : adding document #70000 to Dictionary(59734 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:03,377 : INFO : adding document #80000 to Dictionary(64698 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:03,659 : INFO : adding document #90000 to Dictionary(68921 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:03,984 : INFO : adding document #100000 to Dictionary(74025 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:04,273 : INFO : adding document #110000 to Dictionary(78063 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:04,598 : INFO : adding document #120000 to Dictionary(81932 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:04,892 : INFO : adding document #130000 to Dictionary(85850 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:05,170 : INFO : adding document #140000 to Dictionary(89489 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:05,443 : INFO : adding document #150000 to Dictionary(93441 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:05,701 : INFO : adding document #160000 to Dictionary(97166 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:05,962 : INFO : adding document #170000 to Dictionary(100281 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:06,209 : INFO : adding document #180000 to Dictionary(103372 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:06,486 : INFO : adding document #190000 to Dictionary(106627 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:06,770 : INFO : adding document #200000 to Dictionary(110902 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:07,089 : INFO : adding document #210000 to Dictionary(113686 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:07,357 : INFO : adding document #220000 to Dictionary(117110 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:07,666 : INFO : adding document #230000 to Dictionary(119961 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:07,961 : INFO : adding document #240000 to Dictionary(123182 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:08,227 : INFO : adding document #250000 to Dictionary(125952 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:08,510 : INFO : adding document #260000 to Dictionary(128806 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:08,828 : INFO : adding document #270000 to Dictionary(131361 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:09,112 : INFO : adding document #280000 to Dictionary(133942 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:09,392 : INFO : adding document #290000 to Dictionary(136306 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:09,689 : INFO : adding document #300000 to Dictionary(138957 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:09,989 : INFO : adding document #310000 to Dictionary(141490 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:10,319 : INFO : adding document #320000 to Dictionary(144071 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:10,580 : INFO : adding document #330000 to Dictionary(146510 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:10,827 : INFO : adding document #340000 to Dictionary(149053 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:11,119 : INFO : adding document #350000 to Dictionary(151463 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:11,487 : INFO : adding document #360000 to Dictionary(153612 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:11,797 : INFO : adding document #370000 to Dictionary(156234 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:12,126 : INFO : adding document #380000 to Dictionary(158845 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:12,392 : INFO : adding document #390000 to Dictionary(161029 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:12,717 : INFO : adding document #400000 to Dictionary(163444 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:13,110 : INFO : adding document #410000 to Dictionary(165551 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:13,569 : INFO : adding document #420000 to Dictionary(167864 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:14,207 : INFO : adding document #430000 to Dictionary(169982 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:14,963 : INFO : adding document #440000 to Dictionary(172106 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:15,724 : INFO : adding document #450000 to Dictionary(174128 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:16,110 : INFO : adding document #460000 to Dictionary(176267 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:16,749 : INFO : adding document #470000 to Dictionary(178429 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:17,291 : INFO : adding document #480000 to Dictionary(180738 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:17,956 : INFO : adding document #490000 to Dictionary(182982 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:18,424 : INFO : adding document #500000 to Dictionary(184754 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:19,046 : INFO : adding document #510000 to Dictionary(187327 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:19,455 : INFO : adding document #520000 to Dictionary(189327 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:19,995 : INFO : adding document #530000 to Dictionary(191219 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:20,551 : INFO : adding document #540000 to Dictionary(193182 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:50:21,125 : INFO : adding document #550000 to Dictionary(195951 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:21,644 : INFO : adding document #560000 to Dictionary(197956 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:22,189 : INFO : adding document #570000 to Dictionary(200145 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:22,809 : INFO : adding document #580000 to Dictionary(201859 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:23,466 : INFO : adding document #590000 to Dictionary(203724 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:24,117 : INFO : adding document #600000 to Dictionary(205607 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:24,644 : INFO : adding document #610000 to Dictionary(207387 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:25,229 : INFO : adding document #620000 to Dictionary(209246 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:25,780 : INFO : adding document #630000 to Dictionary(211094 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:26,056 : INFO : adding document #640000 to Dictionary(212963 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:26,314 : INFO : adding document #650000 to Dictionary(214666 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:26,660 : INFO : adding document #660000 to Dictionary(216409 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:26,952 : INFO : adding document #670000 to Dictionary(218264 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:27,225 : INFO : adding document #680000 to Dictionary(220129 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:27,618 : INFO : adding document #690000 to Dictionary(222075 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:28,144 : INFO : adding document #700000 to Dictionary(223880 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:28,760 : INFO : adding document #710000 to Dictionary(225982 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:29,288 : INFO : adding document #720000 to Dictionary(227672 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:29,819 : INFO : adding document #730000 to Dictionary(229371 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:30,570 : INFO : adding document #740000 to Dictionary(231078 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:31,233 : INFO : adding document #750000 to Dictionary(232982 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:31,972 : INFO : adding document #760000 to Dictionary(234746 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:32,476 : INFO : adding document #770000 to Dictionary(236494 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:33,055 : INFO : adding document #780000 to Dictionary(238199 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:33,593 : INFO : adding document #790000 to Dictionary(240021 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:34,145 : INFO : adding document #800000 to Dictionary(242280 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:34,758 : INFO : adding document #810000 to Dictionary(244318 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:35,601 : INFO : adding document #820000 to Dictionary(246133 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:36,147 : INFO : adding document #830000 to Dictionary(247703 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:36,686 : INFO : adding document #840000 to Dictionary(249458 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:37,216 : INFO : adding document #850000 to Dictionary(251278 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:37,675 : INFO : adding document #860000 to Dictionary(252971 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:38,398 : INFO : adding document #870000 to Dictionary(254789 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:38,917 : INFO : adding document #880000 to Dictionary(256483 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:39,484 : INFO : adding document #890000 to Dictionary(258416 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:40,080 : INFO : adding document #900000 to Dictionary(260098 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:40,603 : INFO : adding document #910000 to Dictionary(261700 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:41,161 : INFO : adding document #920000 to Dictionary(263313 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:41,707 : INFO : adding document #930000 to Dictionary(264839 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:42,398 : INFO : adding document #940000 to Dictionary(266327 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:43,193 : INFO : adding document #950000 to Dictionary(267891 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:44,024 : INFO : adding document #960000 to Dictionary(270437 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:44,668 : INFO : adding document #970000 to Dictionary(272420 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:45,208 : INFO : adding document #980000 to Dictionary(274058 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:45,724 : INFO : adding document #990000 to Dictionary(275579 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:46,258 : INFO : adding document #1000000 to Dictionary(277402 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:46,788 : INFO : adding document #1010000 to Dictionary(279035 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:47,518 : INFO : adding document #1020000 to Dictionary(280584 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:48,141 : INFO : adding document #1030000 to Dictionary(282206 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:48,681 : INFO : adding document #1040000 to Dictionary(283570 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:49,336 : INFO : adding document #1050000 to Dictionary(285112 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:49,846 : INFO : adding document #1060000 to Dictionary(286666 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:50,356 : INFO : adding document #1070000 to Dictionary(288122 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:50,829 : INFO : adding document #1080000 to Dictionary(289489 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:51,293 : INFO : adding document #1090000 to Dictionary(291139 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:50:52,177 : INFO : adding document #1100000 to Dictionary(293838 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:52,927 : INFO : adding document #1110000 to Dictionary(295273 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:53,529 : INFO : adding document #1120000 to Dictionary(296816 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:54,091 : INFO : adding document #1130000 to Dictionary(298552 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:54,500 : INFO : adding document #1140000 to Dictionary(299628 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:55,158 : INFO : adding document #1150000 to Dictionary(301139 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:55,844 : INFO : adding document #1160000 to Dictionary(302566 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:56,385 : INFO : adding document #1170000 to Dictionary(304039 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:56,643 : INFO : adding document #1180000 to Dictionary(305503 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:56,880 : INFO : adding document #1190000 to Dictionary(307005 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:57,132 : INFO : adding document #1200000 to Dictionary(308842 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:57,397 : INFO : adding document #1210000 to Dictionary(310414 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:57,665 : INFO : adding document #1220000 to Dictionary(312012 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:57,908 : INFO : adding document #1230000 to Dictionary(313850 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:58,171 : INFO : adding document #1240000 to Dictionary(315829 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:58,457 : INFO : adding document #1250000 to Dictionary(317188 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:58,747 : INFO : adding document #1260000 to Dictionary(318577 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:58,984 : INFO : adding document #1270000 to Dictionary(320245 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:59,268 : INFO : adding document #1280000 to Dictionary(321715 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:59,528 : INFO : adding document #1290000 to Dictionary(323216 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:50:59,857 : INFO : adding document #1300000 to Dictionary(324767 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:00,133 : INFO : adding document #1310000 to Dictionary(326386 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:00,597 : INFO : adding document #1320000 to Dictionary(329383 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:00,849 : INFO : adding document #1330000 to Dictionary(330810 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:01,155 : INFO : adding document #1340000 to Dictionary(332299 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:01,471 : INFO : adding document #1350000 to Dictionary(333664 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:01,787 : INFO : adding document #1360000 to Dictionary(335153 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:02,059 : INFO : adding document #1370000 to Dictionary(336962 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:02,341 : INFO : adding document #1380000 to Dictionary(338540 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:02,595 : INFO : adding document #1390000 to Dictionary(339974 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:02,829 : INFO : adding document #1400000 to Dictionary(341332 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:03,090 : INFO : adding document #1410000 to Dictionary(342864 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:03,329 : INFO : adding document #1420000 to Dictionary(344362 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:03,575 : INFO : adding document #1430000 to Dictionary(345627 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:03,861 : INFO : adding document #1440000 to Dictionary(346909 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:04,214 : INFO : adding document #1450000 to Dictionary(348275 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:04,506 : INFO : adding document #1460000 to Dictionary(349755 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:04,838 : INFO : adding document #1470000 to Dictionary(351025 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:05,113 : INFO : adding document #1480000 to Dictionary(352258 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:05,383 : INFO : adding document #1490000 to Dictionary(353503 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:05,642 : INFO : adding document #1500000 to Dictionary(354943 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:05,906 : INFO : adding document #1510000 to Dictionary(356295 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:06,157 : INFO : adding document #1520000 to Dictionary(357459 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:06,624 : INFO : adding document #1530000 to Dictionary(358666 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:06,895 : INFO : adding document #1540000 to Dictionary(359986 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:07,170 : INFO : adding document #1550000 to Dictionary(361326 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:07,442 : INFO : adding document #1560000 to Dictionary(362609 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:07,727 : INFO : adding document #1570000 to Dictionary(363808 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:08,005 : INFO : adding document #1580000 to Dictionary(365172 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:08,301 : INFO : adding document #1590000 to Dictionary(366433 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:08,578 : INFO : adding document #1600000 to Dictionary(367968 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:08,834 : INFO : adding document #1610000 to Dictionary(369421 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:09,105 : INFO : adding document #1620000 to Dictionary(371631 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:09,361 : INFO : adding document #1630000 to Dictionary(372956 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:51:09,616 : INFO : adding document #1640000 to Dictionary(374282 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:09,885 : INFO : adding document #1650000 to Dictionary(375746 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:10,223 : INFO : adding document #1660000 to Dictionary(377073 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:10,535 : INFO : adding document #1670000 to Dictionary(378393 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:10,805 : INFO : adding document #1680000 to Dictionary(379812 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:11,088 : INFO : adding document #1690000 to Dictionary(380895 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:11,452 : INFO : adding document #1700000 to Dictionary(384739 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:11,720 : INFO : adding document #1710000 to Dictionary(386066 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:12,000 : INFO : adding document #1720000 to Dictionary(387270 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:12,248 : INFO : adding document #1730000 to Dictionary(388385 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:12,513 : INFO : adding document #1740000 to Dictionary(389687 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:12,785 : INFO : adding document #1750000 to Dictionary(390955 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:13,104 : INFO : adding document #1760000 to Dictionary(392540 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:13,369 : INFO : adding document #1770000 to Dictionary(393838 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:13,660 : INFO : adding document #1780000 to Dictionary(395032 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:13,935 : INFO : adding document #1790000 to Dictionary(396178 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:14,242 : INFO : adding document #1800000 to Dictionary(401637 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:14,502 : INFO : adding document #1810000 to Dictionary(402961 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:14,766 : INFO : adding document #1820000 to Dictionary(404423 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:15,022 : INFO : adding document #1830000 to Dictionary(405685 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:15,292 : INFO : adding document #1840000 to Dictionary(406830 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:15,599 : INFO : adding document #1850000 to Dictionary(408042 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:15,860 : INFO : adding document #1860000 to Dictionary(409402 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:16,165 : INFO : adding document #1870000 to Dictionary(410413 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:16,411 : INFO : adding document #1880000 to Dictionary(411819 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:16,663 : INFO : adding document #1890000 to Dictionary(412945 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:16,927 : INFO : adding document #1900000 to Dictionary(414272 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:17,185 : INFO : adding document #1910000 to Dictionary(415361 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:17,457 : INFO : adding document #1920000 to Dictionary(416731 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:17,718 : INFO : adding document #1930000 to Dictionary(419310 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:17,976 : INFO : adding document #1940000 to Dictionary(421794 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:18,244 : INFO : adding document #1950000 to Dictionary(423125 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:18,510 : INFO : adding document #1960000 to Dictionary(424191 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:18,753 : INFO : adding document #1970000 to Dictionary(425372 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:19,010 : INFO : adding document #1980000 to Dictionary(426641 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:19,259 : INFO : adding document #1990000 to Dictionary(427732 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:19,533 : INFO : adding document #2000000 to Dictionary(428904 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:19,782 : INFO : adding document #2010000 to Dictionary(429960 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:20,059 : INFO : adding document #2020000 to Dictionary(431271 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:20,363 : INFO : adding document #2030000 to Dictionary(432825 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:20,644 : INFO : adding document #2040000 to Dictionary(433994 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:20,960 : INFO : adding document #2050000 to Dictionary(436053 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:21,246 : INFO : adding document #2060000 to Dictionary(437115 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:21,514 : INFO : adding document #2070000 to Dictionary(438236 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:21,800 : INFO : adding document #2080000 to Dictionary(439512 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:22,088 : INFO : adding document #2090000 to Dictionary(440671 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:22,338 : INFO : adding document #2100000 to Dictionary(442053 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:22,606 : INFO : adding document #2110000 to Dictionary(443098 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:22,880 : INFO : adding document #2120000 to Dictionary(444469 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:23,158 : INFO : adding document #2130000 to Dictionary(445737 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:23,462 : INFO : adding document #2140000 to Dictionary(447128 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:23,718 : INFO : adding document #2150000 to Dictionary(448352 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:24,030 : INFO : adding document #2160000 to Dictionary(449397 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:24,353 : INFO : adding document #2170000 to Dictionary(450649 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:51:24,664 : INFO : adding document #2180000 to Dictionary(451840 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:24,940 : INFO : adding document #2190000 to Dictionary(453020 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:25,245 : INFO : adding document #2200000 to Dictionary(454160 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:25,496 : INFO : adding document #2210000 to Dictionary(455302 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:25,781 : INFO : adding document #2220000 to Dictionary(456657 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:26,053 : INFO : adding document #2230000 to Dictionary(457752 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:26,316 : INFO : adding document #2240000 to Dictionary(458938 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:26,583 : INFO : adding document #2250000 to Dictionary(460343 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:26,833 : INFO : adding document #2260000 to Dictionary(461426 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:27,081 : INFO : adding document #2270000 to Dictionary(462407 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...)\n",
      "2019-06-17 10:51:27,184 : INFO : built Dictionary(462807 unique tokens: ['blocks', 'cnn', 'facebook', 'minsitry', 'thailand']...) from 2274338 documents (total 40096354 corpus positions)\n",
      "2019-06-17 10:51:28,291 : INFO : collecting all words and their counts\n",
      "2019-06-17 10:51:28,292 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-17 10:51:28,326 : INFO : PROGRESS: at sentence #10000, processed 172808 words, keeping 20088 word types\n",
      "2019-06-17 10:51:28,368 : INFO : PROGRESS: at sentence #20000, processed 345955 words, keeping 29692 word types\n",
      "2019-06-17 10:51:28,426 : INFO : PROGRESS: at sentence #30000, processed 541552 words, keeping 37971 word types\n",
      "2019-06-17 10:51:28,462 : INFO : PROGRESS: at sentence #40000, processed 705233 words, keeping 43930 word types\n",
      "2019-06-17 10:51:28,500 : INFO : PROGRESS: at sentence #50000, processed 868029 words, keeping 49340 word types\n",
      "2019-06-17 10:51:28,546 : INFO : PROGRESS: at sentence #60000, processed 1051114 words, keeping 54734 word types\n",
      "2019-06-17 10:51:28,586 : INFO : PROGRESS: at sentence #70000, processed 1229423 words, keeping 59734 word types\n",
      "2019-06-17 10:51:28,631 : INFO : PROGRESS: at sentence #80000, processed 1420566 words, keeping 64698 word types\n",
      "2019-06-17 10:51:28,671 : INFO : PROGRESS: at sentence #90000, processed 1587554 words, keeping 68921 word types\n",
      "2019-06-17 10:51:28,723 : INFO : PROGRESS: at sentence #100000, processed 1763790 words, keeping 74025 word types\n",
      "2019-06-17 10:51:28,770 : INFO : PROGRESS: at sentence #110000, processed 1938499 words, keeping 78063 word types\n",
      "2019-06-17 10:51:28,819 : INFO : PROGRESS: at sentence #120000, processed 2124701 words, keeping 81932 word types\n",
      "2019-06-17 10:51:28,867 : INFO : PROGRESS: at sentence #130000, processed 2298491 words, keeping 85850 word types\n",
      "2019-06-17 10:51:28,916 : INFO : PROGRESS: at sentence #140000, processed 2485247 words, keeping 89489 word types\n",
      "2019-06-17 10:51:28,959 : INFO : PROGRESS: at sentence #150000, processed 2659752 words, keeping 93441 word types\n",
      "2019-06-17 10:51:29,010 : INFO : PROGRESS: at sentence #160000, processed 2832892 words, keeping 97166 word types\n",
      "2019-06-17 10:51:29,063 : INFO : PROGRESS: at sentence #170000, processed 3005814 words, keeping 100281 word types\n",
      "2019-06-17 10:51:29,117 : INFO : PROGRESS: at sentence #180000, processed 3169805 words, keeping 103372 word types\n",
      "2019-06-17 10:51:29,164 : INFO : PROGRESS: at sentence #190000, processed 3347859 words, keeping 106627 word types\n",
      "2019-06-17 10:51:29,210 : INFO : PROGRESS: at sentence #200000, processed 3527666 words, keeping 110902 word types\n",
      "2019-06-17 10:51:29,258 : INFO : PROGRESS: at sentence #210000, processed 3703354 words, keeping 113686 word types\n",
      "2019-06-17 10:51:29,315 : INFO : PROGRESS: at sentence #220000, processed 3881882 words, keeping 117110 word types\n",
      "2019-06-17 10:51:29,404 : INFO : PROGRESS: at sentence #230000, processed 4050419 words, keeping 119961 word types\n",
      "2019-06-17 10:51:29,460 : INFO : PROGRESS: at sentence #240000, processed 4232284 words, keeping 123182 word types\n",
      "2019-06-17 10:51:29,516 : INFO : PROGRESS: at sentence #250000, processed 4400084 words, keeping 125952 word types\n",
      "2019-06-17 10:51:29,576 : INFO : PROGRESS: at sentence #260000, processed 4582320 words, keeping 128806 word types\n",
      "2019-06-17 10:51:29,654 : INFO : PROGRESS: at sentence #270000, processed 4750501 words, keeping 131361 word types\n",
      "2019-06-17 10:51:29,722 : INFO : PROGRESS: at sentence #280000, processed 4922559 words, keeping 133942 word types\n",
      "2019-06-17 10:51:29,780 : INFO : PROGRESS: at sentence #290000, processed 5090547 words, keeping 136306 word types\n",
      "2019-06-17 10:51:29,842 : INFO : PROGRESS: at sentence #300000, processed 5263679 words, keeping 138957 word types\n",
      "2019-06-17 10:51:29,900 : INFO : PROGRESS: at sentence #310000, processed 5446459 words, keeping 141490 word types\n",
      "2019-06-17 10:51:29,950 : INFO : PROGRESS: at sentence #320000, processed 5623621 words, keeping 144071 word types\n",
      "2019-06-17 10:51:30,002 : INFO : PROGRESS: at sentence #330000, processed 5792646 words, keeping 146510 word types\n",
      "2019-06-17 10:51:30,058 : INFO : PROGRESS: at sentence #340000, processed 5958987 words, keeping 149053 word types\n",
      "2019-06-17 10:51:30,114 : INFO : PROGRESS: at sentence #350000, processed 6151645 words, keeping 151463 word types\n",
      "2019-06-17 10:51:30,164 : INFO : PROGRESS: at sentence #360000, processed 6327069 words, keeping 153612 word types\n",
      "2019-06-17 10:51:30,216 : INFO : PROGRESS: at sentence #370000, processed 6496792 words, keeping 156234 word types\n",
      "2019-06-17 10:51:30,277 : INFO : PROGRESS: at sentence #380000, processed 6704748 words, keeping 158845 word types\n",
      "2019-06-17 10:51:30,322 : INFO : PROGRESS: at sentence #390000, processed 6879316 words, keeping 161029 word types\n",
      "2019-06-17 10:51:30,366 : INFO : PROGRESS: at sentence #400000, processed 7045482 words, keeping 163444 word types\n",
      "2019-06-17 10:51:30,419 : INFO : PROGRESS: at sentence #410000, processed 7230856 words, keeping 165551 word types\n",
      "2019-06-17 10:51:30,469 : INFO : PROGRESS: at sentence #420000, processed 7407466 words, keeping 167864 word types\n",
      "2019-06-17 10:51:30,521 : INFO : PROGRESS: at sentence #430000, processed 7589188 words, keeping 169982 word types\n",
      "2019-06-17 10:51:30,593 : INFO : PROGRESS: at sentence #440000, processed 7773096 words, keeping 172106 word types\n",
      "2019-06-17 10:51:30,645 : INFO : PROGRESS: at sentence #450000, processed 7932149 words, keeping 174128 word types\n",
      "2019-06-17 10:51:30,710 : INFO : PROGRESS: at sentence #460000, processed 8098234 words, keeping 176267 word types\n",
      "2019-06-17 10:51:30,767 : INFO : PROGRESS: at sentence #470000, processed 8272686 words, keeping 178429 word types\n",
      "2019-06-17 10:51:30,819 : INFO : PROGRESS: at sentence #480000, processed 8450596 words, keeping 180738 word types\n",
      "2019-06-17 10:51:30,870 : INFO : PROGRESS: at sentence #490000, processed 8626881 words, keeping 182982 word types\n",
      "2019-06-17 10:51:30,928 : INFO : PROGRESS: at sentence #500000, processed 8803988 words, keeping 184754 word types\n",
      "2019-06-17 10:51:30,983 : INFO : PROGRESS: at sentence #510000, processed 8988004 words, keeping 187327 word types\n",
      "2019-06-17 10:51:31,052 : INFO : PROGRESS: at sentence #520000, processed 9169435 words, keeping 189327 word types\n",
      "2019-06-17 10:51:31,114 : INFO : PROGRESS: at sentence #530000, processed 9338537 words, keeping 191219 word types\n",
      "2019-06-17 10:51:31,175 : INFO : PROGRESS: at sentence #540000, processed 9513704 words, keeping 193182 word types\n",
      "2019-06-17 10:51:31,270 : INFO : PROGRESS: at sentence #550000, processed 9700882 words, keeping 195951 word types\n",
      "2019-06-17 10:51:31,332 : INFO : PROGRESS: at sentence #560000, processed 9892043 words, keeping 197956 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:51:31,385 : INFO : PROGRESS: at sentence #570000, processed 10082223 words, keeping 200145 word types\n",
      "2019-06-17 10:51:31,430 : INFO : PROGRESS: at sentence #580000, processed 10249508 words, keeping 201859 word types\n",
      "2019-06-17 10:51:31,481 : INFO : PROGRESS: at sentence #590000, processed 10413550 words, keeping 203724 word types\n",
      "2019-06-17 10:51:31,553 : INFO : PROGRESS: at sentence #600000, processed 10583886 words, keeping 205607 word types\n",
      "2019-06-17 10:51:31,607 : INFO : PROGRESS: at sentence #610000, processed 10761502 words, keeping 207387 word types\n",
      "2019-06-17 10:51:31,653 : INFO : PROGRESS: at sentence #620000, processed 10937476 words, keeping 209246 word types\n",
      "2019-06-17 10:51:31,693 : INFO : PROGRESS: at sentence #630000, processed 11103087 words, keeping 211094 word types\n",
      "2019-06-17 10:51:31,734 : INFO : PROGRESS: at sentence #640000, processed 11271558 words, keeping 212963 word types\n",
      "2019-06-17 10:51:31,774 : INFO : PROGRESS: at sentence #650000, processed 11438866 words, keeping 214666 word types\n",
      "2019-06-17 10:51:31,824 : INFO : PROGRESS: at sentence #660000, processed 11616418 words, keeping 216409 word types\n",
      "2019-06-17 10:51:31,900 : INFO : PROGRESS: at sentence #670000, processed 11798489 words, keeping 218264 word types\n",
      "2019-06-17 10:51:31,953 : INFO : PROGRESS: at sentence #680000, processed 11970418 words, keeping 220129 word types\n",
      "2019-06-17 10:51:32,022 : INFO : PROGRESS: at sentence #690000, processed 12175811 words, keeping 222075 word types\n",
      "2019-06-17 10:51:32,076 : INFO : PROGRESS: at sentence #700000, processed 12343559 words, keeping 223880 word types\n",
      "2019-06-17 10:51:32,164 : INFO : PROGRESS: at sentence #710000, processed 12565565 words, keeping 225982 word types\n",
      "2019-06-17 10:51:32,219 : INFO : PROGRESS: at sentence #720000, processed 12736259 words, keeping 227672 word types\n",
      "2019-06-17 10:51:32,269 : INFO : PROGRESS: at sentence #730000, processed 12910946 words, keeping 229371 word types\n",
      "2019-06-17 10:51:32,315 : INFO : PROGRESS: at sentence #740000, processed 13086533 words, keeping 231078 word types\n",
      "2019-06-17 10:51:32,359 : INFO : PROGRESS: at sentence #750000, processed 13252162 words, keeping 232982 word types\n",
      "2019-06-17 10:51:32,414 : INFO : PROGRESS: at sentence #760000, processed 13430188 words, keeping 234746 word types\n",
      "2019-06-17 10:51:32,457 : INFO : PROGRESS: at sentence #770000, processed 13599380 words, keeping 236494 word types\n",
      "2019-06-17 10:51:32,501 : INFO : PROGRESS: at sentence #780000, processed 13761922 words, keeping 238199 word types\n",
      "2019-06-17 10:51:32,554 : INFO : PROGRESS: at sentence #790000, processed 13939964 words, keeping 240021 word types\n",
      "2019-06-17 10:51:32,607 : INFO : PROGRESS: at sentence #800000, processed 14171975 words, keeping 242280 word types\n",
      "2019-06-17 10:51:32,653 : INFO : PROGRESS: at sentence #810000, processed 14355425 words, keeping 244318 word types\n",
      "2019-06-17 10:51:32,701 : INFO : PROGRESS: at sentence #820000, processed 14535275 words, keeping 246133 word types\n",
      "2019-06-17 10:51:32,760 : INFO : PROGRESS: at sentence #830000, processed 14712909 words, keeping 247703 word types\n",
      "2019-06-17 10:51:32,816 : INFO : PROGRESS: at sentence #840000, processed 14884617 words, keeping 249458 word types\n",
      "2019-06-17 10:51:32,864 : INFO : PROGRESS: at sentence #850000, processed 15062312 words, keeping 251278 word types\n",
      "2019-06-17 10:51:32,912 : INFO : PROGRESS: at sentence #860000, processed 15240611 words, keeping 252971 word types\n",
      "2019-06-17 10:51:32,961 : INFO : PROGRESS: at sentence #870000, processed 15425624 words, keeping 254789 word types\n",
      "2019-06-17 10:51:33,006 : INFO : PROGRESS: at sentence #880000, processed 15599153 words, keeping 256483 word types\n",
      "2019-06-17 10:51:33,072 : INFO : PROGRESS: at sentence #890000, processed 15766709 words, keeping 258416 word types\n",
      "2019-06-17 10:51:33,125 : INFO : PROGRESS: at sentence #900000, processed 15946022 words, keeping 260098 word types\n",
      "2019-06-17 10:51:33,176 : INFO : PROGRESS: at sentence #910000, processed 16109571 words, keeping 261700 word types\n",
      "2019-06-17 10:51:33,231 : INFO : PROGRESS: at sentence #920000, processed 16285569 words, keeping 263313 word types\n",
      "2019-06-17 10:51:33,279 : INFO : PROGRESS: at sentence #930000, processed 16459265 words, keeping 264839 word types\n",
      "2019-06-17 10:51:33,322 : INFO : PROGRESS: at sentence #940000, processed 16630795 words, keeping 266327 word types\n",
      "2019-06-17 10:51:33,365 : INFO : PROGRESS: at sentence #950000, processed 16809469 words, keeping 267891 word types\n",
      "2019-06-17 10:51:33,415 : INFO : PROGRESS: at sentence #960000, processed 16991225 words, keeping 270437 word types\n",
      "2019-06-17 10:51:33,472 : INFO : PROGRESS: at sentence #970000, processed 17166201 words, keeping 272420 word types\n",
      "2019-06-17 10:51:33,538 : INFO : PROGRESS: at sentence #980000, processed 17337011 words, keeping 274058 word types\n",
      "2019-06-17 10:51:33,604 : INFO : PROGRESS: at sentence #990000, processed 17518793 words, keeping 275579 word types\n",
      "2019-06-17 10:51:33,656 : INFO : PROGRESS: at sentence #1000000, processed 17695697 words, keeping 277402 word types\n",
      "2019-06-17 10:51:33,704 : INFO : PROGRESS: at sentence #1010000, processed 17875525 words, keeping 279035 word types\n",
      "2019-06-17 10:51:33,748 : INFO : PROGRESS: at sentence #1020000, processed 18055144 words, keeping 280584 word types\n",
      "2019-06-17 10:51:33,791 : INFO : PROGRESS: at sentence #1030000, processed 18231637 words, keeping 282206 word types\n",
      "2019-06-17 10:51:33,837 : INFO : PROGRESS: at sentence #1040000, processed 18398878 words, keeping 283570 word types\n",
      "2019-06-17 10:51:33,885 : INFO : PROGRESS: at sentence #1050000, processed 18584353 words, keeping 285112 word types\n",
      "2019-06-17 10:51:33,933 : INFO : PROGRESS: at sentence #1060000, processed 18750269 words, keeping 286666 word types\n",
      "2019-06-17 10:51:33,991 : INFO : PROGRESS: at sentence #1070000, processed 18929960 words, keeping 288122 word types\n",
      "2019-06-17 10:51:34,052 : INFO : PROGRESS: at sentence #1080000, processed 19102851 words, keeping 289489 word types\n",
      "2019-06-17 10:51:34,102 : INFO : PROGRESS: at sentence #1090000, processed 19278476 words, keeping 291139 word types\n",
      "2019-06-17 10:51:34,151 : INFO : PROGRESS: at sentence #1100000, processed 19463665 words, keeping 293838 word types\n",
      "2019-06-17 10:51:34,192 : INFO : PROGRESS: at sentence #1110000, processed 19635307 words, keeping 295273 word types\n",
      "2019-06-17 10:51:34,236 : INFO : PROGRESS: at sentence #1120000, processed 19812865 words, keeping 296816 word types\n",
      "2019-06-17 10:51:34,290 : INFO : PROGRESS: at sentence #1130000, processed 19983578 words, keeping 298552 word types\n",
      "2019-06-17 10:51:34,324 : INFO : PROGRESS: at sentence #1140000, processed 20106292 words, keeping 299628 word types\n",
      "2019-06-17 10:51:34,369 : INFO : PROGRESS: at sentence #1150000, processed 20281010 words, keeping 301139 word types\n",
      "2019-06-17 10:51:34,423 : INFO : PROGRESS: at sentence #1160000, processed 20463992 words, keeping 302566 word types\n",
      "2019-06-17 10:51:34,475 : INFO : PROGRESS: at sentence #1170000, processed 20639845 words, keeping 304039 word types\n",
      "2019-06-17 10:51:34,550 : INFO : PROGRESS: at sentence #1180000, processed 20809920 words, keeping 305503 word types\n",
      "2019-06-17 10:51:34,611 : INFO : PROGRESS: at sentence #1190000, processed 20984247 words, keeping 307005 word types\n",
      "2019-06-17 10:51:34,685 : INFO : PROGRESS: at sentence #1200000, processed 21163937 words, keeping 308842 word types\n",
      "2019-06-17 10:51:34,743 : INFO : PROGRESS: at sentence #1210000, processed 21353983 words, keeping 310414 word types\n",
      "2019-06-17 10:51:34,804 : INFO : PROGRESS: at sentence #1220000, processed 21534830 words, keeping 312012 word types\n",
      "2019-06-17 10:51:34,861 : INFO : PROGRESS: at sentence #1230000, processed 21709272 words, keeping 313850 word types\n",
      "2019-06-17 10:51:34,921 : INFO : PROGRESS: at sentence #1240000, processed 21894484 words, keeping 315829 word types\n",
      "2019-06-17 10:51:34,973 : INFO : PROGRESS: at sentence #1250000, processed 22068690 words, keeping 317188 word types\n",
      "2019-06-17 10:51:35,037 : INFO : PROGRESS: at sentence #1260000, processed 22244101 words, keeping 318577 word types\n",
      "2019-06-17 10:51:35,103 : INFO : PROGRESS: at sentence #1270000, processed 22407248 words, keeping 320245 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:51:35,159 : INFO : PROGRESS: at sentence #1280000, processed 22594585 words, keeping 321715 word types\n",
      "2019-06-17 10:51:35,207 : INFO : PROGRESS: at sentence #1290000, processed 22771530 words, keeping 323216 word types\n",
      "2019-06-17 10:51:35,258 : INFO : PROGRESS: at sentence #1300000, processed 22963365 words, keeping 324767 word types\n",
      "2019-06-17 10:51:35,304 : INFO : PROGRESS: at sentence #1310000, processed 23129072 words, keeping 326386 word types\n",
      "2019-06-17 10:51:35,374 : INFO : PROGRESS: at sentence #1320000, processed 23362428 words, keeping 329383 word types\n",
      "2019-06-17 10:51:35,423 : INFO : PROGRESS: at sentence #1330000, processed 23523119 words, keeping 330810 word types\n",
      "2019-06-17 10:51:35,472 : INFO : PROGRESS: at sentence #1340000, processed 23697659 words, keeping 332299 word types\n",
      "2019-06-17 10:51:35,522 : INFO : PROGRESS: at sentence #1350000, processed 23867127 words, keeping 333664 word types\n",
      "2019-06-17 10:51:35,591 : INFO : PROGRESS: at sentence #1360000, processed 24046933 words, keeping 335153 word types\n",
      "2019-06-17 10:51:35,646 : INFO : PROGRESS: at sentence #1370000, processed 24206322 words, keeping 336962 word types\n",
      "2019-06-17 10:51:35,693 : INFO : PROGRESS: at sentence #1380000, processed 24383841 words, keeping 338540 word types\n",
      "2019-06-17 10:51:35,738 : INFO : PROGRESS: at sentence #1390000, processed 24567114 words, keeping 339974 word types\n",
      "2019-06-17 10:51:35,780 : INFO : PROGRESS: at sentence #1400000, processed 24733374 words, keeping 341332 word types\n",
      "2019-06-17 10:51:35,826 : INFO : PROGRESS: at sentence #1410000, processed 24911546 words, keeping 342864 word types\n",
      "2019-06-17 10:51:35,872 : INFO : PROGRESS: at sentence #1420000, processed 25081231 words, keeping 344362 word types\n",
      "2019-06-17 10:51:35,912 : INFO : PROGRESS: at sentence #1430000, processed 25241099 words, keeping 345627 word types\n",
      "2019-06-17 10:51:35,960 : INFO : PROGRESS: at sentence #1440000, processed 25416659 words, keeping 346909 word types\n",
      "2019-06-17 10:51:36,011 : INFO : PROGRESS: at sentence #1450000, processed 25584691 words, keeping 348275 word types\n",
      "2019-06-17 10:51:36,087 : INFO : PROGRESS: at sentence #1460000, processed 25763232 words, keeping 349755 word types\n",
      "2019-06-17 10:51:36,137 : INFO : PROGRESS: at sentence #1470000, processed 25943095 words, keeping 351025 word types\n",
      "2019-06-17 10:51:36,181 : INFO : PROGRESS: at sentence #1480000, processed 26119119 words, keeping 352258 word types\n",
      "2019-06-17 10:51:36,222 : INFO : PROGRESS: at sentence #1490000, processed 26285945 words, keeping 353503 word types\n",
      "2019-06-17 10:51:36,265 : INFO : PROGRESS: at sentence #1500000, processed 26454584 words, keeping 354943 word types\n",
      "2019-06-17 10:51:36,310 : INFO : PROGRESS: at sentence #1510000, processed 26637032 words, keeping 356295 word types\n",
      "2019-06-17 10:51:36,350 : INFO : PROGRESS: at sentence #1520000, processed 26799446 words, keeping 357459 word types\n",
      "2019-06-17 10:51:36,394 : INFO : PROGRESS: at sentence #1530000, processed 26978249 words, keeping 358666 word types\n",
      "2019-06-17 10:51:36,443 : INFO : PROGRESS: at sentence #1540000, processed 27153165 words, keeping 359986 word types\n",
      "2019-06-17 10:51:36,499 : INFO : PROGRESS: at sentence #1550000, processed 27328146 words, keeping 361326 word types\n",
      "2019-06-17 10:51:36,564 : INFO : PROGRESS: at sentence #1560000, processed 27519824 words, keeping 362609 word types\n",
      "2019-06-17 10:51:36,617 : INFO : PROGRESS: at sentence #1570000, processed 27694120 words, keeping 363808 word types\n",
      "2019-06-17 10:51:36,677 : INFO : PROGRESS: at sentence #1580000, processed 27882692 words, keeping 365172 word types\n",
      "2019-06-17 10:51:36,734 : INFO : PROGRESS: at sentence #1590000, processed 28078298 words, keeping 366433 word types\n",
      "2019-06-17 10:51:36,782 : INFO : PROGRESS: at sentence #1600000, processed 28259932 words, keeping 367968 word types\n",
      "2019-06-17 10:51:36,823 : INFO : PROGRESS: at sentence #1610000, processed 28425483 words, keeping 369421 word types\n",
      "2019-06-17 10:51:36,867 : INFO : PROGRESS: at sentence #1620000, processed 28595513 words, keeping 371631 word types\n",
      "2019-06-17 10:51:36,912 : INFO : PROGRESS: at sentence #1630000, processed 28769505 words, keeping 372956 word types\n",
      "2019-06-17 10:51:36,958 : INFO : PROGRESS: at sentence #1640000, processed 28939090 words, keeping 374282 word types\n",
      "2019-06-17 10:51:37,014 : INFO : PROGRESS: at sentence #1650000, processed 29125521 words, keeping 375746 word types\n",
      "2019-06-17 10:51:37,071 : INFO : PROGRESS: at sentence #1660000, processed 29299976 words, keeping 377073 word types\n",
      "2019-06-17 10:51:37,117 : INFO : PROGRESS: at sentence #1670000, processed 29475570 words, keeping 378393 word types\n",
      "2019-06-17 10:51:37,180 : INFO : PROGRESS: at sentence #1680000, processed 29645432 words, keeping 379812 word types\n",
      "2019-06-17 10:51:37,259 : INFO : PROGRESS: at sentence #1690000, processed 29818315 words, keeping 380895 word types\n",
      "2019-06-17 10:51:37,361 : INFO : PROGRESS: at sentence #1700000, processed 30005971 words, keeping 384739 word types\n",
      "2019-06-17 10:51:37,431 : INFO : PROGRESS: at sentence #1710000, processed 30176935 words, keeping 386066 word types\n",
      "2019-06-17 10:51:37,488 : INFO : PROGRESS: at sentence #1720000, processed 30353224 words, keeping 387270 word types\n",
      "2019-06-17 10:51:37,553 : INFO : PROGRESS: at sentence #1730000, processed 30524099 words, keeping 388385 word types\n",
      "2019-06-17 10:51:37,612 : INFO : PROGRESS: at sentence #1740000, processed 30694784 words, keeping 389687 word types\n",
      "2019-06-17 10:51:37,672 : INFO : PROGRESS: at sentence #1750000, processed 30865134 words, keeping 390955 word types\n",
      "2019-06-17 10:51:37,728 : INFO : PROGRESS: at sentence #1760000, processed 31036964 words, keeping 392540 word types\n",
      "2019-06-17 10:51:37,786 : INFO : PROGRESS: at sentence #1770000, processed 31209787 words, keeping 393838 word types\n",
      "2019-06-17 10:51:37,840 : INFO : PROGRESS: at sentence #1780000, processed 31381693 words, keeping 395032 word types\n",
      "2019-06-17 10:51:37,890 : INFO : PROGRESS: at sentence #1790000, processed 31554124 words, keeping 396178 word types\n",
      "2019-06-17 10:51:37,962 : INFO : PROGRESS: at sentence #1800000, processed 31729616 words, keeping 401637 word types\n",
      "2019-06-17 10:51:38,018 : INFO : PROGRESS: at sentence #1810000, processed 31902990 words, keeping 402961 word types\n",
      "2019-06-17 10:51:38,076 : INFO : PROGRESS: at sentence #1820000, processed 32081329 words, keeping 404423 word types\n",
      "2019-06-17 10:51:38,130 : INFO : PROGRESS: at sentence #1830000, processed 32252980 words, keeping 405685 word types\n",
      "2019-06-17 10:51:38,182 : INFO : PROGRESS: at sentence #1840000, processed 32421414 words, keeping 406830 word types\n",
      "2019-06-17 10:51:38,231 : INFO : PROGRESS: at sentence #1850000, processed 32585345 words, keeping 408042 word types\n",
      "2019-06-17 10:51:38,277 : INFO : PROGRESS: at sentence #1860000, processed 32748405 words, keeping 409402 word types\n",
      "2019-06-17 10:51:38,325 : INFO : PROGRESS: at sentence #1870000, processed 32926235 words, keeping 410413 word types\n",
      "2019-06-17 10:51:38,369 : INFO : PROGRESS: at sentence #1880000, processed 33090777 words, keeping 411819 word types\n",
      "2019-06-17 10:51:38,415 : INFO : PROGRESS: at sentence #1890000, processed 33269427 words, keeping 412945 word types\n",
      "2019-06-17 10:51:38,463 : INFO : PROGRESS: at sentence #1900000, processed 33450367 words, keeping 414272 word types\n",
      "2019-06-17 10:51:38,508 : INFO : PROGRESS: at sentence #1910000, processed 33618588 words, keeping 415361 word types\n",
      "2019-06-17 10:51:38,568 : INFO : PROGRESS: at sentence #1920000, processed 33803241 words, keeping 416731 word types\n",
      "2019-06-17 10:51:38,628 : INFO : PROGRESS: at sentence #1930000, processed 33985967 words, keeping 419310 word types\n",
      "2019-06-17 10:51:38,684 : INFO : PROGRESS: at sentence #1940000, processed 34169107 words, keeping 421794 word types\n",
      "2019-06-17 10:51:38,740 : INFO : PROGRESS: at sentence #1950000, processed 34356523 words, keeping 423125 word types\n",
      "2019-06-17 10:51:38,794 : INFO : PROGRESS: at sentence #1960000, processed 34532003 words, keeping 424191 word types\n",
      "2019-06-17 10:51:38,848 : INFO : PROGRESS: at sentence #1970000, processed 34704604 words, keeping 425372 word types\n",
      "2019-06-17 10:51:38,918 : INFO : PROGRESS: at sentence #1980000, processed 34886895 words, keeping 426641 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:51:38,999 : INFO : PROGRESS: at sentence #1990000, processed 35059892 words, keeping 427732 word types\n",
      "2019-06-17 10:51:39,080 : INFO : PROGRESS: at sentence #2000000, processed 35237154 words, keeping 428904 word types\n",
      "2019-06-17 10:51:39,136 : INFO : PROGRESS: at sentence #2010000, processed 35409658 words, keeping 429960 word types\n",
      "2019-06-17 10:51:39,206 : INFO : PROGRESS: at sentence #2020000, processed 35599655 words, keeping 431271 word types\n",
      "2019-06-17 10:51:39,260 : INFO : PROGRESS: at sentence #2030000, processed 35788909 words, keeping 432825 word types\n",
      "2019-06-17 10:51:39,309 : INFO : PROGRESS: at sentence #2040000, processed 35960123 words, keeping 433994 word types\n",
      "2019-06-17 10:51:39,355 : INFO : PROGRESS: at sentence #2050000, processed 36145529 words, keeping 436053 word types\n",
      "2019-06-17 10:51:39,399 : INFO : PROGRESS: at sentence #2060000, processed 36317031 words, keeping 437115 word types\n",
      "2019-06-17 10:51:39,453 : INFO : PROGRESS: at sentence #2070000, processed 36494774 words, keeping 438236 word types\n",
      "2019-06-17 10:51:39,508 : INFO : PROGRESS: at sentence #2080000, processed 36675860 words, keeping 439512 word types\n",
      "2019-06-17 10:51:39,567 : INFO : PROGRESS: at sentence #2090000, processed 36852776 words, keeping 440671 word types\n",
      "2019-06-17 10:51:39,616 : INFO : PROGRESS: at sentence #2100000, processed 37026875 words, keeping 442053 word types\n",
      "2019-06-17 10:51:39,676 : INFO : PROGRESS: at sentence #2110000, processed 37193537 words, keeping 443098 word types\n",
      "2019-06-17 10:51:39,727 : INFO : PROGRESS: at sentence #2120000, processed 37376099 words, keeping 444469 word types\n",
      "2019-06-17 10:51:39,773 : INFO : PROGRESS: at sentence #2130000, processed 37548033 words, keeping 445737 word types\n",
      "2019-06-17 10:51:39,818 : INFO : PROGRESS: at sentence #2140000, processed 37717063 words, keeping 447128 word types\n",
      "2019-06-17 10:51:39,860 : INFO : PROGRESS: at sentence #2150000, processed 37884639 words, keeping 448352 word types\n",
      "2019-06-17 10:51:39,901 : INFO : PROGRESS: at sentence #2160000, processed 38049158 words, keeping 449397 word types\n",
      "2019-06-17 10:51:39,962 : INFO : PROGRESS: at sentence #2170000, processed 38241528 words, keeping 450649 word types\n",
      "2019-06-17 10:51:40,010 : INFO : PROGRESS: at sentence #2180000, processed 38413267 words, keeping 451840 word types\n",
      "2019-06-17 10:51:40,064 : INFO : PROGRESS: at sentence #2190000, processed 38599896 words, keeping 453020 word types\n",
      "2019-06-17 10:51:40,110 : INFO : PROGRESS: at sentence #2200000, processed 38774142 words, keeping 454160 word types\n",
      "2019-06-17 10:51:40,163 : INFO : PROGRESS: at sentence #2210000, processed 38952307 words, keeping 455302 word types\n",
      "2019-06-17 10:51:40,272 : INFO : PROGRESS: at sentence #2220000, processed 39148420 words, keeping 456657 word types\n",
      "2019-06-17 10:51:40,346 : INFO : PROGRESS: at sentence #2230000, processed 39323321 words, keeping 457752 word types\n",
      "2019-06-17 10:51:40,427 : INFO : PROGRESS: at sentence #2240000, processed 39503997 words, keeping 458938 word types\n",
      "2019-06-17 10:51:40,505 : INFO : PROGRESS: at sentence #2250000, processed 39687270 words, keeping 460343 word types\n",
      "2019-06-17 10:51:40,564 : INFO : PROGRESS: at sentence #2260000, processed 39858294 words, keeping 461426 word types\n",
      "2019-06-17 10:51:40,606 : INFO : PROGRESS: at sentence #2270000, processed 40024958 words, keeping 462407 word types\n",
      "2019-06-17 10:51:40,625 : INFO : collected 462807 word types from a corpus of 40096354 raw words and 2274338 sentences\n",
      "2019-06-17 10:51:40,626 : INFO : Loading a fresh vocabulary\n",
      "2019-06-17 10:51:41,060 : INFO : effective_min_count=5 retains 104360 unique words (22% of original 462807, drops 358447)\n",
      "2019-06-17 10:51:41,062 : INFO : effective_min_count=5 leaves 39565168 word corpus (98% of original 40096354, drops 531186)\n",
      "2019-06-17 10:51:41,484 : INFO : deleting the raw counts dictionary of 462807 items\n",
      "2019-06-17 10:51:41,497 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2019-06-17 10:51:41,499 : INFO : downsampling leaves estimated 38552993 word corpus (97.4% of prior 39565168)\n",
      "2019-06-17 10:51:42,069 : INFO : estimated required memory for 104360 words and 300 dimensions: 302644000 bytes\n",
      "2019-06-17 10:51:42,070 : INFO : resetting layer weights\n",
      "2019-06-17 10:51:43,769 : INFO : training model with 8 workers on 104360 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-17 10:51:44,788 : INFO : EPOCH 1 - PROGRESS: at 1.63% examples, 629426 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:45,791 : INFO : EPOCH 1 - PROGRESS: at 3.48% examples, 671688 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:46,810 : INFO : EPOCH 1 - PROGRESS: at 5.50% examples, 704903 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:51:47,826 : INFO : EPOCH 1 - PROGRESS: at 7.32% examples, 700768 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:48,834 : INFO : EPOCH 1 - PROGRESS: at 9.23% examples, 705042 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:49,851 : INFO : EPOCH 1 - PROGRESS: at 11.28% examples, 716345 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:50,859 : INFO : EPOCH 1 - PROGRESS: at 13.22% examples, 717217 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:51,878 : INFO : EPOCH 1 - PROGRESS: at 15.08% examples, 714515 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:51:52,881 : INFO : EPOCH 1 - PROGRESS: at 16.78% examples, 711713 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:51:53,887 : INFO : EPOCH 1 - PROGRESS: at 18.89% examples, 721792 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:51:54,911 : INFO : EPOCH 1 - PROGRESS: at 20.92% examples, 723424 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:55,919 : INFO : EPOCH 1 - PROGRESS: at 22.61% examples, 717704 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:51:56,929 : INFO : EPOCH 1 - PROGRESS: at 24.19% examples, 709976 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:57,948 : INFO : EPOCH 1 - PROGRESS: at 26.13% examples, 712215 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:51:58,969 : INFO : EPOCH 1 - PROGRESS: at 27.91% examples, 708533 words/s, in_qsize 14, out_qsize 3\n",
      "2019-06-17 10:51:59,980 : INFO : EPOCH 1 - PROGRESS: at 29.62% examples, 704586 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:00,988 : INFO : EPOCH 1 - PROGRESS: at 31.07% examples, 699187 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:01,991 : INFO : EPOCH 1 - PROGRESS: at 32.86% examples, 697935 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:02,996 : INFO : EPOCH 1 - PROGRESS: at 34.62% examples, 695841 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:04,013 : INFO : EPOCH 1 - PROGRESS: at 35.83% examples, 685167 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:05,028 : INFO : EPOCH 1 - PROGRESS: at 37.49% examples, 682621 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:06,058 : INFO : EPOCH 1 - PROGRESS: at 39.61% examples, 687586 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:07,072 : INFO : EPOCH 1 - PROGRESS: at 41.83% examples, 693884 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:52:08,091 : INFO : EPOCH 1 - PROGRESS: at 44.05% examples, 700216 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:52:09,099 : INFO : EPOCH 1 - PROGRESS: at 46.20% examples, 705258 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:10,107 : INFO : EPOCH 1 - PROGRESS: at 48.39% examples, 710246 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:11,114 : INFO : EPOCH 1 - PROGRESS: at 50.66% examples, 713816 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:12,134 : INFO : EPOCH 1 - PROGRESS: at 52.58% examples, 714089 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:52:13,138 : INFO : EPOCH 1 - PROGRESS: at 54.44% examples, 715129 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:52:14,150 : INFO : EPOCH 1 - PROGRESS: at 56.54% examples, 717816 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:15,153 : INFO : EPOCH 1 - PROGRESS: at 58.33% examples, 718355 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:16,173 : INFO : EPOCH 1 - PROGRESS: at 60.32% examples, 718736 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:52:17,182 : INFO : EPOCH 1 - PROGRESS: at 62.13% examples, 717686 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:18,184 : INFO : EPOCH 1 - PROGRESS: at 64.02% examples, 717119 words/s, in_qsize 14, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:52:19,198 : INFO : EPOCH 1 - PROGRESS: at 65.63% examples, 713917 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:52:20,209 : INFO : EPOCH 1 - PROGRESS: at 67.31% examples, 711748 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:21,249 : INFO : EPOCH 1 - PROGRESS: at 69.30% examples, 712944 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:52:22,277 : INFO : EPOCH 1 - PROGRESS: at 71.13% examples, 712527 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:52:23,291 : INFO : EPOCH 1 - PROGRESS: at 72.92% examples, 711716 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:52:24,293 : INFO : EPOCH 1 - PROGRESS: at 74.81% examples, 712079 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:25,294 : INFO : EPOCH 1 - PROGRESS: at 76.79% examples, 712915 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:26,297 : INFO : EPOCH 1 - PROGRESS: at 78.81% examples, 713783 words/s, in_qsize 13, out_qsize 1\n",
      "2019-06-17 10:52:27,308 : INFO : EPOCH 1 - PROGRESS: at 80.55% examples, 712396 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:28,309 : INFO : EPOCH 1 - PROGRESS: at 82.41% examples, 711885 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:29,334 : INFO : EPOCH 1 - PROGRESS: at 84.14% examples, 710181 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:30,337 : INFO : EPOCH 1 - PROGRESS: at 85.70% examples, 708296 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:31,370 : INFO : EPOCH 1 - PROGRESS: at 87.45% examples, 707222 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:32,386 : INFO : EPOCH 1 - PROGRESS: at 88.74% examples, 702886 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:33,388 : INFO : EPOCH 1 - PROGRESS: at 90.08% examples, 699462 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:34,406 : INFO : EPOCH 1 - PROGRESS: at 91.90% examples, 699223 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:35,416 : INFO : EPOCH 1 - PROGRESS: at 93.74% examples, 699073 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:36,423 : INFO : EPOCH 1 - PROGRESS: at 95.85% examples, 700983 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:37,430 : INFO : EPOCH 1 - PROGRESS: at 97.83% examples, 702422 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:38,412 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-17 10:52:38,437 : INFO : EPOCH 1 - PROGRESS: at 99.84% examples, 703642 words/s, in_qsize 6, out_qsize 1\n",
      "2019-06-17 10:52:38,445 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-17 10:52:38,451 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-17 10:52:38,452 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-17 10:52:38,475 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-17 10:52:38,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-17 10:52:38,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-17 10:52:38,503 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-17 10:52:38,504 : INFO : EPOCH - 1 : training on 40096354 raw words (38515047 effective words) took 54.7s, 703815 effective words/s\n",
      "2019-06-17 10:52:39,533 : INFO : EPOCH 2 - PROGRESS: at 1.98% examples, 743818 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:52:40,552 : INFO : EPOCH 2 - PROGRESS: at 4.04% examples, 766222 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:41,557 : INFO : EPOCH 2 - PROGRESS: at 5.84% examples, 742810 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:42,579 : INFO : EPOCH 2 - PROGRESS: at 7.48% examples, 711704 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:43,582 : INFO : EPOCH 2 - PROGRESS: at 9.38% examples, 714319 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:44,590 : INFO : EPOCH 2 - PROGRESS: at 11.35% examples, 720497 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:52:45,608 : INFO : EPOCH 2 - PROGRESS: at 13.28% examples, 719869 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:46,608 : INFO : EPOCH 2 - PROGRESS: at 15.06% examples, 713763 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:52:47,651 : INFO : EPOCH 2 - PROGRESS: at 16.82% examples, 711100 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:48,660 : INFO : EPOCH 2 - PROGRESS: at 18.40% examples, 700163 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:49,664 : INFO : EPOCH 2 - PROGRESS: at 20.44% examples, 705901 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:50,697 : INFO : EPOCH 2 - PROGRESS: at 22.36% examples, 708147 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:51,705 : INFO : EPOCH 2 - PROGRESS: at 24.01% examples, 702723 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:52,727 : INFO : EPOCH 2 - PROGRESS: at 25.96% examples, 705322 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:53,744 : INFO : EPOCH 2 - PROGRESS: at 27.81% examples, 704183 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:52:54,745 : INFO : EPOCH 2 - PROGRESS: at 29.73% examples, 705687 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:52:55,758 : INFO : EPOCH 2 - PROGRESS: at 31.49% examples, 707214 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:56,766 : INFO : EPOCH 2 - PROGRESS: at 33.27% examples, 705341 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:57,802 : INFO : EPOCH 2 - PROGRESS: at 35.22% examples, 705867 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:58,831 : INFO : EPOCH 2 - PROGRESS: at 37.14% examples, 706821 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:52:59,860 : INFO : EPOCH 2 - PROGRESS: at 38.65% examples, 700627 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:53:00,886 : INFO : EPOCH 2 - PROGRESS: at 40.50% examples, 699736 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:01,907 : INFO : EPOCH 2 - PROGRESS: at 42.43% examples, 700712 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:02,912 : INFO : EPOCH 2 - PROGRESS: at 44.19% examples, 700063 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:03,924 : INFO : EPOCH 2 - PROGRESS: at 45.98% examples, 699335 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:53:04,943 : INFO : EPOCH 2 - PROGRESS: at 48.00% examples, 701406 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:05,957 : INFO : EPOCH 2 - PROGRESS: at 50.01% examples, 702296 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:06,969 : INFO : EPOCH 2 - PROGRESS: at 51.86% examples, 701841 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:07,974 : INFO : EPOCH 2 - PROGRESS: at 53.51% examples, 700313 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:08,983 : INFO : EPOCH 2 - PROGRESS: at 55.10% examples, 697569 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:09,991 : INFO : EPOCH 2 - PROGRESS: at 56.90% examples, 697144 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:11,007 : INFO : EPOCH 2 - PROGRESS: at 58.76% examples, 698625 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:12,016 : INFO : EPOCH 2 - PROGRESS: at 60.19% examples, 693518 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:13,022 : INFO : EPOCH 2 - PROGRESS: at 61.83% examples, 691648 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:14,034 : INFO : EPOCH 2 - PROGRESS: at 63.97% examples, 694068 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:15,046 : INFO : EPOCH 2 - PROGRESS: at 65.61% examples, 691909 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:16,061 : INFO : EPOCH 2 - PROGRESS: at 67.67% examples, 694154 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:17,072 : INFO : EPOCH 2 - PROGRESS: at 69.31% examples, 693060 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:18,073 : INFO : EPOCH 2 - PROGRESS: at 71.18% examples, 693900 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:19,081 : INFO : EPOCH 2 - PROGRESS: at 73.24% examples, 696287 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:53:20,088 : INFO : EPOCH 2 - PROGRESS: at 75.20% examples, 697380 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:21,102 : INFO : EPOCH 2 - PROGRESS: at 77.19% examples, 698316 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:22,111 : INFO : EPOCH 2 - PROGRESS: at 79.20% examples, 699414 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:23,112 : INFO : EPOCH 2 - PROGRESS: at 80.97% examples, 698745 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:24,128 : INFO : EPOCH 2 - PROGRESS: at 82.61% examples, 696434 words/s, in_qsize 14, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:53:25,138 : INFO : EPOCH 2 - PROGRESS: at 84.71% examples, 698816 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:26,152 : INFO : EPOCH 2 - PROGRESS: at 86.55% examples, 699089 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:27,162 : INFO : EPOCH 2 - PROGRESS: at 88.20% examples, 697750 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:28,198 : INFO : EPOCH 2 - PROGRESS: at 89.88% examples, 696676 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:53:29,235 : INFO : EPOCH 2 - PROGRESS: at 91.43% examples, 694317 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:30,254 : INFO : EPOCH 2 - PROGRESS: at 93.32% examples, 694702 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:53:31,258 : INFO : EPOCH 2 - PROGRESS: at 95.38% examples, 696182 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:32,283 : INFO : EPOCH 2 - PROGRESS: at 97.30% examples, 696781 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:33,287 : INFO : EPOCH 2 - PROGRESS: at 99.01% examples, 696552 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:33,701 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-17 10:53:33,731 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-17 10:53:33,744 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-17 10:53:33,757 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-17 10:53:33,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-17 10:53:33,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-17 10:53:33,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-17 10:53:33,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-17 10:53:33,813 : INFO : EPOCH - 2 : training on 40096354 raw words (38515452 effective words) took 55.3s, 696512 effective words/s\n",
      "2019-06-17 10:53:34,835 : INFO : EPOCH 3 - PROGRESS: at 1.47% examples, 569709 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:53:35,842 : INFO : EPOCH 3 - PROGRESS: at 3.03% examples, 574857 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:36,853 : INFO : EPOCH 3 - PROGRESS: at 4.60% examples, 584979 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:37,870 : INFO : EPOCH 3 - PROGRESS: at 6.24% examples, 598728 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:38,905 : INFO : EPOCH 3 - PROGRESS: at 7.80% examples, 591714 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:39,911 : INFO : EPOCH 3 - PROGRESS: at 9.38% examples, 594562 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:40,916 : INFO : EPOCH 3 - PROGRESS: at 11.07% examples, 600923 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:41,931 : INFO : EPOCH 3 - PROGRESS: at 12.48% examples, 590749 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:42,933 : INFO : EPOCH 3 - PROGRESS: at 14.26% examples, 601636 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:43,959 : INFO : EPOCH 3 - PROGRESS: at 16.31% examples, 618310 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:44,975 : INFO : EPOCH 3 - PROGRESS: at 18.20% examples, 630072 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:45,985 : INFO : EPOCH 3 - PROGRESS: at 20.18% examples, 639284 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:46,998 : INFO : EPOCH 3 - PROGRESS: at 22.16% examples, 647522 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:53:48,006 : INFO : EPOCH 3 - PROGRESS: at 24.08% examples, 655535 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:49,026 : INFO : EPOCH 3 - PROGRESS: at 26.11% examples, 663143 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:50,081 : INFO : EPOCH 3 - PROGRESS: at 27.97% examples, 663192 words/s, in_qsize 15, out_qsize 3\n",
      "2019-06-17 10:53:51,088 : INFO : EPOCH 3 - PROGRESS: at 29.56% examples, 659476 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:52,094 : INFO : EPOCH 3 - PROGRESS: at 30.54% examples, 644963 words/s, in_qsize 12, out_qsize 3\n",
      "2019-06-17 10:53:53,102 : INFO : EPOCH 3 - PROGRESS: at 31.44% examples, 631545 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:54,129 : INFO : EPOCH 3 - PROGRESS: at 32.29% examples, 615584 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:55,143 : INFO : EPOCH 3 - PROGRESS: at 33.21% examples, 602926 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:56,190 : INFO : EPOCH 3 - PROGRESS: at 34.34% examples, 592680 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:53:57,333 : INFO : EPOCH 3 - PROGRESS: at 35.42% examples, 582742 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:58,372 : INFO : EPOCH 3 - PROGRESS: at 36.40% examples, 573697 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:53:59,391 : INFO : EPOCH 3 - PROGRESS: at 37.37% examples, 565420 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:00,411 : INFO : EPOCH 3 - PROGRESS: at 38.34% examples, 558146 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:01,523 : INFO : EPOCH 3 - PROGRESS: at 39.62% examples, 553340 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:54:02,587 : INFO : EPOCH 3 - PROGRESS: at 40.76% examples, 547535 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:03,625 : INFO : EPOCH 3 - PROGRESS: at 41.80% examples, 541989 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:04,710 : INFO : EPOCH 3 - PROGRESS: at 42.91% examples, 536589 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:05,751 : INFO : EPOCH 3 - PROGRESS: at 43.78% examples, 529868 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:54:06,778 : INFO : EPOCH 3 - PROGRESS: at 44.86% examples, 526130 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:07,780 : INFO : EPOCH 3 - PROGRESS: at 45.84% examples, 521599 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:08,798 : INFO : EPOCH 3 - PROGRESS: at 46.79% examples, 516854 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:09,807 : INFO : EPOCH 3 - PROGRESS: at 47.65% examples, 511436 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:54:10,847 : INFO : EPOCH 3 - PROGRESS: at 48.77% examples, 508913 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:11,884 : INFO : EPOCH 3 - PROGRESS: at 49.83% examples, 505127 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:12,911 : INFO : EPOCH 3 - PROGRESS: at 50.77% examples, 500151 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:54:13,991 : INFO : EPOCH 3 - PROGRESS: at 51.76% examples, 496225 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:15,004 : INFO : EPOCH 3 - PROGRESS: at 52.77% examples, 493521 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:16,011 : INFO : EPOCH 3 - PROGRESS: at 53.56% examples, 489475 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:17,029 : INFO : EPOCH 3 - PROGRESS: at 54.54% examples, 486789 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:18,060 : INFO : EPOCH 3 - PROGRESS: at 55.45% examples, 483259 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:19,136 : INFO : EPOCH 3 - PROGRESS: at 56.36% examples, 479609 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:20,227 : INFO : EPOCH 3 - PROGRESS: at 57.28% examples, 476189 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:21,336 : INFO : EPOCH 3 - PROGRESS: at 58.32% examples, 474319 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:22,348 : INFO : EPOCH 3 - PROGRESS: at 59.33% examples, 472140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:23,398 : INFO : EPOCH 3 - PROGRESS: at 60.40% examples, 470491 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:24,387 : INFO : EPOCH 3 - PROGRESS: at 61.35% examples, 468217 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:25,468 : INFO : EPOCH 3 - PROGRESS: at 62.26% examples, 465105 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:26,488 : INFO : EPOCH 3 - PROGRESS: at 63.27% examples, 463180 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:27,499 : INFO : EPOCH 3 - PROGRESS: at 64.42% examples, 462684 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:28,501 : INFO : EPOCH 3 - PROGRESS: at 65.41% examples, 461049 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:29,523 : INFO : EPOCH 3 - PROGRESS: at 66.82% examples, 462042 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:54:30,613 : INFO : EPOCH 3 - PROGRESS: at 68.24% examples, 462810 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:31,643 : INFO : EPOCH 3 - PROGRESS: at 69.33% examples, 462169 words/s, in_qsize 13, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:54:32,665 : INFO : EPOCH 3 - PROGRESS: at 70.76% examples, 463907 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:33,669 : INFO : EPOCH 3 - PROGRESS: at 72.26% examples, 465559 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:34,675 : INFO : EPOCH 3 - PROGRESS: at 73.91% examples, 468254 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:35,704 : INFO : EPOCH 3 - PROGRESS: at 75.48% examples, 470220 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:36,751 : INFO : EPOCH 3 - PROGRESS: at 76.81% examples, 470460 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:37,846 : INFO : EPOCH 3 - PROGRESS: at 77.93% examples, 468989 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:38,852 : INFO : EPOCH 3 - PROGRESS: at 78.68% examples, 466013 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:40,054 : INFO : EPOCH 3 - PROGRESS: at 79.66% examples, 463130 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:54:41,195 : INFO : EPOCH 3 - PROGRESS: at 80.47% examples, 459836 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:54:42,210 : INFO : EPOCH 3 - PROGRESS: at 81.23% examples, 457071 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:54:43,296 : INFO : EPOCH 3 - PROGRESS: at 82.19% examples, 455038 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:44,329 : INFO : EPOCH 3 - PROGRESS: at 82.99% examples, 452586 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:45,346 : INFO : EPOCH 3 - PROGRESS: at 83.96% examples, 451382 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:46,348 : INFO : EPOCH 3 - PROGRESS: at 85.05% examples, 451076 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:47,352 : INFO : EPOCH 3 - PROGRESS: at 85.81% examples, 449135 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:48,378 : INFO : EPOCH 3 - PROGRESS: at 86.86% examples, 448361 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:49,448 : INFO : EPOCH 3 - PROGRESS: at 87.71% examples, 446307 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:50,476 : INFO : EPOCH 3 - PROGRESS: at 88.75% examples, 445705 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:51,485 : INFO : EPOCH 3 - PROGRESS: at 89.67% examples, 444602 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:52,567 : INFO : EPOCH 3 - PROGRESS: at 90.57% examples, 442967 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:53,572 : INFO : EPOCH 3 - PROGRESS: at 91.52% examples, 442067 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:54:54,574 : INFO : EPOCH 3 - PROGRESS: at 92.48% examples, 441087 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:55,596 : INFO : EPOCH 3 - PROGRESS: at 93.38% examples, 439792 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:54:56,639 : INFO : EPOCH 3 - PROGRESS: at 94.26% examples, 438183 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:57,664 : INFO : EPOCH 3 - PROGRESS: at 95.24% examples, 437283 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:54:58,684 : INFO : EPOCH 3 - PROGRESS: at 96.00% examples, 435529 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:54:59,729 : INFO : EPOCH 3 - PROGRESS: at 96.93% examples, 434457 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:55:00,737 : INFO : EPOCH 3 - PROGRESS: at 97.68% examples, 432933 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:01,781 : INFO : EPOCH 3 - PROGRESS: at 98.62% examples, 431940 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:02,787 : INFO : EPOCH 3 - PROGRESS: at 99.68% examples, 431667 words/s, in_qsize 12, out_qsize 0\n",
      "2019-06-17 10:55:02,870 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-17 10:55:02,872 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-17 10:55:02,888 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-17 10:55:02,929 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-17 10:55:02,943 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-17 10:55:02,946 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-17 10:55:02,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-17 10:55:02,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-17 10:55:02,990 : INFO : EPOCH - 3 : training on 40096354 raw words (38515901 effective words) took 89.2s, 431960 effective words/s\n",
      "2019-06-17 10:55:04,189 : INFO : EPOCH 4 - PROGRESS: at 0.55% examples, 201524 words/s, in_qsize 15, out_qsize 6\n",
      "2019-06-17 10:55:05,191 : INFO : EPOCH 4 - PROGRESS: at 1.45% examples, 276436 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:55:06,191 : INFO : EPOCH 4 - PROGRESS: at 2.86% examples, 361369 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:07,200 : INFO : EPOCH 4 - PROGRESS: at 4.14% examples, 394044 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:08,222 : INFO : EPOCH 4 - PROGRESS: at 5.54% examples, 422101 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:09,227 : INFO : EPOCH 4 - PROGRESS: at 6.90% examples, 438933 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:10,275 : INFO : EPOCH 4 - PROGRESS: at 8.50% examples, 459153 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:11,298 : INFO : EPOCH 4 - PROGRESS: at 9.60% examples, 454336 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:12,320 : INFO : EPOCH 4 - PROGRESS: at 10.23% examples, 429798 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:13,350 : INFO : EPOCH 4 - PROGRESS: at 10.98% examples, 413646 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:14,437 : INFO : EPOCH 4 - PROGRESS: at 11.79% examples, 401941 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:55:15,534 : INFO : EPOCH 4 - PROGRESS: at 12.78% examples, 395727 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:55:16,584 : INFO : EPOCH 4 - PROGRESS: at 13.58% examples, 388342 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:17,586 : INFO : EPOCH 4 - PROGRESS: at 14.47% examples, 384689 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:55:18,677 : INFO : EPOCH 4 - PROGRESS: at 15.49% examples, 382972 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:55:19,733 : INFO : EPOCH 4 - PROGRESS: at 16.33% examples, 378814 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:55:20,741 : INFO : EPOCH 4 - PROGRESS: at 17.14% examples, 375680 words/s, in_qsize 15, out_qsize 4\n",
      "2019-06-17 10:55:21,797 : INFO : EPOCH 4 - PROGRESS: at 18.23% examples, 377089 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:22,853 : INFO : EPOCH 4 - PROGRESS: at 19.14% examples, 375417 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:23,928 : INFO : EPOCH 4 - PROGRESS: at 20.01% examples, 371224 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:55:24,932 : INFO : EPOCH 4 - PROGRESS: at 20.94% examples, 369941 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:25,947 : INFO : EPOCH 4 - PROGRESS: at 21.90% examples, 369881 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:26,977 : INFO : EPOCH 4 - PROGRESS: at 22.76% examples, 368320 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:27,990 : INFO : EPOCH 4 - PROGRESS: at 23.67% examples, 367206 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:29,022 : INFO : EPOCH 4 - PROGRESS: at 24.49% examples, 365526 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:30,052 : INFO : EPOCH 4 - PROGRESS: at 25.43% examples, 365329 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:31,088 : INFO : EPOCH 4 - PROGRESS: at 26.37% examples, 364140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:32,220 : INFO : EPOCH 4 - PROGRESS: at 27.34% examples, 362842 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:33,341 : INFO : EPOCH 4 - PROGRESS: at 28.27% examples, 360802 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:34,395 : INFO : EPOCH 4 - PROGRESS: at 29.24% examples, 360409 words/s, in_qsize 15, out_qsize 3\n",
      "2019-06-17 10:55:35,429 : INFO : EPOCH 4 - PROGRESS: at 30.23% examples, 360807 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:55:36,453 : INFO : EPOCH 4 - PROGRESS: at 31.07% examples, 361458 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:37,500 : INFO : EPOCH 4 - PROGRESS: at 32.64% examples, 367406 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:38,550 : INFO : EPOCH 4 - PROGRESS: at 33.93% examples, 370310 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:39,590 : INFO : EPOCH 4 - PROGRESS: at 35.28% examples, 374278 words/s, in_qsize 14, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:55:40,639 : INFO : EPOCH 4 - PROGRESS: at 36.08% examples, 372243 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:41,721 : INFO : EPOCH 4 - PROGRESS: at 36.84% examples, 369503 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:42,821 : INFO : EPOCH 4 - PROGRESS: at 37.61% examples, 366729 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:43,826 : INFO : EPOCH 4 - PROGRESS: at 38.42% examples, 365442 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:44,891 : INFO : EPOCH 4 - PROGRESS: at 39.06% examples, 361881 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:55:45,908 : INFO : EPOCH 4 - PROGRESS: at 39.74% examples, 359282 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:46,946 : INFO : EPOCH 4 - PROGRESS: at 40.51% examples, 357338 words/s, in_qsize 11, out_qsize 4\n",
      "2019-06-17 10:55:47,952 : INFO : EPOCH 4 - PROGRESS: at 41.26% examples, 355736 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:49,007 : INFO : EPOCH 4 - PROGRESS: at 41.88% examples, 352781 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:50,014 : INFO : EPOCH 4 - PROGRESS: at 42.69% examples, 351935 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:55:51,058 : INFO : EPOCH 4 - PROGRESS: at 43.61% examples, 351648 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:52,109 : INFO : EPOCH 4 - PROGRESS: at 44.35% examples, 350093 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:55:53,120 : INFO : EPOCH 4 - PROGRESS: at 45.26% examples, 349951 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:55:54,167 : INFO : EPOCH 4 - PROGRESS: at 46.15% examples, 349545 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:55:55,208 : INFO : EPOCH 4 - PROGRESS: at 46.97% examples, 348461 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:56,247 : INFO : EPOCH 4 - PROGRESS: at 47.74% examples, 347238 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:57,356 : INFO : EPOCH 4 - PROGRESS: at 48.54% examples, 345922 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:55:58,375 : INFO : EPOCH 4 - PROGRESS: at 49.42% examples, 345633 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:55:59,379 : INFO : EPOCH 4 - PROGRESS: at 50.43% examples, 345394 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:00,409 : INFO : EPOCH 4 - PROGRESS: at 51.12% examples, 343851 words/s, in_qsize 11, out_qsize 4\n",
      "2019-06-17 10:56:01,512 : INFO : EPOCH 4 - PROGRESS: at 52.09% examples, 343573 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:56:02,534 : INFO : EPOCH 4 - PROGRESS: at 52.95% examples, 343471 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:56:03,552 : INFO : EPOCH 4 - PROGRESS: at 53.78% examples, 343083 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:56:04,570 : INFO : EPOCH 4 - PROGRESS: at 54.87% examples, 344402 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:05,600 : INFO : EPOCH 4 - PROGRESS: at 56.23% examples, 347009 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:06,603 : INFO : EPOCH 4 - PROGRESS: at 57.20% examples, 347726 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:07,641 : INFO : EPOCH 4 - PROGRESS: at 57.95% examples, 347312 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:08,673 : INFO : EPOCH 4 - PROGRESS: at 58.61% examples, 345497 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:09,726 : INFO : EPOCH 4 - PROGRESS: at 59.43% examples, 344643 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:10,768 : INFO : EPOCH 4 - PROGRESS: at 60.20% examples, 343544 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:56:11,793 : INFO : EPOCH 4 - PROGRESS: at 60.91% examples, 342472 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:56:12,793 : INFO : EPOCH 4 - PROGRESS: at 61.79% examples, 342371 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:13,880 : INFO : EPOCH 4 - PROGRESS: at 62.72% examples, 341846 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:14,904 : INFO : EPOCH 4 - PROGRESS: at 63.64% examples, 341774 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:15,925 : INFO : EPOCH 4 - PROGRESS: at 64.54% examples, 341852 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:17,008 : INFO : EPOCH 4 - PROGRESS: at 65.50% examples, 341723 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:56:18,081 : INFO : EPOCH 4 - PROGRESS: at 66.33% examples, 341092 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:56:19,117 : INFO : EPOCH 4 - PROGRESS: at 67.32% examples, 341244 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:20,221 : INFO : EPOCH 4 - PROGRESS: at 68.20% examples, 340835 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:56:21,242 : INFO : EPOCH 4 - PROGRESS: at 69.13% examples, 341040 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:22,352 : INFO : EPOCH 4 - PROGRESS: at 69.93% examples, 340604 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:56:23,434 : INFO : EPOCH 4 - PROGRESS: at 70.74% examples, 339836 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:24,441 : INFO : EPOCH 4 - PROGRESS: at 71.61% examples, 339626 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:25,460 : INFO : EPOCH 4 - PROGRESS: at 72.53% examples, 339727 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:26,460 : INFO : EPOCH 4 - PROGRESS: at 73.18% examples, 338644 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:56:27,472 : INFO : EPOCH 4 - PROGRESS: at 74.12% examples, 338784 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:28,547 : INFO : EPOCH 4 - PROGRESS: at 74.98% examples, 338547 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:29,631 : INFO : EPOCH 4 - PROGRESS: at 76.15% examples, 339315 words/s, in_qsize 15, out_qsize 5\n",
      "2019-06-17 10:56:30,644 : INFO : EPOCH 4 - PROGRESS: at 77.55% examples, 341382 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:31,776 : INFO : EPOCH 4 - PROGRESS: at 79.24% examples, 344217 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:56:32,801 : INFO : EPOCH 4 - PROGRESS: at 80.11% examples, 344020 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:33,816 : INFO : EPOCH 4 - PROGRESS: at 80.75% examples, 342835 words/s, in_qsize 16, out_qsize 5\n",
      "2019-06-17 10:56:34,879 : INFO : EPOCH 4 - PROGRESS: at 81.53% examples, 341863 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:35,912 : INFO : EPOCH 4 - PROGRESS: at 82.39% examples, 341686 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:56:37,009 : INFO : EPOCH 4 - PROGRESS: at 83.13% examples, 340545 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:56:38,066 : INFO : EPOCH 4 - PROGRESS: at 83.84% examples, 339581 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:39,075 : INFO : EPOCH 4 - PROGRESS: at 84.53% examples, 338808 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:40,278 : INFO : EPOCH 4 - PROGRESS: at 85.19% examples, 337289 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:56:41,310 : INFO : EPOCH 4 - PROGRESS: at 85.91% examples, 336780 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:42,326 : INFO : EPOCH 4 - PROGRESS: at 86.57% examples, 335844 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:43,360 : INFO : EPOCH 4 - PROGRESS: at 87.43% examples, 335721 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:44,525 : INFO : EPOCH 4 - PROGRESS: at 88.20% examples, 334787 words/s, in_qsize 16, out_qsize 2\n",
      "2019-06-17 10:56:45,527 : INFO : EPOCH 4 - PROGRESS: at 89.08% examples, 334976 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:46,528 : INFO : EPOCH 4 - PROGRESS: at 89.80% examples, 334500 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:56:47,542 : INFO : EPOCH 4 - PROGRESS: at 90.62% examples, 334275 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:48,567 : INFO : EPOCH 4 - PROGRESS: at 91.43% examples, 334024 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:56:49,592 : INFO : EPOCH 4 - PROGRESS: at 92.28% examples, 333860 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:50,605 : INFO : EPOCH 4 - PROGRESS: at 93.23% examples, 334101 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:51,705 : INFO : EPOCH 4 - PROGRESS: at 94.16% examples, 333894 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:52,719 : INFO : EPOCH 4 - PROGRESS: at 95.07% examples, 333953 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:53,794 : INFO : EPOCH 4 - PROGRESS: at 95.91% examples, 333656 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:56:54,827 : INFO : EPOCH 4 - PROGRESS: at 96.97% examples, 334331 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:56:55,829 : INFO : EPOCH 4 - PROGRESS: at 98.02% examples, 335089 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:56,835 : INFO : EPOCH 4 - PROGRESS: at 99.18% examples, 336150 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:56:57,231 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-17 10:56:57,274 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-17 10:56:57,288 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-17 10:56:57,301 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-17 10:56:57,324 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-17 10:56:57,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-17 10:56:57,353 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-17 10:56:57,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-17 10:56:57,403 : INFO : EPOCH - 4 : training on 40096354 raw words (38514699 effective words) took 114.3s, 337073 effective words/s\n",
      "2019-06-17 10:56:58,502 : INFO : EPOCH 5 - PROGRESS: at 1.32% examples, 517134 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:56:59,514 : INFO : EPOCH 5 - PROGRESS: at 2.27% examples, 428308 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:00,514 : INFO : EPOCH 5 - PROGRESS: at 2.97% examples, 378483 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:01,612 : INFO : EPOCH 5 - PROGRESS: at 3.72% examples, 349274 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:02,631 : INFO : EPOCH 5 - PROGRESS: at 4.52% examples, 339670 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:03,697 : INFO : EPOCH 5 - PROGRESS: at 5.29% examples, 330675 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:57:04,778 : INFO : EPOCH 5 - PROGRESS: at 6.19% examples, 330231 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:05,833 : INFO : EPOCH 5 - PROGRESS: at 7.00% examples, 325158 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:06,936 : INFO : EPOCH 5 - PROGRESS: at 8.00% examples, 326764 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:07,936 : INFO : EPOCH 5 - PROGRESS: at 8.86% examples, 327551 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:08,978 : INFO : EPOCH 5 - PROGRESS: at 9.62% examples, 323732 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:10,016 : INFO : EPOCH 5 - PROGRESS: at 10.50% examples, 323629 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:11,105 : INFO : EPOCH 5 - PROGRESS: at 11.35% examples, 321713 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:12,281 : INFO : EPOCH 5 - PROGRESS: at 12.34% examples, 320966 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:57:13,329 : INFO : EPOCH 5 - PROGRESS: at 13.14% examples, 318332 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:14,440 : INFO : EPOCH 5 - PROGRESS: at 13.95% examples, 316701 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:15,474 : INFO : EPOCH 5 - PROGRESS: at 14.87% examples, 317168 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:16,505 : INFO : EPOCH 5 - PROGRESS: at 15.67% examples, 316587 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:17,557 : INFO : EPOCH 5 - PROGRESS: at 16.46% examples, 316260 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:18,560 : INFO : EPOCH 5 - PROGRESS: at 17.38% examples, 318062 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:19,628 : INFO : EPOCH 5 - PROGRESS: at 18.25% examples, 318362 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:20,666 : INFO : EPOCH 5 - PROGRESS: at 19.08% examples, 318216 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:57:21,706 : INFO : EPOCH 5 - PROGRESS: at 20.22% examples, 321559 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:22,713 : INFO : EPOCH 5 - PROGRESS: at 21.20% examples, 323916 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:23,764 : INFO : EPOCH 5 - PROGRESS: at 22.31% examples, 327371 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:24,811 : INFO : EPOCH 5 - PROGRESS: at 23.77% examples, 335522 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:57:25,845 : INFO : EPOCH 5 - PROGRESS: at 24.82% examples, 338475 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:26,862 : INFO : EPOCH 5 - PROGRESS: at 25.72% examples, 338436 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:57:27,879 : INFO : EPOCH 5 - PROGRESS: at 26.57% examples, 337520 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:28,993 : INFO : EPOCH 5 - PROGRESS: at 27.55% examples, 337471 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:30,064 : INFO : EPOCH 5 - PROGRESS: at 28.39% examples, 335803 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:31,114 : INFO : EPOCH 5 - PROGRESS: at 29.53% examples, 338435 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:32,120 : INFO : EPOCH 5 - PROGRESS: at 30.43% examples, 339292 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:33,142 : INFO : EPOCH 5 - PROGRESS: at 31.17% examples, 338908 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:34,183 : INFO : EPOCH 5 - PROGRESS: at 32.07% examples, 338186 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:35,185 : INFO : EPOCH 5 - PROGRESS: at 32.84% examples, 337013 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:36,193 : INFO : EPOCH 5 - PROGRESS: at 33.67% examples, 336421 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:37,272 : INFO : EPOCH 5 - PROGRESS: at 34.55% examples, 335470 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:38,318 : INFO : EPOCH 5 - PROGRESS: at 35.31% examples, 334467 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:39,373 : INFO : EPOCH 5 - PROGRESS: at 36.15% examples, 334055 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:57:40,453 : INFO : EPOCH 5 - PROGRESS: at 37.09% examples, 333906 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:41,455 : INFO : EPOCH 5 - PROGRESS: at 37.84% examples, 333040 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:42,514 : INFO : EPOCH 5 - PROGRESS: at 38.77% examples, 333281 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:43,548 : INFO : EPOCH 5 - PROGRESS: at 39.55% examples, 332238 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:44,672 : INFO : EPOCH 5 - PROGRESS: at 40.58% examples, 332458 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:45,695 : INFO : EPOCH 5 - PROGRESS: at 41.70% examples, 334139 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:46,754 : INFO : EPOCH 5 - PROGRESS: at 42.72% examples, 335159 words/s, in_qsize 15, out_qsize 3\n",
      "2019-06-17 10:57:47,794 : INFO : EPOCH 5 - PROGRESS: at 43.86% examples, 336932 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:57:48,845 : INFO : EPOCH 5 - PROGRESS: at 45.06% examples, 339161 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:49,855 : INFO : EPOCH 5 - PROGRESS: at 46.67% examples, 344349 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:50,902 : INFO : EPOCH 5 - PROGRESS: at 47.50% examples, 343529 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:57:51,946 : INFO : EPOCH 5 - PROGRESS: at 48.27% examples, 342540 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:52,957 : INFO : EPOCH 5 - PROGRESS: at 49.00% examples, 341309 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:53,960 : INFO : EPOCH 5 - PROGRESS: at 50.03% examples, 341512 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:54,962 : INFO : EPOCH 5 - PROGRESS: at 50.89% examples, 341026 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:57:56,017 : INFO : EPOCH 5 - PROGRESS: at 51.85% examples, 341262 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:57:57,039 : INFO : EPOCH 5 - PROGRESS: at 52.79% examples, 341507 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:58,121 : INFO : EPOCH 5 - PROGRESS: at 53.72% examples, 341585 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:57:59,234 : INFO : EPOCH 5 - PROGRESS: at 54.70% examples, 341619 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:00,239 : INFO : EPOCH 5 - PROGRESS: at 55.67% examples, 341954 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:58:01,315 : INFO : EPOCH 5 - PROGRESS: at 56.49% examples, 341295 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:58:02,320 : INFO : EPOCH 5 - PROGRESS: at 57.37% examples, 341479 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:03,339 : INFO : EPOCH 5 - PROGRESS: at 58.15% examples, 341415 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:58:04,363 : INFO : EPOCH 5 - PROGRESS: at 59.13% examples, 341498 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:05,397 : INFO : EPOCH 5 - PROGRESS: at 59.99% examples, 341079 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:06,407 : INFO : EPOCH 5 - PROGRESS: at 60.77% examples, 340374 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:07,520 : INFO : EPOCH 5 - PROGRESS: at 61.81% examples, 340782 words/s, in_qsize 15, out_qsize 1\n",
      "2019-06-17 10:58:08,514 : INFO : EPOCH 5 - PROGRESS: at 62.52% examples, 339592 words/s, in_qsize 16, out_qsize 1\n",
      "2019-06-17 10:58:09,515 : INFO : EPOCH 5 - PROGRESS: at 63.37% examples, 339258 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:10,517 : INFO : EPOCH 5 - PROGRESS: at 64.34% examples, 339726 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:58:11,527 : INFO : EPOCH 5 - PROGRESS: at 65.38% examples, 340404 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:58:12,596 : INFO : EPOCH 5 - PROGRESS: at 66.59% examples, 341673 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:13,635 : INFO : EPOCH 5 - PROGRESS: at 67.98% examples, 343826 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:14,653 : INFO : EPOCH 5 - PROGRESS: at 69.52% examples, 347464 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:15,672 : INFO : EPOCH 5 - PROGRESS: at 70.43% examples, 347597 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:16,687 : INFO : EPOCH 5 - PROGRESS: at 71.39% examples, 347611 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:17,873 : INFO : EPOCH 5 - PROGRESS: at 72.28% examples, 346778 words/s, in_qsize 16, out_qsize 0\n",
      "2019-06-17 10:58:18,911 : INFO : EPOCH 5 - PROGRESS: at 73.17% examples, 346597 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:19,927 : INFO : EPOCH 5 - PROGRESS: at 74.01% examples, 346163 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:20,979 : INFO : EPOCH 5 - PROGRESS: at 74.98% examples, 346381 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:22,053 : INFO : EPOCH 5 - PROGRESS: at 75.70% examples, 345162 words/s, in_qsize 13, out_qsize 2\n",
      "2019-06-17 10:58:23,056 : INFO : EPOCH 5 - PROGRESS: at 76.57% examples, 344923 words/s, in_qsize 14, out_qsize 1\n",
      "2019-06-17 10:58:24,062 : INFO : EPOCH 5 - PROGRESS: at 77.36% examples, 344333 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:25,067 : INFO : EPOCH 5 - PROGRESS: at 78.23% examples, 344100 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:26,069 : INFO : EPOCH 5 - PROGRESS: at 79.08% examples, 343839 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:27,113 : INFO : EPOCH 5 - PROGRESS: at 79.88% examples, 343254 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:28,191 : INFO : EPOCH 5 - PROGRESS: at 80.77% examples, 342874 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:29,265 : INFO : EPOCH 5 - PROGRESS: at 81.77% examples, 342716 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:30,408 : INFO : EPOCH 5 - PROGRESS: at 82.68% examples, 342214 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:31,449 : INFO : EPOCH 5 - PROGRESS: at 83.57% examples, 342099 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:32,482 : INFO : EPOCH 5 - PROGRESS: at 84.55% examples, 342319 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:33,522 : INFO : EPOCH 5 - PROGRESS: at 85.43% examples, 342321 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:34,557 : INFO : EPOCH 5 - PROGRESS: at 86.21% examples, 341906 words/s, in_qsize 16, out_qsize 3\n",
      "2019-06-17 10:58:35,553 : INFO : EPOCH 5 - PROGRESS: at 87.33% examples, 342759 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:36,556 : INFO : EPOCH 5 - PROGRESS: at 88.55% examples, 344124 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:37,648 : INFO : EPOCH 5 - PROGRESS: at 90.31% examples, 347331 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:38,667 : INFO : EPOCH 5 - PROGRESS: at 91.43% examples, 348096 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:39,701 : INFO : EPOCH 5 - PROGRESS: at 92.17% examples, 347382 words/s, in_qsize 12, out_qsize 3\n",
      "2019-06-17 10:58:40,732 : INFO : EPOCH 5 - PROGRESS: at 92.93% examples, 346701 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:41,870 : INFO : EPOCH 5 - PROGRESS: at 93.65% examples, 345580 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:42,917 : INFO : EPOCH 5 - PROGRESS: at 94.46% examples, 344967 words/s, in_qsize 15, out_qsize 2\n",
      "2019-06-17 10:58:43,950 : INFO : EPOCH 5 - PROGRESS: at 95.33% examples, 344760 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:44,957 : INFO : EPOCH 5 - PROGRESS: at 96.22% examples, 344740 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:45,960 : INFO : EPOCH 5 - PROGRESS: at 96.96% examples, 344194 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:47,034 : INFO : EPOCH 5 - PROGRESS: at 97.60% examples, 343260 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:48,097 : INFO : EPOCH 5 - PROGRESS: at 98.31% examples, 342474 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:49,098 : INFO : EPOCH 5 - PROGRESS: at 99.12% examples, 342214 words/s, in_qsize 15, out_qsize 0\n",
      "2019-06-17 10:58:49,770 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-06-17 10:58:49,796 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-06-17 10:58:49,810 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-06-17 10:58:49,812 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-17 10:58:49,827 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-17 10:58:49,842 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-17 10:58:49,853 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-17 10:58:49,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-17 10:58:49,888 : INFO : EPOCH - 5 : training on 40096354 raw words (38515769 effective words) took 112.4s, 342702 effective words/s\n",
      "2019-06-17 10:58:49,891 : INFO : training on a 200481770 raw words (192576868 effective words) took 426.1s, 451930 effective words/s\n",
      "2019-06-17 10:58:50,191 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fa0201bf978>\n",
      "2019-06-17 10:58:50,251 : INFO : iterating over columns in tf-idf order\n",
      "2019-06-17 10:58:54,722 : INFO : PROGRESS: at 0.00% columns (1 / 462807, 0.000216% density, 0.000216% projected density)\n",
      "2019-06-17 10:58:55,477 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 10:58:56,617 : INFO : PROGRESS: at 0.22% columns (1001 / 462807, 0.000216% density, 0.000345% projected density)\n",
      "2019-06-17 10:58:56,733 : INFO : PROGRESS: at 0.43% columns (2001 / 462807, 0.000216% density, 0.000302% projected density)\n",
      "2019-06-17 10:58:57,066 : INFO : PROGRESS: at 0.65% columns (3001 / 462807, 0.000217% density, 0.000345% projected density)\n",
      "2019-06-17 10:58:57,162 : INFO : PROGRESS: at 0.86% columns (4001 / 462807, 0.000217% density, 0.000324% projected density)\n",
      "2019-06-17 10:58:57,421 : INFO : PROGRESS: at 1.08% columns (5001 / 462807, 0.000217% density, 0.000337% projected density)\n",
      "2019-06-17 10:58:57,646 : INFO : PROGRESS: at 1.30% columns (6001 / 462807, 0.000218% density, 0.000338% projected density)\n",
      "2019-06-17 10:58:57,834 : INFO : PROGRESS: at 1.51% columns (7001 / 462807, 0.000218% density, 0.000339% projected density)\n",
      "2019-06-17 10:58:58,107 : INFO : PROGRESS: at 1.73% columns (8001 / 462807, 0.000218% density, 0.000345% projected density)\n",
      "2019-06-17 10:58:58,163 : INFO : PROGRESS: at 1.94% columns (9001 / 462807, 0.000218% density, 0.000335% projected density)\n",
      "2019-06-17 10:58:58,522 : INFO : PROGRESS: at 2.16% columns (10001 / 462807, 0.000219% density, 0.000349% projected density)\n",
      "2019-06-17 10:58:58,616 : INFO : PROGRESS: at 2.38% columns (11001 / 462807, 0.000219% density, 0.000337% projected density)\n",
      "2019-06-17 10:58:58,834 : INFO : PROGRESS: at 2.59% columns (12001 / 462807, 0.000219% density, 0.000345% projected density)\n",
      "2019-06-17 10:58:58,983 : INFO : PROGRESS: at 2.81% columns (13001 / 462807, 0.000220% density, 0.000342% projected density)\n",
      "2019-06-17 10:58:59,086 : INFO : PROGRESS: at 3.03% columns (14001 / 462807, 0.000220% density, 0.000336% projected density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:58:59,536 : INFO : PROGRESS: at 3.24% columns (15001 / 462807, 0.000221% density, 0.000354% projected density)\n",
      "2019-06-17 10:58:59,556 : INFO : PROGRESS: at 3.46% columns (16001 / 462807, 0.000221% density, 0.000345% projected density)\n",
      "2019-06-17 10:58:59,923 : INFO : PROGRESS: at 3.67% columns (17001 / 462807, 0.000221% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:00,094 : INFO : PROGRESS: at 3.89% columns (18001 / 462807, 0.000222% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:00,175 : INFO : PROGRESS: at 4.11% columns (19001 / 462807, 0.000222% density, 0.000352% projected density)\n",
      "2019-06-17 10:59:00,349 : INFO : PROGRESS: at 4.32% columns (20001 / 462807, 0.000222% density, 0.000352% projected density)\n",
      "2019-06-17 10:59:00,563 : INFO : PROGRESS: at 4.54% columns (21001 / 462807, 0.000222% density, 0.000352% projected density)\n",
      "2019-06-17 10:59:00,680 : INFO : PROGRESS: at 4.75% columns (22001 / 462807, 0.000223% density, 0.000351% projected density)\n",
      "2019-06-17 10:59:00,771 : INFO : PROGRESS: at 4.97% columns (23001 / 462807, 0.000223% density, 0.000347% projected density)\n",
      "2019-06-17 10:59:01,060 : INFO : PROGRESS: at 5.19% columns (24001 / 462807, 0.000223% density, 0.000353% projected density)\n",
      "2019-06-17 10:59:01,265 : INFO : PROGRESS: at 5.40% columns (25001 / 462807, 0.000224% density, 0.000354% projected density)\n",
      "2019-06-17 10:59:01,380 : INFO : PROGRESS: at 5.62% columns (26001 / 462807, 0.000224% density, 0.000350% projected density)\n",
      "2019-06-17 10:59:01,652 : INFO : PROGRESS: at 5.83% columns (27001 / 462807, 0.000224% density, 0.000350% projected density)\n",
      "2019-06-17 10:59:01,857 : INFO : PROGRESS: at 6.05% columns (28001 / 462807, 0.000224% density, 0.000351% projected density)\n",
      "2019-06-17 10:59:01,966 : INFO : PROGRESS: at 6.27% columns (29001 / 462807, 0.000225% density, 0.000351% projected density)\n",
      "2019-06-17 10:59:01,987 : INFO : PROGRESS: at 6.48% columns (30001 / 462807, 0.000225% density, 0.000346% projected density)\n",
      "2019-06-17 10:59:02,217 : INFO : PROGRESS: at 6.70% columns (31001 / 462807, 0.000225% density, 0.000348% projected density)\n",
      "2019-06-17 10:59:02,417 : INFO : PROGRESS: at 6.91% columns (32001 / 462807, 0.000225% density, 0.000348% projected density)\n",
      "2019-06-17 10:59:02,459 : INFO : PROGRESS: at 7.13% columns (33001 / 462807, 0.000225% density, 0.000346% projected density)\n",
      "2019-06-17 10:59:02,715 : INFO : PROGRESS: at 7.35% columns (34001 / 462807, 0.000226% density, 0.000347% projected density)\n",
      "2019-06-17 10:59:03,203 : INFO : PROGRESS: at 7.56% columns (35001 / 462807, 0.000226% density, 0.000353% projected density)\n",
      "2019-06-17 10:59:03,930 : INFO : PROGRESS: at 7.78% columns (36001 / 462807, 0.000227% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:04,040 : INFO : PROGRESS: at 7.99% columns (37001 / 462807, 0.000227% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:04,389 : INFO : PROGRESS: at 8.21% columns (38001 / 462807, 0.000228% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:04,661 : INFO : PROGRESS: at 8.43% columns (39001 / 462807, 0.000228% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:04,809 : INFO : PROGRESS: at 8.64% columns (40001 / 462807, 0.000228% density, 0.000357% projected density)\n",
      "2019-06-17 10:59:05,134 : INFO : PROGRESS: at 8.86% columns (41001 / 462807, 0.000229% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:05,273 : INFO : PROGRESS: at 9.08% columns (42001 / 462807, 0.000229% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:05,404 : INFO : PROGRESS: at 9.29% columns (43001 / 462807, 0.000229% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:05,498 : INFO : PROGRESS: at 9.51% columns (44001 / 462807, 0.000230% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:05,742 : INFO : PROGRESS: at 9.72% columns (45001 / 462807, 0.000230% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:06,067 : INFO : PROGRESS: at 9.94% columns (46001 / 462807, 0.000230% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:06,149 : INFO : PROGRESS: at 10.16% columns (47001 / 462807, 0.000230% density, 0.000357% projected density)\n",
      "2019-06-17 10:59:06,484 : INFO : PROGRESS: at 10.37% columns (48001 / 462807, 0.000231% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:06,546 : INFO : PROGRESS: at 10.59% columns (49001 / 462807, 0.000231% density, 0.000355% projected density)\n",
      "2019-06-17 10:59:06,811 : INFO : PROGRESS: at 10.80% columns (50001 / 462807, 0.000231% density, 0.000355% projected density)\n",
      "2019-06-17 10:59:06,971 : INFO : PROGRESS: at 11.02% columns (51001 / 462807, 0.000231% density, 0.000353% projected density)\n",
      "2019-06-17 10:59:07,205 : INFO : PROGRESS: at 11.24% columns (52001 / 462807, 0.000231% density, 0.000353% projected density)\n",
      "2019-06-17 10:59:07,402 : INFO : PROGRESS: at 11.45% columns (53001 / 462807, 0.000232% density, 0.000352% projected density)\n",
      "2019-06-17 10:59:07,533 : INFO : PROGRESS: at 11.67% columns (54001 / 462807, 0.000232% density, 0.000351% projected density)\n",
      "2019-06-17 10:59:07,703 : INFO : PROGRESS: at 11.88% columns (55001 / 462807, 0.000232% density, 0.000351% projected density)\n",
      "2019-06-17 10:59:08,451 : INFO : PROGRESS: at 12.10% columns (56001 / 462807, 0.000233% density, 0.000357% projected density)\n",
      "2019-06-17 10:59:08,651 : INFO : PROGRESS: at 12.32% columns (57001 / 462807, 0.000233% density, 0.000356% projected density)\n",
      "2019-06-17 10:59:08,757 : INFO : PROGRESS: at 12.53% columns (58001 / 462807, 0.000233% density, 0.000355% projected density)\n",
      "2019-06-17 10:59:09,003 : INFO : PROGRESS: at 12.75% columns (59001 / 462807, 0.000234% density, 0.000357% projected density)\n",
      "2019-06-17 10:59:10,126 : INFO : PROGRESS: at 12.96% columns (60001 / 462807, 0.000236% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:10,365 : INFO : PROGRESS: at 13.18% columns (61001 / 462807, 0.000237% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:10,888 : INFO : PROGRESS: at 13.40% columns (62001 / 462807, 0.000238% density, 0.000377% projected density)\n",
      "2019-06-17 10:59:11,244 : INFO : PROGRESS: at 13.61% columns (63001 / 462807, 0.000238% density, 0.000378% projected density)\n",
      "2019-06-17 10:59:11,497 : INFO : PROGRESS: at 13.83% columns (64001 / 462807, 0.000238% density, 0.000378% projected density)\n",
      "2019-06-17 10:59:11,647 : INFO : PROGRESS: at 14.04% columns (65001 / 462807, 0.000239% density, 0.000376% projected density)\n",
      "2019-06-17 10:59:11,923 : INFO : PROGRESS: at 14.26% columns (66001 / 462807, 0.000239% density, 0.000376% projected density)\n",
      "2019-06-17 10:59:12,017 : INFO : PROGRESS: at 14.48% columns (67001 / 462807, 0.000239% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:12,267 : INFO : PROGRESS: at 14.69% columns (68001 / 462807, 0.000239% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:12,459 : INFO : PROGRESS: at 14.91% columns (69001 / 462807, 0.000240% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:12,616 : INFO : PROGRESS: at 15.13% columns (70001 / 462807, 0.000240% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:12,818 : INFO : PROGRESS: at 15.34% columns (71001 / 462807, 0.000240% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:13,243 : INFO : PROGRESS: at 15.56% columns (72001 / 462807, 0.000241% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:13,401 : INFO : PROGRESS: at 15.77% columns (73001 / 462807, 0.000241% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:13,500 : INFO : PROGRESS: at 15.99% columns (74001 / 462807, 0.000241% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:13,918 : INFO : PROGRESS: at 16.21% columns (75001 / 462807, 0.000242% density, 0.000376% projected density)\n",
      "2019-06-17 10:59:14,143 : INFO : PROGRESS: at 16.42% columns (76001 / 462807, 0.000242% density, 0.000376% projected density)\n",
      "2019-06-17 10:59:14,262 : INFO : PROGRESS: at 16.64% columns (77001 / 462807, 0.000243% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:14,382 : INFO : PROGRESS: at 16.85% columns (78001 / 462807, 0.000243% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:14,553 : INFO : PROGRESS: at 17.07% columns (79001 / 462807, 0.000243% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:14,721 : INFO : PROGRESS: at 17.29% columns (80001 / 462807, 0.000243% density, 0.000372% projected density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:59:14,945 : INFO : PROGRESS: at 17.50% columns (81001 / 462807, 0.000243% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:15,083 : INFO : PROGRESS: at 17.72% columns (82001 / 462807, 0.000244% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:15,156 : INFO : PROGRESS: at 17.93% columns (83001 / 462807, 0.000244% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:15,193 : INFO : PROGRESS: at 18.15% columns (84001 / 462807, 0.000244% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:15,392 : INFO : PROGRESS: at 18.37% columns (85001 / 462807, 0.000244% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:15,548 : INFO : PROGRESS: at 18.58% columns (86001 / 462807, 0.000245% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:15,651 : INFO : PROGRESS: at 18.80% columns (87001 / 462807, 0.000245% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:15,730 : INFO : PROGRESS: at 19.01% columns (88001 / 462807, 0.000245% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:15,784 : INFO : PROGRESS: at 19.23% columns (89001 / 462807, 0.000245% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:15,806 : INFO : PROGRESS: at 19.45% columns (90001 / 462807, 0.000245% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:15,887 : INFO : PROGRESS: at 19.66% columns (91001 / 462807, 0.000245% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:16,141 : INFO : PROGRESS: at 19.88% columns (92001 / 462807, 0.000246% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:16,260 : INFO : PROGRESS: at 20.09% columns (93001 / 462807, 0.000246% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:16,368 : INFO : PROGRESS: at 20.31% columns (94001 / 462807, 0.000246% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:16,484 : INFO : PROGRESS: at 20.53% columns (95001 / 462807, 0.000246% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:16,661 : INFO : PROGRESS: at 20.74% columns (96001 / 462807, 0.000246% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:16,821 : INFO : PROGRESS: at 20.96% columns (97001 / 462807, 0.000246% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:16,932 : INFO : PROGRESS: at 21.18% columns (98001 / 462807, 0.000247% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:17,160 : INFO : PROGRESS: at 21.39% columns (99001 / 462807, 0.000247% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:17,299 : INFO : PROGRESS: at 21.61% columns (100001 / 462807, 0.000247% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:17,645 : INFO : PROGRESS: at 21.82% columns (101001 / 462807, 0.000248% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:17,776 : INFO : PROGRESS: at 22.04% columns (102001 / 462807, 0.000248% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:17,821 : INFO : PROGRESS: at 22.26% columns (103001 / 462807, 0.000248% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:18,199 : INFO : PROGRESS: at 22.47% columns (104001 / 462807, 0.000249% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:18,548 : INFO : PROGRESS: at 22.69% columns (105001 / 462807, 0.000249% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:18,683 : INFO : PROGRESS: at 22.90% columns (106001 / 462807, 0.000250% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:18,821 : INFO : PROGRESS: at 23.12% columns (107001 / 462807, 0.000250% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:19,052 : INFO : PROGRESS: at 23.34% columns (108001 / 462807, 0.000250% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:19,218 : INFO : PROGRESS: at 23.55% columns (109001 / 462807, 0.000250% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:19,584 : INFO : PROGRESS: at 23.77% columns (110001 / 462807, 0.000251% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:19,954 : INFO : PROGRESS: at 23.98% columns (111001 / 462807, 0.000251% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:20,135 : INFO : PROGRESS: at 24.20% columns (112001 / 462807, 0.000252% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:20,230 : INFO : PROGRESS: at 24.42% columns (113001 / 462807, 0.000252% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:20,396 : INFO : PROGRESS: at 24.63% columns (114001 / 462807, 0.000252% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:20,861 : INFO : PROGRESS: at 24.85% columns (115001 / 462807, 0.000252% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:21,263 : INFO : PROGRESS: at 25.06% columns (116001 / 462807, 0.000253% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:21,485 : INFO : PROGRESS: at 25.28% columns (117001 / 462807, 0.000253% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:21,512 : INFO : PROGRESS: at 25.50% columns (118001 / 462807, 0.000253% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:21,748 : INFO : PROGRESS: at 25.71% columns (119001 / 462807, 0.000253% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:22,043 : INFO : PROGRESS: at 25.93% columns (120001 / 462807, 0.000254% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:22,147 : INFO : PROGRESS: at 26.15% columns (121001 / 462807, 0.000254% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:22,427 : INFO : PROGRESS: at 26.36% columns (122001 / 462807, 0.000254% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:22,577 : INFO : PROGRESS: at 26.58% columns (123001 / 462807, 0.000255% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:23,060 : INFO : PROGRESS: at 26.79% columns (124001 / 462807, 0.000255% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:23,915 : INFO : PROGRESS: at 27.01% columns (125001 / 462807, 0.000257% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:24,056 : INFO : PROGRESS: at 27.23% columns (126001 / 462807, 0.000257% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:24,498 : INFO : PROGRESS: at 27.44% columns (127001 / 462807, 0.000258% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:24,761 : INFO : PROGRESS: at 27.66% columns (128001 / 462807, 0.000258% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:24,871 : INFO : PROGRESS: at 27.87% columns (129001 / 462807, 0.000258% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:25,078 : INFO : PROGRESS: at 28.09% columns (130001 / 462807, 0.000258% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:25,161 : INFO : PROGRESS: at 28.31% columns (131001 / 462807, 0.000258% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:25,258 : INFO : PROGRESS: at 28.52% columns (132001 / 462807, 0.000259% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:25,297 : INFO : PROGRESS: at 28.74% columns (133001 / 462807, 0.000259% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:25,555 : INFO : PROGRESS: at 28.95% columns (134001 / 462807, 0.000259% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:25,672 : INFO : PROGRESS: at 29.17% columns (135001 / 462807, 0.000259% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:25,735 : INFO : PROGRESS: at 29.39% columns (136001 / 462807, 0.000260% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:25,908 : INFO : PROGRESS: at 29.60% columns (137001 / 462807, 0.000260% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:26,178 : INFO : PROGRESS: at 29.82% columns (138001 / 462807, 0.000260% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:26,423 : INFO : PROGRESS: at 30.03% columns (139001 / 462807, 0.000261% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:26,683 : INFO : PROGRESS: at 30.25% columns (140001 / 462807, 0.000261% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:26,860 : INFO : PROGRESS: at 30.47% columns (141001 / 462807, 0.000262% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:26,932 : INFO : PROGRESS: at 30.68% columns (142001 / 462807, 0.000262% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:26,996 : INFO : PROGRESS: at 30.90% columns (143001 / 462807, 0.000262% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:27,128 : INFO : PROGRESS: at 31.11% columns (144001 / 462807, 0.000262% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:27,159 : INFO : PROGRESS: at 31.33% columns (145001 / 462807, 0.000262% density, 0.000364% projected density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:59:27,252 : INFO : PROGRESS: at 31.55% columns (146001 / 462807, 0.000263% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:27,328 : INFO : PROGRESS: at 31.76% columns (147001 / 462807, 0.000263% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:27,399 : INFO : PROGRESS: at 31.98% columns (148001 / 462807, 0.000263% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:27,498 : INFO : PROGRESS: at 32.20% columns (149001 / 462807, 0.000263% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:27,727 : INFO : PROGRESS: at 32.41% columns (150001 / 462807, 0.000264% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:27,858 : INFO : PROGRESS: at 32.63% columns (151001 / 462807, 0.000264% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:27,938 : INFO : PROGRESS: at 32.84% columns (152001 / 462807, 0.000264% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:28,166 : INFO : PROGRESS: at 33.06% columns (153001 / 462807, 0.000265% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:28,255 : INFO : PROGRESS: at 33.28% columns (154001 / 462807, 0.000265% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:28,481 : INFO : PROGRESS: at 33.49% columns (155001 / 462807, 0.000266% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:28,519 : INFO : PROGRESS: at 33.71% columns (156001 / 462807, 0.000266% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:28,682 : INFO : PROGRESS: at 33.92% columns (157001 / 462807, 0.000266% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:28,716 : INFO : PROGRESS: at 34.14% columns (158001 / 462807, 0.000266% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:28,772 : INFO : PROGRESS: at 34.36% columns (159001 / 462807, 0.000266% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:28,874 : INFO : PROGRESS: at 34.57% columns (160001 / 462807, 0.000266% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:28,914 : INFO : PROGRESS: at 34.79% columns (161001 / 462807, 0.000267% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:28,986 : INFO : PROGRESS: at 35.00% columns (162001 / 462807, 0.000267% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:29,095 : INFO : PROGRESS: at 35.22% columns (163001 / 462807, 0.000267% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:29,181 : INFO : PROGRESS: at 35.44% columns (164001 / 462807, 0.000267% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:29,309 : INFO : PROGRESS: at 35.65% columns (165001 / 462807, 0.000268% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:29,371 : INFO : PROGRESS: at 35.87% columns (166001 / 462807, 0.000268% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:29,447 : INFO : PROGRESS: at 36.08% columns (167001 / 462807, 0.000268% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:29,671 : INFO : PROGRESS: at 36.30% columns (168001 / 462807, 0.000268% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:29,850 : INFO : PROGRESS: at 36.52% columns (169001 / 462807, 0.000269% density, 0.000361% projected density)\n",
      "2019-06-17 10:59:29,914 : INFO : PROGRESS: at 36.73% columns (170001 / 462807, 0.000269% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:30,043 : INFO : PROGRESS: at 36.95% columns (171001 / 462807, 0.000269% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:30,171 : INFO : PROGRESS: at 37.16% columns (172001 / 462807, 0.000269% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:30,366 : INFO : PROGRESS: at 37.38% columns (173001 / 462807, 0.000270% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:30,447 : INFO : PROGRESS: at 37.60% columns (174001 / 462807, 0.000270% density, 0.000360% projected density)\n",
      "2019-06-17 10:59:30,510 : INFO : PROGRESS: at 37.81% columns (175001 / 462807, 0.000270% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:30,631 : INFO : PROGRESS: at 38.03% columns (176001 / 462807, 0.000270% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:30,855 : INFO : PROGRESS: at 38.25% columns (177001 / 462807, 0.000271% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:30,962 : INFO : PROGRESS: at 38.46% columns (178001 / 462807, 0.000271% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:31,022 : INFO : PROGRESS: at 38.68% columns (179001 / 462807, 0.000271% density, 0.000359% projected density)\n",
      "2019-06-17 10:59:31,093 : INFO : PROGRESS: at 38.89% columns (180001 / 462807, 0.000271% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,228 : INFO : PROGRESS: at 39.11% columns (181001 / 462807, 0.000272% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,326 : INFO : PROGRESS: at 39.33% columns (182001 / 462807, 0.000272% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,526 : INFO : PROGRESS: at 39.54% columns (183001 / 462807, 0.000272% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,644 : INFO : PROGRESS: at 39.76% columns (184001 / 462807, 0.000273% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,761 : INFO : PROGRESS: at 39.97% columns (185001 / 462807, 0.000273% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,816 : INFO : PROGRESS: at 40.19% columns (186001 / 462807, 0.000273% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:31,950 : INFO : PROGRESS: at 40.41% columns (187001 / 462807, 0.000273% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:32,150 : INFO : PROGRESS: at 40.62% columns (188001 / 462807, 0.000274% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:32,298 : INFO : PROGRESS: at 40.84% columns (189001 / 462807, 0.000274% density, 0.000358% projected density)\n",
      "2019-06-17 10:59:33,642 : INFO : PROGRESS: at 41.05% columns (190001 / 462807, 0.000277% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:34,498 : INFO : PROGRESS: at 41.27% columns (191001 / 462807, 0.000280% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:34,582 : INFO : PROGRESS: at 41.49% columns (192001 / 462807, 0.000280% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:34,713 : INFO : PROGRESS: at 41.70% columns (193001 / 462807, 0.000280% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:34,764 : INFO : PROGRESS: at 41.92% columns (194001 / 462807, 0.000280% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:34,848 : INFO : PROGRESS: at 42.13% columns (195001 / 462807, 0.000280% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:34,912 : INFO : PROGRESS: at 42.35% columns (196001 / 462807, 0.000281% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:35,077 : INFO : PROGRESS: at 42.57% columns (197001 / 462807, 0.000281% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:35,164 : INFO : PROGRESS: at 42.78% columns (198001 / 462807, 0.000281% density, 0.000369% projected density)\n",
      "2019-06-17 10:59:35,226 : INFO : PROGRESS: at 43.00% columns (199001 / 462807, 0.000281% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:35,382 : INFO : PROGRESS: at 43.21% columns (200001 / 462807, 0.000282% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:35,644 : INFO : PROGRESS: at 43.43% columns (201001 / 462807, 0.000282% density, 0.000368% projected density)\n",
      "2019-06-17 10:59:35,677 : INFO : PROGRESS: at 43.65% columns (202001 / 462807, 0.000282% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:35,732 : INFO : PROGRESS: at 43.86% columns (203001 / 462807, 0.000282% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:35,769 : INFO : PROGRESS: at 44.08% columns (204001 / 462807, 0.000282% density, 0.000367% projected density)\n",
      "2019-06-17 10:59:35,802 : INFO : PROGRESS: at 44.30% columns (205001 / 462807, 0.000282% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:35,882 : INFO : PROGRESS: at 44.51% columns (206001 / 462807, 0.000283% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:36,004 : INFO : PROGRESS: at 44.73% columns (207001 / 462807, 0.000283% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:36,188 : INFO : PROGRESS: at 44.94% columns (208001 / 462807, 0.000283% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:36,242 : INFO : PROGRESS: at 45.16% columns (209001 / 462807, 0.000283% density, 0.000365% projected density)\n",
      "2019-06-17 10:59:36,277 : INFO : PROGRESS: at 45.38% columns (210001 / 462807, 0.000283% density, 0.000365% projected density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:59:36,331 : INFO : PROGRESS: at 45.59% columns (211001 / 462807, 0.000284% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:36,391 : INFO : PROGRESS: at 45.81% columns (212001 / 462807, 0.000284% density, 0.000364% projected density)\n",
      "2019-06-17 10:59:36,453 : INFO : PROGRESS: at 46.02% columns (213001 / 462807, 0.000284% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:36,556 : INFO : PROGRESS: at 46.24% columns (214001 / 462807, 0.000284% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:36,711 : INFO : PROGRESS: at 46.46% columns (215001 / 462807, 0.000284% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:36,781 : INFO : PROGRESS: at 46.67% columns (216001 / 462807, 0.000284% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:36,945 : INFO : PROGRESS: at 46.89% columns (217001 / 462807, 0.000285% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:36,997 : INFO : PROGRESS: at 47.10% columns (218001 / 462807, 0.000285% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:37,030 : INFO : PROGRESS: at 47.32% columns (219001 / 462807, 0.000285% density, 0.000362% projected density)\n",
      "2019-06-17 10:59:37,195 : INFO : PROGRESS: at 47.54% columns (220001 / 462807, 0.000286% density, 0.000363% projected density)\n",
      "2019-06-17 10:59:37,746 : INFO : PROGRESS: at 47.75% columns (221001 / 462807, 0.000288% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:37,868 : INFO : PROGRESS: at 47.97% columns (222001 / 462807, 0.000288% density, 0.000366% projected density)\n",
      "2019-06-17 10:59:39,728 : INFO : PROGRESS: at 48.18% columns (223001 / 462807, 0.000292% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:40,099 : INFO : PROGRESS: at 48.40% columns (224001 / 462807, 0.000293% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:40,476 : INFO : PROGRESS: at 48.62% columns (225001 / 462807, 0.000293% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:40,592 : INFO : PROGRESS: at 48.83% columns (226001 / 462807, 0.000293% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:40,792 : INFO : PROGRESS: at 49.05% columns (227001 / 462807, 0.000294% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:40,932 : INFO : PROGRESS: at 49.26% columns (228001 / 462807, 0.000294% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:41,100 : INFO : PROGRESS: at 49.48% columns (229001 / 462807, 0.000294% density, 0.000374% projected density)\n",
      "2019-06-17 10:59:41,306 : INFO : PROGRESS: at 49.70% columns (230001 / 462807, 0.000294% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:41,517 : INFO : PROGRESS: at 49.91% columns (231001 / 462807, 0.000294% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:41,684 : INFO : PROGRESS: at 50.13% columns (232001 / 462807, 0.000295% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:41,811 : INFO : PROGRESS: at 50.35% columns (233001 / 462807, 0.000295% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:42,098 : INFO : PROGRESS: at 50.56% columns (234001 / 462807, 0.000295% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:42,215 : INFO : PROGRESS: at 50.78% columns (235001 / 462807, 0.000296% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:42,282 : INFO : PROGRESS: at 50.99% columns (236001 / 462807, 0.000296% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:42,367 : INFO : PROGRESS: at 51.21% columns (237001 / 462807, 0.000296% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:42,501 : INFO : PROGRESS: at 51.43% columns (238001 / 462807, 0.000296% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:42,552 : INFO : PROGRESS: at 51.64% columns (239001 / 462807, 0.000296% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:42,680 : INFO : PROGRESS: at 51.86% columns (240001 / 462807, 0.000296% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:42,945 : INFO : PROGRESS: at 52.07% columns (241001 / 462807, 0.000297% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:43,157 : INFO : PROGRESS: at 52.29% columns (242001 / 462807, 0.000297% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:43,231 : INFO : PROGRESS: at 52.51% columns (243001 / 462807, 0.000297% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:43,559 : INFO : PROGRESS: at 52.72% columns (244001 / 462807, 0.000297% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:43,730 : INFO : PROGRESS: at 52.94% columns (245001 / 462807, 0.000297% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:44,273 : INFO : PROGRESS: at 53.15% columns (246001 / 462807, 0.000299% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:44,444 : INFO : PROGRESS: at 53.37% columns (247001 / 462807, 0.000299% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:44,648 : INFO : PROGRESS: at 53.59% columns (248001 / 462807, 0.000299% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:44,723 : INFO : PROGRESS: at 53.80% columns (249001 / 462807, 0.000299% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:45,199 : INFO : PROGRESS: at 54.02% columns (250001 / 462807, 0.000300% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:45,362 : INFO : PROGRESS: at 54.23% columns (251001 / 462807, 0.000300% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:45,431 : INFO : PROGRESS: at 54.45% columns (252001 / 462807, 0.000300% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:45,491 : INFO : PROGRESS: at 54.67% columns (253001 / 462807, 0.000300% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:45,825 : INFO : PROGRESS: at 54.88% columns (254001 / 462807, 0.000301% density, 0.000370% projected density)\n",
      "2019-06-17 10:59:47,182 : INFO : PROGRESS: at 55.10% columns (255001 / 462807, 0.000302% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:47,431 : INFO : PROGRESS: at 55.31% columns (256001 / 462807, 0.000303% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:47,470 : INFO : PROGRESS: at 55.53% columns (257001 / 462807, 0.000303% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:47,718 : INFO : PROGRESS: at 55.75% columns (258001 / 462807, 0.000303% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:47,986 : INFO : PROGRESS: at 55.96% columns (259001 / 462807, 0.000303% density, 0.000372% projected density)\n",
      "2019-06-17 10:59:48,056 : INFO : PROGRESS: at 56.18% columns (260001 / 462807, 0.000303% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:48,230 : INFO : PROGRESS: at 56.40% columns (261001 / 462807, 0.000304% density, 0.000371% projected density)\n",
      "2019-06-17 10:59:48,825 : INFO : PROGRESS: at 56.61% columns (262001 / 462807, 0.000305% density, 0.000373% projected density)\n",
      "2019-06-17 10:59:49,559 : INFO : PROGRESS: at 56.83% columns (263001 / 462807, 0.000306% density, 0.000375% projected density)\n",
      "2019-06-17 10:59:50,540 : INFO : PROGRESS: at 57.04% columns (264001 / 462807, 0.000308% density, 0.000377% projected density)\n",
      "2019-06-17 10:59:51,456 : INFO : PROGRESS: at 57.26% columns (265001 / 462807, 0.000309% density, 0.000379% projected density)\n",
      "2019-06-17 10:59:51,951 : INFO : PROGRESS: at 57.48% columns (266001 / 462807, 0.000310% density, 0.000380% projected density)\n",
      "2019-06-17 10:59:52,872 : INFO : PROGRESS: at 57.69% columns (267001 / 462807, 0.000312% density, 0.000382% projected density)\n",
      "2019-06-17 10:59:53,873 : INFO : PROGRESS: at 57.91% columns (268001 / 462807, 0.000313% density, 0.000384% projected density)\n",
      "2019-06-17 10:59:54,380 : INFO : PROGRESS: at 58.12% columns (269001 / 462807, 0.000314% density, 0.000385% projected density)\n",
      "2019-06-17 10:59:55,470 : INFO : PROGRESS: at 58.34% columns (270001 / 462807, 0.000316% density, 0.000387% projected density)\n",
      "2019-06-17 10:59:55,917 : INFO : PROGRESS: at 58.56% columns (271001 / 462807, 0.000316% density, 0.000387% projected density)\n",
      "2019-06-17 10:59:56,580 : INFO : PROGRESS: at 58.77% columns (272001 / 462807, 0.000317% density, 0.000388% projected density)\n",
      "2019-06-17 10:59:57,230 : INFO : PROGRESS: at 58.99% columns (273001 / 462807, 0.000319% density, 0.000390% projected density)\n",
      "2019-06-17 10:59:57,820 : INFO : PROGRESS: at 59.20% columns (274001 / 462807, 0.000319% density, 0.000391% projected density)\n",
      "2019-06-17 10:59:58,372 : INFO : PROGRESS: at 59.42% columns (275001 / 462807, 0.000320% density, 0.000392% projected density)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 10:59:59,075 : INFO : PROGRESS: at 59.64% columns (276001 / 462807, 0.000321% density, 0.000393% projected density)\n",
      "2019-06-17 10:59:59,593 : INFO : PROGRESS: at 59.85% columns (277001 / 462807, 0.000322% density, 0.000393% projected density)\n",
      "2019-06-17 11:00:00,401 : INFO : PROGRESS: at 60.07% columns (278001 / 462807, 0.000323% density, 0.000395% projected density)\n",
      "2019-06-17 11:00:01,359 : INFO : PROGRESS: at 60.28% columns (279001 / 462807, 0.000325% density, 0.000396% projected density)\n",
      "2019-06-17 11:00:02,584 : INFO : PROGRESS: at 60.50% columns (280001 / 462807, 0.000327% density, 0.000399% projected density)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float32 object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/git/gensim/gensim/similarities/termsim.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, dictionary, tfidf, symmetric, positive_definite, nonzero_limit, dtype)\u001b[0m\n\u001b[1;32m    232\u001b[0m             most_similar = [\n\u001b[1;32m    233\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 if term in dictionary.token2id]\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/gensim/gensim/similarities/termsim.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonzero_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnum_nonzero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             most_similar = [\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 if term in dictionary.token2id]\n",
      "\u001b[0;32m~/git/gensim/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, t1, topn)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mmost_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float32 object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "\n",
    "dictionary = Dictionary(corpus)\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "w2v_model = Word2Vec(corpus, workers=cpu_count(), min_count=5, size=300, seed=12345)\n",
    "similarity_index = WordEmbeddingSimilarityIndex(w2v_model.wv)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Next, we will load the validation and test datasets that were used by the SemEval 2016 and 2017 contestants. The datasets contain 208 original questions posted by the forum members. For each question, there is a list of 10 threads with a human annotation denoting whether or not the thread is relevant to the original question. Our task will be to order the threads so that relevant threads rank above irrelevant threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:04,683 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "datasets = api.load(\"semeval-2016-2017-task3-subtaskBC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform an evaluation to compare three unsupervised similarity measures – the Soft Cosine Measure, two different implementations of the [Word Mover's Distance][wmd], and standard cosine similarity. We will use the [Mean Average Precision (MAP)][map] as an evaluation measure and 10-fold cross-validation to get an estimate of the variance of MAP for each similarity measure.\n",
    "\n",
    "[wmd]: http://vene.ro/blog/word-movers-distance-in-python.html\n",
    "[map]: https://medium.com/@pds.bangalore/mean-average-precision-abd77d0b9a7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wmd in /home/misha/envs/gensim/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/misha/git/gensim/.eggs/numpy-1.15.4-py3.7-linux-x86_64.egg (from wmd) (1.15.4)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "from time import time\n",
    "\n",
    "from gensim.similarities import MatrixSimilarity, WmdSimilarity, SoftCosineSimilarity\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from wmd import WMD\n",
    "\n",
    "def produce_test_data(dataset):\n",
    "    for orgquestion in datasets[dataset]:\n",
    "        query = preprocess(orgquestion[\"OrgQSubject\"]) + preprocess(orgquestion[\"OrgQBody\"])\n",
    "        documents = [\n",
    "            preprocess(thread[\"RelQuestion\"][\"RelQSubject\"]) + preprocess(thread[\"RelQuestion\"][\"RelQBody\"])\n",
    "            for thread in orgquestion[\"Threads\"]]\n",
    "        relevance = [\n",
    "            thread[\"RelQuestion\"][\"RELQ_RELEVANCE2ORGQ\"] in (\"PerfectMatch\", \"Relevant\")\n",
    "            for thread in orgquestion[\"Threads\"]]\n",
    "        yield query, documents, relevance\n",
    "\n",
    "def cossim(query, documents):\n",
    "    # Compute cosine similarity between the query and the documents.\n",
    "    query = tfidf[dictionary.doc2bow(query)]\n",
    "    index = MatrixSimilarity(\n",
    "        tfidf[[dictionary.doc2bow(document) for document in documents]],\n",
    "        num_features=len(dictionary))\n",
    "    similarities = index[query]\n",
    "    return similarities\n",
    "\n",
    "def softcossim(query, documents):\n",
    "    # Compute Soft Cosine Measure between the query and the documents.\n",
    "    query = tfidf[dictionary.doc2bow(query)]\n",
    "    index = SoftCosineSimilarity(\n",
    "        tfidf[[dictionary.doc2bow(document) for document in documents]],\n",
    "        similarity_matrix)\n",
    "    similarities = index[query]\n",
    "    return similarities\n",
    "\n",
    "def wmd_gensim(query, documents):\n",
    "    # Compute Word Mover's Distance as implemented in PyEMD by William Mayner\n",
    "    # between the query and the documents.\n",
    "    index = WmdSimilarity(documents, w2v_model)\n",
    "    similarities = index[query]\n",
    "    return similarities\n",
    "\n",
    "def wmd_relax(query, documents):\n",
    "    # Compute Word Mover's Distance as implemented in WMD by Source{d}\n",
    "    # between the query and the documents.\n",
    "    words = [word for word in set(chain(query, *documents)) if word in w2v_model.wv]\n",
    "    indices, words = zip(*sorted((\n",
    "        (index, word) for (index, _), word in zip(dictionary.doc2bow(words), words))))\n",
    "    query = dict(tfidf[dictionary.doc2bow(query)])\n",
    "    query = [\n",
    "        (new_index, query[dict_index])\n",
    "        for new_index, dict_index in enumerate(indices)\n",
    "        if dict_index in query]\n",
    "    documents = [dict(tfidf[dictionary.doc2bow(document)]) for document in documents]\n",
    "    documents = [[\n",
    "        (new_index, document[dict_index])\n",
    "        for new_index, dict_index in enumerate(indices)\n",
    "        if dict_index in document] for document in documents]\n",
    "    embeddings = np.array([w2v_model.wv[word] for word in words], dtype=np.float32)\n",
    "    nbow = dict(((index, list(chain([None], zip(*document)))) for index, document in enumerate(documents)))\n",
    "    nbow[\"query\"] = tuple([None] + list(zip(*query)))\n",
    "    distances = WMD(embeddings, nbow, vocabulary_min=1).nearest_neighbors(\"query\")\n",
    "    similarities = [-distance for _, distance in sorted(distances)]\n",
    "    return similarities\n",
    "\n",
    "strategies = {\n",
    "    \"cossim\" : cossim,\n",
    "    \"softcossim\": softcossim,\n",
    "    \"wmd-gensim\": wmd_gensim,\n",
    "    \"wmd-relax\": wmd_relax}\n",
    "\n",
    "def evaluate(split, strategy):\n",
    "    # Perform a single round of evaluation.\n",
    "    results = []\n",
    "    start_time = time()\n",
    "    for query, documents, relevance in split:\n",
    "        similarities = strategies[strategy](query, documents)\n",
    "        assert len(similarities) == len(documents)\n",
    "        precision = [\n",
    "            (num_correct + 1) / (num_total + 1) for num_correct, num_total in enumerate(\n",
    "                num_total for num_total, (_, relevant) in enumerate(\n",
    "                    sorted(zip(similarities, relevance), reverse=True)) if relevant)]\n",
    "        average_precision = np.mean(precision) if precision else 0.0\n",
    "        results.append(average_precision)\n",
    "    return (np.mean(results) * 100, time() - start_time)\n",
    "\n",
    "def crossvalidate(args):\n",
    "    # Perform a cross-validation.\n",
    "    dataset, strategy = args\n",
    "    test_data = np.array(list(produce_test_data(dataset)))\n",
    "    kf = KFold(n_splits=10)\n",
    "    samples = []\n",
    "    for _, test_index in kf.split(test_data):\n",
    "        samples.append(evaluate(test_data[test_index], strategy))\n",
    "    return (np.mean(samples, axis=0), np.std(samples, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:13,689 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:13,822 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:14,044 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:14,270 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:14,795 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:14,757 : INFO : Vocabulary size: 35 500\n",
      "2019-06-17 11:00:14,907 : INFO : WCD\n",
      "2019-06-17 11:00:14,927 : INFO : 0.0\n",
      "2019-06-17 11:00:14,919 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:14,950 : INFO : First K WMD\n",
      "2019-06-17 11:00:15,013 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,186 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:15,209 : INFO : WCD\n",
      "2019-06-17 11:00:15,184 : INFO : [(-19.50642204284668, 6), (-18.93822479248047, 4), (-19.099836349487305, 7), (-18.120803833007812, 5), (-18.768550872802734, 3), (-17.821372985839844, 9), (-16.620975494384766, 2), (-17.2498722076416, 8), (-0.0, 1), (-16.496601104736328, 0)]\n",
      "2019-06-17 11:00:15,234 : INFO : 0.3\n",
      "2019-06-17 11:00:15,236 : INFO : 0.0\n",
      "2019-06-17 11:00:15,274 : INFO : First K WMD\n",
      "2019-06-17 11:00:15,283 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,292 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,254 : INFO : P&P\n",
      "2019-06-17 11:00:15,348 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:15,474 : INFO : Vocabulary size: 22 500\n",
      "2019-06-17 11:00:15,482 : INFO : WCD\n",
      "2019-06-17 11:00:15,459 : INFO : [(-22.284854888916016, 2), (-20.642396926879883, 1), (-19.43754768371582, 7), (-19.818540573120117, 0), (-20.160484313964844, 9), (-18.545433044433594, 4), (-19.263004302978516, 5), (-19.687625885009766, 3), (-18.807912826538086, 8), (-18.607440948486328, 6)]\n",
      "2019-06-17 11:00:15,513 : INFO : 0.0\n",
      "2019-06-17 11:00:15,519 : INFO : 0.2\n",
      "2019-06-17 11:00:15,527 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,535 : INFO : First K WMD\n",
      "2019-06-17 11:00:15,502 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,545 : INFO : P&P\n",
      "2019-06-17 11:00:15,575 : INFO : [(-19.88299560546875, 4), (-19.570404052734375, 2), (-19.015090942382812, 7), (-19.5378360748291, 6), (-19.245037078857422, 9), (-18.205718994140625, 0), (-18.70973014831543, 8), (-0.0, 1), (-18.25179100036621, 5), (-18.702421188354492, 3)]\n",
      "2019-06-17 11:00:15,584 : INFO : 0.0\n",
      "2019-06-17 11:00:15,583 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:15,625 : INFO : P&P\n",
      "2019-06-17 11:00:15,696 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,670 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,676 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:15,813 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:15,814 : INFO : WCD\n",
      "2019-06-17 11:00:15,846 : INFO : 0.0\n",
      "2019-06-17 11:00:15,874 : INFO : First K WMD\n",
      "2019-06-17 11:00:15,906 : INFO : Vocabulary size: 35 500\n",
      "2019-06-17 11:00:15,914 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:15,915 : INFO : WCD\n",
      "2019-06-17 11:00:15,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,087 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:15,953 : INFO : [(-18.63705062866211, 4), (-18.16960334777832, 9), (-17.212848663330078, 1), (-17.97890853881836, 8), (-17.71758270263672, 2), (-14.772705078125, 7), (-17.19535255432129, 5), (-16.690874099731445, 3), (-16.197166442871094, 6), (-16.350788116455078, 0)]\n",
      "2019-06-17 11:00:15,975 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,415 : INFO : built Dictionary(58 unique tokens: ['american', 'bachelor', 'bayt', 'com', 'cv']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:16,422 : INFO : 0.0\n",
      "2019-06-17 11:00:16,446 : INFO : First K WMD\n",
      "2019-06-17 11:00:16,414 : INFO : 0.5\n",
      "2019-06-17 11:00:16,414 : INFO : built Dictionary(31 unique tokens: ['days', 'embassy', 'get', 'greece', 'idea']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:16,450 : INFO : P&P\n",
      "2019-06-17 11:00:16,480 : INFO : [(-21.315460205078125, 9), (-19.83448600769043, 8), (-17.198699951171875, 2), (-16.903573989868164, 0), (-16.132722854614258, 3), (-16.038570404052734, 7), (-14.74287223815918, 5), (-0.0, 1), (-15.953986167907715, 4), (-12.188965797424316, 6)]\n",
      "2019-06-17 11:00:16,495 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:16,466 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:16,508 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,491 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,512 : INFO : built Dictionary(28 unique tokens: ['anyone', 'attention', 'btw', 'good', 'issued']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:16,522 : INFO : 0.1\n",
      "2019-06-17 11:00:16,537 : INFO : P&P\n",
      "2019-06-17 11:00:16,552 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,561 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:16,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,571 : INFO : built Dictionary(26 unique tokens: ['anyone', 'doha', 'embassy', 'hi', 'swiss']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:16,590 : INFO : Vocabulary size: 10 500\n",
      "2019-06-17 11:00:16,611 : INFO : WCD\n",
      "2019-06-17 11:00:16,626 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,671 : INFO : built Dictionary(53 unique tokens: ['around', 'bundle', 'car', 'card', 'care']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:16,674 : INFO : 0.0\n",
      "2019-06-17 11:00:16,680 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,692 : INFO : First K WMD\n",
      "2019-06-17 11:00:16,727 : INFO : [(-20.971342086791992, 7), (-20.006893157958984, 5), (-19.6937255859375, 8), (-19.63027572631836, 4), (-19.275415420532227, 6), (-18.86374855041504, 9), (-19.516380310058594, 0), (-18.209640502929688, 1), (-19.462051391601562, 3), (-17.854942321777344, 2)]\n",
      "2019-06-17 11:00:16,763 : INFO : 0.0\n",
      "2019-06-17 11:00:16,767 : INFO : P&P\n",
      "2019-06-17 11:00:16,772 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:16,799 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:16,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,802 : INFO : WCD\n",
      "2019-06-17 11:00:16,826 : INFO : built Dictionary(35 unique tokens: ['almost', 'also', 'applied', 'back', 'bayt']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:00:16,845 : INFO : 0.0\n",
      "2019-06-17 11:00:16,856 : INFO : First K WMD\n",
      "2019-06-17 11:00:16,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,889 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,890 : INFO : built Dictionary(56 unique tokens: ['advice', 'airport', 'alone', 'arrive', 'beth']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:16,890 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:16,923 : INFO : [(-19.77895736694336, 3), (-18.3629093170166, 6), (-18.59762191772461, 2), (-17.96289825439453, 7), (-17.25493621826172, 5), (-16.75144386291504, 8), (-14.48732852935791, 1), (-17.926095962524414, 9), (-13.503809928894043, 0), (-11.997895240783691, 4)]\n",
      "2019-06-17 11:00:16,934 : INFO : 0.1\n",
      "2019-06-17 11:00:16,920 : INFO : WCD\n",
      "2019-06-17 11:00:16,940 : INFO : P&P\n",
      "2019-06-17 11:00:16,945 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:16,947 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:16,954 : INFO : 0.0\n",
      "2019-06-17 11:00:16,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:16,967 : INFO : built Dictionary(70 unique tokens: ['actually', 'anybody', 'anyone', 'applied', 'come']...) from 2 documents (total 90 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:16,974 : INFO : First K WMD\n",
      "2019-06-17 11:00:16,997 : INFO : [(-18.612998962402344, 7), (-17.614749908447266, 0), (-17.181663513183594, 6), (-16.649444580078125, 8), (-16.556007385253906, 9), (-12.592411041259766, 3), (-16.570131301879883, 5), (-11.052681922912598, 2), (-16.47679328918457, 4), (-16.522056579589844, 1)]\n",
      "2019-06-17 11:00:17,027 : INFO : 0.0\n",
      "2019-06-17 11:00:17,045 : INFO : P&P\n",
      "2019-06-17 11:00:17,053 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,093 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,121 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:17,113 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,136 : INFO : WCD\n",
      "2019-06-17 11:00:17,131 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,158 : INFO : Vocabulary size: 24 500\n",
      "2019-06-17 11:00:17,170 : INFO : built Dictionary(35 unique tokens: ['abu', 'airport', 'apply', 'architect', 'arrival']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:17,173 : INFO : 0.0\n",
      "2019-06-17 11:00:17,194 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,177 : INFO : WCD\n",
      "2019-06-17 11:00:17,203 : INFO : 0.0\n",
      "2019-06-17 11:00:17,212 : INFO : [(-20.11184310913086, 6), (-19.872629165649414, 8), (-18.20384407043457, 3), (-19.25678062438965, 9), (-19.32880401611328, 7), (-0.0, 0), (-18.057579040527344, 2), (-17.955053329467773, 5), (-18.148595809936523, 4), (-17.65687370300293, 1)]\n",
      "2019-06-17 11:00:17,217 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,222 : INFO : 0.0\n",
      "2019-06-17 11:00:17,230 : INFO : P&P\n",
      "2019-06-17 11:00:17,232 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,242 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,227 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:17,258 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,263 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,274 : INFO : built Dictionary(52 unique tokens: ['advice', 'al', 'also', 'anyone', 'checked']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:17,271 : INFO : [(-19.990562438964844, 5), (-19.76280403137207, 8), (-18.714017868041992, 7), (-18.305086135864258, 3), (-18.21134376525879, 9), (-16.144176483154297, 4), (-14.203519821166992, 1), (-16.05072021484375, 0), (-17.27656364440918, 6), (-16.997817993164062, 2)]\n",
      "2019-06-17 11:00:17,313 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,312 : INFO : 0.1\n",
      "2019-06-17 11:00:17,326 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:17,341 : INFO : P&P\n",
      "2019-06-17 11:00:17,353 : INFO : Vocabulary size: 3 500\n",
      "2019-06-17 11:00:17,343 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,369 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,376 : INFO : built Dictionary(40 unique tokens: ['aravind', 'doha', 'experience', 'hi', 'job']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:17,360 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:17,377 : INFO : WCD\n",
      "2019-06-17 11:00:17,393 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:17,378 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,402 : INFO : 0.0\n",
      "2019-06-17 11:00:17,416 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,404 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,385 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,435 : INFO : built Dictionary(45 unique tokens: ['abroad', 'anyone', 'bank', 'blamed', 'brazilian']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:17,441 : INFO : built Dictionary(57 unique tokens: ['appreciate', 'arabic', 'born', 'comments', 'dubai']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:17,434 : INFO : [(-21.66366195678711, 3), (-21.585311889648438, 2), (-20.517797470092773, 5), (-20.95993995666504, 6), (-21.518665313720703, 9), (-19.70362663269043, 7), (-18.033660888671875, 0), (-19.783864974975586, 1), (-20.32853126525879, 4), (-19.783138275146484, 8)]\n",
      "2019-06-17 11:00:17,456 : INFO : 0.0\n",
      "2019-06-17 11:00:17,490 : INFO : P&P\n",
      "2019-06-17 11:00:17,506 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,546 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,507 : INFO : Vocabulary size: 21 500\n",
      "2019-06-17 11:00:17,565 : INFO : WCD\n",
      "2019-06-17 11:00:17,579 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,569 : INFO : 0.0\n",
      "2019-06-17 11:00:17,601 : INFO : built Dictionary(52 unique tokens: ['along', 'appointement', 'company', 'congratulated', 'contacts']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:17,579 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:17,603 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,611 : INFO : WCD\n",
      "2019-06-17 11:00:17,617 : INFO : 0.0\n",
      "2019-06-17 11:00:17,629 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,669 : INFO : [(-20.792640686035156, 9), (-20.70882797241211, 0), (-19.472810745239258, 4), (-19.24982452392578, 1), (-20.015117645263672, 7), (-19.24006462097168, 5), (-19.447006225585938, 8), (-17.57625961303711, 2), (-19.106557846069336, 3), (-19.90739631652832, 6)]\n",
      "2019-06-17 11:00:17,665 : INFO : built Dictionary(67 unique tokens: ['already', 'arabic', 'chef', 'doha', 'end']...) from 2 documents (total 75 corpus positions)\n",
      "2019-06-17 11:00:17,681 : INFO : 0.1\n",
      "2019-06-17 11:00:17,695 : INFO : P&P\n",
      "2019-06-17 11:00:17,698 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,714 : INFO : [(-21.044832229614258, 6), (-21.012939453125, 8), (-20.935142517089844, 9), (-20.39023780822754, 4), (-20.131868362426758, 7), (-20.312881469726562, 1), (-20.340091705322266, 3), (-20.13840675354004, 2), (-18.799840927124023, 0), (-19.298742294311523, 5)]\n",
      "2019-06-17 11:00:17,758 : INFO : 0.1\n",
      "2019-06-17 11:00:17,775 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,780 : INFO : P&P\n",
      "2019-06-17 11:00:17,812 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:17,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:17,776 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:17,822 : INFO : built Dictionary(58 unique tokens: ['advice', 'also', 'anyone', 'appreciated', 'arrive']...) from 2 documents (total 71 corpus positions)\n",
      "2019-06-17 11:00:17,762 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,819 : INFO : WCD\n",
      "2019-06-17 11:00:17,832 : INFO : 0.0\n",
      "2019-06-17 11:00:17,858 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,898 : INFO : Vocabulary size: 23 500\n",
      "2019-06-17 11:00:17,883 : INFO : [(-19.59184455871582, 1), (-19.18979263305664, 3), (-19.05352783203125, 5), (-18.789079666137695, 2), (-17.056068420410156, 8), (-17.228107452392578, 4), (-16.887685775756836, 0), (-17.025480270385742, 6), (-14.934661865234375, 7), (-16.47694206237793, 9)]\n",
      "2019-06-17 11:00:17,914 : INFO : WCD\n",
      "2019-06-17 11:00:17,917 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:17,919 : INFO : 0.1\n",
      "2019-06-17 11:00:17,935 : INFO : 0.0\n",
      "2019-06-17 11:00:17,944 : INFO : First K WMD\n",
      "2019-06-17 11:00:17,949 : INFO : P&P\n",
      "2019-06-17 11:00:17,976 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,002 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:18,011 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:18,032 : INFO : [(-21.09128189086914, 8), (-19.800609588623047, 3), (-19.8898983001709, 7), (-19.483251571655273, 1), (-19.76327896118164, 4), (-19.22492027282715, 6), (-19.699356079101562, 2), (-0.0, 0), (-18.392732620239258, 9), (-18.335771560668945, 5)]\n",
      "2019-06-17 11:00:18,037 : INFO : built Dictionary(48 unique tokens: ['agencies', 'agent', 'better', 'doha', 'hi']...) from 2 documents (total 57 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:18,065 : INFO : 0.1\n",
      "2019-06-17 11:00:18,102 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:18,111 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,147 : INFO : P&P\n",
      "2019-06-17 11:00:18,150 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:18,169 : INFO : built Dictionary(63 unique tokens: ['accepted', 'advice', 'anyone', 'appreciated', 'april']...) from 2 documents (total 77 corpus positions)\n",
      "2019-06-17 11:00:18,174 : INFO : WCD\n",
      "2019-06-17 11:00:18,182 : INFO : 0.0\n",
      "2019-06-17 11:00:18,221 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,204 : INFO : First K WMD\n",
      "2019-06-17 11:00:18,268 : INFO : [(-20.066619873046875, 4), (-19.894027709960938, 5), (-19.66065788269043, 2), (-19.0922908782959, 1), (-19.50944709777832, 7), (-18.448623657226562, 3), (-14.045564651489258, 0), (-18.474266052246094, 8), (-18.948467254638672, 9), (-18.842411041259766, 6)]\n",
      "2019-06-17 11:00:18,260 : INFO : Vocabulary size: 29 500\n",
      "2019-06-17 11:00:18,279 : INFO : WCD\n",
      "2019-06-17 11:00:18,281 : INFO : 0.0\n",
      "2019-06-17 11:00:18,290 : INFO : P&P\n",
      "2019-06-17 11:00:18,288 : INFO : 0.0\n",
      "2019-06-17 11:00:18,299 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,315 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,347 : INFO : First K WMD\n",
      "2019-06-17 11:00:18,404 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,410 : INFO : [(-20.723052978515625, 6), (-19.34109878540039, 8), (-20.61148452758789, 5), (-18.90216636657715, 7), (-16.943523406982422, 4), (-17.999187469482422, 2), (-17.958593368530273, 9), (-18.163028717041016, 3), (-18.593242645263672, 1), (-0.0, 0)]\n",
      "2019-06-17 11:00:18,426 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:18,431 : INFO : 0.1\n",
      "2019-06-17 11:00:18,446 : INFO : P&P\n",
      "2019-06-17 11:00:18,445 : INFO : WCD\n",
      "2019-06-17 11:00:18,465 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,458 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,468 : INFO : 0.0\n",
      "2019-06-17 11:00:18,506 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,501 : INFO : First K WMD\n",
      "2019-06-17 11:00:18,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:18,548 : INFO : [(-19.778383255004883, 3), (-19.669401168823242, 6), (-19.37554168701172, 1), (-19.62013816833496, 5), (-19.08390235900879, 8), (-17.030838012695312, 2), (-17.24604034423828, 7), (-18.489704132080078, 9), (-17.38992691040039, 0), (-18.608549118041992, 4)]\n",
      "2019-06-17 11:00:18,567 : INFO : 0.0\n",
      "2019-06-17 11:00:18,580 : INFO : P&P\n",
      "2019-06-17 11:00:18,552 : INFO : built Dictionary(57 unique tokens: ['ad', 'amount', 'anyone', 'data', 'entry']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:18,633 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,547 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:18,646 : INFO : WCD\n",
      "2019-06-17 11:00:18,663 : INFO : 0.0\n",
      "2019-06-17 11:00:18,677 : INFO : First K WMD\n",
      "2019-06-17 11:00:18,603 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,703 : INFO : [(-19.669340133666992, 4), (-19.222414016723633, 7), (-19.051572799682617, 3), (-19.058101654052734, 2), (-18.74559211730957, 6), (-18.956188201904297, 8), (-17.73834800720215, 9), (-14.42180061340332, 0), (-18.00208282470703, 1), (-18.716665267944336, 5)]\n",
      "2019-06-17 11:00:18,695 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,718 : INFO : 0.0\n",
      "2019-06-17 11:00:18,754 : INFO : P&P\n",
      "2019-06-17 11:00:18,785 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,776 : INFO : Vocabulary size: 5 500\n",
      "2019-06-17 11:00:18,810 : INFO : WCD\n",
      "2019-06-17 11:00:18,814 : INFO : 0.0\n",
      "2019-06-17 11:00:18,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:18,826 : INFO : built Dictionary(57 unique tokens: ['agency', 'application', 'apply', 'applying', 'bayt']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:18,828 : INFO : First K WMD\n",
      "2019-06-17 11:00:18,837 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,853 : INFO : [(-21.06051254272461, 9), (-19.226543426513672, 8), (-18.44504165649414, 7), (-18.96554946899414, 5), (-18.861547470092773, 2), (-17.53408432006836, 0), (-15.307819366455078, 3), (-17.408960342407227, 1), (-18.556110382080078, 4), (-16.743637084960938, 6)]\n",
      "2019-06-17 11:00:18,861 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:18,874 : INFO : 0.0\n",
      "2019-06-17 11:00:18,895 : INFO : P&P\n",
      "2019-06-17 11:00:18,914 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:18,920 : INFO : built Dictionary(46 unique tokens: ['account', 'advice', 'already', 'anyone', 'applying']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:18,909 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:18,930 : INFO : WCD\n",
      "2019-06-17 11:00:18,968 : INFO : 0.0\n",
      "2019-06-17 11:00:18,937 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:18,946 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:18,989 : INFO : Vocabulary size: 20 500\n",
      "2019-06-17 11:00:18,976 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,019 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,035 : INFO : [(-22.022193908691406, 7), (-20.422021865844727, 8), (-18.208757400512695, 6), (-17.778650283813477, 5), (-19.247949600219727, 9), (-18.15403175354004, 3), (-14.78189468383789, 1), (-0.0, 0), (-16.134225845336914, 4), (-16.84610366821289, 2)]\n",
      "2019-06-17 11:00:19,005 : INFO : WCD\n",
      "2019-06-17 11:00:19,048 : INFO : 0.0\n",
      "2019-06-17 11:00:19,045 : INFO : built Dictionary(34 unique tokens: ['advise', 'application', 'bank', 'consecutive', 'documents']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:19,046 : INFO : 0.0\n",
      "2019-06-17 11:00:19,076 : INFO : P&P\n",
      "2019-06-17 11:00:19,066 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,097 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,087 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,156 : INFO : built Dictionary(40 unique tokens: ['application', 'applied', 'b', 'certificate', 'counter']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:19,135 : INFO : [(-20.61992645263672, 9), (-17.73233985900879, 3), (-19.079103469848633, 6), (-16.099700927734375, 2), (-15.8208646774292, 4), (-18.733854293823242, 7), (-15.518646240234375, 0), (-15.60511302947998, 5), (-16.02710723876953, 8), (-15.451652526855469, 1)]\n",
      "2019-06-17 11:00:19,176 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:19,197 : INFO : 0.1\n",
      "2019-06-17 11:00:19,207 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,204 : INFO : WCD\n",
      "2019-06-17 11:00:19,209 : INFO : P&P\n",
      "2019-06-17 11:00:19,235 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,234 : INFO : 0.0\n",
      "2019-06-17 11:00:19,214 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,245 : INFO : built Dictionary(41 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:19,256 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,294 : INFO : [(-21.622915267944336, 7), (-19.682754516601562, 6), (-20.263898849487305, 2), (-19.36916160583496, 1), (-19.36738395690918, 3), (-20.001909255981445, 8), (-18.81780433654785, 5), (-19.211259841918945, 4), (-19.300949096679688, 9), (-17.754709243774414, 0)]\n",
      "2019-06-17 11:00:19,316 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:19,320 : INFO : 0.0\n",
      "2019-06-17 11:00:19,322 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,336 : INFO : P&P\n",
      "2019-06-17 11:00:19,354 : INFO : WCD\n",
      "2019-06-17 11:00:19,348 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,361 : INFO : 0.0\n",
      "2019-06-17 11:00:19,383 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,348 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:19,388 : INFO : built Dictionary(34 unique tokens: ['applied', 'applying', 'ask', 'certificate', 'change']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:19,405 : INFO : [(-22.74655532836914, 8), (-22.712932586669922, 9), (-22.07628059387207, 5), (-21.98308563232422, 2), (-19.768983840942383, 4), (-21.54038429260254, 6), (-21.95960235595703, 0), (-18.395387649536133, 3), (-21.559354782104492, 7), (-19.146608352661133, 1)]\n",
      "2019-06-17 11:00:19,390 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,407 : INFO : 0.0\n",
      "2019-06-17 11:00:19,427 : INFO : Vocabulary size: 27 500\n",
      "2019-06-17 11:00:19,432 : INFO : P&P\n",
      "2019-06-17 11:00:19,439 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,440 : INFO : WCD\n",
      "2019-06-17 11:00:19,457 : INFO : 0.0\n",
      "2019-06-17 11:00:19,462 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,473 : INFO : built Dictionary(37 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:19,513 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,518 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:19,535 : INFO : [(-21.225101470947266, 1), (-20.257780075073242, 3), (-19.359134674072266, 2), (-18.866626739501953, 6), (-18.411348342895508, 9), (-19.22258758544922, 8), (-18.70754623413086, 4), (-18.66252899169922, 7), (-18.42574691772461, 0), (-17.901491165161133, 5)]\n",
      "2019-06-17 11:00:19,530 : INFO : WCD\n",
      "2019-06-17 11:00:19,563 : INFO : 0.0\n",
      "2019-06-17 11:00:19,545 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,559 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,572 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,567 : INFO : built Dictionary(25 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:19,553 : INFO : 0.1\n",
      "2019-06-17 11:00:19,600 : INFO : [(-21.82332992553711, 8), (-21.059734344482422, 2), (-20.882959365844727, 9), (-20.135671615600586, 1), (-18.970979690551758, 4), (-19.042600631713867, 5), (-17.663551330566406, 0), (-19.046167373657227, 6), (-17.20755958557129, 7), (-18.62795639038086, 3)]\n",
      "2019-06-17 11:00:19,607 : INFO : 0.0\n",
      "2019-06-17 11:00:19,613 : INFO : P&P\n",
      "2019-06-17 11:00:19,606 : INFO : P&P\n",
      "2019-06-17 11:00:19,629 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,622 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,632 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:19,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,661 : INFO : built Dictionary(35 unique tokens: ['answer', 'apply', 'applying', 'arrive', 'asap']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:19,695 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,710 : INFO : Vocabulary size: 24 500\n",
      "2019-06-17 11:00:19,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,716 : INFO : WCD\n",
      "2019-06-17 11:00:19,701 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:19,724 : INFO : built Dictionary(43 unique tokens: ['africa', 'asia', 'believes', 'completely', 'countries']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:19,720 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:19,733 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,729 : INFO : 0.0\n",
      "2019-06-17 11:00:19,730 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,739 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,763 : INFO : built Dictionary(51 unique tokens: ['accept', 'accepted', 'advance', 'advise', 'application']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:19,756 : INFO : WCD\n",
      "2019-06-17 11:00:19,793 : INFO : 0.0\n",
      "2019-06-17 11:00:19,818 : INFO : [(-20.964574813842773, 1), (-19.85633659362793, 9), (-19.008264541625977, 6), (-18.324031829833984, 3), (-19.506528854370117, 7), (-18.817325592041016, 8), (-18.594560623168945, 5), (-0.0, 0), (-18.11375617980957, 4), (-19.20870018005371, 2)]\n",
      "2019-06-17 11:00:19,816 : INFO : First K WMD\n",
      "2019-06-17 11:00:19,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,845 : INFO : built Dictionary(22 unique tokens: ['bads', 'become', 'common', 'could', 'details']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:19,833 : INFO : 0.1\n",
      "2019-06-17 11:00:19,857 : INFO : P&P\n",
      "2019-06-17 11:00:19,847 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,870 : INFO : [(-20.884735107421875, 9), (-20.2757511138916, 8), (-19.506959915161133, 6), (-19.68661880493164, 5), (-18.931859970092773, 4), (-17.352336883544922, 1), (-18.628686904907227, 7), (-16.856956481933594, 0), (-17.290552139282227, 2), (-15.988212585449219, 3)]\n",
      "2019-06-17 11:00:19,869 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,886 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:19,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,887 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:19,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:19,889 : INFO : 0.1\n",
      "2019-06-17 11:00:19,928 : INFO : built Dictionary(33 unique tokens: ['common', 'country', 'guys', 'lot', 'need']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:19,916 : INFO : built Dictionary(27 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:19,927 : INFO : P&P\n",
      "2019-06-17 11:00:19,945 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:19,957 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:19,995 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,024 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,016 : INFO : Vocabulary size: 5 500\n",
      "2019-06-17 11:00:20,021 : INFO : built Dictionary(28 unique tokens: ['boy', 'dark', 'fare', 'girl', 'married']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:20,060 : INFO : WCD\n",
      "2019-06-17 11:00:20,059 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,083 : INFO : 0.0\n",
      "2019-06-17 11:00:20,087 : INFO : built Dictionary(38 unique tokens: ['confusion', 'enjoy', 'first', 'fourth', 'interviews']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:20,106 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,113 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,137 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:20,138 : INFO : [(-20.12354278564453, 4), (-19.330928802490234, 3), (-19.68625259399414, 9), (-19.320236206054688, 5), (-18.498048782348633, 1), (-18.70110321044922, 6), (-18.4248046875, 0), (-19.288490295410156, 7), (-18.523818969726562, 2), (-18.072237014770508, 8)]\n",
      "2019-06-17 11:00:20,148 : INFO : WCD\n",
      "2019-06-17 11:00:20,157 : INFO : 0.0\n",
      "2019-06-17 11:00:20,182 : INFO : 0.0\n",
      "2019-06-17 11:00:20,182 : INFO : P&P\n",
      "2019-06-17 11:00:20,184 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,201 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:20,188 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,232 : INFO : built Dictionary(31 unique tokens: ['beatiful', 'beautiful', 'egyptians', 'girls', 'heard']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:20,264 : INFO : [(-20.386171340942383, 9), (-18.01150894165039, 7), (-16.592327117919922, 8), (-16.407611846923828, 6), (-17.911773681640625, 1), (-12.767983436584473, 2), (-15.204957962036133, 0), (-13.164169311523438, 4), (-14.287120819091797, 5), (-15.469461441040039, 3)]\n",
      "2019-06-17 11:00:20,295 : INFO : Vocabulary size: 23 500\n",
      "2019-06-17 11:00:20,284 : INFO : 0.1\n",
      "2019-06-17 11:00:20,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:20,305 : INFO : WCD\n",
      "2019-06-17 11:00:20,311 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,310 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,306 : INFO : P&P\n",
      "2019-06-17 11:00:20,319 : INFO : 0.0\n",
      "2019-06-17 11:00:20,334 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,327 : INFO : built Dictionary(36 unique tokens: ['around', 'closed', 'expired', 'garvey', 'got']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:20,330 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:20,361 : INFO : [(-19.928001403808594, 4), (-19.701377868652344, 9), (-19.740556716918945, 5), (-19.489063262939453, 8), (-14.717412948608398, 3), (-19.454524993896484, 1), (-18.943920135498047, 7), (-18.111894607543945, 6), (-17.66033935546875, 2), (-0.0, 0)]\n",
      "2019-06-17 11:00:20,369 : INFO : 0.0\n",
      "2019-06-17 11:00:20,379 : INFO : P&P\n",
      "2019-06-17 11:00:20,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,384 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:20,406 : INFO : built Dictionary(32 unique tokens: ['accepted', 'already', 'church', 'cousin', 'like']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:20,431 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,396 : INFO : Vocabulary size: 10 500\n",
      "2019-06-17 11:00:20,469 : INFO : WCD\n",
      "2019-06-17 11:00:20,486 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,488 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,522 : INFO : 0.0\n",
      "2019-06-17 11:00:20,554 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,581 : INFO : [(-20.013511657714844, 7), (-19.239303588867188, 1), (-18.320966720581055, 5), (-18.22110939025879, 4), (-18.58061408996582, 0), (-16.569042205810547, 8), (-18.305482864379883, 9), (-17.526578903198242, 2), (-17.708402633666992, 6), (-15.194838523864746, 3)]\n",
      "2019-06-17 11:00:20,589 : INFO : 0.0\n",
      "2019-06-17 11:00:20,445 : INFO : built Dictionary(30 unique tokens: ['comments', 'erotic', 'exotic', 'fill', 'indian']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:20,601 : INFO : P&P\n",
      "2019-06-17 11:00:20,614 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:20,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,631 : INFO : built Dictionary(49 unique tokens: ['born', 'care', 'concerned', 'countries', 'doesnt']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:20,609 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,662 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:20,675 : INFO : WCD\n",
      "2019-06-17 11:00:20,697 : INFO : 0.0\n",
      "2019-06-17 11:00:20,706 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,731 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,767 : INFO : [(-18.810142517089844, 5), (-18.789100646972656, 4), (-16.503429412841797, 7), (-16.736772537231445, 9), (-16.01210594177246, 8), (-16.422739028930664, 3), (-16.188203811645508, 1), (-16.56192970275879, 2), (-16.532358169555664, 6), (-0.0, 0)]\n",
      "2019-06-17 11:00:20,790 : INFO : 0.0\n",
      "2019-06-17 11:00:20,806 : INFO : P&P\n",
      "2019-06-17 11:00:20,808 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:20,798 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:20,804 : INFO : Vocabulary size: 10 500\n",
      "2019-06-17 11:00:20,822 : INFO : WCD\n",
      "2019-06-17 11:00:20,843 : INFO : 0.0\n",
      "2019-06-17 11:00:20,846 : INFO : First K WMD\n",
      "2019-06-17 11:00:20,854 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,859 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,934 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:20,926 : INFO : built Dictionary(39 unique tokens: ['also', 'back', 'breaking', 'cons', 'expected']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:20,967 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:20,953 : INFO : WCD\n",
      "2019-06-17 11:00:20,980 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:20,978 : INFO : 0.0\n",
      "2019-06-17 11:00:20,991 : INFO : built Dictionary(41 unique tokens: ['advise', 'appreciated', 'asking', 'certificate', 'conditions']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:20,916 : INFO : [(-21.67673110961914, 5), (-21.479244232177734, 9), (-21.578922271728516, 1), (-21.346134185791016, 3), (-20.572608947753906, 0), (-21.335363388061523, 7), (-20.747163772583008, 4), (-19.958662033081055, 6), (-21.19645118713379, 2), (-19.6876163482666, 8)]\n",
      "2019-06-17 11:00:20,997 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,028 : INFO : [(-20.87758445739746, 7), (-19.972414016723633, 1), (-19.854808807373047, 8), (-18.431907653808594, 4), (-18.72998809814453, 9), (-14.937641143798828, 6), (-18.18296241760254, 3), (-0.0, 0), (-14.82392406463623, 5), (-16.290706634521484, 2)]\n",
      "2019-06-17 11:00:21,014 : INFO : 0.1\n",
      "2019-06-17 11:00:21,041 : INFO : 0.0\n",
      "2019-06-17 11:00:21,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,044 : INFO : P&P\n",
      "2019-06-17 11:00:21,066 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,059 : INFO : built Dictionary(17 unique tokens: ['cancel', 'cancellation', 'even', 'guys', 'passport']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:00:21,058 : INFO : P&P\n",
      "2019-06-17 11:00:21,077 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,082 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,118 : INFO : built Dictionary(38 unique tokens: ['care', 'days', 'delivery', 'expecting', 'hired']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:21,121 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,227 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,265 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:21,264 : INFO : built Dictionary(24 unique tokens: ['agency', 'bring', 'ethiopia', 'help', 'hiring']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:21,274 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,270 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,280 : INFO : WCD\n",
      "2019-06-17 11:00:21,271 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:21,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,294 : INFO : 0.0\n",
      "2019-06-17 11:00:21,292 : INFO : WCD\n",
      "2019-06-17 11:00:21,305 : INFO : built Dictionary(35 unique tokens: ['able', 'advance', 'anyone', 'approval', 'b']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:21,313 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,319 : INFO : 0.0\n",
      "2019-06-17 11:00:21,333 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,377 : INFO : [(-19.667552947998047, 2), (-19.66010284423828, 6), (-18.77987289428711, 1), (-18.70496368408203, 8), (-19.54026222229004, 3), (-18.396299362182617, 9), (-17.53272247314453, 4), (-18.696979522705078, 7), (-17.450464248657227, 5), (-19.26309585571289, 0)]\n",
      "2019-06-17 11:00:21,385 : INFO : [(-19.03515625, 2), (-17.79197120666504, 9), (-17.12276840209961, 5), (-17.395830154418945, 8), (-16.71596908569336, 4), (-0.0, 0), (-16.52887725830078, 6), (-16.414045333862305, 1), (-15.892051696777344, 3), (-16.633621215820312, 7)]\n",
      "2019-06-17 11:00:21,390 : INFO : 0.1\n",
      "2019-06-17 11:00:21,413 : INFO : P&P\n",
      "2019-06-17 11:00:21,420 : INFO : 0.1\n",
      "2019-06-17 11:00:21,425 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,439 : INFO : P&P\n",
      "2019-06-17 11:00:21,455 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,440 : INFO : built Dictionary(34 unique tokens: ['advice', 'anyone', 'could', 'currently', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:21,450 : INFO : stopped by early_stop condition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:21,472 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:21,475 : INFO : WCD\n",
      "2019-06-17 11:00:21,478 : INFO : 0.0\n",
      "2019-06-17 11:00:21,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,494 : INFO : built Dictionary(45 unique tokens: ['able', 'accommodation', 'apartment', 'considering', 'cost']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:21,481 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,523 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,536 : INFO : [(-18.4628963470459, 1), (-16.511249542236328, 4), (-18.106136322021484, 8), (-15.929932594299316, 7), (-15.801773071289062, 5), (-12.384505271911621, 0), (-16.102802276611328, 2), (-15.169452667236328, 3), (-13.54975700378418, 9), (-15.740694999694824, 6)]\n",
      "2019-06-17 11:00:21,557 : INFO : 0.0\n",
      "2019-06-17 11:00:21,550 : INFO : Vocabulary size: 24 500\n",
      "2019-06-17 11:00:21,565 : INFO : P&P\n",
      "2019-06-17 11:00:21,573 : INFO : WCD\n",
      "2019-06-17 11:00:21,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,598 : INFO : 0.0\n",
      "2019-06-17 11:00:21,583 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,629 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,608 : INFO : built Dictionary(40 unique tokens: ['baby', 'begining', 'cannot', 'case', 'change']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:21,671 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,688 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,711 : INFO : [(-19.250457763671875, 3), (-18.85906410217285, 4), (-18.769018173217773, 8), (-17.918420791625977, 2), (-18.744565963745117, 9), (-18.29644203186035, 7), (-17.58534049987793, 5), (-17.727991104125977, 1), (-0.0, 0), (-17.380163192749023, 6)]\n",
      "2019-06-17 11:00:21,727 : INFO : 0.1\n",
      "2019-06-17 11:00:21,714 : INFO : built Dictionary(30 unique tokens: ['alternative', 'ban', 'change', 'employee', 'employer']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:21,705 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,744 : INFO : P&P\n",
      "2019-06-17 11:00:21,734 : INFO : built Dictionary(51 unique tokens: ['allowances', 'better', 'dear', 'engineer', 'experience']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:00:21,722 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:21,730 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,751 : INFO : WCD\n",
      "2019-06-17 11:00:21,763 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,755 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:21,772 : INFO : 0.0\n",
      "2019-06-17 11:00:21,802 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,861 : INFO : [(-21.039222717285156, 8), (-20.15747833251953, 9), (-17.522485733032227, 1), (-16.2148380279541, 0), (-19.940343856811523, 7), (-17.510330200195312, 2), (-13.029369354248047, 5), (-13.951584815979004, 4), (-13.90832805633545, 3), (-16.406095504760742, 6)]\n",
      "2019-06-17 11:00:21,852 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:21,875 : INFO : WCD\n",
      "2019-06-17 11:00:21,882 : INFO : 0.0\n",
      "2019-06-17 11:00:21,899 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,863 : INFO : 0.0\n",
      "2019-06-17 11:00:21,892 : INFO : First K WMD\n",
      "2019-06-17 11:00:21,911 : INFO : P&P\n",
      "2019-06-17 11:00:21,912 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:21,935 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:21,945 : INFO : built Dictionary(35 unique tokens: ['abt', 'accept', 'accommodation', 'air', 'company']...) from 2 documents (total 80 corpus positions)\n",
      "2019-06-17 11:00:21,942 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:21,968 : INFO : [(-21.430776596069336, 7), (-20.564456939697266, 8), (-20.104063034057617, 6), (-20.141925811767578, 9), (-16.082399368286133, 3), (-19.606534957885742, 5), (-16.55998992919922, 0), (-18.965049743652344, 4), (-16.65601921081543, 1), (-15.463759422302246, 2)]\n",
      "2019-06-17 11:00:22,036 : INFO : 0.1\n",
      "2019-06-17 11:00:22,034 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,048 : INFO : Vocabulary size: 20 500\n",
      "2019-06-17 11:00:22,086 : INFO : P&P\n",
      "2019-06-17 11:00:22,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,097 : INFO : WCD\n",
      "2019-06-17 11:00:22,116 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,121 : INFO : 0.0\n",
      "2019-06-17 11:00:22,116 : INFO : built Dictionary(41 unique tokens: ['approx', 'engineer', 'experience', 'good', 'hi']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:22,134 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,141 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,143 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,188 : INFO : [(-19.72854995727539, 3), (-19.402931213378906, 7), (-19.092586517333984, 5), (-19.142333984375, 8), (-18.676002502441406, 0), (-18.122026443481445, 9), (-19.085140228271484, 6), (-16.499950408935547, 4), (-17.093196868896484, 2), (-17.905433654785156, 1)]\n",
      "2019-06-17 11:00:22,204 : INFO : 0.1\n",
      "2019-06-17 11:00:22,184 : INFO : Vocabulary size: 31 500\n",
      "2019-06-17 11:00:22,195 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,224 : INFO : P&P\n",
      "2019-06-17 11:00:22,222 : INFO : WCD\n",
      "2019-06-17 11:00:22,224 : INFO : built Dictionary(63 unique tokens: ['accomodation', 'advance', 'allowance', 'australia', 'basic']...) from 2 documents (total 82 corpus positions)\n",
      "2019-06-17 11:00:22,236 : INFO : 0.0\n",
      "2019-06-17 11:00:22,230 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,242 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,290 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,303 : INFO : [(-20.056838989257812, 6), (-19.226882934570312, 1), (-18.571245193481445, 9), (-16.567798614501953, 5), (-16.235095977783203, 7), (-17.919570922851562, 3), (-15.500161170959473, 8), (-16.03974723815918, 0), (-0.0, 4), (-16.14720916748047, 2)]\n",
      "2019-06-17 11:00:22,301 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:22,325 : INFO : 0.1\n",
      "2019-06-17 11:00:22,321 : INFO : WCD\n",
      "2019-06-17 11:00:22,343 : INFO : 0.0\n",
      "2019-06-17 11:00:22,341 : INFO : P&P\n",
      "2019-06-17 11:00:22,358 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,367 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,378 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,355 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,417 : INFO : built Dictionary(33 unique tokens: ['becuase', 'capable', 'children', 'company', 'current']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:22,422 : INFO : Vocabulary size: 32 500\n",
      "2019-06-17 11:00:22,445 : INFO : WCD\n",
      "2019-06-17 11:00:22,452 : INFO : 0.0\n",
      "2019-06-17 11:00:22,461 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,448 : INFO : [(-21.482257843017578, 8), (-19.855825424194336, 7), (-19.809064865112305, 9), (-17.341535568237305, 3), (-19.41337776184082, 6), (-17.812976837158203, 5), (-16.061477661132812, 1), (-14.578228950500488, 0), (-11.360544204711914, 2), (-19.014596939086914, 4)]\n",
      "2019-06-17 11:00:22,448 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,472 : INFO : built Dictionary(36 unique tokens: ['also', 'around', 'asked', 'company', 'current']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:22,471 : INFO : 0.1\n",
      "2019-06-17 11:00:22,489 : INFO : P&P\n",
      "2019-06-17 11:00:22,486 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,493 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,495 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,497 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,536 : INFO : built Dictionary(15 unique tokens: ['change', 'family', 'husband', 'new', 'sponsor']...) from 2 documents (total 29 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:22,535 : INFO : built Dictionary(61 unique tokens: ['airport', 'alcohol', 'apartment', 'cafeteria', 'close']...) from 2 documents (total 76 corpus positions)\n",
      "2019-06-17 11:00:22,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,521 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,607 : INFO : [(-22.983196258544922, 2), (-20.509382247924805, 4), (-20.981613159179688, 6), (-20.342670440673828, 7), (-19.96017074584961, 9), (-19.425537109375, 5), (-20.297460556030273, 0), (-19.357925415039062, 8), (-19.886138916015625, 3), (-19.932920455932617, 1)]\n",
      "2019-06-17 11:00:22,617 : INFO : built Dictionary(16 unique tokens: ['advise', 'change', 'company', 'even', 'husband']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:00:22,616 : INFO : 0.1\n",
      "2019-06-17 11:00:22,654 : INFO : P&P\n",
      "2019-06-17 11:00:22,665 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,682 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,703 : INFO : built Dictionary(18 unique tokens: ['away', 'exit', 'get', 'go', 'going']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:22,733 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,729 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:22,738 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,752 : INFO : built Dictionary(24 unique tokens: ['anyone', 'changing', 'could', 'guide', 'husband']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:22,754 : INFO : WCD\n",
      "2019-06-17 11:00:22,779 : INFO : 0.0\n",
      "2019-06-17 11:00:22,784 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,798 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,801 : INFO : built Dictionary(25 unique tokens: ['advise', 'anyone', 'appreciated', 'change', 'changing']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:22,773 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:22,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,816 : INFO : WCD\n",
      "2019-06-17 11:00:22,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,829 : INFO : built Dictionary(29 unique tokens: ['advance', 'confused', 'dates', 'dear', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:22,828 : INFO : [(-22.046415328979492, 9), (-20.95157241821289, 1), (-20.582632064819336, 0), (-20.551929473876953, 6), (-19.506080627441406, 3), (-20.08598518371582, 7), (-19.04819107055664, 5), (-15.837325096130371, 4), (-17.99116325378418, 8), (-19.36410903930664, 2)]\n",
      "2019-06-17 11:00:22,837 : INFO : built Dictionary(58 unique tokens: ['accomodation', 'advice', 'air', 'back', 'choice']...) from 2 documents (total 82 corpus positions)\n",
      "2019-06-17 11:00:22,840 : INFO : 0.0\n",
      "2019-06-17 11:00:22,854 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,856 : INFO : 0.0\n",
      "2019-06-17 11:00:22,865 : INFO : built Dictionary(29 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:22,854 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,869 : INFO : First K WMD\n",
      "2019-06-17 11:00:22,873 : INFO : P&P\n",
      "2019-06-17 11:00:22,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:22,883 : INFO : [(-21.64759063720703, 8), (-20.698009490966797, 6), (-20.19049644470215, 3), (-20.217205047607422, 7), (-20.401790618896484, 2), (-20.058513641357422, 0), (-19.606748580932617, 1), (-19.6198787689209, 9), (-19.903806686401367, 5), (-20.314977645874023, 4)]\n",
      "2019-06-17 11:00:22,902 : INFO : built Dictionary(30 unique tokens: ['advise', 'already', 'anyone', 'appreciate', 'company']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:22,897 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:22,900 : INFO : 0.0\n",
      "2019-06-17 11:00:22,932 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:22,951 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:22,939 : INFO : P&P\n",
      "2019-06-17 11:00:22,970 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:23,010 : INFO : Vocabulary size: 10 500\n",
      "2019-06-17 11:00:23,040 : INFO : WCD\n",
      "2019-06-17 11:00:23,035 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,059 : INFO : 0.0\n",
      "2019-06-17 11:00:23,074 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,096 : INFO : Vocabulary size: 32 500\n",
      "2019-06-17 11:00:23,095 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,068 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:23,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,133 : INFO : built Dictionary(44 unique tokens: ['accept', 'accomodation', 'air', 'basically', 'decent']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:23,126 : INFO : WCD\n",
      "2019-06-17 11:00:23,118 : INFO : [(-20.45170021057129, 6), (-19.22339630126953, 7), (-19.231740951538086, 1), (-18.897192001342773, 9), (-18.976207733154297, 8), (-15.086923599243164, 4), (-19.05516815185547, 2), (-18.404979705810547, 3), (-18.666227340698242, 5), (-17.626728057861328, 0)]\n",
      "2019-06-17 11:00:23,144 : INFO : 0.0\n",
      "2019-06-17 11:00:23,158 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,152 : INFO : 0.1\n",
      "2019-06-17 11:00:23,162 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,198 : INFO : P&P\n",
      "2019-06-17 11:00:23,221 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:23,232 : INFO : [(-20.24945068359375, 7), (-19.241369247436523, 4), (-18.42218589782715, 2), (-18.234771728515625, 9), (-18.983125686645508, 8), (-17.934825897216797, 3), (-17.387073516845703, 5), (-16.71825408935547, 1), (-0.0, 0), (-18.127037048339844, 6)]\n",
      "2019-06-17 11:00:23,247 : INFO : 0.1\n",
      "2019-06-17 11:00:23,220 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,256 : INFO : P&P\n",
      "2019-06-17 11:00:23,270 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:23,288 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,292 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:23,301 : INFO : built Dictionary(51 unique tokens: ['adjusment', 'allowance', 'bad', 'bank', 'basic']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:00:23,321 : INFO : WCD\n",
      "2019-06-17 11:00:23,328 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,343 : INFO : 0.0\n",
      "2019-06-17 11:00:23,357 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,362 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:23,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,383 : INFO : built Dictionary(45 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:23,474 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,468 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,495 : INFO : [(-19.615846633911133, 8), (-19.122394561767578, 1), (-19.068479537963867, 9), (-17.899232864379883, 3), (-17.1294002532959, 6), (-14.97187614440918, 0), (-15.362873077392578, 4), (-15.59569263458252, 2), (-16.632450103759766, 7), (-15.283309936523438, 5)]\n",
      "2019-06-17 11:00:23,481 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:23,522 : INFO : 0.1\n",
      "2019-06-17 11:00:23,534 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:23,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,541 : INFO : P&P\n",
      "2019-06-17 11:00:23,551 : INFO : built Dictionary(50 unique tokens: ['assume', 'bank', 'benifits', 'best', 'edge']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:23,550 : INFO : WCD\n",
      "2019-06-17 11:00:23,562 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:23,576 : INFO : 0.0\n",
      "2019-06-17 11:00:23,590 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,583 : INFO : creating matrix with 10 documents and 462807 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:23,637 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:23,662 : INFO : [(-20.357885360717773, 1), (-19.780426025390625, 7), (-19.14809799194336, 8), (-18.854463577270508, 9), (-19.412431716918945, 4), (-18.396207809448242, 6), (-0.0, 0), (-18.080116271972656, 5), (-14.599279403686523, 2), (-18.860294342041016, 3)]\n",
      "2019-06-17 11:00:23,679 : INFO : 0.1\n",
      "2019-06-17 11:00:23,685 : INFO : P&P\n",
      "2019-06-17 11:00:23,692 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:23,695 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:23,709 : INFO : WCD\n",
      "2019-06-17 11:00:23,717 : INFO : 0.0\n",
      "2019-06-17 11:00:23,714 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,736 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,780 : INFO : built Dictionary(38 unique tokens: ['best', 'craftsmanship', 'doha', 'kind', 'looking']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:23,791 : INFO : Vocabulary size: 21 500\n",
      "2019-06-17 11:00:23,832 : INFO : [(-19.677589416503906, 7), (-18.47458267211914, 3), (-17.61203384399414, 9), (-18.383403778076172, 8), (-16.384395599365234, 5), (-17.225461959838867, 6), (-13.274375915527344, 2), (-15.138846397399902, 4), (-17.881616592407227, 0), (-13.639961242675781, 1)]\n",
      "2019-06-17 11:00:23,824 : INFO : WCD\n",
      "2019-06-17 11:00:23,850 : INFO : 0.0\n",
      "2019-06-17 11:00:23,869 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:23,856 : INFO : 0.1\n",
      "2019-06-17 11:00:23,876 : INFO : First K WMD\n",
      "2019-06-17 11:00:23,920 : INFO : P&P\n",
      "2019-06-17 11:00:23,882 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,974 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:23,894 : INFO : built Dictionary(37 unique tokens: ['abaya', 'anyone', 'black', 'casual', 'cloaks']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:23,981 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,004 : INFO : [(-20.820114135742188, 7), (-20.19756317138672, 1), (-19.597253799438477, 3), (-19.890138626098633, 8), (-20.00615119934082, 6), (-19.59061622619629, 2), (-19.46162223815918, 4), (-18.416481018066406, 5), (-19.87225341796875, 9), (-0.0, 0)]\n",
      "2019-06-17 11:00:24,018 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,058 : INFO : built Dictionary(43 unique tokens: ['advance', 'blouse', 'churidar', 'dress', 'dresses']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:24,051 : INFO : 0.1\n",
      "2019-06-17 11:00:24,070 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:24,108 : INFO : WCD\n",
      "2019-06-17 11:00:24,119 : INFO : P&P\n",
      "2019-06-17 11:00:24,116 : INFO : 0.0\n",
      "2019-06-17 11:00:24,135 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,142 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,150 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,198 : INFO : [(-19.877304077148438, 8), (-19.085084915161133, 7), (-18.657848358154297, 3), (-19.042797088623047, 2), (-18.211782455444336, 6), (-17.016071319580078, 5), (-18.421659469604492, 9), (-17.998512268066406, 4), (-13.507487297058105, 1), (-16.135478973388672, 0)]\n",
      "2019-06-17 11:00:24,238 : INFO : 0.1\n",
      "2019-06-17 11:00:24,230 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,249 : INFO : built Dictionary(31 unique tokens: ['beating', 'car', 'considered', 'cross', 'detect']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:24,215 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,250 : INFO : P&P\n",
      "2019-06-17 11:00:24,279 : INFO : built Dictionary(33 unique tokens: ['anyone', 'clothes', 'decent', 'doha', 'indian']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:24,277 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,258 : INFO : Vocabulary size: 34 500\n",
      "2019-06-17 11:00:24,297 : INFO : WCD\n",
      "2019-06-17 11:00:24,310 : INFO : 0.0\n",
      "2019-06-17 11:00:24,341 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,320 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,338 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,337 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,359 : INFO : built Dictionary(42 unique tokens: ['brand', 'design', 'designer', 'doha', 'dress']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:24,351 : INFO : built Dictionary(36 unique tokens: ['ask', 'car', 'dated', 'didin', 'driving']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:24,413 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:24,412 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:24,443 : INFO : WCD\n",
      "2019-06-17 11:00:24,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,454 : INFO : 0.0\n",
      "2019-06-17 11:00:24,442 : INFO : [(-20.2683048248291, 7), (-19.58823585510254, 9), (-18.286088943481445, 5), (-18.73045539855957, 4), (-17.102840423583984, 1), (-18.136104583740234, 3), (-16.952558517456055, 2), (-0.0, 0), (-17.65007972717285, 6), (-16.91690444946289, 8)]\n",
      "2019-06-17 11:00:24,462 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,448 : INFO : built Dictionary(35 unique tokens: ['ago', 'anyone', 'around', 'broke', 'bt']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:24,481 : INFO : 0.1\n",
      "2019-06-17 11:00:24,485 : INFO : P&P\n",
      "2019-06-17 11:00:24,500 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,497 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,518 : INFO : built Dictionary(53 unique tokens: ['advice', 'buy', 'buying', 'clothes', 'comments']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:24,522 : INFO : [(-20.28169059753418, 6), (-20.092449188232422, 2), (-20.0845890045166, 9), (-18.82201385498047, 0), (-18.483827590942383, 1), (-19.605558395385742, 8), (-17.517024993896484, 5), (-18.1574649810791, 4), (-18.211830139160156, 3), (-17.6107120513916, 7)]\n",
      "2019-06-17 11:00:24,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,527 : INFO : 0.0\n",
      "2019-06-17 11:00:24,532 : INFO : P&P\n",
      "2019-06-17 11:00:24,535 : INFO : built Dictionary(23 unique tokens: ['always', 'beating', 'cameras', 'car', 'friend']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:24,542 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,573 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,581 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,582 : INFO : built Dictionary(15 unique tokens: ['anyone', 'jumping', 'know', 'light', 'penalty']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:00:24,596 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,595 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:24,603 : INFO : built Dictionary(41 unique tokens: ['advise', 'already', 'blinking', 'cid', 'drive']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:24,605 : INFO : WCD\n",
      "2019-06-17 11:00:24,623 : INFO : 0.0\n",
      "2019-06-17 11:00:24,630 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,644 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:24,655 : INFO : [(-19.60178565979004, 4), (-17.11741828918457, 5), (-19.558208465576172, 8), (-17.10744285583496, 6), (-16.72115135192871, 9), (-12.865962028503418, 3), (-17.641164779663086, 7), (-14.12519359588623, 2), (-0.0, 0), (-15.129334449768066, 1)]\n",
      "2019-06-17 11:00:24,673 : INFO : built Dictionary(46 unique tokens: ['also', 'anyone', 'area', 'around', 'best']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:24,667 : INFO : WCD\n",
      "2019-06-17 11:00:24,675 : INFO : 0.0\n",
      "2019-06-17 11:00:24,682 : INFO : 0.0\n",
      "2019-06-17 11:00:24,689 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:24,686 : INFO : P&P\n",
      "2019-06-17 11:00:24,703 : INFO : built Dictionary(47 unique tokens: ['advice', 'al', 'anybody', 'anyone', 'around']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:24,711 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,714 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,720 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,737 : INFO : [(-22.86701011657715, 6), (-20.402727127075195, 7), (-20.48593521118164, 8), (-18.18132781982422, 2), (-19.93043327331543, 4), (-20.44795799255371, 9), (-16.477418899536133, 3), (-14.835026741027832, 1), (-16.64773941040039, 0), (-18.959447860717773, 5)]\n",
      "2019-06-17 11:00:24,741 : INFO : 0.0\n",
      "2019-06-17 11:00:24,753 : INFO : P&P\n",
      "2019-06-17 11:00:24,756 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,768 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:24,781 : INFO : built Dictionary(38 unique tokens: ['buy', 'covers', 'curtains', 'cushions', 'greetings']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:24,779 : INFO : WCD\n",
      "2019-06-17 11:00:24,794 : INFO : 0.0\n",
      "2019-06-17 11:00:24,806 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,802 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,814 : INFO : Vocabulary size: 5 500\n",
      "2019-06-17 11:00:24,759 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,832 : INFO : WCD\n",
      "2019-06-17 11:00:24,832 : INFO : built Dictionary(35 unique tokens: ['already', 'anyone', 'discussion', 'fined', 'god']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:24,836 : INFO : [(-20.62563133239746, 3), (-19.602008819580078, 9), (-19.610998153686523, 1), (-19.522390365600586, 6), (-18.27522850036621, 8), (-19.052762985229492, 7), (-18.87635612487793, 5), (-18.857219696044922, 2), (-0.0, 0), (-18.253019332885742, 4)]\n",
      "2019-06-17 11:00:24,846 : INFO : 0.0\n",
      "2019-06-17 11:00:24,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,867 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,855 : INFO : 0.0\n",
      "2019-06-17 11:00:24,864 : INFO : built Dictionary(40 unique tokens: ['advise', 'also', 'bombay', 'buy', 'doha']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:24,858 : INFO : First K WMD\n",
      "2019-06-17 11:00:24,870 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:24,877 : INFO : P&P\n",
      "2019-06-17 11:00:24,906 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:24,888 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,903 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:24,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,934 : INFO : [(-22.541810989379883, 4), (-22.24071502685547, 9), (-21.51754379272461, 7), (-22.111560821533203, 0), (-20.859777450561523, 3), (-20.45640754699707, 2), (-19.84976577758789, 6), (-20.50175666809082, 8), (-19.71261978149414, 5), (-19.719711303710938, 1)]\n",
      "2019-06-17 11:00:24,920 : INFO : built Dictionary(34 unique tokens: ['always', 'around', 'camera', 'car', 'department']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:24,953 : INFO : built Dictionary(61 unique tokens: ['belly', 'benefits', 'called', 'come', 'cool']...) from 2 documents (total 71 corpus positions)\n",
      "2019-06-17 11:00:24,958 : INFO : 0.0\n",
      "2019-06-17 11:00:24,975 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:24,983 : INFO : P&P\n",
      "2019-06-17 11:00:25,010 : INFO : built Dictionary(44 unique tokens: ['abt', 'approx', 'around', 'average', 'colleague']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:25,035 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,045 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,065 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,021 : INFO : Vocabulary size: 2 500\n",
      "2019-06-17 11:00:25,093 : INFO : WCD\n",
      "2019-06-17 11:00:25,111 : INFO : 0.0\n",
      "2019-06-17 11:00:25,142 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:25,135 : INFO : First K WMD\n",
      "2019-06-17 11:00:25,157 : INFO : [(-23.72178840637207, 7), (-23.49480628967285, 1), (-23.435033798217773, 4), (-23.4432430267334, 2), (-22.612407684326172, 5), (-22.263782501220703, 3), (-20.57079315185547, 6), (-22.94523048400879, 9), (-22.279651641845703, 0), (-21.54509735107422, 8)]\n",
      "2019-06-17 11:00:25,181 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:25,179 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:25,202 : INFO : WCD\n",
      "2019-06-17 11:00:25,174 : INFO : 0.0\n",
      "2019-06-17 11:00:25,237 : INFO : P&P\n",
      "2019-06-17 11:00:25,247 : INFO : 0.0\n",
      "2019-06-17 11:00:25,249 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,261 : INFO : First K WMD\n",
      "2019-06-17 11:00:25,312 : INFO : [(-20.907976150512695, 6), (-20.65557098388672, 7), (-20.209787368774414, 2), (-19.42686653137207, 8), (-20.1729679107666, 5), (-19.85382843017578, 9), (-19.46387481689453, 4), (-17.561071395874023, 1), (-18.731647491455078, 3), (-16.913681030273438, 0)]\n",
      "2019-06-17 11:00:25,322 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,324 : INFO : 0.0\n",
      "2019-06-17 11:00:25,345 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,354 : INFO : P&P\n",
      "2019-06-17 11:00:25,359 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,484 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:25,496 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,511 : INFO : WCD\n",
      "2019-06-17 11:00:25,524 : INFO : 0.0\n",
      "2019-06-17 11:00:25,513 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,535 : INFO : First K WMD\n",
      "2019-06-17 11:00:25,573 : INFO : [(-18.27536392211914, 8), (-15.975988388061523, 4), (-17.401443481445312, 7), (-15.573257446289062, 3), (-15.129974365234375, 2), (-0.0, 5), (-14.290162086486816, 0), (-15.054178237915039, 9), (-14.32775592803955, 1), (-14.90622329711914, 6)]\n",
      "2019-06-17 11:00:25,571 : INFO : Vocabulary size: 8 500\n",
      "2019-06-17 11:00:25,588 : INFO : WCD\n",
      "2019-06-17 11:00:25,598 : INFO : 0.0\n",
      "2019-06-17 11:00:25,587 : INFO : 0.0\n",
      "2019-06-17 11:00:25,611 : INFO : P&P\n",
      "2019-06-17 11:00:25,619 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,613 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,603 : INFO : First K WMD\n",
      "2019-06-17 11:00:25,684 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:25,728 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,715 : INFO : WCD\n",
      "2019-06-17 11:00:25,722 : INFO : [(-20.551462173461914, 4), (-20.480745315551758, 6), (-20.408477783203125, 8), (-19.62327766418457, 1), (-20.389354705810547, 9), (-19.718387603759766, 7), (-19.19757652282715, 2), (-18.760740280151367, 3), (-17.99998664855957, 0), (-19.891403198242188, 5)]\n",
      "2019-06-17 11:00:25,785 : INFO : 0.0\n",
      "2019-06-17 11:00:25,788 : INFO : First K WMD\n",
      "2019-06-17 11:00:25,800 : INFO : [(-20.81613540649414, 6), (-20.19727325439453, 2), (-20.517271041870117, 7), (-19.849313735961914, 8), (-20.178733825683594, 0), (-18.20035171508789, 9), (-20.421390533447266, 4), (-18.042539596557617, 5), (-18.46993637084961, 3), (-19.27248191833496, 1)]\n",
      "2019-06-17 11:00:25,789 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:25,822 : INFO : 0.0\n",
      "2019-06-17 11:00:25,830 : INFO : P&P\n",
      "2019-06-17 11:00:25,846 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,825 : INFO : 0.1\n",
      "2019-06-17 11:00:25,852 : INFO : P&P\n",
      "2019-06-17 11:00:25,857 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:25,945 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:25,953 : INFO : WCD\n",
      "2019-06-17 11:00:25,963 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,009 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:25,985 : INFO : 0.0\n",
      "2019-06-17 11:00:26,022 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,011 : INFO : WCD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:26,017 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,075 : INFO : [(-20.992015838623047, 9), (-20.75328826904297, 7), (-19.498641967773438, 8), (-19.870206832885742, 4), (-19.648998260498047, 5), (-17.791488647460938, 0), (-0.0, 2), (-17.486867904663086, 6), (-18.01848793029785, 1), (-18.784202575683594, 3)]\n",
      "2019-06-17 11:00:26,038 : INFO : 0.0\n",
      "2019-06-17 11:00:26,087 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,087 : INFO : 0.0\n",
      "2019-06-17 11:00:26,105 : INFO : [(-18.782733917236328, 6), (-18.279544830322266, 7), (-18.260786056518555, 4), (-16.14962387084961, 8), (-18.257122039794922, 2), (-16.6293888092041, 0), (-17.138071060180664, 5), (-15.686863899230957, 3), (-15.035643577575684, 1), (-17.576824188232422, 9)]\n",
      "2019-06-17 11:00:26,107 : INFO : P&P\n",
      "2019-06-17 11:00:26,108 : INFO : 0.0\n",
      "2019-06-17 11:00:26,110 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,118 : INFO : P&P\n",
      "2019-06-17 11:00:26,122 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,182 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,237 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:26,247 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,261 : INFO : WCD\n",
      "2019-06-17 11:00:26,275 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:26,286 : INFO : 0.0\n",
      "2019-06-17 11:00:26,305 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,298 : INFO : WCD\n",
      "2019-06-17 11:00:26,317 : INFO : 0.0\n",
      "2019-06-17 11:00:26,318 : INFO : [(-21.374256134033203, 9), (-20.23324966430664, 4), (-20.285512924194336, 8), (-19.678295135498047, 6), (-19.059524536132812, 5), (-18.52311897277832, 2), (-20.156221389770508, 7), (-19.374723434448242, 3), (-16.942718505859375, 1), (-17.47613525390625, 0)]\n",
      "2019-06-17 11:00:26,328 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:26,335 : INFO : 0.0\n",
      "2019-06-17 11:00:26,341 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,333 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,352 : INFO : P&P\n",
      "2019-06-17 11:00:26,363 : INFO : built Dictionary(33 unique tokens: ['acid', 'anyone', 'away', 'doc', 'gerd']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:26,367 : INFO : [(-22.269243240356445, 3), (-20.06961441040039, 2), (-19.588472366333008, 6), (-19.80489730834961, 4), (-19.480934143066406, 1), (-19.43011474609375, 5), (-18.992307662963867, 0), (-19.175397872924805, 7), (-16.802125930786133, 9), (-18.527618408203125, 8)]\n",
      "2019-06-17 11:00:26,382 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,383 : INFO : 0.0\n",
      "2019-06-17 11:00:26,395 : INFO : P&P\n",
      "2019-06-17 11:00:26,411 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,443 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,432 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,412 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:26,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,453 : INFO : built Dictionary(6 unique tokens: ['advice', 'body', 'get', 'gold', 'loan']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:00:26,459 : INFO : built Dictionary(62 unique tokens: ['advertisement', 'al', 'also', 'asking', 'bridge']...) from 2 documents (total 71 corpus positions)\n",
      "2019-06-17 11:00:26,477 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,496 : INFO : built Dictionary(25 unique tokens: ['accept', 'advise', 'already', 'buy', 'cash']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:26,488 : INFO : Vocabulary size: 21 500\n",
      "2019-06-17 11:00:26,506 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,517 : INFO : WCD\n",
      "2019-06-17 11:00:26,525 : INFO : 0.0\n",
      "2019-06-17 11:00:26,518 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:26,536 : INFO : WCD\n",
      "2019-06-17 11:00:26,529 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,550 : INFO : 0.0\n",
      "2019-06-17 11:00:26,561 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,577 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,581 : INFO : built Dictionary(17 unique tokens: ['bank', 'banks', 'commercial', 'experiences', 'good']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:26,589 : INFO : [(-18.041109085083008, 7), (-17.922582626342773, 0), (-17.66966438293457, 6), (-17.207416534423828, 9), (-16.908185958862305, 2), (-17.32042121887207, 4), (-17.24117088317871, 8), (-16.24907875061035, 1), (-16.990314483642578, 5), (-14.12224292755127, 3)]\n",
      "2019-06-17 11:00:26,608 : INFO : 0.0\n",
      "2019-06-17 11:00:26,604 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,612 : INFO : P&P\n",
      "2019-06-17 11:00:26,619 : INFO : built Dictionary(35 unique tokens: ['anything', 'apply', 'away', 'bank', 'best']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:26,600 : INFO : [(-23.455646514892578, 4), (-22.065065383911133, 2), (-20.45237159729004, 9), (-21.475576400756836, 5), (-18.329002380371094, 7), (-18.838115692138672, 6), (-18.952970504760742, 3), (-19.68659782409668, 8), (-0.0, 0), (-17.867162704467773, 1)]\n",
      "2019-06-17 11:00:26,634 : INFO : 0.1\n",
      "2019-06-17 11:00:26,633 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,644 : INFO : P&P\n",
      "2019-06-17 11:00:26,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,655 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,663 : INFO : built Dictionary(37 unique tokens: ['anyone', 'appreciated', 'approval', 'bank', 'company']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:26,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,689 : INFO : built Dictionary(32 unique tokens: ['accessories', 'bought', 'country', 'crazy', 'extraordinarily']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:26,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,706 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,709 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:26,691 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,712 : INFO : built Dictionary(33 unique tokens: ['actually', 'al', 'answer', 'bank', 'banks']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:26,752 : INFO : WCD\n",
      "2019-06-17 11:00:26,743 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:26,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,764 : INFO : 0.0\n",
      "2019-06-17 11:00:26,745 : INFO : Vocabulary size: 22 500\n",
      "2019-06-17 11:00:26,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,773 : INFO : built Dictionary(9 unique tokens: ['best', 'investment', 'qr', 'advice', 'body']...) from 2 documents (total 12 corpus positions)\n",
      "2019-06-17 11:00:26,794 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,786 : INFO : WCD\n",
      "2019-06-17 11:00:26,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,828 : INFO : [(-21.035198211669922, 6), (-19.697490692138672, 4), (-17.247283935546875, 3), (-18.055545806884766, 9), (-16.85856819152832, 8), (-13.834127426147461, 5), (-17.196884155273438, 0), (-17.693374633789062, 7), (-15.961370468139648, 2), (-13.881464004516602, 1)]\n",
      "2019-06-17 11:00:26,809 : INFO : built Dictionary(38 unique tokens: ['anyone', 'exam', 'feedback', 'got', 'help']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:26,882 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:26,865 : INFO : built Dictionary(17 unique tokens: ['confirm', 'department', 'hr', 'informed', 'must']...) from 2 documents (total 20 corpus positions)\n",
      "2019-06-17 11:00:26,874 : INFO : 0.0\n",
      "2019-06-17 11:00:26,856 : INFO : 0.1\n",
      "2019-06-17 11:00:26,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,890 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:26,902 : INFO : P&P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:26,919 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:26,903 : INFO : First K WMD\n",
      "2019-06-17 11:00:26,920 : INFO : built Dictionary(33 unique tokens: ['advise', 'alcohol', 'answers', 'anywhere', 'bottle']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:26,921 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:26,956 : INFO : built Dictionary(51 unique tokens: ['april', 'better', 'cannot', 'country', 'daughter']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:26,968 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:26,972 : INFO : [(-19.62661361694336, 9), (-18.843427658081055, 5), (-18.94264030456543, 7), (-18.63849449157715, 8), (-18.411895751953125, 1), (-17.88629150390625, 0), (-18.74211883544922, 3), (-18.212522506713867, 2), (-18.46287727355957, 6), (-18.2197322845459, 4)]\n",
      "2019-06-17 11:00:26,977 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:26,999 : INFO : WCD\n",
      "2019-06-17 11:00:26,998 : INFO : 0.1\n",
      "2019-06-17 11:00:26,978 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,008 : INFO : 0.0\n",
      "2019-06-17 11:00:27,021 : INFO : P&P\n",
      "2019-06-17 11:00:27,033 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,067 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,086 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,096 : INFO : [(-16.850013732910156, 8), (-16.278112411499023, 4), (-16.343198776245117, 6), (-16.093955993652344, 0), (-16.06368637084961, 2), (-15.517644882202148, 3), (-15.101765632629395, 1), (-15.20785903930664, 7), (-15.09005355834961, 5), (-13.519165992736816, 9)]\n",
      "2019-06-17 11:00:27,074 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,102 : INFO : 0.0\n",
      "2019-06-17 11:00:27,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,105 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:27,110 : INFO : P&P\n",
      "2019-06-17 11:00:27,122 : INFO : built Dictionary(51 unique tokens: ['accommodation', 'allowance', 'allowances', 'allownaces', 'applied']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:27,125 : INFO : WCD\n",
      "2019-06-17 11:00:27,119 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,132 : INFO : 0.0\n",
      "2019-06-17 11:00:27,139 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,177 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,187 : INFO : [(-20.353586196899414, 6), (-20.067359924316406, 9), (-18.538522720336914, 4), (-17.606855392456055, 2), (-17.57813835144043, 7), (-17.336700439453125, 3), (-18.209636688232422, 8), (-12.104432106018066, 0), (-17.344139099121094, 5), (-0.0, 1)]\n",
      "2019-06-17 11:00:27,209 : INFO : 0.1\n",
      "2019-06-17 11:00:27,223 : INFO : P&P\n",
      "2019-06-17 11:00:27,243 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,236 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:27,243 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,293 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,295 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,313 : INFO : built Dictionary(61 unique tokens: ['affected', 'affects', 'anyone', 'appreciated', 'asset']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:27,272 : INFO : WCD\n",
      "2019-06-17 11:00:27,340 : INFO : 0.0\n",
      "2019-06-17 11:00:27,324 : INFO : Vocabulary size: 4 500\n",
      "2019-06-17 11:00:27,345 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,334 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,355 : INFO : WCD\n",
      "2019-06-17 11:00:27,378 : INFO : 0.0\n",
      "2019-06-17 11:00:27,382 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,395 : INFO : [(-21.555646896362305, 1), (-20.454160690307617, 9), (-20.98645782470703, 6), (-20.008569717407227, 2), (-19.962316513061523, 4), (-20.12238121032715, 7), (-18.93120002746582, 8), (-19.593385696411133, 5), (-17.651859283447266, 3), (-15.80506706237793, 0)]\n",
      "2019-06-17 11:00:27,379 : INFO : [(-22.21565818786621, 3), (-20.122690200805664, 9), (-19.663360595703125, 4), (-18.386571884155273, 7), (-18.788270950317383, 1), (-18.17585563659668, 5), (-16.408733367919922, 2), (-18.073129653930664, 6), (-18.14422607421875, 0), (-17.42237663269043, 8)]\n",
      "2019-06-17 11:00:27,401 : INFO : 0.0\n",
      "2019-06-17 11:00:27,414 : INFO : 0.1\n",
      "2019-06-17 11:00:27,426 : INFO : P&P\n",
      "2019-06-17 11:00:27,443 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,447 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,457 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,471 : INFO : built Dictionary(32 unique tokens: ['arabic', 'course', 'good', 'group', 'lessons']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:27,488 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,430 : INFO : P&P\n",
      "2019-06-17 11:00:27,513 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,518 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,551 : INFO : Vocabulary size: 29 500\n",
      "2019-06-17 11:00:27,575 : INFO : WCD\n",
      "2019-06-17 11:00:27,509 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,579 : INFO : 0.0\n",
      "2019-06-17 11:00:27,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,596 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,607 : INFO : built Dictionary(46 unique tokens: ['burqa', 'conservatively', 'culture', 'disrespectful', 'dressed']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:27,587 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:27,639 : INFO : WCD\n",
      "2019-06-17 11:00:27,674 : INFO : 0.0\n",
      "2019-06-17 11:00:27,685 : INFO : [(-20.94194793701172, 4), (-19.31439208984375, 8), (-17.978097915649414, 1), (-18.033113479614258, 3), (-18.472368240356445, 7), (-17.24334144592285, 5), (-17.712953567504883, 6), (-0.0, 0), (-16.23072624206543, 9), (-16.584203720092773, 2)]\n",
      "2019-06-17 11:00:27,694 : INFO : 0.1\n",
      "2019-06-17 11:00:27,694 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,727 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,733 : INFO : [(-22.453466415405273, 2), (-22.17247772216797, 1), (-21.491403579711914, 9), (-22.023984909057617, 4), (-22.132368087768555, 5), (-20.552640914916992, 6), (-20.7470760345459, 8), (-15.549776077270508, 0), (-20.574033737182617, 3), (-18.723716735839844, 7)]\n",
      "2019-06-17 11:00:27,731 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,711 : INFO : P&P\n",
      "2019-06-17 11:00:27,704 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,739 : INFO : 0.0\n",
      "2019-06-17 11:00:27,756 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,772 : INFO : P&P\n",
      "2019-06-17 11:00:27,761 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,780 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:27,784 : INFO : built Dictionary(55 unique tokens: ['accent', 'asked', 'attention', 'bill', 'boost']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:00:27,813 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,868 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:27,891 : INFO : WCD\n",
      "2019-06-17 11:00:27,899 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,897 : INFO : Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,914 : INFO : 0.0\n",
      "2019-06-17 11:00:27,923 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,936 : INFO : First K WMD\n",
      "2019-06-17 11:00:27,941 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:27,934 : INFO : built Dictionary(10 unique tokens: ['ayala', 'ceo', 'clan', 'first', 'job']...) from 2 documents (total 12 corpus positions)\n",
      "2019-06-17 11:00:27,939 : INFO : built Dictionary(37 unique tokens: ['anyone', 'back', 'bass', 'brands', 'buy']...) from 2 documents (total 41 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:27,961 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:27,961 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:27,975 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,997 : INFO : built Dictionary(18 unique tokens: ['brand', 'car', 'dog', 'first', 'good']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:27,986 : INFO : [(-21.977235794067383, 7), (-20.514877319335938, 8), (-19.21251678466797, 5), (-19.176013946533203, 1), (-18.162734985351562, 4), (-19.07843589782715, 9), (-18.020183563232422, 6), (-0.0, 0), (-14.783650398254395, 3), (-13.305310249328613, 2)]\n",
      "2019-06-17 11:00:28,002 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:27,990 : INFO : WCD\n",
      "2019-06-17 11:00:28,015 : INFO : built Dictionary(9 unique tokens: ['anyone', 'experience', 'needed', 'qaws', 'recently']...) from 2 documents (total 9 corpus positions)\n",
      "2019-06-17 11:00:27,994 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,018 : INFO : 0.1\n",
      "2019-06-17 11:00:28,016 : INFO : 0.0\n",
      "2019-06-17 11:00:28,042 : INFO : P&P\n",
      "2019-06-17 11:00:28,036 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:28,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,041 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,078 : INFO : built Dictionary(21 unique tokens: ['buy', 'c', 'call', 'child', 'even']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:00:28,094 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,037 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:28,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,123 : INFO : [(-21.062255859375, 4), (-20.485973358154297, 7), (-19.610925674438477, 1), (-19.744382858276367, 0), (-19.80144500732422, 9), (-18.889549255371094, 8), (-19.154850006103516, 3), (-18.105464935302734, 6), (-18.93995475769043, 5), (-19.263092041015625, 2)]\n",
      "2019-06-17 11:00:28,129 : INFO : built Dictionary(27 unique tokens: ['bad', 'career', 'countries', 'end', 'equally']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:28,139 : INFO : 0.0\n",
      "2019-06-17 11:00:28,151 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,166 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,171 : INFO : built Dictionary(22 unique tokens: ['bout', 'buy', 'cartoon', 'case', 'character']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:00:28,156 : INFO : P&P\n",
      "2019-06-17 11:00:28,178 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,175 : INFO : Vocabulary size: 26 500\n",
      "2019-06-17 11:00:28,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,213 : INFO : built Dictionary(46 unique tokens: ['actually', 'back', 'bar', 'bed', 'bit']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:28,213 : INFO : WCD\n",
      "2019-06-17 11:00:28,240 : INFO : 0.0\n",
      "2019-06-17 11:00:28,255 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,276 : INFO : built Dictionary(10 unique tokens: ['adventures', 'cool', 'get', 'interesting', 'still']...) from 2 documents (total 10 corpus positions)\n",
      "2019-06-17 11:00:28,287 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,295 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,321 : INFO : [(-21.8565731048584, 8), (-21.204713821411133, 2), (-20.658161163330078, 1), (-21.179067611694336, 6), (-20.874967575073242, 3), (-20.409217834472656, 5), (-19.645164489746094, 7), (-20.295358657836914, 4), (-20.622304916381836, 9), (-17.459487915039062, 0)]\n",
      "2019-06-17 11:00:28,325 : INFO : built Dictionary(37 unique tokens: ['activities', 'adha', 'away', 'boring', 'celebrations']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:28,310 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:28,338 : INFO : WCD\n",
      "2019-06-17 11:00:28,338 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:28,345 : INFO : 0.1\n",
      "2019-06-17 11:00:28,359 : INFO : 0.0\n",
      "2019-06-17 11:00:28,361 : INFO : P&P\n",
      "2019-06-17 11:00:28,392 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,364 : INFO : built Dictionary(12 unique tokens: ['answer', 'b', 'bad', 'c', 'good']...) from 2 documents (total 13 corpus positions)\n",
      "2019-06-17 11:00:28,402 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,410 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:28,419 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,460 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,465 : INFO : [(-18.01692008972168, 8), (-16.514070510864258, 6), (-15.622930526733398, 0), (-15.550285339355469, 7), (-14.403300285339355, 5), (-13.951655387878418, 4), (-14.783037185668945, 1), (-14.121346473693848, 9), (-13.982943534851074, 3), (-11.930872917175293, 2)]\n",
      "2019-06-17 11:00:28,479 : INFO : Vocabulary size: 30 500\n",
      "2019-06-17 11:00:28,500 : INFO : WCD\n",
      "2019-06-17 11:00:28,490 : INFO : 0.0\n",
      "2019-06-17 11:00:28,506 : INFO : 0.0\n",
      "2019-06-17 11:00:28,515 : INFO : P&P\n",
      "2019-06-17 11:00:28,538 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,520 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,576 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:28,581 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,611 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,617 : INFO : [(-21.027233123779297, 9), (-20.083112716674805, 8), (-19.397388458251953, 7), (-19.660768508911133, 6), (-19.4161376953125, 4), (-17.85083770751953, 1), (-18.94464683532715, 2), (-17.982450485229492, 3), (-0.0, 0), (-18.07908058166504, 5)]\n",
      "2019-06-17 11:00:28,630 : INFO : 0.1\n",
      "2019-06-17 11:00:28,593 : INFO : WCD\n",
      "2019-06-17 11:00:28,659 : INFO : P&P\n",
      "2019-06-17 11:00:28,687 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,651 : INFO : 0.0\n",
      "2019-06-17 11:00:28,711 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,739 : INFO : Vocabulary size: 32 500\n",
      "2019-06-17 11:00:28,751 : INFO : WCD\n",
      "2019-06-17 11:00:28,754 : INFO : [(-23.14484977722168, 5), (-22.619564056396484, 7), (-23.134672164916992, 9), (-21.432825088500977, 2), (-21.3194637298584, 4), (-19.984485626220703, 1), (-21.27737045288086, 8), (-19.216136932373047, 0), (-20.86276626586914, 6), (-16.77802085876465, 3)]\n",
      "2019-06-17 11:00:28,760 : INFO : 0.0\n",
      "2019-06-17 11:00:28,757 : INFO : 0.0\n",
      "2019-06-17 11:00:28,774 : INFO : First K WMD\n",
      "2019-06-17 11:00:28,778 : INFO : P&P\n",
      "2019-06-17 11:00:28,796 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,809 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,853 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:28,851 : INFO : [(-21.7929630279541, 4), (-20.990459442138672, 9), (-19.990171432495117, 8), (-19.875844955444336, 3), (-19.566560745239258, 6), (-19.362686157226562, 7), (-19.42097282409668, 1), (-18.863811492919922, 5), (-0.0, 0), (-16.420063018798828, 2)]\n",
      "2019-06-17 11:00:28,878 : INFO : 0.1\n",
      "2019-06-17 11:00:28,906 : INFO : P&P\n",
      "2019-06-17 11:00:28,924 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:28,974 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:28,989 : INFO : WCD\n",
      "2019-06-17 11:00:28,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,022 : INFO : 0.0\n",
      "2019-06-17 11:00:28,984 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:29,027 : INFO : WCD\n",
      "2019-06-17 11:00:29,012 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,012 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,030 : INFO : built Dictionary(34 unique tokens: ['airfare', 'airlines', 'airway', 'airways', 'bangkok']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:29,034 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,036 : INFO : 0.0\n",
      "2019-06-17 11:00:29,055 : INFO : First K WMD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:29,070 : INFO : [(-20.862916946411133, 6), (-20.249042510986328, 7), (-19.542098999023438, 4), (-19.406902313232422, 8), (-19.896913528442383, 9), (-18.920698165893555, 1), (-16.545400619506836, 0), (-17.868898391723633, 3), (-16.870378494262695, 5), (-17.386886596679688, 2)]\n",
      "2019-06-17 11:00:29,078 : INFO : 0.0\n",
      "2019-06-17 11:00:29,103 : INFO : P&P\n",
      "2019-06-17 11:00:29,100 : INFO : [(-21.121061325073242, 7), (-20.979509353637695, 4), (-20.974580764770508, 8), (-20.244028091430664, 9), (-18.75907325744629, 5), (-18.502172470092773, 3), (-19.219619750976562, 6), (-17.862863540649414, 1), (-18.3883113861084, 0), (-15.199790954589844, 2)]\n",
      "2019-06-17 11:00:29,108 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,121 : INFO : 0.0\n",
      "2019-06-17 11:00:29,137 : INFO : P&P\n",
      "2019-06-17 11:00:29,154 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,150 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,195 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,212 : INFO : built Dictionary(37 unique tokens: ['airways', 'anyone', 'anything', 'commercial', 'currently']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:29,230 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,239 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:29,259 : INFO : WCD\n",
      "2019-06-17 11:00:29,252 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:29,264 : INFO : 0.0\n",
      "2019-06-17 11:00:29,271 : INFO : WCD\n",
      "2019-06-17 11:00:29,257 : INFO : built Dictionary(43 unique tokens: ['advertisement', 'advice', 'agent', 'apartment', 'areas']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:29,278 : INFO : 0.0\n",
      "2019-06-17 11:00:29,276 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,292 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,301 : INFO : [(-21.77652931213379, 7), (-21.4781551361084, 6), (-21.440885543823242, 8), (-20.657028198242188, 9), (-20.836076736450195, 5), (-19.31797981262207, 4), (-19.60320281982422, 0), (-19.210739135742188, 2), (-20.401050567626953, 3), (-15.302748680114746, 1)]\n",
      "2019-06-17 11:00:29,313 : INFO : 0.0\n",
      "2019-06-17 11:00:29,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,333 : INFO : P&P\n",
      "2019-06-17 11:00:29,338 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,328 : INFO : [(-20.644161224365234, 9), (-20.414745330810547, 0), (-19.340627670288086, 7), (-19.41282081604004, 3), (-19.389419555664062, 6), (-19.236597061157227, 2), (-19.194141387939453, 1), (-18.88278579711914, 4), (-19.073816299438477, 5), (-18.44911766052246, 8)]\n",
      "2019-06-17 11:00:29,341 : INFO : 0.0\n",
      "2019-06-17 11:00:29,348 : INFO : P&P\n",
      "2019-06-17 11:00:29,355 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,327 : INFO : built Dictionary(34 unique tokens: ['advice', 'airways', 'baby', 'born', 'come']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:29,343 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,357 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,377 : INFO : built Dictionary(35 unique tokens: ['advice', 'apartment', 'apartments', 'best', 'contract']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:29,382 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:29,395 : INFO : WCD\n",
      "2019-06-17 11:00:29,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,408 : INFO : built Dictionary(44 unique tokens: ['airways', 'anybody', 'arrive', 'ask', 'baggage']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:29,407 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,411 : INFO : 0.0\n",
      "2019-06-17 11:00:29,420 : INFO : built Dictionary(41 unique tokens: ['also', 'car', 'come', 'every', 'fine']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:29,416 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:29,437 : INFO : WCD\n",
      "2019-06-17 11:00:29,428 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,460 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:29,477 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,494 : INFO : built Dictionary(42 unique tokens: ['already', 'anybody', 'appartments', 'appreciate', 'area']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:29,481 : INFO : 0.0\n",
      "2019-06-17 11:00:29,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,510 : INFO : built Dictionary(38 unique tokens: ['airline', 'airways', 'anyone', 'attended', 'back']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:29,521 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,529 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,482 : INFO : [(-20.97886085510254, 7), (-20.73546028137207, 0), (-19.62263298034668, 2), (-19.749256134033203, 5), (-19.198741912841797, 9), (-18.735034942626953, 3), (-19.37714958190918, 4), (-18.78215980529785, 1), (-19.186920166015625, 8), (-17.750608444213867, 6)]\n",
      "2019-06-17 11:00:29,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,550 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,559 : INFO : built Dictionary(43 unique tokens: ['bought', 'buying', 'considered', 'decision', 'e']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:29,562 : INFO : 0.1\n",
      "2019-06-17 11:00:29,573 : INFO : built Dictionary(46 unique tokens: ['afraid', 'airlines', 'airways', 'although', 'anybody']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:29,575 : INFO : [(-20.2460880279541, 3), (-19.868133544921875, 5), (-19.763158798217773, 2), (-19.842336654663086, 7), (-18.585250854492188, 4), (-19.14208221435547, 0), (-17.078927993774414, 9), (-19.233854293823242, 1), (-16.912384033203125, 6), (-16.558300018310547, 8)]\n",
      "2019-06-17 11:00:29,577 : INFO : P&P\n",
      "2019-06-17 11:00:29,605 : INFO : 0.1\n",
      "2019-06-17 11:00:29,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,614 : INFO : P&P\n",
      "2019-06-17 11:00:29,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,613 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,625 : INFO : built Dictionary(39 unique tokens: ['able', 'apartment', 'away', 'back', 'cancelling']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:29,627 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,629 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,639 : INFO : built Dictionary(31 unique tokens: ['air', 'airline', 'airways', 'bad', 'better']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:29,676 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,708 : INFO : built Dictionary(42 unique tokens: ['advance', 'bank', 'big', 'card', 'company']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:29,700 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:29,725 : INFO : Vocabulary size: 26 500\n",
      "2019-06-17 11:00:29,712 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,746 : INFO : built Dictionary(34 unique tokens: ['advice', 'airways', 'aviation', 'bad', 'cabin']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:29,742 : INFO : WCD\n",
      "2019-06-17 11:00:29,747 : INFO : WCD\n",
      "2019-06-17 11:00:29,763 : INFO : 0.0\n",
      "2019-06-17 11:00:29,769 : INFO : 0.0\n",
      "2019-06-17 11:00:29,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,784 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,781 : INFO : First K WMD\n",
      "2019-06-17 11:00:29,792 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,826 : INFO : [(-18.400894165039062, 7), (-17.91080093383789, 9), (-14.332581520080566, 5), (-15.574594497680664, 4), (-16.727848052978516, 8), (-13.601612091064453, 3), (-14.171646118164062, 2), (-15.121397018432617, 1), (-14.556406021118164, 6), (-14.622307777404785, 0)]\n",
      "2019-06-17 11:00:29,796 : INFO : built Dictionary(41 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 53 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:29,836 : INFO : [(-19.3093318939209, 7), (-17.731443405151367, 9), (-18.60076332092285, 8), (-17.314767837524414, 1), (-17.476215362548828, 6), (-17.00689697265625, 3), (-13.628423690795898, 4), (-16.6080322265625, 0), (-14.79593276977539, 5), (-17.467424392700195, 2)]\n",
      "2019-06-17 11:00:29,848 : INFO : 0.0\n",
      "2019-06-17 11:00:29,857 : INFO : 0.0\n",
      "2019-06-17 11:00:29,861 : INFO : P&P\n",
      "2019-06-17 11:00:29,880 : INFO : P&P\n",
      "2019-06-17 11:00:29,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,891 : INFO : built Dictionary(46 unique tokens: ['advance', 'airways', 'allowance', 'also', 'analyst']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:29,898 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,884 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:29,895 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,937 : INFO : built Dictionary(43 unique tokens: ['anything', 'apply', 'away', 'bank', 'best']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:29,995 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:29,994 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:29,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:29,989 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:30,013 : INFO : WCD\n",
      "2019-06-17 11:00:30,017 : INFO : built Dictionary(27 unique tokens: ['advice', 'airways', 'ask', 'cabin', 'could']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:30,010 : INFO : WCD\n",
      "2019-06-17 11:00:30,035 : INFO : 0.0\n",
      "2019-06-17 11:00:30,057 : INFO : 0.0\n",
      "2019-06-17 11:00:30,063 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,073 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,016 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,072 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:30,089 : INFO : [(-19.602678298950195, 6), (-18.6185245513916, 9), (-18.542539596557617, 8), (-18.018573760986328, 2), (-18.223575592041016, 5), (-17.944637298583984, 7), (-17.143938064575195, 3), (-17.378173828125, 4), (-17.546092987060547, 0), (-16.883371353149414, 1)]\n",
      "2019-06-17 11:00:30,078 : INFO : built Dictionary(37 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:30,109 : INFO : [(-21.00978660583496, 8), (-20.83885383605957, 5), (-20.752784729003906, 4), (-20.11821937561035, 6), (-20.06441879272461, 2), (-18.68817138671875, 3), (-20.25462532043457, 7), (-17.17200469970703, 0), (-18.989503860473633, 9), (-19.150390625, 1)]\n",
      "2019-06-17 11:00:30,114 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,121 : INFO : 0.0\n",
      "2019-06-17 11:00:30,126 : INFO : 0.0\n",
      "2019-06-17 11:00:30,140 : INFO : P&P\n",
      "2019-06-17 11:00:30,147 : INFO : P&P\n",
      "2019-06-17 11:00:30,165 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,165 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,170 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:30,234 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:30,245 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:30,250 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,261 : INFO : WCD\n",
      "2019-06-17 11:00:30,265 : INFO : 0.0\n",
      "2019-06-17 11:00:30,255 : INFO : WCD\n",
      "2019-06-17 11:00:30,295 : INFO : 0.0\n",
      "2019-06-17 11:00:30,321 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,352 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,281 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,379 : INFO : [(-20.366222381591797, 8), (-20.320045471191406, 3), (-20.081510543823242, 7), (-18.559356689453125, 1), (-18.538646697998047, 4), (-0.0, 0), (-19.319894790649414, 2), (-17.319419860839844, 5), (-18.12623405456543, 6), (-17.797597885131836, 9)]\n",
      "2019-06-17 11:00:30,390 : INFO : [(-21.40264129638672, 8), (-20.722829818725586, 7), (-18.176223754882812, 9), (-20.056838989257812, 1), (-18.98577308654785, 6), (-17.408966064453125, 5), (-16.87538719177246, 3), (-18.540672302246094, 4), (-18.069774627685547, 2), (-15.758721351623535, 0)]\n",
      "2019-06-17 11:00:30,402 : INFO : 0.0\n",
      "2019-06-17 11:00:30,390 : INFO : 0.0\n",
      "2019-06-17 11:00:30,408 : INFO : P&P\n",
      "2019-06-17 11:00:30,412 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,413 : INFO : P&P\n",
      "2019-06-17 11:00:30,431 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,515 : INFO : Vocabulary size: 27 500\n",
      "2019-06-17 11:00:30,534 : INFO : WCD\n",
      "2019-06-17 11:00:30,555 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,549 : INFO : 0.0\n",
      "2019-06-17 11:00:30,570 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,550 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:30,597 : INFO : WCD\n",
      "2019-06-17 11:00:30,607 : INFO : 0.0\n",
      "2019-06-17 11:00:30,615 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,617 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,607 : INFO : [(-20.123703002929688, 8), (-19.88127899169922, 5), (-19.22214126586914, 7), (-18.89466667175293, 0), (-19.83743667602539, 3), (-18.064085006713867, 4), (-19.16490364074707, 2), (-18.442626953125, 1), (-18.403837203979492, 9), (-19.154579162597656, 6)]\n",
      "2019-06-17 11:00:30,632 : INFO : [(-20.939878463745117, 4), (-18.451101303100586, 2), (-19.002378463745117, 3), (-18.02260398864746, 1), (-17.672014236450195, 0), (-16.197465896606445, 9), (-18.914541244506836, 6), (-16.754568099975586, 8), (-17.168359756469727, 5), (-17.38541030883789, 7)]\n",
      "2019-06-17 11:00:30,629 : INFO : 0.0\n",
      "2019-06-17 11:00:30,620 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,643 : INFO : built Dictionary(23 unique tokens: ['allowances', 'best', 'budget', 'buying', 'call']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:30,638 : INFO : 0.0\n",
      "2019-06-17 11:00:30,645 : INFO : P&P\n",
      "2019-06-17 11:00:30,648 : INFO : P&P\n",
      "2019-06-17 11:00:30,654 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,655 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,687 : INFO : built Dictionary(49 unique tokens: ['advice', 'around', 'avail', 'banks', 'brand']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:00:30,711 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:30,733 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,755 : INFO : built Dictionary(18 unique tokens: ['cat', 'help', 'lost', 'lp', 'anyone']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:30,755 : INFO : Vocabulary size: 26 500\n",
      "2019-06-17 11:00:30,770 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:30,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,785 : INFO : WCD\n",
      "2019-06-17 11:00:30,798 : INFO : built Dictionary(39 unique tokens: ['asked', 'conversation', 'days', 'definite', 'english']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:30,762 : INFO : Vocabulary size: 21 500\n",
      "2019-06-17 11:00:30,809 : INFO : WCD\n",
      "2019-06-17 11:00:30,820 : INFO : 0.0\n",
      "2019-06-17 11:00:30,807 : INFO : 0.0\n",
      "2019-06-17 11:00:30,802 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:30,824 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,823 : INFO : First K WMD\n",
      "2019-06-17 11:00:30,839 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:30,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,876 : INFO : [(-19.32832908630371, 5), (-18.815444946289062, 6), (-17.8533992767334, 2), (-17.964111328125, 3), (-18.089637756347656, 8), (-16.659833908081055, 0), (-17.620195388793945, 4), (-17.122276306152344, 7), (-17.49424934387207, 1), (-17.28972625732422, 9)]\n",
      "2019-06-17 11:00:30,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:30,891 : INFO : 0.0\n",
      "2019-06-17 11:00:30,889 : INFO : built Dictionary(50 unique tokens: ['accent', 'asked', 'attention', 'bill', 'boost']...) from 2 documents (total 60 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:30,907 : INFO : P&P\n",
      "2019-06-17 11:00:30,890 : INFO : [(-18.945558547973633, 1), (-18.90532875061035, 0), (-16.798765182495117, 6), (-18.410327911376953, 5), (-17.673742294311523, 8), (-14.911725044250488, 3), (-15.577156066894531, 7), (-17.493745803833008, 4), (-16.80230712890625, 2), (-17.353836059570312, 9)]\n",
      "2019-06-17 11:00:30,905 : INFO : built Dictionary(44 unique tokens: ['also', 'car', 'care', 'considering', 'cost']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:30,922 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,931 : INFO : 0.1\n",
      "2019-06-17 11:00:30,947 : INFO : P&P\n",
      "2019-06-17 11:00:30,966 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:30,974 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,000 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:31,019 : INFO : built Dictionary(43 unique tokens: ['alyson', 'appreciate', 'bring', 'buy', 'called']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:31,020 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:31,039 : INFO : WCD\n",
      "2019-06-17 11:00:31,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,056 : INFO : 0.0\n",
      "2019-06-17 11:00:31,065 : INFO : built Dictionary(35 unique tokens: ['couple', 'enough', 'good', 'k', 'kids']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:31,071 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,071 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:31,063 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,088 : INFO : WCD\n",
      "2019-06-17 11:00:31,097 : INFO : 0.0\n",
      "2019-06-17 11:00:31,098 : INFO : [(-21.237926483154297, 8), (-20.931133270263672, 4), (-19.53594398498535, 1), (-20.011028289794922, 9), (-19.38066864013672, 2), (-19.483915328979492, 7), (-17.8912353515625, 5), (-19.19415283203125, 3), (-18.333831787109375, 6), (-0.0, 0)]\n",
      "2019-06-17 11:00:31,114 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,112 : INFO : 0.0\n",
      "2019-06-17 11:00:31,133 : INFO : P&P\n",
      "2019-06-17 11:00:31,129 : INFO : built Dictionary(30 unique tokens: ['buying', 'cats', 'could', 'doha', 'finding']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:31,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,145 : INFO : built Dictionary(45 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:31,139 : INFO : [(-21.21497917175293, 0), (-20.223201751708984, 6), (-20.88553810119629, 3), (-19.245086669921875, 4), (-19.793455123901367, 5), (-19.373207092285156, 9), (-19.764842987060547, 1), (-17.590715408325195, 8), (-17.958223342895508, 2), (-19.538572311401367, 7)]\n",
      "2019-06-17 11:00:31,158 : INFO : 0.0\n",
      "2019-06-17 11:00:31,152 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,154 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,168 : INFO : P&P\n",
      "2019-06-17 11:00:31,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,188 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,197 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:31,188 : INFO : built Dictionary(46 unique tokens: ['also', 'annoying', 'asian', 'depends', 'end']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:31,263 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,311 : INFO : built Dictionary(58 unique tokens: ['advice', 'bedrooms', 'car', 'care', 'completion']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:00:31,314 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:31,305 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:31,289 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,330 : INFO : built Dictionary(57 unique tokens: ['anyone', 'ask', 'bags', 'brand', 'buying']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:31,337 : INFO : WCD\n",
      "2019-06-17 11:00:31,325 : INFO : WCD\n",
      "2019-06-17 11:00:31,348 : INFO : 0.0\n",
      "2019-06-17 11:00:31,362 : INFO : 0.0\n",
      "2019-06-17 11:00:31,376 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,369 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,402 : INFO : [(-20.870853424072266, 9), (-20.743717193603516, 1), (-19.487890243530273, 6), (-19.746864318847656, 7), (-19.00180435180664, 3), (-19.32524871826172, 8), (-19.019563674926758, 2), (-19.37403678894043, 4), (-18.634815216064453, 5), (-0.0, 0)]\n",
      "2019-06-17 11:00:31,429 : INFO : [(-20.068523406982422, 7), (-19.560813903808594, 9), (-19.817136764526367, 6), (-19.024555206298828, 8), (-18.089645385742188, 4), (-19.315662384033203, 5), (-17.600624084472656, 0), (-16.996206283569336, 3), (-15.252201080322266, 2), (-16.952898025512695, 1)]\n",
      "2019-06-17 11:00:31,434 : INFO : creating matrix with 10 documents and 462807 features\n",
      "2019-06-17 11:00:31,451 : INFO : 0.1\n",
      "2019-06-17 11:00:31,441 : INFO : 0.0\n",
      "2019-06-17 11:00:31,469 : INFO : P&P\n",
      "2019-06-17 11:00:31,472 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,473 : INFO : P&P\n",
      "2019-06-17 11:00:31,479 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,522 : INFO : Vocabulary size: 35 500\n",
      "2019-06-17 11:00:31,534 : INFO : WCD\n",
      "2019-06-17 11:00:31,515 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,547 : INFO : 0.0\n",
      "2019-06-17 11:00:31,558 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,550 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,571 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:31,590 : INFO : built Dictionary(41 unique tokens: ['ago', 'anyone', 'arrived', 'doha', 'everyone']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:31,596 : INFO : WCD\n",
      "2019-06-17 11:00:31,601 : INFO : 0.0\n",
      "2019-06-17 11:00:31,611 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,617 : INFO : [(-19.37356185913086, 8), (-19.122310638427734, 3), (-19.230640411376953, 9), (-17.534046173095703, 2), (-18.711584091186523, 4), (-18.05825424194336, 6), (-18.555828094482422, 7), (-16.679759979248047, 1), (-0.0, 0), (-18.67510414123535, 5)]\n",
      "2019-06-17 11:00:31,563 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,625 : INFO : 0.1\n",
      "2019-06-17 11:00:31,635 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,649 : INFO : P&P\n",
      "2019-06-17 11:00:31,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,658 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,669 : INFO : built Dictionary(43 unique tokens: ['actually', 'anyone', 'bay', 'beaches', 'calling']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:31,633 : INFO : built Dictionary(50 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:00:31,688 : INFO : [(-19.523452758789062, 4), (-19.429969787597656, 1), (-18.90572166442871, 7), (-19.30421257019043, 0), (-18.837142944335938, 5), (-18.411298751831055, 2), (-15.300297737121582, 6), (-18.655675888061523, 8), (-19.107013702392578, 3), (-17.92129135131836, 9)]\n",
      "2019-06-17 11:00:31,710 : INFO : 0.1\n",
      "2019-06-17 11:00:31,711 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:31,725 : INFO : P&P\n",
      "2019-06-17 11:00:31,725 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,738 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:31,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,741 : INFO : built Dictionary(46 unique tokens: ['amongst', 'anyone', 'area', 'aside', 'aspiring']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:31,774 : INFO : built Dictionary(54 unique tokens: ['abt', 'approx', 'around', 'average', 'colleague']...) from 2 documents (total 75 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:31,750 : INFO : Vocabulary size: 36 500\n",
      "2019-06-17 11:00:31,809 : INFO : WCD\n",
      "2019-06-17 11:00:31,814 : INFO : 0.0\n",
      "2019-06-17 11:00:31,832 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,827 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:31,834 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:31,842 : INFO : WCD\n",
      "2019-06-17 11:00:31,851 : INFO : 0.0\n",
      "2019-06-17 11:00:31,862 : INFO : First K WMD\n",
      "2019-06-17 11:00:31,861 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:31,944 : INFO : [(-19.713876724243164, 2), (-18.643831253051758, 9), (-18.276973724365234, 0), (-17.69195556640625, 8), (-18.381807327270508, 7), (-16.744121551513672, 3), (-15.881484985351562, 6), (-16.615751266479492, 5), (-16.494796752929688, 1), (-16.381183624267578, 4)]\n",
      "2019-06-17 11:00:31,941 : INFO : built Dictionary(42 unique tokens: ['account', 'anyone', 'bank', 'directly', 'employee']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:31,969 : INFO : [(-20.705909729003906, 3), (-19.65243911743164, 1), (-19.778379440307617, 8), (-19.13143539428711, 7), (-19.506011962890625, 6), (-17.314760208129883, 4), (-0.0, 0), (-18.687602996826172, 2), (-18.05893898010254, 5), (-19.337446212768555, 9)]\n",
      "2019-06-17 11:00:31,970 : INFO : 0.1\n",
      "2019-06-17 11:00:31,984 : INFO : 0.1\n",
      "2019-06-17 11:00:31,991 : INFO : P&P\n",
      "2019-06-17 11:00:31,997 : INFO : P&P\n",
      "2019-06-17 11:00:32,011 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,023 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,039 : INFO : built Dictionary(41 unique tokens: ['appartment', 'applied', 'around', 'enough', 'good']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:32,083 : INFO : Vocabulary size: 5 500\n",
      "2019-06-17 11:00:32,094 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:32,115 : INFO : WCD\n",
      "2019-06-17 11:00:32,109 : INFO : WCD\n",
      "2019-06-17 11:00:32,132 : INFO : 0.0\n",
      "2019-06-17 11:00:32,123 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:32,145 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,146 : INFO : 0.0\n",
      "2019-06-17 11:00:32,165 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,214 : INFO : [(-18.63098907470703, 4), (-17.700000762939453, 5), (-16.462350845336914, 2), (-16.901535034179688, 0), (-16.488506317138672, 1), (-16.318065643310547, 7), (-15.095454216003418, 8), (-11.010178565979004, 9), (-15.703067779541016, 6), (-14.420193672180176, 3)]\n",
      "2019-06-17 11:00:32,228 : INFO : 0.0\n",
      "2019-06-17 11:00:32,236 : INFO : P&P\n",
      "2019-06-17 11:00:32,255 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,266 : INFO : [(-21.06643295288086, 3), (-20.65537452697754, 8), (-20.383665084838867, 5), (-19.254892349243164, 4), (-19.983257293701172, 9), (-19.38749122619629, 7), (-16.88791847229004, 0), (-18.811683654785156, 1), (-18.53022003173828, 6), (-17.580408096313477, 2)]\n",
      "2019-06-17 11:00:32,284 : INFO : 0.0\n",
      "2019-06-17 11:00:32,302 : INFO : P&P\n",
      "2019-06-17 11:00:32,320 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,333 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:32,364 : INFO : WCD\n",
      "2019-06-17 11:00:32,378 : INFO : 0.0\n",
      "2019-06-17 11:00:32,385 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,393 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:32,410 : INFO : WCD\n",
      "2019-06-17 11:00:32,421 : INFO : 0.0\n",
      "2019-06-17 11:00:32,433 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,451 : INFO : [(-20.693185806274414, 9), (-19.54066276550293, 7), (-19.544017791748047, 2), (-17.098793029785156, 0), (-18.72410774230957, 8), (-16.843175888061523, 4), (-14.32939338684082, 1), (-13.396100997924805, 5), (-16.414615631103516, 3), (-17.783702850341797, 6)]\n",
      "2019-06-17 11:00:32,429 : INFO : [(-22.097414016723633, 4), (-20.794246673583984, 6), (-20.22679901123047, 7), (-20.516145706176758, 9), (-18.487123489379883, 1), (-20.148447036743164, 3), (-18.71928596496582, 5), (-20.35823631286621, 8), (-0.0, 0), (-18.284427642822266, 2)]\n",
      "2019-06-17 11:00:32,461 : INFO : 0.1\n",
      "2019-06-17 11:00:32,470 : INFO : 0.0\n",
      "2019-06-17 11:00:32,473 : INFO : P&P\n",
      "2019-06-17 11:00:32,477 : INFO : P&P\n",
      "2019-06-17 11:00:32,495 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,486 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,492 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,540 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:32,515 : INFO : built Dictionary(32 unique tokens: ['also', 'anyone', 'day', 'dubai', 'else']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:32,543 : INFO : WCD\n",
      "2019-06-17 11:00:32,557 : INFO : 0.0\n",
      "2019-06-17 11:00:32,590 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,590 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:32,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,612 : INFO : built Dictionary(47 unique tokens: ['affidavit', 'anybody', 'bank', 'citizen', 'consul']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:00:32,615 : INFO : WCD\n",
      "2019-06-17 11:00:32,623 : INFO : 0.0\n",
      "2019-06-17 11:00:32,642 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,629 : INFO : [(-20.879165649414062, 4), (-20.013179779052734, 7), (-19.777036666870117, 8), (-19.601316452026367, 5), (-17.886882781982422, 1), (-18.925846099853516, 0), (-19.38395881652832, 6), (-18.354297637939453, 2), (-18.212135314941406, 9), (-16.6771183013916, 3)]\n",
      "2019-06-17 11:00:32,648 : INFO : 0.1\n",
      "2019-06-17 11:00:32,661 : INFO : P&P\n",
      "2019-06-17 11:00:32,679 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,688 : INFO : [(-18.080730438232422, 8), (-16.670108795166016, 7), (-16.81732177734375, 6), (-16.34855079650879, 9), (-15.838838577270508, 5), (-15.221599578857422, 3), (-14.975289344787598, 1), (-14.735886573791504, 4), (-14.203593254089355, 2), (-15.477485656738281, 0)]\n",
      "2019-06-17 11:00:32,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,734 : INFO : built Dictionary(32 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:32,732 : INFO : 0.1\n",
      "2019-06-17 11:00:32,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,759 : INFO : P&P\n",
      "2019-06-17 11:00:32,763 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,770 : INFO : Vocabulary size: 32 500\n",
      "2019-06-17 11:00:32,785 : INFO : WCD\n",
      "2019-06-17 11:00:32,771 : INFO : built Dictionary(29 unique tokens: ['accomodation', 'company', 'cristian', 'doha', 'engineer']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:32,796 : INFO : 0.0\n",
      "2019-06-17 11:00:32,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,812 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:32,834 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,823 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,854 : INFO : WCD\n",
      "2019-06-17 11:00:32,857 : INFO : built Dictionary(46 unique tokens: ['advise', 'allowance', 'ashghal', 'doha', 'enough']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:32,845 : INFO : built Dictionary(20 unique tokens: ['agencies', 'everybody', 'hi', 'know', 'qatar']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:32,859 : INFO : 0.0\n",
      "2019-06-17 11:00:32,867 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,865 : INFO : First K WMD\n",
      "2019-06-17 11:00:32,885 : INFO : built Dictionary(48 unique tokens: ['anybody', 'around', 'called', 'come', 'company']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:32,903 : INFO : [(-19.180747985839844, 4), (-19.161819458007812, 7), (-18.23743438720703, 8), (-16.87937355041504, 1), (-18.217662811279297, 6), (-15.245537757873535, 9), (-16.962942123413086, 2), (-16.20942497253418, 5), (-15.413349151611328, 3), (-0.0, 0)]\n",
      "2019-06-17 11:00:32,907 : INFO : [(-22.97454071044922, 8), (-21.440460205078125, 7), (-19.91413116455078, 6), (-20.932754516601562, 9), (-15.500402450561523, 2), (-13.398838996887207, 1), (-19.09998893737793, 4), (-16.108665466308594, 0), (-18.564666748046875, 5), (-13.41885757446289, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:32,929 : INFO : 0.0\n",
      "2019-06-17 11:00:32,920 : INFO : 0.1\n",
      "2019-06-17 11:00:32,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:32,943 : INFO : P&P\n",
      "2019-06-17 11:00:32,946 : INFO : P&P\n",
      "2019-06-17 11:00:32,957 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,956 : INFO : built Dictionary(43 unique tokens: ['companies', 'dolphin', 'energy', 'engineer', 'etc']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:32,967 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:32,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,017 : INFO : built Dictionary(23 unique tokens: ['anyone', 'city', 'come', 'compare', 'open']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:33,014 : INFO : Vocabulary size: 8 500\n",
      "2019-06-17 11:00:33,038 : INFO : WCD\n",
      "2019-06-17 11:00:33,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,087 : INFO : built Dictionary(38 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:33,118 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,102 : INFO : 0.0\n",
      "2019-06-17 11:00:33,134 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,141 : INFO : built Dictionary(40 unique tokens: ['aircraft', 'airways', 'anyone', 'called', 'engineer']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:33,144 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:33,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,166 : INFO : WCD\n",
      "2019-06-17 11:00:33,175 : INFO : [(-22.233287811279297, 8), (-21.784374237060547, 5), (-20.658559799194336, 3), (-20.910507202148438, 6), (-20.607698440551758, 2), (-16.963151931762695, 4), (-16.65375328063965, 9), (-12.143669128417969, 1), (-19.722387313842773, 7), (-12.151820182800293, 0)]\n",
      "2019-06-17 11:00:33,182 : INFO : built Dictionary(31 unique tokens: ['drive', 'dubai', 'fun', 'get', 'go']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:33,195 : INFO : 0.0\n",
      "2019-06-17 11:00:33,188 : INFO : 0.0\n",
      "2019-06-17 11:00:33,225 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,248 : INFO : built Dictionary(45 unique tokens: ['advance', 'benefits', 'company', 'engineer', 'everyone']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:33,218 : INFO : P&P\n",
      "2019-06-17 11:00:33,283 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,300 : INFO : [(-22.007612228393555, 1), (-21.19927406311035, 3), (-21.07072639465332, 6), (-19.662084579467773, 7), (-19.85196304321289, 5), (-18.490304946899414, 4), (-20.543115615844727, 0), (-19.420085906982422, 8), (-19.2573299407959, 9), (-19.482419967651367, 2)]\n",
      "2019-06-17 11:00:33,312 : INFO : 0.1\n",
      "2019-06-17 11:00:33,326 : INFO : P&P\n",
      "2019-06-17 11:00:33,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,338 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,320 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,351 : INFO : built Dictionary(50 unique tokens: ['advance', 'airways', 'answered', 'anybody', 'apply']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:33,350 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:33,376 : INFO : built Dictionary(54 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:00:33,425 : INFO : WCD\n",
      "2019-06-17 11:00:33,447 : INFO : Vocabulary size: 24 500\n",
      "2019-06-17 11:00:33,447 : INFO : 0.0\n",
      "2019-06-17 11:00:33,455 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,470 : INFO : WCD\n",
      "2019-06-17 11:00:33,478 : INFO : [(-20.295320510864258, 6), (-19.63300323486328, 5), (-20.22649574279785, 9), (-18.950098037719727, 4), (-19.31886863708496, 8), (-19.00516128540039, 7), (-20.127485275268555, 3), (-15.96928596496582, 0), (-18.376943588256836, 2), (-14.258912086486816, 1)]\n",
      "2019-06-17 11:00:33,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,481 : INFO : 0.0\n",
      "2019-06-17 11:00:33,485 : INFO : built Dictionary(34 unique tokens: ['drake', 'hear', 'one', 'scull', 'stable']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:33,499 : INFO : 0.0\n",
      "2019-06-17 11:00:33,496 : INFO : built Dictionary(24 unique tokens: ['airline', 'dubai', 'fly', 'know', 'let']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:33,507 : INFO : P&P\n",
      "2019-06-17 11:00:33,499 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,525 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:33,526 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,547 : INFO : [(-20.79355239868164, 7), (-20.671812057495117, 6), (-19.848337173461914, 9), (-19.950206756591797, 3), (-19.51067543029785, 5), (-18.237428665161133, 1), (-19.728437423706055, 8), (-15.822040557861328, 0), (-17.829906463623047, 2), (-0.0, 4)]\n",
      "2019-06-17 11:00:33,554 : INFO : built Dictionary(51 unique tokens: ['advance', 'amount', 'company', 'dear', 'excluding']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:33,559 : INFO : 0.0\n",
      "2019-06-17 11:00:33,602 : INFO : P&P\n",
      "2019-06-17 11:00:33,615 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,669 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:33,673 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:33,694 : INFO : WCD\n",
      "2019-06-17 11:00:33,682 : INFO : WCD\n",
      "2019-06-17 11:00:33,703 : INFO : 0.0\n",
      "2019-06-17 11:00:33,709 : INFO : 0.0\n",
      "2019-06-17 11:00:33,745 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,732 : INFO : First K WMD\n",
      "2019-06-17 11:00:33,762 : INFO : [(-23.576061248779297, 3), (-18.994909286499023, 2), (-20.149356842041016, 1), (-18.084823608398438, 6), (-18.527414321899414, 7), (-16.774330139160156, 0), (-19.62653160095215, 4), (-15.04141616821289, 9), (-17.703624725341797, 8), (-15.920472145080566, 5)]\n",
      "2019-06-17 11:00:33,779 : INFO : [(-20.58319854736328, 8), (-20.03719139099121, 7), (-19.538331985473633, 2), (-19.237489700317383, 3), (-19.334184646606445, 5), (-15.136723518371582, 1), (-18.533306121826172, 4), (-17.901466369628906, 6), (-16.575166702270508, 0), (-17.849380493164062, 9)]\n",
      "2019-06-17 11:00:33,793 : INFO : 0.0\n",
      "2019-06-17 11:00:33,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,790 : INFO : 0.0\n",
      "2019-06-17 11:00:33,806 : INFO : P&P\n",
      "2019-06-17 11:00:33,806 : INFO : built Dictionary(47 unique tokens: ['airways', 'comments', 'dear', 'doha', 'find']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:33,798 : INFO : P&P\n",
      "2019-06-17 11:00:33,828 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,832 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:33,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:33,946 : INFO : built Dictionary(61 unique tokens: ['another', 'average', 'basic', 'benefits', 'change']...) from 2 documents (total 79 corpus positions)\n",
      "2019-06-17 11:00:33,940 : INFO : Vocabulary size: 10 500\n",
      "2019-06-17 11:00:33,975 : INFO : WCD\n",
      "2019-06-17 11:00:33,981 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:33,995 : INFO : WCD\n",
      "2019-06-17 11:00:33,987 : INFO : 0.0\n",
      "2019-06-17 11:00:34,001 : INFO : 0.0\n",
      "2019-06-17 11:00:34,013 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,011 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,037 : INFO : [(-19.450517654418945, 4), (-19.081039428710938, 7), (-18.029870986938477, 6), (-17.9783878326416, 5), (-16.567235946655273, 3), (-16.557945251464844, 2), (-17.975421905517578, 9), (-16.490108489990234, 1), (-16.365474700927734, 8), (-0.0, 0)]\n",
      "2019-06-17 11:00:34,056 : INFO : 0.0\n",
      "2019-06-17 11:00:34,050 : INFO : [(-21.43976593017578, 1), (-20.52419090270996, 3), (-20.118268966674805, 7), (-20.430696487426758, 6), (-19.727731704711914, 8), (-19.106203079223633, 9), (-18.14591407775879, 5), (-17.760683059692383, 2), (-18.439157485961914, 4), (-18.366701126098633, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:34,079 : INFO : P&P\n",
      "2019-06-17 11:00:34,064 : INFO : 0.0\n",
      "2019-06-17 11:00:34,109 : INFO : P&P\n",
      "2019-06-17 11:00:34,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,091 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,130 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,132 : INFO : built Dictionary(16 unique tokens: ['assuming', 'budget', 'enough', 'food', 'month']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:34,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,186 : INFO : built Dictionary(25 unique tokens: ['assist', 'confusing', 'contact', 'delivery', 'doha']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:34,221 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:34,214 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:34,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,237 : INFO : built Dictionary(16 unique tokens: ['afican', 'african', 'doha', 'eating', 'food']...) from 2 documents (total 20 corpus positions)\n",
      "2019-06-17 11:00:34,239 : INFO : WCD\n",
      "2019-06-17 11:00:34,262 : INFO : 0.0\n",
      "2019-06-17 11:00:34,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,250 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:34,268 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,279 : INFO : WCD\n",
      "2019-06-17 11:00:34,284 : INFO : built Dictionary(10 unique tokens: ['best', 'chinese', 'doha', 'food', 'go']...) from 2 documents (total 11 corpus positions)\n",
      "2019-06-17 11:00:34,291 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:34,295 : INFO : 0.0\n",
      "2019-06-17 11:00:34,305 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,324 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,346 : INFO : built Dictionary(20 unique tokens: ['also', 'appreciate', 'cooking', 'doha', 'find']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:34,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,360 : INFO : built Dictionary(18 unique tokens: ['ali', 'arabic', 'dates', 'discriminate', 'favourites']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:34,338 : INFO : [(-18.15898323059082, 7), (-18.002897262573242, 3), (-16.057205200195312, 6), (-17.622173309326172, 4), (-16.408206939697266, 2), (-15.448098182678223, 8), (-15.1015625, 1), (-15.41435718536377, 5), (-17.120269775390625, 9), (-0.0, 0)]\n",
      "2019-06-17 11:00:34,366 : INFO : 0.1\n",
      "2019-06-17 11:00:34,379 : INFO : [(-20.192272186279297, 7), (-19.46892547607422, 6), (-18.489145278930664, 9), (-18.166622161865234, 5), (-16.937883377075195, 0), (-17.94074249267578, 2), (-18.25009536743164, 4), (-18.041271209716797, 8), (-16.206748962402344, 1), (-15.568445205688477, 3)]\n",
      "2019-06-17 11:00:34,372 : INFO : P&P\n",
      "2019-06-17 11:00:34,397 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,397 : INFO : 0.1\n",
      "2019-06-17 11:00:34,414 : INFO : P&P\n",
      "2019-06-17 11:00:34,403 : INFO : built Dictionary(15 unique tokens: ['available', 'best', 'cost', 'doha', 'food']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:34,430 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,461 : INFO : built Dictionary(34 unique tokens: ['acids', 'advice', 'amino', 'anyone', 'bodybuilding']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:34,474 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,475 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:34,488 : INFO : Vocabulary size: 16 500\n",
      "2019-06-17 11:00:34,493 : INFO : WCD\n",
      "2019-06-17 11:00:34,497 : INFO : 0.0\n",
      "2019-06-17 11:00:34,499 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,490 : INFO : WCD\n",
      "2019-06-17 11:00:34,491 : INFO : built Dictionary(51 unique tokens: ['able', 'anchovies', 'answers', 'apart', 'bear']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:34,513 : INFO : 0.0\n",
      "2019-06-17 11:00:34,539 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,565 : INFO : [(-20.412973403930664, 2), (-19.5238037109375, 9), (-20.271774291992188, 7), (-18.71141815185547, 3), (-19.249799728393555, 6), (-18.591630935668945, 5), (-17.961729049682617, 0), (-18.10434913635254, 1), (-18.14888572692871, 4), (-17.337181091308594, 8)]\n",
      "2019-06-17 11:00:34,559 : INFO : [(-20.01292610168457, 6), (-19.246118545532227, 5), (-18.699932098388672, 7), (-16.427471160888672, 0), (-18.362825393676758, 2), (-18.555654525756836, 8), (-14.655370712280273, 4), (-16.09160041809082, 3), (-16.41191291809082, 1), (-15.264312744140625, 9)]\n",
      "2019-06-17 11:00:34,577 : INFO : 0.0\n",
      "2019-06-17 11:00:34,582 : INFO : 0.1\n",
      "2019-06-17 11:00:34,588 : INFO : P&P\n",
      "2019-06-17 11:00:34,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,600 : INFO : P&P\n",
      "2019-06-17 11:00:34,601 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,602 : INFO : built Dictionary(42 unique tokens: ['able', 'also', 'answer', 'asked', 'baby']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:34,612 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,662 : INFO : Vocabulary size: 20 500\n",
      "2019-06-17 11:00:34,663 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:34,674 : INFO : WCD\n",
      "2019-06-17 11:00:34,696 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:34,724 : INFO : 0.0\n",
      "2019-06-17 11:00:34,740 : INFO : WCD\n",
      "2019-06-17 11:00:34,738 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,766 : INFO : 0.0\n",
      "2019-06-17 11:00:34,785 : INFO : First K WMD\n",
      "2019-06-17 11:00:34,775 : INFO : [(-21.972305297851562, 9), (-19.445903778076172, 1), (-20.091201782226562, 7), (-18.83397674560547, 4), (-19.0318603515625, 3), (-18.150190353393555, 8), (-19.299009323120117, 5), (-17.99772071838379, 6), (-0.0, 0), (-18.651206970214844, 2)]\n",
      "2019-06-17 11:00:34,796 : INFO : 0.0\n",
      "2019-06-17 11:00:34,814 : INFO : P&P\n",
      "2019-06-17 11:00:34,814 : INFO : [(-21.42551612854004, 2), (-21.140594482421875, 4), (-19.901439666748047, 8), (-20.594717025756836, 3), (-20.422121047973633, 7), (-18.116193771362305, 0), (-19.37249755859375, 6), (-18.77257537841797, 5), (-19.99769401550293, 9), (-19.709524154663086, 1)]\n",
      "2019-06-17 11:00:34,838 : INFO : 0.0\n",
      "2019-06-17 11:00:34,832 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,852 : INFO : P&P\n",
      "2019-06-17 11:00:34,869 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:34,910 : INFO : Vocabulary size: 27 500\n",
      "2019-06-17 11:00:34,935 : INFO : WCD\n",
      "2019-06-17 11:00:34,929 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:34,956 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:34,971 : INFO : 0.0\n",
      "2019-06-17 11:00:34,966 : INFO : WCD\n",
      "2019-06-17 11:00:34,978 : INFO : built Dictionary(20 unique tokens: ['change', 'climate', 'conference', 'cop', 'details']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:34,983 : INFO : 0.0\n",
      "2019-06-17 11:00:34,987 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,002 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,002 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,029 : INFO : built Dictionary(23 unique tokens: ['arrested', 'campaigners', 'climate', 'copenhagen', 'dead']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:35,053 : INFO : [(-20.496992111206055, 7), (-20.00928497314453, 2), (-19.1842041015625, 8), (-19.98302459716797, 5), (-18.43427848815918, 3), (-17.037803649902344, 6), (-18.044445037841797, 9), (-17.45364761352539, 0), (-18.161466598510742, 4), (-16.765296936035156, 1)]\n",
      "2019-06-17 11:00:35,047 : INFO : [(-20.217315673828125, 8), (-20.136913299560547, 7), (-19.348432540893555, 4), (-20.0974063873291, 6), (-18.35012435913086, 5), (-18.308691024780273, 3), (-18.69338607788086, 1), (-18.09669303894043, 2), (-18.769285202026367, 9), (-0.0, 0)]\n",
      "2019-06-17 11:00:35,056 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,067 : INFO : 0.0\n",
      "2019-06-17 11:00:35,071 : INFO : built Dictionary(48 unique tokens: ['able', 'arrive', 'besides', 'busy', 'concerned']...) from 2 documents (total 56 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:35,077 : INFO : P&P\n",
      "2019-06-17 11:00:35,067 : INFO : 0.1\n",
      "2019-06-17 11:00:35,105 : INFO : P&P\n",
      "2019-06-17 11:00:35,120 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,111 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,133 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,120 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,151 : INFO : built Dictionary(41 unique tokens: ['application', 'applied', 'approval', 'approved', 'changed']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:35,157 : INFO : built Dictionary(27 unique tokens: ['ask', 'authorities', 'caught', 'couple', 'happen']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:35,166 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:35,191 : INFO : Vocabulary size: 17 500\n",
      "2019-06-17 11:00:35,192 : INFO : WCD\n",
      "2019-06-17 11:00:35,198 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,208 : INFO : 0.0\n",
      "2019-06-17 11:00:35,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,224 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,233 : INFO : built Dictionary(22 unique tokens: ['absolutely', 'back', 'everyone', 'go', 'gr']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:35,216 : INFO : WCD\n",
      "2019-06-17 11:00:35,251 : INFO : 0.0\n",
      "2019-06-17 11:00:35,251 : INFO : [(-20.12743377685547, 9), (-18.46717071533203, 6), (-18.671781539916992, 7), (-17.12271499633789, 4), (-17.668659210205078, 1), (-17.568227767944336, 2), (-17.15428924560547, 8), (-15.747174263000488, 5), (-0.0, 0), (-16.206281661987305, 3)]\n",
      "2019-06-17 11:00:35,257 : INFO : 0.0\n",
      "2019-06-17 11:00:35,261 : INFO : Removed 32 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,256 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,273 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,263 : INFO : P&P\n",
      "2019-06-17 11:00:35,273 : INFO : built Dictionary(46 unique tokens: ['anybody', 'application', 'changes', 'ect', 'experience']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:35,286 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,290 : INFO : built Dictionary(29 unique tokens: ['cooperation', 'english', 'forum', 'group', 'lay']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:35,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,315 : INFO : built Dictionary(24 unique tokens: ['anyone', 'child', 'feedback', 'house', 'like']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:35,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,334 : INFO : built Dictionary(23 unique tokens: ['allowances', 'choose', 'country', 'guys', 'looking']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:35,289 : INFO : [(-20.816585540771484, 8), (-18.787384033203125, 9), (-17.63273811340332, 0), (-17.24789810180664, 7), (-18.65730094909668, 3), (-15.876364707946777, 1), (-15.174938201904297, 5), (-16.36451530456543, 2), (-14.644176483154297, 4), (-17.744613647460938, 6)]\n",
      "2019-06-17 11:00:35,345 : INFO : 0.1\n",
      "2019-06-17 11:00:35,355 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:35,347 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,351 : INFO : P&P\n",
      "2019-06-17 11:00:35,363 : INFO : built Dictionary(38 unique tokens: ['afternoon', 'anyone', 'bus', 'child', 'children']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:35,402 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,364 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,387 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,394 : INFO : WCD\n",
      "2019-06-17 11:00:35,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,419 : INFO : 0.0\n",
      "2019-06-17 11:00:35,417 : INFO : built Dictionary(31 unique tokens: ['allowed', 'american', 'boyfriend', 'coming', 'doha']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:35,433 : INFO : built Dictionary(30 unique tokens: ['andi', 'anyone', 'application', 'ask', 'captain']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:35,453 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,444 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:35,463 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,474 : INFO : [(-20.87978172302246, 7), (-20.51575469970703, 3), (-20.86862564086914, 9), (-19.08833122253418, 4), (-20.039094924926758, 1), (-16.549318313598633, 0), (-18.61134147644043, 8), (-18.56856346130371, 6), (-17.65003776550293, 2), (-17.57465934753418, 5)]\n",
      "2019-06-17 11:00:35,481 : INFO : 0.0\n",
      "2019-06-17 11:00:35,470 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,482 : INFO : Vocabulary size: 8 500\n",
      "2019-06-17 11:00:35,485 : INFO : P&P\n",
      "2019-06-17 11:00:35,497 : INFO : WCD\n",
      "2019-06-17 11:00:35,494 : INFO : built Dictionary(52 unique tokens: ['accept', 'accepted', 'advance', 'advise', 'application']...) from 2 documents (total 79 corpus positions)\n",
      "2019-06-17 11:00:35,509 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,525 : INFO : 0.0\n",
      "2019-06-17 11:00:35,555 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,583 : INFO : [(-21.688377380371094, 4), (-21.14757537841797, 1), (-21.547412872314453, 0), (-19.63936424255371, 5), (-20.30988311767578, 9), (-20.919878005981445, 3), (-19.8194522857666, 8), (-17.768611907958984, 6), (-18.275617599487305, 7), (-20.218931198120117, 2)]\n",
      "2019-06-17 11:00:35,608 : INFO : 0.0\n",
      "2019-06-17 11:00:35,626 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,633 : INFO : P&P\n",
      "2019-06-17 11:00:35,646 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,663 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,667 : INFO : built Dictionary(31 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:35,694 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,696 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,715 : INFO : Vocabulary size: 8 500\n",
      "2019-06-17 11:00:35,737 : INFO : WCD\n",
      "2019-06-17 11:00:35,741 : INFO : built Dictionary(45 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:35,750 : INFO : 0.0\n",
      "2019-06-17 11:00:35,773 : INFO : First K WMD\n",
      "2019-06-17 11:00:35,783 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,801 : INFO : [(-19.445392608642578, 9), (-18.134946823120117, 5), (-18.296350479125977, 8), (-17.199392318725586, 1), (-17.12928581237793, 2), (-11.231815338134766, 0), (-16.944690704345703, 4), (-16.572744369506836, 6), (-15.609818458557129, 3), (-15.289205551147461, 7)]\n",
      "2019-06-17 11:00:35,816 : INFO : 0.0\n",
      "2019-06-17 11:00:35,798 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:35,833 : INFO : P&P\n",
      "2019-06-17 11:00:35,841 : INFO : built Dictionary(44 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:35,895 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:35,947 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:35,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,000 : INFO : built Dictionary(45 unique tokens: ['application', 'applied', 'b', 'certificate', 'counter']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:36,004 : INFO : Vocabulary size: 11 500\n",
      "2019-06-17 11:00:36,029 : INFO : WCD\n",
      "2019-06-17 11:00:36,041 : INFO : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:36,051 : INFO : First K WMD\n",
      "2019-06-17 11:00:36,082 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:36,067 : INFO : [(-20.949689865112305, 6), (-19.396881103515625, 8), (-19.628931045532227, 4), (-19.34319305419922, 5), (-17.90311622619629, 7), (-16.35881233215332, 0), (-19.283071517944336, 3), (-15.986547470092773, 2), (-18.581565856933594, 9), (-16.03264045715332, 1)]\n",
      "2019-06-17 11:00:36,106 : INFO : 0.0\n",
      "2019-06-17 11:00:36,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,113 : INFO : P&P\n",
      "2019-06-17 11:00:36,122 : INFO : built Dictionary(26 unique tokens: ['cost', 'document', 'family', 'hi', 'much']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:36,129 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:36,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,164 : INFO : built Dictionary(16 unique tokens: ['accommodation', 'airways', 'allowance', 'british', 'grade']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:36,159 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:36,175 : INFO : Vocabulary size: 7 500\n",
      "2019-06-17 11:00:36,174 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,181 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,186 : INFO : WCD\n",
      "2019-06-17 11:00:36,189 : INFO : built Dictionary(30 unique tokens: ['airways', 'basic', 'benefits', 'could', 'figures']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:36,206 : INFO : 0.0\n",
      "2019-06-17 11:00:36,189 : INFO : built Dictionary(41 unique tokens: ['applied', 'applying', 'ask', 'certificate', 'change']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:36,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,258 : INFO : built Dictionary(44 unique tokens: ['airways', 'allowance', 'appreciate', 'basic', 'btw']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:36,267 : INFO : First K WMD\n",
      "2019-06-17 11:00:36,284 : INFO : [(-19.167402267456055, 1), (-19.1055908203125, 6), (-17.88391876220703, 5), (-18.9915828704834, 3), (-17.738367080688477, 7), (-15.83198356628418, 0), (-17.41509437561035, 2), (-16.774913787841797, 4), (-14.511051177978516, 9), (-17.28350257873535, 8)]\n",
      "2019-06-17 11:00:36,309 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:36,310 : INFO : 0.0\n",
      "2019-06-17 11:00:36,336 : INFO : P&P\n",
      "2019-06-17 11:00:36,342 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:36,364 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,380 : INFO : built Dictionary(39 unique tokens: ['airways', 'appalling', 'convince', 'course', 'day']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:36,422 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:36,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,437 : INFO : built Dictionary(35 unique tokens: ['able', 'airlines', 'airways', 'anyone', 'benefits']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:36,446 : INFO : WCD\n",
      "2019-06-17 11:00:36,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,509 : INFO : 0.0\n",
      "2019-06-17 11:00:36,520 : INFO : First K WMD\n",
      "2019-06-17 11:00:36,528 : INFO : built Dictionary(29 unique tokens: ['advise', 'airways', 'ground', 'guys', 'hi']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:36,560 : INFO : [(-21.731332778930664, 9), (-20.71062660217285, 5), (-20.011201858520508, 3), (-20.441579818725586, 0), (-19.973024368286133, 6), (-19.774911880493164, 7), (-19.786239624023438, 2), (-20.43289566040039, 4), (-19.005836486816406, 8), (-18.6771297454834, 1)]\n",
      "2019-06-17 11:00:36,565 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,586 : INFO : 0.1\n",
      "2019-06-17 11:00:36,593 : INFO : built Dictionary(34 unique tokens: ['airways', 'comments', 'dear', 'doha', 'find']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:36,617 : INFO : P&P\n",
      "2019-06-17 11:00:36,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,640 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:36,685 : INFO : built Dictionary(22 unique tokens: ['application', 'credential', 'hmc', 'right', 'stage']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:36,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,744 : INFO : built Dictionary(45 unique tokens: ['anything', 'apply', 'away', 'bank', 'best']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:36,797 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:36,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:36,854 : INFO : built Dictionary(49 unique tokens: ['advise', 'airways', 'anybody', 'british', 'children']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:36,894 : INFO : WCD\n",
      "2019-06-17 11:00:36,913 : INFO : 0.0\n",
      "2019-06-17 11:00:36,935 : INFO : First K WMD\n",
      "2019-06-17 11:00:36,942 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:36,975 : INFO : [(-20.481361389160156, 7), (-19.7364559173584, 3), (-19.929903030395508, 9), (-18.935733795166016, 0), (-19.586103439331055, 5), (-19.302993774414062, 6), (-17.773727416992188, 4), (-17.244417190551758, 1), (-17.52622413635254, 2), (-18.34647560119629, 8)]\n",
      "2019-06-17 11:00:37,031 : INFO : 0.1\n",
      "2019-06-17 11:00:37,051 : INFO : P&P\n",
      "2019-06-17 11:00:37,066 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:37,154 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:37,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,175 : INFO : built Dictionary(21 unique tokens: ['anybody', 'currently', 'doha', 'extend', 'futher']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:37,175 : INFO : WCD\n",
      "2019-06-17 11:00:37,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,194 : INFO : 0.0\n",
      "2019-06-17 11:00:37,211 : INFO : First K WMD\n",
      "2019-06-17 11:00:37,212 : INFO : built Dictionary(32 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:37,251 : INFO : [(-21.87194061279297, 6), (-21.52432632446289, 8), (-20.98164939880371, 4), (-21.176498413085938, 3), (-20.657711029052734, 1), (-19.16114044189453, 2), (-19.983736038208008, 5), (-21.009477615356445, 9), (-18.037687301635742, 0), (-20.454893112182617, 7)]\n",
      "2019-06-17 11:00:37,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,273 : INFO : 0.0\n",
      "2019-06-17 11:00:37,281 : INFO : P&P\n",
      "2019-06-17 11:00:37,275 : INFO : built Dictionary(33 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:37,290 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:37,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,328 : INFO : built Dictionary(16 unique tokens: ['business', 'driving', 'family', 'indian', 'license']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:37,345 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,356 : INFO : built Dictionary(18 unique tokens: ['expiration', 'going', 'help', 'husband', 'husbands']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:37,377 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,383 : INFO : built Dictionary(44 unique tokens: ['apply', 'called', 'card', 'company', 'cost']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:37,381 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:37,401 : INFO : WCD\n",
      "2019-06-17 11:00:37,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,420 : INFO : built Dictionary(22 unique tokens: ['baby', 'born', 'bring', 'dear', 'help']...) from 2 documents (total 31 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:37,412 : INFO : 0.0\n",
      "2019-06-17 11:00:37,434 : INFO : First K WMD\n",
      "2019-06-17 11:00:37,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,475 : INFO : [(-19.762784957885742, 3), (-19.206506729125977, 4), (-19.526235580444336, 1), (-19.05494499206543, 8), (-17.926799774169922, 7), (-18.369455337524414, 2), (-17.78753089904785, 5), (-17.48784065246582, 6), (-17.665884017944336, 0), (-16.571170806884766, 9)]\n",
      "2019-06-17 11:00:37,483 : INFO : 0.0\n",
      "2019-06-17 11:00:37,480 : INFO : built Dictionary(20 unique tokens: ['embassy', 'experiences', 'go', 'loved', 'ones']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:37,496 : INFO : P&P\n",
      "2019-06-17 11:00:37,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,517 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:37,524 : INFO : built Dictionary(24 unique tokens: ['backward', 'get', 'intervals', 'muslims', 'nobel']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:37,519 : INFO : built Dictionary(21 unique tokens: ['baby', 'best', 'childbirth', 'deliver', 'doctors']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:00:37,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,552 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,566 : INFO : built Dictionary(16 unique tokens: ['extend', 'extension', 'hotel', 'month', 'one']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:37,572 : INFO : built Dictionary(20 unique tokens: ['attitude', 'audio', 'block', 'conversation', 'crap']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:00:37,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,593 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:37,604 : INFO : built Dictionary(43 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:37,588 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:37,626 : INFO : WCD\n",
      "2019-06-17 11:00:37,643 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,654 : INFO : 0.0\n",
      "2019-06-17 11:00:37,655 : INFO : built Dictionary(42 unique tokens: ['advice', 'buy', 'buying', 'clothes', 'comments']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:37,681 : INFO : First K WMD\n",
      "2019-06-17 11:00:37,706 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,720 : INFO : [(-21.050922393798828, 9), (-20.895610809326172, 2), (-19.826833724975586, 5), (-19.209794998168945, 7), (-20.10151481628418, 6), (-18.33381462097168, 3), (-19.04155731201172, 4), (-14.180174827575684, 1), (-18.471216201782227, 8), (-16.98177719116211, 0)]\n",
      "2019-06-17 11:00:37,742 : INFO : built Dictionary(26 unique tokens: ['advise', 'anyone', 'bring', 'bringing', 'confiscated']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:37,743 : INFO : 0.0\n",
      "2019-06-17 11:00:37,759 : INFO : P&P\n",
      "2019-06-17 11:00:37,768 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:37,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,809 : INFO : built Dictionary(41 unique tokens: ['advise', 'already', 'appreciated', 'banned', 'country']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:37,849 : INFO : Vocabulary size: 9 500\n",
      "2019-06-17 11:00:37,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,894 : INFO : WCD\n",
      "2019-06-17 11:00:37,894 : INFO : built Dictionary(38 unique tokens: ['advice', 'anyone', 'appreciated', 'back', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:37,907 : INFO : 0.0\n",
      "2019-06-17 11:00:37,917 : INFO : First K WMD\n",
      "2019-06-17 11:00:37,932 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,945 : INFO : built Dictionary(20 unique tokens: ['anyone', 'doha', 'done', 'give', 'lasik']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:37,953 : INFO : [(-20.403234481811523, 7), (-19.119047164916992, 6), (-19.638460159301758, 3), (-18.922842025756836, 8), (-17.707704544067383, 0), (-17.156421661376953, 4), (-18.131568908691406, 2), (-18.605148315429688, 1), (-14.9379301071167, 5), (-17.35120964050293, 9)]\n",
      "2019-06-17 11:00:37,956 : INFO : 0.0\n",
      "2019-06-17 11:00:37,972 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:37,981 : INFO : P&P\n",
      "2019-06-17 11:00:37,987 : INFO : built Dictionary(23 unique tokens: ['burke', 'edmund', 'evident', 'evil', 'good']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:37,992 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:38,019 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,053 : INFO : built Dictionary(47 unique tokens: ['alone', 'atheletes', 'built', 'city', 'closed']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:38,070 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:38,113 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:38,115 : INFO : WCD\n",
      "2019-06-17 11:00:38,154 : INFO : 0.0\n",
      "2019-06-17 11:00:38,177 : INFO : First K WMD\n",
      "2019-06-17 11:00:38,199 : INFO : [(-24.28228187561035, 6), (-21.832162857055664, 2), (-21.999034881591797, 1), (-21.730714797973633, 3), (-20.52125358581543, 8), (-20.101045608520508, 4), (-21.892850875854492, 7), (-20.590497970581055, 9), (-21.15671730041504, 5), (-18.731416702270508, 0)]\n",
      "2019-06-17 11:00:38,228 : INFO : 0.0\n",
      "2019-06-17 11:00:38,240 : INFO : P&P\n",
      "2019-06-17 11:00:38,267 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:38,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,341 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:38,353 : INFO : built Dictionary(22 unique tokens: ['abt', 'bike', 'buy', 'doha', 'get']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:38,365 : INFO : WCD\n",
      "2019-06-17 11:00:38,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,386 : INFO : 0.0\n",
      "2019-06-17 11:00:38,401 : INFO : built Dictionary(43 unique tokens: ['around', 'arriving', 'buy', 'bycicle', 'car']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:38,400 : INFO : First K WMD\n",
      "2019-06-17 11:00:38,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,459 : INFO : [(-17.764511108398438, 6), (-17.709978103637695, 8), (-15.994205474853516, 0), (-16.619455337524414, 9), (-15.931408882141113, 2), (-14.984450340270996, 1), (-13.635215759277344, 7), (-14.509904861450195, 5), (-15.160906791687012, 3), (-14.881619453430176, 4)]\n",
      "2019-06-17 11:00:38,462 : INFO : 0.0\n",
      "2019-06-17 11:00:38,461 : INFO : built Dictionary(23 unique tokens: ['also', 'costs', 'cycle', 'cycles', 'doha']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:38,498 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,484 : INFO : P&P\n",
      "2019-06-17 11:00:38,507 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,505 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:38,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,563 : INFO : built Dictionary(31 unique tokens: ['anybody', 'attempted', 'bcoz', 'behavior', 'driving']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:38,574 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,591 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,587 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:38,601 : INFO : built Dictionary(55 unique tokens: ['amount', 'anybody', 'asked', 'br', 'calls']...) from 2 documents (total 71 corpus positions)\n",
      "2019-06-17 11:00:38,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:38,620 : INFO : WCD\n",
      "2019-06-17 11:00:38,633 : INFO : built Dictionary(34 unique tokens: ['anybody', 'asd', 'country', 'driving', 'drving']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:38,638 : INFO : 0.0\n",
      "2019-06-17 11:00:38,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,661 : INFO : First K WMD\n",
      "2019-06-17 11:00:38,663 : INFO : built Dictionary(25 unique tokens: ['apply', 'class', 'direct', 'documents', 'driving']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:38,704 : INFO : [(-21.377159118652344, 3), (-21.08753776550293, 6), (-19.832258224487305, 9), (-20.14006233215332, 7), (-19.665321350097656, 5), (-19.657665252685547, 8), (-18.030359268188477, 4), (-18.450210571289062, 0), (-19.70598793029785, 1), (-15.693595886230469, 2)]\n",
      "2019-06-17 11:00:38,685 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,709 : INFO : 0.0\n",
      "2019-06-17 11:00:38,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,717 : INFO : P&P\n",
      "2019-06-17 11:00:38,731 : INFO : built Dictionary(29 unique tokens: ['advance', 'case', 'comments', 'dear', 'doha']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:38,734 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:38,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,802 : INFO : Vocabulary size: 15 500\n",
      "2019-06-17 11:00:38,798 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,798 : INFO : built Dictionary(31 unique tokens: ['back', 'convert', 'driving', 'get', 'getting']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:38,827 : INFO : built Dictionary(30 unique tokens: ['customer', 'number', 'qtel', 'service', 'anyone']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:38,846 : INFO : WCD\n",
      "2019-06-17 11:00:38,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,862 : INFO : 0.0\n",
      "2019-06-17 11:00:38,861 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:38,884 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,885 : INFO : First K WMD\n",
      "2019-06-17 11:00:38,873 : INFO : built Dictionary(20 unique tokens: ['anyone', 'exam', 'giving', 'help', 'information']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:38,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:38,910 : INFO : built Dictionary(34 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:38,897 : INFO : built Dictionary(60 unique tokens: ['advice', 'also', 'basis', 'best', 'billions']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:38,918 : INFO : [(-20.13083839416504, 4), (-19.07439422607422, 5), (-18.750486373901367, 9), (-18.271827697753906, 7), (-18.402454376220703, 6), (-17.867551803588867, 1), (-16.922332763671875, 8), (-15.350641250610352, 0), (-18.01175308227539, 2), (-13.096017837524414, 3)]\n",
      "2019-06-17 11:00:38,935 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:38,949 : INFO : 0.0\n",
      "2019-06-17 11:00:38,974 : INFO : P&P\n",
      "2019-06-17 11:00:39,001 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:39,040 : INFO : Vocabulary size: 19 500\n",
      "2019-06-17 11:00:39,046 : INFO : WCD\n",
      "2019-06-17 11:00:39,055 : INFO : 0.0\n",
      "2019-06-17 11:00:39,059 : INFO : First K WMD\n",
      "2019-06-17 11:00:39,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,066 : INFO : built Dictionary(33 unique tokens: ['affordable', 'calling', 'must', 'qtel', 'rates']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:39,091 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,114 : INFO : built Dictionary(37 unique tokens: ['anything', 'calls', 'dubai', 'eid', 'going']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:39,106 : INFO : [(-20.230304718017578, 0), (-19.479509353637695, 9), (-17.710174560546875, 4), (-18.35860252380371, 3), (-19.444610595703125, 5), (-15.621187210083008, 6), (-15.525400161743164, 7), (-16.300935745239258, 1), (-17.063791275024414, 8), (-19.43621826171875, 2)]\n",
      "2019-06-17 11:00:39,134 : INFO : 0.1\n",
      "2019-06-17 11:00:39,179 : INFO : P&P\n",
      "2019-06-17 11:00:39,190 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:39,165 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,226 : INFO : built Dictionary(56 unique tokens: ['adsl', 'advice', 'ask', 'cable', 'connection']...) from 2 documents (total 73 corpus positions)\n",
      "2019-06-17 11:00:39,290 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:39,330 : INFO : WCD\n",
      "2019-06-17 11:00:39,383 : INFO : 0.0\n",
      "2019-06-17 11:00:39,391 : INFO : First K WMD\n",
      "2019-06-17 11:00:39,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,443 : INFO : [(-17.119346618652344, 8), (-16.076509475708008, 3), (-16.589311599731445, 9), (-16.007556915283203, 5), (-15.58163833618164, 6), (-15.27847671508789, 1), (-14.926919937133789, 4), (-15.67658519744873, 2), (-14.558967590332031, 7), (-15.210012435913086, 0)]\n",
      "2019-06-17 11:00:39,443 : INFO : built Dictionary(54 unique tokens: ['access', 'anyone', 'apn', 'btw', 'called']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:39,485 : INFO : 0.1\n",
      "2019-06-17 11:00:39,507 : INFO : P&P\n",
      "2019-06-17 11:00:39,529 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:39,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,608 : INFO : Vocabulary size: 18 500\n",
      "2019-06-17 11:00:39,596 : INFO : built Dictionary(32 unique tokens: ['asian', 'easy', 'expatriate', 'formalities', 'getting']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:39,625 : INFO : WCD\n",
      "2019-06-17 11:00:39,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,640 : INFO : 0.0\n",
      "2019-06-17 11:00:39,646 : INFO : built Dictionary(30 unique tokens: ['aquiring', 'doha', 'easy', 'everyone', 'get']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:39,654 : INFO : First K WMD\n",
      "2019-06-17 11:00:39,703 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,712 : INFO : built Dictionary(36 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:39,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,723 : INFO : [(-22.838077545166016, 3), (-19.12660026550293, 7), (-20.452198028564453, 8), (-18.19191551208496, 6), (-18.192567825317383, 0), (-18.645906448364258, 4), (-19.086807250976562, 9), (-16.502981185913086, 2), (-18.089052200317383, 1), (-17.613845825195312, 5)]\n",
      "2019-06-17 11:00:39,767 : INFO : 0.1\n",
      "2019-06-17 11:00:39,771 : INFO : P&P\n",
      "2019-06-17 11:00:39,781 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:39,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,762 : INFO : built Dictionary(31 unique tokens: ['anyone', 'broadband', 'internet', 'new', 'qtel']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:39,789 : INFO : built Dictionary(41 unique tokens: ['anybody', 'care', 'change', 'circumstance', 'comes']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:39,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,814 : INFO : built Dictionary(42 unique tokens: ['able', 'august', 'currently', 'hi', 'iphone']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:39,865 : INFO : Vocabulary size: 13 500\n",
      "2019-06-17 11:00:39,872 : INFO : WCD\n",
      "2019-06-17 11:00:39,884 : INFO : 0.0\n",
      "2019-06-17 11:00:39,887 : INFO : First K WMD\n",
      "2019-06-17 11:00:39,886 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,901 : INFO : built Dictionary(40 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:39,910 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:39,927 : INFO : built Dictionary(44 unique tokens: ['anyone', 'connection', 'general', 'get', 'house']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:39,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:39,988 : INFO : built Dictionary(47 unique tokens: ['bank', 'citizens', 'exemption', 'fill', 'forget']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:39,979 : INFO : [(-21.04570770263672, 4), (-20.171222686767578, 8), (-20.69675064086914, 2), (-19.5819034576416, 3), (-18.434484481811523, 6), (-19.40779685974121, 0), (-18.442502975463867, 7), (-18.39457893371582, 5), (-19.11724090576172, 9), (-17.09741973876953, 1)]\n",
      "2019-06-17 11:00:40,013 : INFO : 0.1\n",
      "2019-06-17 11:00:40,016 : INFO : P&P\n",
      "2019-06-17 11:00:40,044 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:40,039 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:40,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,074 : INFO : built Dictionary(53 unique tokens: ['advice', 'also', 'anyone', 'appreciated', 'arrive']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:00:40,106 : INFO : Vocabulary size: 12 500\n",
      "2019-06-17 11:00:40,118 : INFO : WCD\n",
      "2019-06-17 11:00:40,146 : INFO : 0.0\n",
      "2019-06-17 11:00:40,159 : INFO : First K WMD\n",
      "2019-06-17 11:00:40,182 : INFO : [(-19.365873336791992, 8), (-18.663850784301758, 2), (-17.512544631958008, 7), (-18.311302185058594, 5), (-17.288000106811523, 3), (-17.059988021850586, 9), (-14.581818580627441, 6), (-16.77338409423828, 1), (-17.331825256347656, 0), (-15.349908828735352, 4)]\n",
      "2019-06-17 11:00:40,185 : INFO : 0.0\n",
      "2019-06-17 11:00:40,166 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:40,192 : INFO : P&P\n",
      "2019-06-17 11:00:40,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,197 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:40,203 : INFO : built Dictionary(42 unique tokens: ['abroad', 'anyone', 'bank', 'blamed', 'brazilian']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:40,244 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:40,265 : INFO : Vocabulary size: 6 500\n",
      "2019-06-17 11:00:40,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,280 : INFO : built Dictionary(43 unique tokens: ['ago', 'ahmed', 'anyone', 'best', 'friends']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:40,298 : INFO : WCD\n",
      "2019-06-17 11:00:40,308 : INFO : 0.0\n",
      "2019-06-17 11:00:40,310 : INFO : First K WMD\n",
      "2019-06-17 11:00:40,342 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,338 : INFO : [(-20.010656356811523, 6), (-19.918170928955078, 1), (-19.132266998291016, 0), (-19.825809478759766, 3), (-19.092330932617188, 8), (-19.018573760986328, 7), (-16.99293327331543, 2), (-17.552520751953125, 5), (-19.49000358581543, 9), (-16.874366760253906, 4)]\n",
      "2019-06-17 11:00:40,353 : INFO : built Dictionary(42 unique tokens: ['business', 'changes', 'countries', 'current', 'every']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:40,352 : INFO : 0.0\n",
      "2019-06-17 11:00:40,373 : INFO : P&P\n",
      "2019-06-17 11:00:40,408 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:40,405 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:40,482 : INFO : Vocabulary size: 14 500\n",
      "2019-06-17 11:00:40,503 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,525 : INFO : WCD\n",
      "2019-06-17 11:00:40,535 : INFO : built Dictionary(24 unique tokens: ['available', 'clicked', 'email', 'happend', 'hmmmm']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:40,542 : INFO : 0.0\n",
      "2019-06-17 11:00:40,563 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,552 : INFO : First K WMD\n",
      "2019-06-17 11:00:40,568 : INFO : built Dictionary(30 unique tokens: ['fill', 'island', 'palm', 'someone', 'story']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:40,579 : INFO : [(-21.750587463378906, 7), (-20.090139389038086, 4), (-19.499101638793945, 3), (-19.887256622314453, 9), (-19.635231018066406, 5), (-18.89453125, 0), (-19.363637924194336, 6), (-19.32002830505371, 8), (-16.862781524658203, 2), (-16.79352378845215, 1)]\n",
      "2019-06-17 11:00:40,595 : INFO : 0.0\n",
      "2019-06-17 11:00:40,614 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,627 : INFO : P&P\n",
      "2019-06-17 11:00:40,646 : INFO : built Dictionary(59 unique tokens: ['around', 'bundle', 'car', 'card', 'care']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:00:40,648 : INFO : stopped by early_stop condition\n",
      "2019-06-17 11:00:40,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,837 : INFO : built Dictionary(48 unique tokens: ['better', 'description', 'even', 'everything', 'exactly']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:40,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,938 : INFO : built Dictionary(27 unique tokens: ['enough', 'painful', 'qatar', 'available', 'clicked']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:40,945 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,948 : INFO : built Dictionary(37 unique tokens: ['around', 'closed', 'expired', 'garvey', 'got']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:40,968 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:40,987 : INFO : built Dictionary(29 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:40,999 : INFO : built Dictionary(55 unique tokens: ['anyone', 'appropiate', 'behind', 'dirty', 'eastern']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:41,016 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,038 : INFO : built Dictionary(48 unique tokens: ['anyone', 'approval', 'based', 'citizens', 'conditions']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:41,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,116 : INFO : built Dictionary(18 unique tokens: ['convert', 'exit', 'flying', 'need', 'permit']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:41,124 : INFO : built Dictionary(39 unique tokens: ['beware', 'culture', 'gun', 'help', 'increasing']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:41,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,171 : INFO : built Dictionary(33 unique tokens: ['airport', 'also', 'arrange', 'bak', 'changed']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:41,176 : INFO : built Dictionary(51 unique tokens: ['abaya', 'bloody', 'buy', 'considering', 'daft']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:41,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,202 : INFO : built Dictionary(24 unique tokens: ['advice', 'business', 'change', 'convert', 'employer']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:41,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,265 : INFO : built Dictionary(28 unique tokens: ['ago', 'already', 'another', 'apply', 'business']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:41,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,329 : INFO : built Dictionary(26 unique tokens: ['book', 'country', 'day', 'days', 'due']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:41,330 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:41,347 : INFO : built Dictionary(41 unique tokens: ['atleast', 'call', 'could', 'inform', 'know']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:41,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,379 : INFO : built Dictionary(28 unique tokens: ['american', 'bu', 'chance', 'country', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:41,389 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:41,406 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:41,429 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,453 : INFO : built Dictionary(23 unique tokens: ['answers', 'asap', 'business', 'company', 'follow']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:41,480 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,506 : INFO : built Dictionary(20 unique tokens: ['enlighten', 'hi', 'hw', 'kindly', 'kinds']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:41,528 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:41,746 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:41,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,802 : INFO : built Dictionary(30 unique tokens: ['anything', 'appreciate', 'buy', 'dhw', 'doha']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:41,828 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,843 : INFO : built Dictionary(26 unique tokens: ['advance', 'blouse', 'churidar', 'dress', 'dresses']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:41,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,876 : INFO : built Dictionary(21 unique tokens: ['abaya', 'anyone', 'black', 'casual', 'cloaks']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:41,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,917 : INFO : built Dictionary(14 unique tokens: ['advanced', 'coming', 'hi', 'mallus', 'onam']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:00:41,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,958 : INFO : built Dictionary(21 unique tokens: ['al', 'already', 'area', 'deera', 'demolished']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:41,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:41,993 : INFO : built Dictionary(20 unique tokens: ['advise', 'also', 'bombay', 'buy', 'doha']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:00:42,008 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,028 : INFO : built Dictionary(25 unique tokens: ['age', 'anybody', 'code', 'could', 'doha']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:42,071 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:42,076 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:42,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,112 : INFO : built Dictionary(21 unique tokens: ['anyone', 'anything', 'appropriate', 'comfortable', 'especially']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:42,110 : INFO : built Dictionary(41 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:42,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,133 : INFO : built Dictionary(20 unique tokens: ['able', 'anyone', 'brought', 'get', 'gonna']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:00:42,152 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,158 : INFO : built Dictionary(28 unique tokens: ['appreciated', 'august', 'company', 'doha', 'embassy']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:42,174 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:42,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,188 : INFO : built Dictionary(38 unique tokens: ['advises', 'asks', 'brought', 'cesarean', 'checks']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:42,201 : INFO : built Dictionary(43 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:42,220 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:42,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,258 : INFO : built Dictionary(33 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:42,297 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:42,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,323 : INFO : built Dictionary(28 unique tokens: ['b', 'documents', 'doha', 'done', 'ect']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:42,351 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,373 : INFO : built Dictionary(44 unique tokens: ['advance', 'anybody', 'appointment', 'approval', 'ask']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:42,410 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,425 : INFO : built Dictionary(25 unique tokens: ['aquiring', 'doha', 'easy', 'everyone', 'get']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:42,450 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,452 : INFO : built Dictionary(39 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:42,487 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,495 : INFO : built Dictionary(36 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:42,499 : INFO : built Dictionary(23 unique tokens: ['british', 'construction', 'cost', 'currently', 'deal']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:42,526 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,528 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,537 : INFO : built Dictionary(44 unique tokens: ['absolutely', 'anyone', 'asked', 'available', 'car']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:42,551 : INFO : built Dictionary(41 unique tokens: ['already', 'anyone', 'application', 'apply', 'aside']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:42,591 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:42,580 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,620 : INFO : built Dictionary(35 unique tokens: ['allowance', 'company', 'deal', 'doha', 'good']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:42,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,665 : INFO : built Dictionary(32 unique tokens: ['benefit', 'cost', 'deal', 'doha', 'family']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:42,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,704 : INFO : built Dictionary(41 unique tokens: ['anybody', 'anywhere', 'appreciate', 'best', 'carlton']...) from 2 documents (total 48 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:42,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,738 : INFO : built Dictionary(56 unique tokens: ['advance', 'agencies', 'almost', 'anyone', 'best']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:42,809 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,822 : INFO : built Dictionary(48 unique tokens: ['advice', 'also', 'anyone', 'around', 'buying']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:42,885 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,907 : INFO : built Dictionary(39 unique tokens: ['clothes', 'deals', 'doha', 'get', 'good']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:42,930 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,935 : INFO : built Dictionary(22 unique tokens: ['embassy', 'experiences', 'go', 'loved', 'ones']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:42,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:42,961 : INFO : built Dictionary(34 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:42,969 : INFO : built Dictionary(46 unique tokens: ['advices', 'agent', 'auto', 'best', 'brand']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:42,995 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,001 : INFO : built Dictionary(26 unique tokens: ['advice', 'appointment', 'foundation', 'give', 'hired']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:43,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,049 : INFO : built Dictionary(33 unique tokens: ['able', 'apply', 'august', 'away', 'back']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:43,057 : INFO : built Dictionary(45 unique tokens: ['advantage', 'advert', 'ask', 'asking', 'big']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:43,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,085 : INFO : built Dictionary(36 unique tokens: ['advise', 'authentication', 'business', 'days', 'embassy']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:43,113 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,106 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:43,124 : INFO : built Dictionary(45 unique tokens: ['able', 'access', 'account', 'also', 'appreciated']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:43,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,175 : INFO : built Dictionary(38 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:43,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,222 : INFO : built Dictionary(34 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:43,258 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,289 : INFO : built Dictionary(41 unique tokens: ['advise', 'agency', 'already', 'anyone', 'applied']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:43,349 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:43,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,390 : INFO : built Dictionary(47 unique tokens: ['another', 'change', 'co', 'company', 'contract']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:43,453 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,453 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:43,455 : INFO : built Dictionary(18 unique tokens: ['country', 'done', 'enter', 'extension', 'hi']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:43,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,469 : INFO : built Dictionary(30 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:43,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,480 : INFO : built Dictionary(45 unique tokens: ['approximately', 'arriving', 'contract', 'country', 'december']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:43,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,513 : INFO : built Dictionary(40 unique tokens: ['ant', 'application', 'daughter', 'day', 'earning']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:43,530 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,532 : INFO : built Dictionary(29 unique tokens: ['change', 'family', 'husband', 'new', 'sponsor']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:43,545 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,547 : INFO : built Dictionary(48 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:43,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,576 : INFO : built Dictionary(30 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:43,596 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,605 : INFO : built Dictionary(39 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:43,626 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,630 : INFO : built Dictionary(49 unique tokens: ['airport', 'allowance', 'assigned', 'calling', 'co']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:43,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,637 : INFO : built Dictionary(40 unique tokens: ['already', 'application', 'apply', 'coming', 'company']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:43,654 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:43,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,665 : INFO : built Dictionary(31 unique tokens: ['acceptable', 'amounting', 'answer', 'cost', 'dont']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:43,665 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,667 : INFO : built Dictionary(51 unique tokens: ['allow', 'already', 'answers', 'arranged', 'arrival']...) from 2 documents (total 76 corpus positions)\n",
      "2019-06-17 11:00:43,681 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,710 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:43,707 : INFO : built Dictionary(24 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:43,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,757 : INFO : built Dictionary(41 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:43,805 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,825 : INFO : built Dictionary(31 unique tokens: ['accommodation', 'allowed', 'another', 'everyone', 'female']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:43,867 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:43,887 : INFO : built Dictionary(40 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:43,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:43,945 : INFO : built Dictionary(48 unique tokens: ['apartment', 'aprtment', 'bed', 'bedroom', 'cable']...) from 2 documents (total 77 corpus positions)\n",
      "2019-06-17 11:00:43,995 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,017 : INFO : built Dictionary(24 unique tokens: ['couple', 'enough', 'good', 'k', 'kids']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:44,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,057 : INFO : built Dictionary(33 unique tokens: ['acommodation', 'allowance', 'ask', 'basic', 'company']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:44,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,112 : INFO : built Dictionary(31 unique tokens: ['accomodation', 'admin', 'also', 'assistant', 'care']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:44,118 : INFO : built Dictionary(13 unique tokens: ['able', 'beaches', 'drive', 'good', 'leading']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:44,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,131 : INFO : built Dictionary(20 unique tokens: ['bay', 'beach', 'best', 'go', 'maybe']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:44,140 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:44,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,148 : INFO : built Dictionary(40 unique tokens: ['actually', 'anyone', 'bay', 'beaches', 'calling']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:44,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,181 : INFO : built Dictionary(48 unique tokens: ['ago', 'amazing', 'around', 'beach', 'beaches']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:44,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,218 : INFO : built Dictionary(27 unique tokens: ['beach', 'best', 'dukhan', 'friends', 'go']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:44,232 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,234 : INFO : built Dictionary(25 unique tokens: ['anyone', 'beaches', 'coming', 'family', 'good']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:44,246 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,260 : INFO : built Dictionary(20 unique tokens: ['beach', 'beaches', 'booze', 'know', 'let']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:44,268 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,277 : INFO : built Dictionary(26 unique tokens: ['beach', 'best', 'camping', 'family', 'front']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:44,293 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,295 : INFO : built Dictionary(27 unique tokens: ['beach', 'best', 'clean', 'crowded', 'easy']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:44,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,310 : INFO : built Dictionary(46 unique tokens: ['beach', 'best', 'chat', 'chit', 'confused']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:44,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,325 : INFO : built Dictionary(27 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:44,342 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:44,352 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,406 : INFO : built Dictionary(46 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:44,457 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,474 : INFO : built Dictionary(39 unique tokens: ['anyone', 'apply', 'completed', 'country', 'exit']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:44,521 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,542 : INFO : built Dictionary(27 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:44,541 : INFO : built Dictionary(19 unique tokens: ['dads', 'doha', 'full', 'groups', 'hi']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:44,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,569 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,583 : INFO : built Dictionary(37 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:44,585 : INFO : built Dictionary(38 unique tokens: ['admin', 'anyway', 'full', 'good', 'great']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:44,608 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,612 : INFO : built Dictionary(46 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:44,613 : INFO : built Dictionary(41 unique tokens: ['average', 'child', 'clothes', 'dad', 'due']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:44,642 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,666 : INFO : built Dictionary(42 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:00:44,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,692 : INFO : built Dictionary(51 unique tokens: ['age', 'alot', 'children', 'daddies', 'dads']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:44,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,740 : INFO : built Dictionary(26 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:44,760 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,777 : INFO : built Dictionary(44 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:44,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,798 : INFO : built Dictionary(41 unique tokens: ['anybody', 'anything', 'around', 'chat', 'chit']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:44,806 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,811 : INFO : built Dictionary(35 unique tokens: ['agreement', 'along', 'application', 'apply', 'called']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:44,828 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:44,828 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,837 : INFO : built Dictionary(34 unique tokens: ['advise', 'aged', 'anyone', 'dhothi', 'dhoti']...) from 2 documents (total 43 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:44,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,866 : INFO : built Dictionary(47 unique tokens: ['account', 'also', 'bank', 'born', 'cheapest']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:44,899 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,901 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,904 : INFO : built Dictionary(29 unique tokens: ['actually', 'anyone', 'call', 'doctor', 'gone']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:44,914 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,918 : INFO : built Dictionary(48 unique tokens: ['also', 'bring', 'care', 'careful', 'child']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:44,939 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,942 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,944 : INFO : built Dictionary(40 unique tokens: ['anyone', 'books', 'buy', 'darling', 'daughter']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:44,966 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:44,989 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:44,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:44,993 : INFO : built Dictionary(23 unique tokens: ['ago', 'anyone', 'close', 'doha', 'eid']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:45,004 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,008 : INFO : built Dictionary(17 unique tokens: ['aladdin', 'aqua', 'doha', 'feedback', 'kingdom']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:45,012 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,016 : INFO : built Dictionary(28 unique tokens: ['animals', 'awareness', 'doha', 'ever', 'first']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:45,025 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,030 : INFO : built Dictionary(14 unique tokens: ['hi', 'monkeys', 'please', 'qatar', 'regards']...) from 2 documents (total 19 corpus positions)\n",
      "2019-06-17 11:00:45,035 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,036 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,038 : INFO : built Dictionary(12 unique tokens: ['doha', 'know', 'one', 'open', 'ramadan']...) from 2 documents (total 16 corpus positions)\n",
      "2019-06-17 11:00:45,041 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,045 : INFO : built Dictionary(12 unique tokens: ['doha', 'open', 'timings', 'today', 'wondering']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:00:45,048 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,051 : INFO : built Dictionary(25 unique tokens: ['actually', 'book', 'doha', 'drive', 'dug']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:45,062 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,066 : INFO : built Dictionary(15 unique tokens: ['aquarium', 'asked', 'doha', 'know', 'open']...) from 2 documents (total 19 corpus positions)\n",
      "2019-06-17 11:00:45,071 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,074 : INFO : built Dictionary(13 unique tokens: ['nephew', 'places', 'qatar', 'tourists', 'visit']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:00:45,078 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,080 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,082 : INFO : built Dictionary(17 unique tokens: ['anyone', 'bus', 'buses', 'centre', 'city']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:45,088 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:45,115 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,117 : INFO : built Dictionary(24 unique tokens: ['adv', 'anyone', 'call', 'company', 'could']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:45,132 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,134 : INFO : built Dictionary(42 unique tokens: ['anybody', 'bayt', 'consider', 'cv', 'etc']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:45,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,151 : INFO : built Dictionary(52 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:45,174 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,177 : INFO : built Dictionary(43 unique tokens: ['asked', 'correct', 'courteous', 'displeasure', 'go']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:45,201 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,204 : INFO : built Dictionary(43 unique tokens: ['dads', 'doha', 'full', 'groups', 'hi']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:45,219 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,221 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,223 : INFO : built Dictionary(57 unique tokens: ['abroad', 'accept', 'accommodation', 'admin', 'allowance']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:45,230 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,232 : INFO : built Dictionary(26 unique tokens: ['finger', 'get', 'issued', 'long', 'medical']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:45,242 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,248 : INFO : built Dictionary(35 unique tokens: ['approval', 'fingerprinting', 'get', 'got', 'medical']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:45,266 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,269 : INFO : built Dictionary(32 unique tokens: ['days', 'done', 'finger', 'get', 'got']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:00:45,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,278 : INFO : built Dictionary(43 unique tokens: ['across', 'also', 'american', 'article', 'campuses']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:45,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,294 : INFO : built Dictionary(44 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:45,303 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,305 : INFO : built Dictionary(49 unique tokens: ['advance', 'bank', 'big', 'card', 'company']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:45,323 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,331 : INFO : built Dictionary(43 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 56 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:45,332 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,335 : INFO : built Dictionary(53 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:45,375 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,405 : INFO : built Dictionary(39 unique tokens: ['boy', 'boys', 'brother', 'calling', 'calls']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:45,416 : INFO : built Dictionary(38 unique tokens: ['anyone', 'carry', 'duplicate', 'expire', 'get']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:45,463 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:45,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,477 : INFO : built Dictionary(48 unique tokens: ['ask', 'aug', 'authenticated', 'bless', 'clearance']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:45,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,572 : INFO : built Dictionary(41 unique tokens: ['appointment', 'curious', 'date', 'due', 'first']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:45,612 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,641 : INFO : built Dictionary(38 unique tokens: ['answer', 'anybody', 'appreciated', 'ask', 'cid']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:45,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,679 : INFO : built Dictionary(39 unique tokens: ['adults', 'anyone', 'certain', 'comments', 'country']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:45,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,720 : INFO : built Dictionary(34 unique tokens: ['days', 'documents', 'family', 'get', 'issued']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:45,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,767 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:45,764 : INFO : built Dictionary(38 unique tokens: ['almost', 'back', 'cheery', 'empty', 'even']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:45,842 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,858 : INFO : built Dictionary(25 unique tokens: ['advance', 'anyone', 'c', 'friday', 'know']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:45,883 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,889 : INFO : built Dictionary(28 unique tokens: ['avail', 'blocks', 'buy', 'center', 'city']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:45,916 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:45,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:45,931 : INFO : built Dictionary(49 unique tokens: ['anything', 'appears', 'bands', 'bars', 'bored']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:45,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,038 : INFO : built Dictionary(36 unique tokens: ['able', 'baby', 'brand', 'decent', 'doha']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:46,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,094 : INFO : built Dictionary(27 unique tokens: ['anything', 'booze', 'bring', 'brought', 'country']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:46,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,160 : INFO : built Dictionary(26 unique tokens: ['buy', 'carseat', 'carseats', 'children', 'doha']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:46,185 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:46,192 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,206 : INFO : built Dictionary(28 unique tokens: ['brewing', 'condom', 'condoms', 'debate', 'india']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:46,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,237 : INFO : built Dictionary(30 unique tokens: ['buying', 'cats', 'could', 'doha', 'finding']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:46,275 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:46,297 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:46,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,348 : INFO : built Dictionary(36 unique tokens: ['advance', 'case', 'comments', 'dear', 'doha']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:46,394 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:46,462 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,476 : INFO : built Dictionary(42 unique tokens: ['anybody', 'asd', 'country', 'driving', 'drving']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:46,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,586 : INFO : built Dictionary(31 unique tokens: ['allowed', 'back', 'cancelled', 'drive', 'driving']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:46,697 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,714 : INFO : built Dictionary(32 unique tokens: ['arrived', 'drive', 'driver', 'folks', 'got']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:46,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,789 : INFO : built Dictionary(49 unique tokens: ['abu', 'advice', 'biased', 'course', 'dhabi']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:46,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,827 : INFO : built Dictionary(34 unique tokens: ['convertible', 'correct', 'dl', 'dubai', 'everyone']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:46,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,868 : INFO : built Dictionary(39 unique tokens: ['also', 'assist', 'complications', 'dl', 'driving']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:46,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:46,980 : INFO : built Dictionary(26 unique tokens: ['car', 'doha', 'drive', 'holder', 'long']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:46,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,022 : INFO : built Dictionary(47 unique tokens: ['abudhabi', 'advance', 'doha', 'friends', 'great']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:47,032 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,058 : INFO : built Dictionary(37 unique tokens: ['allows', 'amount', 'certain', 'companies', 'contractors']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:47,093 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,106 : INFO : built Dictionary(40 unique tokens: ['anyone', 'doha', 'furniture', 'guys', 'hey']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:47,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,165 : INFO : built Dictionary(60 unique tokens: ['abu', 'another', 'appreciate', 'bedroom', 'concerns']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:47,205 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,227 : INFO : built Dictionary(36 unique tokens: ['advisable', 'application', 'appreciate', 'canada', 'consultants']...) from 2 documents (total 42 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:47,282 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:47,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,412 : INFO : built Dictionary(63 unique tokens: ['abu', 'ahead', 'clean', 'contest', 'dhabi']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:47,666 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,698 : INFO : built Dictionary(31 unique tokens: ['abu', 'allowed', 'alone', 'best', 'culture']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:00:47,752 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,801 : INFO : built Dictionary(49 unique tokens: ['abu', 'abudhabi', 'assist', 'benefits', 'better']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:47,900 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,945 : INFO : built Dictionary(40 unique tokens: ['anyone', 'best', 'canadian', 'courier', 'documents']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:47,961 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:47,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:47,998 : INFO : built Dictionary(51 unique tokens: ['accident', 'afterwards', 'already', 'back', 'came']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:48,015 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,033 : INFO : built Dictionary(50 unique tokens: ['abu', 'addition', 'child', 'costs', 'covered']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:00:48,060 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:48,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,130 : INFO : built Dictionary(25 unique tokens: ['bumper', 'car', 'minor', 'need', 'paint']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:48,155 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,161 : INFO : built Dictionary(36 unique tokens: ['accent', 'answer', 'arabic', 'authority', 'callers']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:48,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,217 : INFO : built Dictionary(44 unique tokens: ['accident', 'acknowledge', 'agree', 'also', 'ask']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:48,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,255 : INFO : built Dictionary(42 unique tokens: ['abu', 'anyone', 'dhabi', 'fest', 'film']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:48,261 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,302 : INFO : built Dictionary(25 unique tokens: ['another', 'applying', 'certificate', 'clearance', 'help']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:48,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,387 : INFO : built Dictionary(62 unique tokens: ['abudhabi', 'advice', 'ajman', 'among', 'best']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:00:48,377 : INFO : built Dictionary(26 unique tokens: ['anonymous', 'anonymously', 'country', 'expired', 'good']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:48,407 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,413 : INFO : built Dictionary(24 unique tokens: ['anybody', 'cbq', 'cheque', 'etc', 'issued']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:00:48,432 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,457 : INFO : built Dictionary(45 unique tokens: ['accident', 'airbag', 'back', 'car', 'cost']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:48,506 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:48,515 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,540 : INFO : built Dictionary(32 unique tokens: ['anyone', 'authorities', 'certificate', 'clearance', 'everyone']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:48,577 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:48,595 : INFO : built Dictionary(18 unique tokens: ['id', 'lost', 'mine', 'qatar', 'advice']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:00:48,613 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:49,304 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,363 : INFO : built Dictionary(52 unique tokens: ['ago', 'also', 'big', 'cards', 'credit']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:00:49,470 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,506 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,524 : INFO : built Dictionary(56 unique tokens: ['access', 'assistance', 'contact', 'denied', 'dont']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:49,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,577 : INFO : built Dictionary(38 unique tokens: ['apply', 'approval', 'approve', 'daughter', 'days']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:49,596 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,648 : INFO : built Dictionary(34 unique tokens: ['accepted', 'accomadation', 'advance', 'advice', 'application']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:49,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,677 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,702 : INFO : built Dictionary(37 unique tokens: ['application', 'applied', 'b', 'certificate', 'counter']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:49,726 : INFO : built Dictionary(35 unique tokens: ['aids', 'positive', 'problem', 'qatar', 'airport']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:49,772 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,782 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,821 : INFO : built Dictionary(39 unique tokens: ['application', 'approval', 'checking', 'deferred', 'found']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:49,807 : INFO : built Dictionary(43 unique tokens: ['advance', 'anybody', 'appointment', 'approval', 'ask']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:49,852 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:49,851 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,897 : INFO : built Dictionary(21 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:49,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,911 : INFO : built Dictionary(30 unique tokens: ['applied', 'applying', 'ask', 'certificate', 'change']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:49,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:49,918 : INFO : built Dictionary(58 unique tokens: ['abroad', 'anyone', 'bank', 'blamed', 'brazilian']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:49,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:49,965 : INFO : built Dictionary(17 unique tokens: ['application', 'approval', 'checking', 'deferred', 'found']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:49,985 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,008 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,018 : INFO : built Dictionary(35 unique tokens: ['application', 'applied', 'approval', 'approved', 'changed']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:50,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,098 : INFO : built Dictionary(33 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:00:50,110 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,174 : INFO : built Dictionary(55 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:50,165 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,199 : INFO : built Dictionary(42 unique tokens: ['advice', 'anyone', 'application', 'approval', 'approved']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:50,232 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:50,318 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,357 : INFO : built Dictionary(64 unique tokens: ['al', 'allows', 'also', 'another', 'course']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:50,541 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,578 : INFO : built Dictionary(66 unique tokens: ['agreement', 'back', 'call', 'cancelled', 'chance']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:50,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,824 : INFO : built Dictionary(27 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:50,818 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,853 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,873 : INFO : built Dictionary(33 unique tokens: ['air', 'anyone', 'anything', 'cant', 'come']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:50,875 : INFO : built Dictionary(47 unique tokens: ['asked', 'coming', 'friend', 'girlfriend', 'hotel']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:50,910 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,931 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:50,946 : INFO : built Dictionary(22 unique tokens: ['coming', 'extend', 'february', 'many', 'months']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:50,967 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,980 : INFO : built Dictionary(36 unique tokens: ['ant', 'application', 'daughter', 'day', 'earning']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:51,003 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:50,985 : INFO : built Dictionary(48 unique tokens: ['also', 'arrested', 'childhood', 'expat', 'female']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:51,016 : INFO : built Dictionary(33 unique tokens: ['back', 'come', 'doha', 'find', 'fly']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:00:51,038 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,050 : INFO : built Dictionary(24 unique tokens: ['agency', 'apply', 'arrange', 'arrival', 'cant']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:51,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,073 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:51,092 : INFO : built Dictionary(32 unique tokens: ['also', 'awaited', 'brought', 'earliest', 'extend']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:51,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,141 : INFO : built Dictionary(27 unique tokens: ['aquiring', 'doha', 'easy', 'everyone', 'get']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:51,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,207 : INFO : built Dictionary(52 unique tokens: ['able', 'advice', 'age', 'alaykum', 'apply']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:51,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,396 : INFO : built Dictionary(48 unique tokens: ['abt', 'appreciated', 'cant', 'card', 'could']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:51,442 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:51,536 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,558 : INFO : built Dictionary(23 unique tokens: ['answers', 'aqua', 'best', 'blackcat', 'comments']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:51,590 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,617 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,634 : INFO : built Dictionary(15 unique tokens: ['al', 'anyone', 'get', 'international', 'jazeera']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:00:51,648 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,660 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,675 : INFO : built Dictionary(9 unique tokens: ['cornich', 'go', 'running', 'che', 'famous']...) from 2 documents (total 11 corpus positions)\n",
      "2019-06-17 11:00:51,687 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,714 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,737 : INFO : built Dictionary(23 unique tokens: ['advise', 'anyone', 'apply', 'everyone', 'hey']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:51,770 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,813 : INFO : built Dictionary(23 unique tokens: ['absurd', 'andy', 'anyone', 'ask', 'bit']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:51,834 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,841 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,861 : INFO : built Dictionary(53 unique tokens: ['appreciated', 'around', 'available', 'best', 'chicken']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:00:51,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,870 : INFO : built Dictionary(35 unique tokens: ['accommodation', 'al', 'anyone', 'appreciate', 'ask']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:51,902 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:51,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:51,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:51,965 : INFO : built Dictionary(12 unique tokens: ['afternoon', 'become', 'evening', 'good', 'harder']...) from 2 documents (total 15 corpus positions)\n",
      "2019-06-17 11:00:51,985 : INFO : built Dictionary(32 unique tokens: ['america', 'anyone', 'company', 'deliver', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:51,989 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:52,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,025 : INFO : built Dictionary(17 unique tokens: ['anyone', 'details', 'doha', 'good', 'know']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:00:52,023 : INFO : built Dictionary(29 unique tokens: ['anyone', 'center', 'curves', 'help', 'hi']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:52,044 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:52,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,090 : INFO : built Dictionary(45 unique tokens: ['anyone', 'buy', 'days', 'diet', 'effective']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:52,076 : INFO : built Dictionary(11 unique tokens: ['baby', 'board', 'car', 'funny', 'get']...) from 2 documents (total 14 corpus positions)\n",
      "2019-06-17 11:00:52,133 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:52,136 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,147 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,152 : INFO : built Dictionary(33 unique tokens: ['awesome', 'cross', 'fit', 'goal', 'group']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:52,163 : INFO : built Dictionary(38 unique tokens: ['advise', 'although', 'around', 'bar', 'birthday']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:52,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,191 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:52,201 : INFO : built Dictionary(59 unique tokens: ['anyone', 'ask', 'bags', 'brand', 'buying']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:52,324 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,351 : INFO : built Dictionary(52 unique tokens: ['also', 'alternatives', 'anyone', 'available', 'buttons']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:52,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,477 : INFO : built Dictionary(21 unique tokens: ['anyone', 'doha', 'hypnosis', 'know', 'anything']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:00:52,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,499 : INFO : built Dictionary(51 unique tokens: ['anyone', 'come', 'comments', 'contact', 'desperate']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:52,567 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,600 : INFO : built Dictionary(43 unique tokens: ['anyone', 'boxing', 'diagnosed', 'first', 'fitness']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:00:52,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,671 : INFO : built Dictionary(32 unique tokens: ['also', 'ask', 'become', 'book', 'business']...) from 2 documents (total 86 corpus positions)\n",
      "2019-06-17 11:00:52,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,682 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:52,727 : INFO : built Dictionary(54 unique tokens: ['advice', 'anyone', 'could', 'currently', 'doha']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:52,795 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:52,807 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,825 : INFO : built Dictionary(64 unique tokens: ['agreement', 'back', 'call', 'cancelled', 'chance']...) from 2 documents (total 82 corpus positions)\n",
      "2019-06-17 11:00:52,953 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:52,969 : INFO : built Dictionary(53 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:53,076 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,129 : INFO : built Dictionary(51 unique tokens: ['ago', 'back', 'canceled', 'cancelled', 'company']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:53,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,214 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:53,230 : INFO : built Dictionary(22 unique tokens: ['adult', 'anybody', 'body', 'circumcision', 'clinic']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:53,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,252 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:53,278 : INFO : built Dictionary(63 unique tokens: ['appreciate', 'ban', 'bank', 'banned', 'continue']...) from 2 documents (total 87 corpus positions)\n",
      "2019-06-17 11:00:53,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,301 : INFO : built Dictionary(31 unique tokens: ['advise', 'anybody', 'avilable', 'brother', 'circumcision']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:53,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,354 : INFO : built Dictionary(27 unique tokens: ['agree', 'aids', 'benefits', 'circumscision', 'curb']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:53,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,399 : INFO : built Dictionary(24 unique tokens: ['circumsized', 'forbidden', 'mod', 'moved', 'note']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:53,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,450 : INFO : built Dictionary(29 unique tokens: ['considered', 'crime', 'honour', 'immoral', 'imply']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:53,475 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,484 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,494 : INFO : built Dictionary(53 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 77 corpus positions)\n",
      "2019-06-17 11:00:53,504 : INFO : built Dictionary(21 unique tokens: ['anybody', 'artist', 'hehehehe', 'knows', 'tattoo']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:00:53,525 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:53,541 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,585 : INFO : built Dictionary(46 unique tokens: ['account', 'confession', 'could', 'create', 'decided']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:53,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,637 : INFO : built Dictionary(28 unique tokens: ['among', 'atheists', 'confident', 'due', 'earnest']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:53,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,668 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,671 : INFO : built Dictionary(53 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 72 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:53,690 : INFO : built Dictionary(17 unique tokens: ['one', 'painful', 'associates', 'babies', 'baby']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:00:53,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,751 : INFO : built Dictionary(26 unique tokens: ['anniversary', 'bf', 'gift', 'good', 'guys']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:00:53,761 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,783 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:53,785 : INFO : built Dictionary(40 unique tokens: ['cancel', 'cancellation', 'even', 'guys', 'passport']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:53,830 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:53,849 : INFO : built Dictionary(59 unique tokens: ['allow', 'anyone', 'company', 'contract', 'court']...) from 2 documents (total 84 corpus positions)\n",
      "2019-06-17 11:00:53,935 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:54,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,046 : INFO : built Dictionary(24 unique tokens: ['also', 'anyone', 'apply', 'believe', 'free']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:54,060 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,067 : INFO : built Dictionary(37 unique tokens: ['applying', 'b', 'basically', 'c', 'checked']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:54,086 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:54,093 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,106 : INFO : built Dictionary(35 unique tokens: ['body', 'civil', 'collect', 'desktop', 'engineer']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:54,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,135 : INFO : built Dictionary(36 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:54,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,166 : INFO : built Dictionary(29 unique tokens: ['anything', 'boy', 'check', 'disease', 'e']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:54,182 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:54,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,210 : INFO : built Dictionary(35 unique tokens: ['anyone', 'came', 'change', 'doha', 'due']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:54,277 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,296 : INFO : built Dictionary(31 unique tokens: ['accomodation', 'centre', 'corp', 'criteria', 'culture']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:54,346 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,365 : INFO : built Dictionary(20 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:00:54,381 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:54,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,425 : INFO : built Dictionary(37 unique tokens: ['alahly', 'alot', 'also', 'best', 'clinics']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:54,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,525 : INFO : built Dictionary(39 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:54,558 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:54,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,615 : INFO : built Dictionary(12 unique tokens: ['anyone', 'england', 'everyone', 'hi', 'hows']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:54,627 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,644 : INFO : built Dictionary(46 unique tokens: ['accept', 'alex', 'appropriate', 'become', 'behavior']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:54,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,707 : INFO : built Dictionary(18 unique tokens: ['dubai', 'family', 'hows', 'law', 'like']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:00:54,721 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,746 : INFO : built Dictionary(46 unique tokens: ['abt', 'approx', 'around', 'average', 'colleague']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:54,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,827 : INFO : built Dictionary(31 unique tokens: ['ask', 'choice', 'country', 'coz', 'describe']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:54,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,930 : INFO : built Dictionary(31 unique tokens: ['anywhere', 'bars', 'belly', 'c', 'comin']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:54,951 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:54,974 : INFO : built Dictionary(46 unique tokens: ['abu', 'ahead', 'clean', 'contest', 'dhabi']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:54,979 : INFO : built Dictionary(38 unique tokens: ['anybody', 'care', 'change', 'circumstance', 'comes']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:55,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,017 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,045 : INFO : built Dictionary(16 unique tokens: ['approval', 'female', 'get', 'hard', 'qatar']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:00:55,058 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,066 : INFO : built Dictionary(42 unique tokens: ['affidavit', 'anybody', 'bank', 'citizen', 'consul']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:00:55,050 : INFO : built Dictionary(34 unique tokens: ['body', 'cancelled', 'council', 'fresh', 'get']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:00:55,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,131 : INFO : built Dictionary(39 unique tokens: ['acs', 'africa', 'american', 'anyone', 'anything']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:55,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,144 : INFO : built Dictionary(25 unique tokens: ['anybody', 'currently', 'doha', 'extend', 'futher']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:55,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,193 : INFO : built Dictionary(45 unique tokens: ['already', 'arabic', 'chef', 'doha', 'end']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:55,224 : INFO : built Dictionary(32 unique tokens: ['al', 'also', 'anyone', 'appears', 'aussie']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:55,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,262 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:55,303 : INFO : built Dictionary(40 unique tokens: ['accommodation', 'allowance', 'allowances', 'allownaces', 'applied']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:00:55,424 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:55,434 : INFO : built Dictionary(13 unique tokens: ['freelance', 'visa', 'anyone', 'appreciated', 'female']...) from 2 documents (total 20 corpus positions)\n",
      "2019-06-17 11:00:55,443 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,460 : INFO : built Dictionary(23 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:55,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,484 : INFO : built Dictionary(34 unique tokens: ['accommodation', 'allowed', 'another', 'everyone', 'female']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:00:55,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,544 : INFO : built Dictionary(37 unique tokens: ['ahli', 'al', 'anyone', 'clear', 'coverage']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:00:55,582 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:55,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:55,965 : INFO : built Dictionary(29 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:00:55,982 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,003 : INFO : built Dictionary(30 unique tokens: ['ask', 'car', 'complication', 'didnt', 'file']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:56,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,057 : INFO : built Dictionary(43 unique tokens: ['able', 'advice', 'age', 'alaykum', 'apply']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:56,078 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:56,092 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,103 : INFO : built Dictionary(42 unique tokens: ['accept', 'accepted', 'advance', 'advise', 'application']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:00:56,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,178 : INFO : built Dictionary(31 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:56,213 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:56,210 : INFO : built Dictionary(21 unique tokens: ['best', 'car', 'clocked', 'contest', 'drive']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:56,227 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,245 : INFO : built Dictionary(27 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:56,256 : INFO : built Dictionary(27 unique tokens: ['budget', 'got', 'guys', 'help', 'money']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:56,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,293 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:56,299 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,297 : INFO : built Dictionary(25 unique tokens: ['advise', 'also', 'anyone', 'bring', 'cost']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:56,313 : INFO : built Dictionary(42 unique tokens: ['come', 'contact', 'couple', 'designed', 'details']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:56,319 : INFO : Removed 11 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:56,341 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,346 : INFO : built Dictionary(40 unique tokens: ['ang', 'another', 'aq', 'ay', 'business']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:00:56,357 : INFO : built Dictionary(59 unique tokens: ['applying', 'asked', 'asking', 'assalamualaikum', 'blank']...) from 2 documents (total 76 corpus positions)\n",
      "2019-06-17 11:00:56,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,377 : INFO : built Dictionary(34 unique tokens: ['btw', 'cons', 'couple', 'doha', 'employer']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:00:56,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,430 : INFO : built Dictionary(30 unique tokens: ['accomodation', 'advise', 'al', 'appreciated', 'bedroom']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:56,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,450 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:56,452 : INFO : built Dictionary(47 unique tokens: ['advance', 'bank', 'big', 'card', 'company']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:56,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,561 : INFO : built Dictionary(49 unique tokens: ['acceptable', 'accomodation', 'ask', 'company', 'contract']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:56,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,657 : INFO : built Dictionary(47 unique tokens: ['business', 'changes', 'countries', 'current', 'every']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:56,740 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,761 : INFO : built Dictionary(35 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:56,825 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:56,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,850 : INFO : built Dictionary(53 unique tokens: ['abroad', 'accept', 'accommodation', 'admin', 'allowance']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:56,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:56,956 : INFO : built Dictionary(49 unique tokens: ['alot', 'challenging', 'day', 'dont', 'embrace']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:00:57,024 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:57,035 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:57,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,061 : INFO : built Dictionary(46 unique tokens: ['age', 'ang', 'apply', 'baka', 'business']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:57,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,147 : INFO : built Dictionary(23 unique tokens: ['anybody', 'currently', 'doha', 'extend', 'futher']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:00:57,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,166 : INFO : built Dictionary(34 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:57,189 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,200 : INFO : built Dictionary(41 unique tokens: ['anybody', 'anyone', 'come', 'company', 'contract']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:57,249 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,282 : INFO : built Dictionary(24 unique tokens: ['anyone', 'bachelors', 'batchelors', 'complete', 'could']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:57,302 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:57,315 : INFO : built Dictionary(41 unique tokens: ['accommodation', 'allowance', 'allowances', 'allownaces', 'applied']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:57,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,363 : INFO : built Dictionary(28 unique tokens: ['account', 'anyone', 'bank', 'directly', 'employee']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:00:57,400 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:57,413 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,439 : INFO : built Dictionary(23 unique tokens: ['available', 'cameroonian', 'currently', 'everyone', 'hello']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:00:57,460 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:57,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,525 : INFO : built Dictionary(44 unique tokens: ['admissions', 'admit', 'advance', 'back', 'birla']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:57,546 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,573 : INFO : built Dictionary(44 unique tokens: ['almost', 'anyone', 'baby', 'bad', 'boy']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:00:57,595 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:57,724 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,735 : INFO : built Dictionary(34 unique tokens: ['accomadation', 'advise', 'anyone', 'children', 'company']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:00:57,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:57,842 : INFO : built Dictionary(47 unique tokens: ['anyone', 'c', 'heading', 'hey', 'know']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:00:57,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,024 : INFO : built Dictionary(48 unique tokens: ['anyone', 'anything', 'companies', 'company', 'damaging']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:00:58,065 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:58,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,082 : INFO : built Dictionary(59 unique tokens: ['advice', 'anyone', 'april', 'books', 'bringing']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:00:58,131 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,153 : INFO : built Dictionary(35 unique tokens: ['account', 'also', 'bank', 'born', 'cheapest']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:58,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,217 : INFO : built Dictionary(25 unique tokens: ['accnt', 'anybody', 'charges', 'commission', 'electronic']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:00:58,237 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:58,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,277 : INFO : built Dictionary(63 unique tokens: ['advice', 'appreciate', 'appreciated', 'clubs', 'concern']...) from 2 documents (total 79 corpus positions)\n",
      "2019-06-17 11:00:58,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,286 : INFO : built Dictionary(44 unique tokens: ['account', 'across', 'aka', 'axis', 'bank']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:00:58,313 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,334 : INFO : built Dictionary(33 unique tokens: ['accounts', 'around', 'bank', 'banks', 'best']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:00:58,366 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:58,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,390 : INFO : built Dictionary(35 unique tokens: ['abroad', 'anyone', 'bank', 'blamed', 'brazilian']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:00:58,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,414 : INFO : built Dictionary(68 unique tokens: ['able', 'anyone', 'avoid', 'cabin', 'cant']...) from 2 documents (total 82 corpus positions)\n",
      "2019-06-17 11:00:58,423 : INFO : built Dictionary(14 unique tokens: ['anyone', 'cost', 'ipad', 'know', 'much']...) from 2 documents (total 20 corpus positions)\n",
      "2019-06-17 11:00:58,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,477 : INFO : built Dictionary(39 unique tokens: ['already', 'appreciated', 'arrest', 'back', 'bank']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:00:58,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,552 : INFO : built Dictionary(36 unique tokens: ['academic', 'also', 'anyone', 'appreciate', 'august']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:00:58,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,601 : INFO : built Dictionary(26 unique tokens: ['anyone', 'anything', 'companies', 'company', 'damaging']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:00:58,628 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,621 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:58,643 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,642 : INFO : built Dictionary(35 unique tokens: ['afternoon', 'anyone', 'bus', 'child', 'children']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:00:58,661 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:58,663 : INFO : built Dictionary(62 unique tokens: ['adapted', 'advance', 'areas', 'breed', 'bring']...) from 2 documents (total 73 corpus positions)\n",
      "2019-06-17 11:00:58,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,876 : INFO : built Dictionary(48 unique tokens: ['bangalore', 'clearance', 'customs', 'doha', 'door']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:58,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:58,947 : INFO : built Dictionary(65 unique tokens: ['allowance', 'anyone', 'attending', 'banks', 'best']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:00:59,210 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:59,211 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,248 : INFO : built Dictionary(37 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:00:59,246 : INFO : built Dictionary(56 unique tokens: ['adapt', 'anybody', 'anyone', 'behaviour', 'closer']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:00:59,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,321 : INFO : built Dictionary(21 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:00:59,348 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:00:59,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,387 : INFO : built Dictionary(41 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:00:59,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,431 : INFO : built Dictionary(28 unique tokens: ['ant', 'application', 'daughter', 'day', 'earning']...) from 2 documents (total 42 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:00:59,440 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:59,467 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,483 : INFO : built Dictionary(27 unique tokens: ['anybody', 'authenticated', 'business', 'company', 'friend']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:59,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,528 : INFO : built Dictionary(19 unique tokens: ['coming', 'enter', 'family', 'january', 'kindly']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:00:59,546 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,561 : INFO : built Dictionary(25 unique tokens: ['also', 'document', 'documents', 'entry', 'find']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:00:59,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,594 : INFO : built Dictionary(22 unique tokens: ['aquiring', 'doha', 'easy', 'everyone', 'get']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:59,617 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,621 : INFO : built Dictionary(27 unique tokens: ['agents', 'almost', 'asked', 'available', 'deal']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:00:59,652 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:00:59,677 : INFO : built Dictionary(19 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:00:59,704 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:00:59,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,009 : INFO : built Dictionary(9 unique tokens: ['best', 'buy', 'doha', 'furniture', 'good']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:00,019 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,037 : INFO : built Dictionary(20 unique tokens: ['advice', 'considering', 'doha', 'furnished', 'furniture']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:00,066 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,095 : INFO : built Dictionary(19 unique tokens: ['bed', 'buy', 'cheap', 'doha', 'dresser']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:00,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,147 : INFO : built Dictionary(14 unique tokens: ['center', 'doha', 'furniture', 'home', 'prices']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:00,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,192 : INFO : built Dictionary(19 unique tokens: ['buy', 'classical', 'furnitures', 'looking', 'lot']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:00,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,238 : INFO : built Dictionary(41 unique tokens: ['artistic', 'bargained', 'besides', 'bulky', 'buy']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:00,276 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,307 : INFO : built Dictionary(17 unique tokens: ['abhaya', 'buy', 'could', 'doha', 'get']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:00,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,346 : INFO : built Dictionary(28 unique tokens: ['around', 'arrived', 'available', 'best', 'catch']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:00,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,388 : INFO : built Dictionary(53 unique tokens: ['anyone', 'appreciate', 'buy', 'could', 'curtains']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:00,437 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,445 : INFO : built Dictionary(14 unique tokens: ['anyone', 'doha', 'good', 'know', 'mattress']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:00,461 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:00,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,550 : INFO : built Dictionary(36 unique tokens: ['authority', 'basic', 'benefits', 'calculated', 'calculation']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:00,577 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:00,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,632 : INFO : built Dictionary(48 unique tokens: ['according', 'action', 'advise', 'allow', 'back']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:00,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,710 : INFO : built Dictionary(25 unique tokens: ['company', 'get', 'gratuity', 'help', 'hi']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:00,721 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,739 : INFO : built Dictionary(24 unique tokens: ['benefit', 'benefits', 'computation', 'confirm', 'end']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:00,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,759 : INFO : built Dictionary(42 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:00,803 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,830 : INFO : built Dictionary(26 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:00,873 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,893 : INFO : built Dictionary(48 unique tokens: ['another', 'average', 'basic', 'benefits', 'change']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:00,944 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:00,995 : INFO : built Dictionary(37 unique tokens: ['accomodation', 'b', 'company', 'doha', 'e']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:01,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,037 : INFO : built Dictionary(19 unique tokens: ['apartments', 'bring', 'bringing', 'doha', 'fully']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:01,055 : INFO : built Dictionary(42 unique tokens: ['accommodation', 'bachelor', 'company', 'decent', 'engineer']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:01,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,094 : INFO : built Dictionary(41 unique tokens: ['advice', 'appreciated', 'august', 'comments', 'documents']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:01,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,128 : INFO : built Dictionary(35 unique tokens: ['coincided', 'company', 'days', 'deducted', 'deduction']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:01,160 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:01,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,209 : INFO : built Dictionary(29 unique tokens: ['bring', 'bringing', 'company', 'entitled', 'hi']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:01,238 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,275 : INFO : built Dictionary(26 unique tokens: ['around', 'doha', 'hi', 'ikea', 'kindly']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:01,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:01,311 : INFO : built Dictionary(39 unique tokens: ['basis', 'car', 'classifieds', 'collect', 'find']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:01,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,409 : INFO : built Dictionary(44 unique tokens: ['accomodation', 'ago', 'also', 'dear', 'details']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:01,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,515 : INFO : built Dictionary(45 unique tokens: ['advance', 'amount', 'company', 'dear', 'excluding']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:01,623 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,632 : INFO : built Dictionary(47 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:01,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,718 : INFO : built Dictionary(35 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:01,727 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,797 : INFO : built Dictionary(44 unique tokens: ['also', 'apartment', 'apartments', 'bringing', 'cuz']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:01,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,806 : INFO : built Dictionary(31 unique tokens: ['applied', 'applying', 'ask', 'certificate', 'change']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:01,857 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:01,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,896 : INFO : built Dictionary(47 unique tokens: ['accept', 'accepted', 'advance', 'advise', 'application']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:01:01,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:01,964 : INFO : built Dictionary(39 unique tokens: ['anyone', 'appreciate', 'cant', 'closest', 'doha']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:01,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,014 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:02,037 : INFO : built Dictionary(16 unique tokens: ['advice', 'changing', 'long', 'please', 'procedure']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:02,068 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,082 : INFO : built Dictionary(25 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:02,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,172 : INFO : built Dictionary(24 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:02,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,245 : INFO : built Dictionary(44 unique tokens: ['allow', 'already', 'answers', 'arranged', 'arrival']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:02,292 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,329 : INFO : built Dictionary(39 unique tokens: ['application', 'applied', 'b', 'certificate', 'counter']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:02,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,391 : INFO : built Dictionary(30 unique tokens: ['applied', 'call', 'correct', 'delivered', 'entered']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:02,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,449 : INFO : built Dictionary(27 unique tokens: ['abu', 'airport', 'apply', 'architect', 'arrival']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:02,474 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:02,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,676 : INFO : built Dictionary(12 unique tokens: ['aladdin', 'aqua', 'doha', 'feedback', 'kingdom']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:02,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,729 : INFO : built Dictionary(31 unique tokens: ['advices', 'aqua', 'body', 'boring', 'come']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:02,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,787 : INFO : built Dictionary(7 unique tokens: ['know', 'park', 'qatar', 'theme', 'water']...) from 2 documents (total 13 corpus positions)\n",
      "2019-06-17 11:01:02,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,799 : INFO : built Dictionary(14 unique tokens: ['anyone', 'aqua', 'check', 'found', 'havent']...) from 2 documents (total 17 corpus positions)\n",
      "2019-06-17 11:01:02,808 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,819 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:02,815 : INFO : built Dictionary(17 unique tokens: ['advance', 'aqua', 'concern', 'fill', 'issue']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:02,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,865 : INFO : built Dictionary(36 unique tokens: ['asia', 'ban', 'benefit', 'comments', 'countries']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:02,871 : INFO : built Dictionary(14 unique tokens: ['anything', 'aqua', 'heard', 'industrial', 'might']...) from 2 documents (total 16 corpus positions)\n",
      "2019-06-17 11:01:02,909 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,924 : INFO : built Dictionary(9 unique tokens: ['aquarium', 'asked', 'doha', 'know', 'open']...) from 2 documents (total 14 corpus positions)\n",
      "2019-06-17 11:01:02,941 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,955 : INFO : built Dictionary(10 unique tokens: ['build', 'disneyland', 'guys', 'oh', 'perfect']...) from 2 documents (total 15 corpus positions)\n",
      "2019-06-17 11:01:02,972 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:02,969 : INFO : built Dictionary(20 unique tokens: ['ask', 'ban', 'cancelled', 'comeback', 'job']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:02,991 : INFO : built Dictionary(21 unique tokens: ['answer', 'anybody', 'appreciated', 'ask', 'cid']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:01:02,999 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:03,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,034 : INFO : built Dictionary(42 unique tokens: ['appreciate', 'ban', 'bank', 'banned', 'continue']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:03,044 : INFO : built Dictionary(11 unique tokens: ['eid', 'exciting', 'happenings', 'holidays', 'new']...) from 2 documents (total 15 corpus positions)\n",
      "2019-06-17 11:01:03,073 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:03,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,119 : INFO : built Dictionary(29 unique tokens: ['ban', 'coming', 'currently', 'downed', 'fiancé']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:03,175 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:03,217 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,245 : INFO : built Dictionary(47 unique tokens: ['agreement', 'back', 'call', 'cancelled', 'chance']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:03,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,334 : INFO : built Dictionary(23 unique tokens: ['answer', 'back', 'ban', 'come', 'contract']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:03,361 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,392 : INFO : built Dictionary(25 unique tokens: ['alternative', 'banned', 'getting', 'guys', 'hello']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:03,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,418 : INFO : built Dictionary(30 unique tokens: ['ago', 'back', 'canceled', 'cancelled', 'company']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:03,435 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,441 : INFO : built Dictionary(41 unique tokens: ['ago', 'already', 'applying', 'april', 'ask']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:03,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,482 : INFO : built Dictionary(22 unique tokens: ['anyone', 'apply', 'banned', 'cannot', 'employment']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:03,497 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:03,650 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:03,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,696 : INFO : built Dictionary(30 unique tokens: ['airways', 'basic', 'benefits', 'could', 'figures']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:03,730 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:03,749 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,774 : INFO : built Dictionary(26 unique tokens: ['advise', 'airways', 'ground', 'guys', 'hi']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:03,802 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:03,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,869 : INFO : built Dictionary(26 unique tokens: ['aircraft', 'airways', 'anyone', 'called', 'engineer']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:03,901 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:03,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:03,950 : INFO : built Dictionary(50 unique tokens: ['advance', 'airways', 'allowance', 'also', 'analyst']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:04,062 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,066 : INFO : built Dictionary(36 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:04,066 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,145 : INFO : built Dictionary(34 unique tokens: ['airways', 'annual', 'basic', 'bonus', 'cabin']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:04,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,203 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,205 : INFO : built Dictionary(40 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:04,245 : INFO : built Dictionary(17 unique tokens: ['admin', 'airways', 'appreciate', 'assistant', 'could']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:04,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,285 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,287 : INFO : built Dictionary(28 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:04,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,335 : INFO : built Dictionary(29 unique tokens: ['accommodation', 'airways', 'allowance', 'british', 'grade']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:04,340 : INFO : built Dictionary(26 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:04,365 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,389 : INFO : built Dictionary(40 unique tokens: ['aircraft', 'airways', 'allowance', 'around', 'bring']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:04,392 : INFO : built Dictionary(37 unique tokens: ['anyone', 'apply', 'completed', 'country', 'exit']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:04,435 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,450 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,464 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,466 : INFO : built Dictionary(36 unique tokens: ['answer', 'apply', 'applying', 'arrive', 'asap']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:04,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,513 : INFO : built Dictionary(39 unique tokens: ['airways', 'anyone', 'anything', 'commercial', 'currently']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:04,507 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,535 : INFO : built Dictionary(44 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:04,577 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,594 : INFO : built Dictionary(34 unique tokens: ['airways', 'comments', 'dear', 'doha', 'find']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:04,617 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,624 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:04,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,657 : INFO : built Dictionary(45 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:04,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,725 : INFO : built Dictionary(41 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:04,828 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:04,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:04,904 : INFO : built Dictionary(33 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 52 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:04,957 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:05,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,203 : INFO : built Dictionary(19 unique tokens: ['advance', 'ago', 'anyone', 'bottle', 'cheers']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:05,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,293 : INFO : built Dictionary(19 unique tokens: ['cost', 'etc', 'getting', 'give', 'hi']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:01:05,321 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,345 : INFO : built Dictionary(19 unique tokens: ['al', 'alcohol', 'beer', 'clubs', 'cold']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:05,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,392 : INFO : built Dictionary(16 unique tokens: ['events', 'light', 'pearl', 'qdc', 'reason']...) from 2 documents (total 17 corpus positions)\n",
      "2019-06-17 11:01:05,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,433 : INFO : built Dictionary(26 unique tokens: ['ale', 'apparently', 'beer', 'brits', 'calling']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:05,461 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,473 : INFO : built Dictionary(27 unique tokens: ['back', 'beer', 'bit', 'could', 'cruiser']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:05,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,556 : INFO : built Dictionary(15 unique tokens: ['alcohol', 'also', 'americans', 'apply', 'ban']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:01:05,564 : INFO : Removed 4 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:05,582 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,601 : INFO : built Dictionary(41 unique tokens: ['al', 'americans', 'anyone', 'anything', 'area']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:05,602 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:05,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,630 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,653 : INFO : built Dictionary(38 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:05,648 : INFO : built Dictionary(14 unique tokens: ['fasting', 'last', 'lot', 'must', 'people']...) from 2 documents (total 14 corpus positions)\n",
      "2019-06-17 11:01:05,675 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,700 : INFO : built Dictionary(34 unique tokens: ['anybody', 'apart', 'available', 'clubbing', 'cool']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:05,731 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:05,720 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:05,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,797 : INFO : built Dictionary(40 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:05,876 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:05,889 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,896 : INFO : built Dictionary(20 unique tokens: ['advise', 'help', 'need', 'please', 'sponsorship']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:05,914 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:05,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:05,974 : INFO : built Dictionary(28 unique tokens: ['ask', 'ban', 'cancelled', 'comeback', 'job']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:06,041 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,057 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,063 : INFO : built Dictionary(51 unique tokens: ['according', 'another', 'answers', 'anyone', 'appreciated']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:06,116 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,131 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,140 : INFO : built Dictionary(33 unique tokens: ['advice', 'business', 'change', 'convert', 'employer']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:06,175 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,256 : INFO : built Dictionary(28 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:06,282 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,289 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,313 : INFO : built Dictionary(27 unique tokens: ['advise', 'change', 'company', 'even', 'husband']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:06,345 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,399 : INFO : built Dictionary(37 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:06,475 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,444 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,514 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,511 : INFO : built Dictionary(31 unique tokens: ['amount', 'attendants', 'boy', 'car', 'coffee']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:06,531 : INFO : built Dictionary(47 unique tokens: ['approximately', 'arriving', 'contract', 'country', 'december']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:06,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,599 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:06,620 : INFO : built Dictionary(18 unique tokens: ['beauty', 'haircut', 'idea', 'manicure', 'massage']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:01:06,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,682 : INFO : built Dictionary(6 unique tokens: ['back', 'home', 'leave', 'much', 'percent']...) from 2 documents (total 14 corpus positions)\n",
      "2019-06-17 11:01:06,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,713 : INFO : built Dictionary(40 unique tokens: ['acceptable', 'advice', 'also', 'assisting', 'ave']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:06,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,809 : INFO : built Dictionary(23 unique tokens: ['dinner', 'discusse', 'example', 'give', 'good']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:06,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,867 : INFO : built Dictionary(22 unique tokens: ['bell', 'boy', 'carry', 'day', 'even']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:01:06,891 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,909 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,945 : INFO : built Dictionary(33 unique tokens: ['ask', 'bartender', 'bartenders', 'come', 'doha']...) from 2 documents (total 40 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:06,973 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:06,982 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:06,986 : INFO : built Dictionary(25 unique tokens: ['advice', 'airport', 'arrive', 'bring', 'card']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:07,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,052 : INFO : built Dictionary(22 unique tokens: ['baby', 'born', 'bring', 'dear', 'help']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:07,041 : INFO : built Dictionary(23 unique tokens: ['business', 'certain', 'doha', 'dress', 'enquiring']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:07,064 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:07,071 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,084 : INFO : built Dictionary(36 unique tokens: ['able', 'amount', 'area', 'asking', 'bad']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:07,088 : INFO : built Dictionary(34 unique tokens: ['advance', 'baby', 'bcg', 'better', 'child']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:07,108 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,123 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:07,118 : INFO : built Dictionary(26 unique tokens: ['amount', 'asking', 'bringing', 'help', 'idea']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:07,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,168 : INFO : built Dictionary(50 unique tokens: ['able', 'advice', 'age', 'alaykum', 'apply']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:07,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,223 : INFO : built Dictionary(37 unique tokens: ['another', 'baby', 'babysitter', 'come', 'common']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:07,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,306 : INFO : built Dictionary(35 unique tokens: ['advice', 'change', 'cid', 'clearance', 'company']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:07,353 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,358 : INFO : built Dictionary(35 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:07,388 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,417 : INFO : built Dictionary(43 unique tokens: ['account', 'advice', 'already', 'anyone', 'applying']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:07,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,505 : INFO : built Dictionary(33 unique tokens: ['anyone', 'baby', 'bees', 'best', 'busy']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:07,543 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,564 : INFO : built Dictionary(41 unique tokens: ['big', 'bored', 'contact', 'daughter', 'doha']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:07,608 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:07,620 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,635 : INFO : built Dictionary(37 unique tokens: ['almost', 'annoying', 'candies', 'cashiers', 'change']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:07,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,738 : INFO : built Dictionary(41 unique tokens: ['always', 'atm', 'back', 'boss', 'businesses']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:07,767 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,774 : INFO : built Dictionary(28 unique tokens: ['almost', 'cashier', 'come', 'comment', 'even']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:07,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,807 : INFO : built Dictionary(23 unique tokens: ['account', 'advise', 'around', 'bank', 'best']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:07,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,853 : INFO : built Dictionary(23 unique tokens: ['beauty', 'haircut', 'idea', 'manicure', 'massage']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:07,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,884 : INFO : built Dictionary(40 unique tokens: ['behavior', 'boils', 'cashiers', 'cellphone', 'chat']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:07,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,926 : INFO : built Dictionary(28 unique tokens: ['account', 'anyone', 'bank', 'directly', 'employee']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:07,950 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:07,985 : INFO : built Dictionary(30 unique tokens: ['along', 'came', 'created', 'customer', 'doha']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:07,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,016 : INFO : built Dictionary(43 unique tokens: ['act', 'additionally', 'anyone', 'arrogant', 'behave']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:08,019 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,045 : INFO : built Dictionary(41 unique tokens: ['alive', 'amigos', 'around', 'attendance', 'away']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:08,043 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:08,061 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,067 : INFO : built Dictionary(21 unique tokens: ['abdulla', 'al', 'arab', 'come', 'filipinos']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:08,088 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:08,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,109 : INFO : built Dictionary(52 unique tokens: ['asd', 'assessed', 'assessments', 'autism', 'autistic']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:08,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,226 : INFO : built Dictionary(30 unique tokens: ['anyone', 'around', 'cockroaches', 'company', 'control']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:08,256 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,276 : INFO : built Dictionary(38 unique tokens: ['ago', 'anyone', 'arrived', 'doha', 'everyone']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:08,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,337 : INFO : built Dictionary(36 unique tokens: ['anyone', 'basically', 'belong', 'cats', 'compound']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:08,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,387 : INFO : built Dictionary(22 unique tokens: ['anyone', 'details', 'doha', 'good', 'know']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:08,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,427 : INFO : built Dictionary(23 unique tokens: ['cavity', 'child', 'cost', 'dentist', 'hi']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:08,456 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:08,480 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,495 : INFO : built Dictionary(51 unique tokens: ['anyone', 'anything', 'anywhere', 'away', 'awesome']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:08,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,569 : INFO : built Dictionary(28 unique tokens: ['admin', 'cut', 'delete', 'found', 'least']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:08,595 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:08,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,633 : INFO : built Dictionary(28 unique tokens: ['american', 'anyone', 'chiropractor', 'clinic', 'closed']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:08,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,640 : INFO : built Dictionary(21 unique tokens: ['accept', 'bought', 'carrier', 'get', 'help']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:08,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,662 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:08,681 : INFO : built Dictionary(48 unique tokens: ['anyone', 'buy', 'cheers', 'comment', 'currently']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:08,773 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:08,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,824 : INFO : built Dictionary(32 unique tokens: ['anyone', 'apple', 'doha', 'get', 'got']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:08,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,856 : INFO : built Dictionary(54 unique tokens: ['around', 'blow', 'driving', 'easier', 'east']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:08,945 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:08,965 : INFO : built Dictionary(27 unique tokens: ['anyone', 'cost', 'ipad', 'know', 'much']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:08,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,015 : INFO : built Dictionary(31 unique tokens: ['buy', 'cheapest', 'curious', 'decided', 'finally']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:09,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,054 : INFO : built Dictionary(43 unique tokens: ['approx', 'buy', 'capacity', 'cheapest', 'copied']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:09,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,128 : INFO : built Dictionary(41 unique tokens: ['able', 'announced', 'browse', 'content', 'end']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:09,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,188 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,193 : INFO : built Dictionary(35 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:09,190 : INFO : built Dictionary(50 unique tokens: ['ago', 'barely', 'better', 'blackberry', 'carrefour']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:09,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,237 : INFO : built Dictionary(38 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:01:09,244 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,247 : INFO : built Dictionary(47 unique tokens: ['accessories', 'bought', 'country', 'crazy', 'extraordinarily']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:09,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,295 : INFO : built Dictionary(23 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:09,284 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:09,314 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,328 : INFO : built Dictionary(24 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:09,352 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,385 : INFO : built Dictionary(34 unique tokens: ['anyone', 'apply', 'completed', 'country', 'exit']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:09,414 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,437 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,453 : INFO : built Dictionary(33 unique tokens: ['answer', 'apply', 'applying', 'arrive', 'asap']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:09,488 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,495 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,498 : INFO : built Dictionary(41 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:09,521 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,539 : INFO : built Dictionary(43 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:09,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,557 : INFO : built Dictionary(57 unique tokens: ['already', 'alternatives', 'anyway', 'apartment', 'apartments']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:09,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,585 : INFO : built Dictionary(38 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:09,608 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,615 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,622 : INFO : built Dictionary(31 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:09,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,642 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:09,670 : INFO : built Dictionary(35 unique tokens: ['anyone', 'anything', 'british', 'doha', 'family']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:09,701 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,721 : INFO : built Dictionary(44 unique tokens: ['advice', 'anyone', 'appreciated', 'back', 'doha']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:09,793 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,810 : INFO : built Dictionary(45 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:09,852 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,854 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,858 : INFO : built Dictionary(40 unique tokens: ['corporate', 'dear', 'doha', 'fun', 'give']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:09,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,880 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:09,882 : INFO : built Dictionary(50 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:09,881 : INFO : built Dictionary(32 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:09,894 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,895 : INFO : built Dictionary(43 unique tokens: ['account', 'advice', 'already', 'anyone', 'applying']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:09,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,910 : INFO : built Dictionary(50 unique tokens: ['applying', 'b', 'basically', 'c', 'checked']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:09,915 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,921 : INFO : built Dictionary(36 unique tokens: ['approach', 'done', 'end', 'extension', 'family']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:09,933 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,939 : INFO : built Dictionary(52 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:09,940 : INFO : built Dictionary(37 unique tokens: ['account', 'additinal', 'another', 'automatically', 'back']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:09,950 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,951 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,954 : INFO : built Dictionary(45 unique tokens: ['agreement', 'back', 'call', 'cancelled', 'chance']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:09,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,965 : INFO : built Dictionary(45 unique tokens: ['business', 'changes', 'countries', 'current', 'every']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:09,971 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,973 : INFO : built Dictionary(20 unique tokens: ['extend', 'extension', 'hotel', 'month', 'one']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:09,985 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,986 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:09,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:09,994 : INFO : built Dictionary(30 unique tokens: ['advance', 'case', 'comments', 'dear', 'doha']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:09,993 : INFO : built Dictionary(53 unique tokens: ['agency', 'al', 'another', 'anyone', 'applying']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:10,003 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,005 : INFO : built Dictionary(23 unique tokens: ['asking', 'father', 'friend', 'hi', 'husband']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:10,013 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,023 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:10,016 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,045 : INFO : built Dictionary(40 unique tokens: ['afp', 'allow', 'amnesty', 'arab', 'doha']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:10,086 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,093 : INFO : built Dictionary(43 unique tokens: ['advise', 'agency', 'already', 'anyone', 'applied']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:10,126 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:10,262 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,267 : INFO : built Dictionary(24 unique tokens: ['answer', 'back', 'ban', 'come', 'contract']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:10,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,321 : INFO : built Dictionary(13 unique tokens: ['apart', 'back', 'ban', 'country', 'doesnt']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:10,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,333 : INFO : built Dictionary(45 unique tokens: ['ban', 'bank', 'banks', 'blacklisted', 'cannot']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:10,353 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,358 : INFO : built Dictionary(46 unique tokens: ['agreement', 'back', 'call', 'cancelled', 'chance']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:10,358 : INFO : built Dictionary(21 unique tokens: ['days', 'finish', 'many', 'permit', 'processing']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:10,370 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,376 : INFO : built Dictionary(39 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:10,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,389 : INFO : built Dictionary(45 unique tokens: ['also', 'another', 'answers', 'black', 'countries']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:10,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,403 : INFO : built Dictionary(20 unique tokens: ['c', 'get', 'hcv', 'hepatitis', 'permit']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:10,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,415 : INFO : built Dictionary(37 unique tokens: ['africa', 'anyone', 'beleive', 'cant', 'come']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:10,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,421 : INFO : built Dictionary(19 unique tokens: ['ask', 'banned', 'dubai', 'possible', 'qatar']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:01:10,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,439 : INFO : built Dictionary(37 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:10,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,444 : INFO : built Dictionary(41 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:10,456 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,465 : INFO : built Dictionary(37 unique tokens: ['asia', 'ban', 'benefit', 'comments', 'countries']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:10,467 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,494 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,508 : INFO : built Dictionary(33 unique tokens: ['accompanied', 'arabia', 'ban', 'bicycles', 'country']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:10,499 : INFO : built Dictionary(45 unique tokens: ['also', 'ask', 'become', 'book', 'business']...) from 2 documents (total 64 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:10,530 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,537 : INFO : built Dictionary(38 unique tokens: ['answer', 'anymore', 'apply', 'appreciated', 'company']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:10,541 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,565 : INFO : built Dictionary(33 unique tokens: ['advisable', 'application', 'appreciate', 'canada', 'consultants']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:10,567 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:10,589 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,601 : INFO : built Dictionary(36 unique tokens: ['appreciated', 'august', 'company', 'doha', 'embassy']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:10,628 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,658 : INFO : built Dictionary(51 unique tokens: ['another', 'change', 'co', 'company', 'contract']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:10,724 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,757 : INFO : built Dictionary(46 unique tokens: ['agency', 'almost', 'already', 'answer', 'anyone']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:10,811 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:10,819 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,826 : INFO : built Dictionary(15 unique tokens: ['anyone', 'apple', 'doha', 'get', 'got']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:10,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,833 : INFO : built Dictionary(16 unique tokens: ['best', 'brands', 'buy', 'doha', 'etc']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:10,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,841 : INFO : built Dictionary(30 unique tokens: ['anyone', 'apple', 'best', 'bit', 'bought']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:10,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,853 : INFO : built Dictionary(12 unique tokens: ['buy', 'clothes', 'doha', 'favorite', 'let']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:10,857 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,859 : INFO : built Dictionary(24 unique tokens: ['aslo', 'birthday', 'cannot', 'come', 'day']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:10,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,868 : INFO : built Dictionary(15 unique tokens: ['anyone', 'best', 'compounds', 'doha', 'go']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:01:10,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,880 : INFO : built Dictionary(22 unique tokens: ['cracked', 'edge', 'everything', 'except', 'fine']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:10,888 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,890 : INFO : built Dictionary(35 unique tokens: ['also', 'banana', 'best', 'canal', 'comparable']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:10,901 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,903 : INFO : built Dictionary(18 unique tokens: ['chk', 'ibm', 'laptop', 'lets', 'mine']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:01:10,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,909 : INFO : built Dictionary(11 unique tokens: ['anyone', 'doha', 'good', 'know', 'mattress']...) from 2 documents (total 16 corpus positions)\n",
      "2019-06-17 11:01:10,912 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:10,972 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,976 : INFO : built Dictionary(30 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:10,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,989 : INFO : built Dictionary(20 unique tokens: ['affordable', 'anybody', 'apply', 'assist', 'attestation']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:10,995 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:10,997 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:10,999 : INFO : built Dictionary(40 unique tokens: ['accomodation', 'agreement', 'attach', 'attestation', 'baladiya']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:11,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,015 : INFO : built Dictionary(14 unique tokens: ['arrange', 'contact', 'cost', 'family', 'requirement']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:11,020 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,022 : INFO : built Dictionary(23 unique tokens: ['anybody', 'appreciated', 'call', 'contact', 'could']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:11,029 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:11,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,036 : INFO : built Dictionary(23 unique tokens: ['bank', 'designation', 'employer', 'enough', 'family']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:11,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,045 : INFO : built Dictionary(42 unique tokens: ['advance', 'anybody', 'appointment', 'approval', 'ask']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:11,063 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,066 : INFO : built Dictionary(21 unique tokens: ['application', 'apply', 'committee', 'family', 'got']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:11,071 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,071 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,073 : INFO : built Dictionary(29 unique tokens: ['accommodation', 'anyone', 'appreciated', 'central', 'corporation']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:11,077 : INFO : built Dictionary(22 unique tokens: ['anyone', 'help', 'hey', 'knows', 'looking']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:01:11,082 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:11,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,087 : INFO : built Dictionary(41 unique tokens: ['activities', 'al', 'anyone', 'benefit', 'car']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:11,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,102 : INFO : built Dictionary(42 unique tokens: ['doctor', 'example', 'expat', 'hmc', 'hospital']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:11,109 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:11,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,128 : INFO : built Dictionary(55 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:11,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,185 : INFO : built Dictionary(36 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 51 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:11,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,210 : INFO : built Dictionary(34 unique tokens: ['application', 'credential', 'hmc', 'right', 'stage']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:11,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,253 : INFO : built Dictionary(55 unique tokens: ['accommodation', 'allowance', 'allowances', 'allownaces', 'applied']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:01:11,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,344 : INFO : built Dictionary(36 unique tokens: ['anyone', 'apply', 'complicated', 'confuses', 'french']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:11,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,399 : INFO : built Dictionary(42 unique tokens: ['baby', 'best', 'childbirth', 'deliver', 'doctors']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:11,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,423 : INFO : built Dictionary(52 unique tokens: ['appreciated', 'around', 'available', 'best', 'chicken']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:11,449 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,465 : INFO : built Dictionary(52 unique tokens: ['anyone', 'apparently', 'appointment', 'around', 'decides']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:11,504 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:11,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,551 : INFO : built Dictionary(41 unique tokens: ['adopt', 'anybody', 'buy', 'cheers', 'find']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:11,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,560 : INFO : built Dictionary(42 unique tokens: ['also', 'anyone', 'apply', 'believe', 'free']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:11,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,601 : INFO : built Dictionary(31 unique tokens: ['baby', 'boys', 'everyone', 'girls', 'help']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:11,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,636 : INFO : built Dictionary(46 unique tokens: ['ago', 'allowed', 'arrival', 'arrived', 'assistant']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:11,607 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,646 : INFO : built Dictionary(46 unique tokens: ['around', 'birth', 'cause', 'clinic', 'cost']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:01:11,679 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,709 : INFO : built Dictionary(35 unique tokens: ['aaa', 'assistance', 'breaks', 'call', 'calls']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:11,714 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:11,736 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:11,770 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,787 : INFO : built Dictionary(33 unique tokens: ['advice', 'black', 'causes', 'coke', 'completely']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:11,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,866 : INFO : built Dictionary(43 unique tokens: ['also', 'answer', 'approximate', 'birth', 'bring']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:11,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:11,979 : INFO : built Dictionary(49 unique tokens: ['anymore', 'anyone', 'bad', 'could', 'dermatologists']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:12,060 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:12,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,115 : INFO : built Dictionary(36 unique tokens: ['anybody', 'appreciated', 'approach', 'badly', 'called']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:12,154 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:12,180 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,201 : INFO : built Dictionary(17 unique tokens: ['doha', 'even', 'get', 'got', 'husband']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:12,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,239 : INFO : built Dictionary(36 unique tokens: ['allowed', 'corporation', 'family', 'female', 'females']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:12,269 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,297 : INFO : built Dictionary(26 unique tokens: ['advise', 'change', 'company', 'even', 'husband']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:12,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,353 : INFO : built Dictionary(28 unique tokens: ['asking', 'father', 'friend', 'hi', 'husband']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:12,425 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,430 : INFO : built Dictionary(34 unique tokens: ['active', 'anybody', 'anyone', 'badminton', 'comment']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:12,428 : INFO : built Dictionary(39 unique tokens: ['advance', 'amount', 'business', 'company', 'day']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:12,468 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:12,487 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,501 : INFO : built Dictionary(37 unique tokens: ['area', 'badminton', 'club', 'college', 'court']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:12,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,562 : INFO : built Dictionary(17 unique tokens: ['badminton', 'doha', 'go', 'hi', 'looking']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:01:12,499 : INFO : built Dictionary(49 unique tokens: ['affidavit', 'anybody', 'bank', 'citizen', 'consul']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:01:12,569 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:12,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,594 : INFO : built Dictionary(34 unique tokens: ['already', 'badminton', 'club', 'college', 'doha']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:12,624 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:12,628 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,643 : INFO : built Dictionary(33 unique tokens: ['american', 'bu', 'chance', 'country', 'doha']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:12,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,650 : INFO : built Dictionary(39 unique tokens: ['akin', 'ako', 'ba', 'badminton', 'berks']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:12,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,691 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,681 : INFO : built Dictionary(23 unique tokens: ['apply', 'conditions', 'foreigner', 'marry', 'muslim']...) from 2 documents (total 35 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:12,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,736 : INFO : built Dictionary(31 unique tokens: ['birth', 'child', 'give', 'gots', 'happens']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:12,764 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,734 : INFO : built Dictionary(23 unique tokens: ['badminton', 'club', 'details', 'group', 'grp']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:12,788 : INFO : built Dictionary(47 unique tokens: ['american', 'answers', 'considering', 'currently', 'dating']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:12,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,805 : INFO : built Dictionary(37 unique tokens: ['anybody', 'badminton', 'boring', 'crowded', 'doha']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:12,830 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,848 : INFO : built Dictionary(24 unique tokens: ['able', 'ballistic', 'could', 'daughter', 'doha']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:12,867 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,877 : INFO : built Dictionary(31 unique tokens: ['also', 'anyone', 'appreciated', 'basis', 'cheers']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:12,882 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:12,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:12,921 : INFO : built Dictionary(41 unique tokens: ['accept', 'advance', 'advice', 'also', 'around']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:12,968 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:13,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,713 : INFO : built Dictionary(26 unique tokens: ['decided', 'favorite', 'hearing', 'interested', 'new']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:13,703 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:13,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,762 : INFO : built Dictionary(44 unique tokens: ['advance', 'case', 'comments', 'dear', 'doha']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:13,749 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,790 : INFO : built Dictionary(50 unique tokens: ['advance', 'alone', 'attached', 'behind', 'com']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:13,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,871 : INFO : built Dictionary(21 unique tokens: ['best', 'mention', 'place', 'plz', 'romantic']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:01:13,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,904 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,923 : INFO : built Dictionary(39 unique tokens: ['boring', 'doha', 'get', 'gets', 'guess']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:13,937 : INFO : built Dictionary(18 unique tokens: ['best', 'hang', 'place', 'qatar', 'alkhor']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:01:13,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:13,983 : INFO : built Dictionary(32 unique tokens: ['already', 'appreciated', 'april', 'country', 'days']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:13,991 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:14,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,013 : INFO : built Dictionary(18 unique tokens: ['chill', 'doha', 'favorite', 'place', 'alkhor']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:01:14,023 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:14,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,032 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,035 : INFO : built Dictionary(23 unique tokens: ['cuisine', 'dosa', 'dosas', 'find', 'good']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:14,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,034 : INFO : built Dictionary(34 unique tokens: ['avoid', 'boredom', 'boring', 'doha', 'ixzz']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:14,074 : INFO : built Dictionary(31 unique tokens: ['anyone', 'btw', 'dress', 'dresses', 'gay']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:14,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,100 : INFO : built Dictionary(38 unique tokens: ['anyone', 'besides', 'bodybuilding', 'buy', 'expensive']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:14,126 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:14,136 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:14,147 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,153 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,161 : INFO : built Dictionary(44 unique tokens: ['advance', 'christans', 'different', 'engagement', 'help']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:14,166 : INFO : built Dictionary(27 unique tokens: ['anyone', 'big', 'fish', 'fishing', 'friends']...) from 2 documents (total 31 corpus positions)\n",
      "2019-06-17 11:01:14,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,197 : INFO : built Dictionary(35 unique tokens: ['able', 'advance', 'advantage', 'advice', 'decide']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:14,250 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,238 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:14,281 : INFO : built Dictionary(41 unique tokens: ['events', 'friends', 'go', 'home', 'make']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:14,327 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,368 : INFO : built Dictionary(37 unique tokens: ['arabic', 'course', 'good', 'group', 'lessons']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:14,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,411 : INFO : built Dictionary(60 unique tokens: ['advise', 'airways', 'anybody', 'british', 'children']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:01:14,531 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,569 : INFO : built Dictionary(43 unique tokens: ['anyone', 'arrogant', 'care', 'encounters', 'ever']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:14,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,611 : INFO : built Dictionary(55 unique tokens: ['arab', 'bring', 'care', 'children', 'countries']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:01:14,768 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:14,892 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:14,920 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,937 : INFO : built Dictionary(26 unique tokens: ['anyother', 'available', 'better', 'cook', 'cooking']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:14,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:14,979 : INFO : built Dictionary(30 unique tokens: ['accomodations', 'area', 'doha', 'drive', 'entertainment']...) from 2 documents (total 40 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:15,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,023 : INFO : built Dictionary(32 unique tokens: ['buy', 'carrefour', 'cook', 'enough', 'etc']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:15,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,076 : INFO : built Dictionary(41 unique tokens: ['add', 'also', 'answers', 'anyone', 'away']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:15,163 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,181 : INFO : built Dictionary(24 unique tokens: ['anyone', 'average', 'breakfast', 'cost', 'could']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:15,207 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,223 : INFO : built Dictionary(27 unique tokens: ['anyone', 'bring', 'case', 'christmas', 'coming']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:15,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,275 : INFO : built Dictionary(41 unique tokens: ['anyone', 'appropiate', 'behind', 'dirty', 'eastern']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:15,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,333 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:15,366 : INFO : built Dictionary(30 unique tokens: ['accountant', 'accounting', 'achive', 'advice', 'anyone']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:15,363 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,395 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,407 : INFO : built Dictionary(50 unique tokens: ['advice', 'anyone', 'could', 'currently', 'doha']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:15,397 : INFO : built Dictionary(36 unique tokens: ['also', 'answer', 'ask', 'company', 'delhi']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:15,435 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:15,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,472 : INFO : built Dictionary(33 unique tokens: ['air', 'arabia', 'bag', 'baggage', 'compensation']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:15,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,531 : INFO : built Dictionary(36 unique tokens: ['anyone', 'buy', 'days', 'diet', 'effective']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:15,576 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:15,565 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,605 : INFO : built Dictionary(47 unique tokens: ['apply', 'cost', 'get', 'hello', 'im']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:15,725 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,745 : INFO : built Dictionary(48 unique tokens: ['able', 'airlines', 'airways', 'anyone', 'benefits']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:15,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:15,902 : INFO : built Dictionary(59 unique tokens: ['ask', 'attend', 'clinics', 'days', 'doha']...) from 2 documents (total 71 corpus positions)\n",
      "2019-06-17 11:01:16,111 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,140 : INFO : built Dictionary(57 unique tokens: ['actually', 'also', 'anybody', 'anyone', 'anything']...) from 2 documents (total 72 corpus positions)\n",
      "2019-06-17 11:01:16,276 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,317 : INFO : built Dictionary(47 unique tokens: ['appreciated', 'around', 'available', 'best', 'chicken']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:16,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,375 : INFO : built Dictionary(39 unique tokens: ['anyone', 'center', 'enough', 'filipino', 'getting']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:16,435 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,463 : INFO : built Dictionary(40 unique tokens: ['anyone', 'buy', 'days', 'diet', 'effective']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:16,485 : INFO : built Dictionary(62 unique tokens: ['also', 'back', 'care', 'collect', 'cost']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:01:16,527 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,566 : INFO : built Dictionary(30 unique tokens: ['believe', 'calories', 'counting', 'days', 'dieting']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:16,591 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,614 : INFO : built Dictionary(55 unique tokens: ['anyone', 'ask', 'bags', 'brand', 'buying']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:16,782 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,772 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:16,812 : INFO : built Dictionary(20 unique tokens: ['anybody', 'herbalife', 'know', 'losing', 'products']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:01:16,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,847 : INFO : built Dictionary(53 unique tokens: ['abroad', 'anyone', 'bank', 'blamed', 'brazilian']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:01:16,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,863 : INFO : built Dictionary(24 unique tokens: ['anyone', 'center', 'curves', 'help', 'hi']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:16,885 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,914 : INFO : built Dictionary(41 unique tokens: ['body', 'challenge', 'cm', 'diet', 'dress']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:16,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:16,963 : INFO : built Dictionary(37 unique tokens: ['download', 'english', 'free', 'hindi', 'like']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:16,944 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:16,986 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,005 : INFO : built Dictionary(34 unique tokens: ['actrim', 'anyone', 'best', 'give', 'healthy']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:16,998 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:17,041 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,061 : INFO : built Dictionary(26 unique tokens: ['bread', 'diet', 'eat', 'effective', 'fitness']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:17,105 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:17,126 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,168 : INFO : built Dictionary(26 unique tokens: ['anyone', 'aspire', 'ball', 'fee', 'fitness']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:17,205 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:17,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,490 : INFO : built Dictionary(32 unique tokens: ['bad', 'company', 'could', 'doha', 'dubai']...) from 2 documents (total 86 corpus positions)\n",
      "2019-06-17 11:01:17,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,544 : INFO : built Dictionary(42 unique tokens: ['appropriate', 'cooler', 'find', 'fridge', 'get']...) from 2 documents (total 61 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:17,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,653 : INFO : built Dictionary(55 unique tokens: ['american', 'bachelor', 'bayt', 'com', 'cv']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:01:17,754 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:17,769 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,781 : INFO : built Dictionary(53 unique tokens: ['car', 'charger', 'clear', 'comments', 'cost']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:01:17,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,861 : INFO : built Dictionary(34 unique tokens: ['doha', 'find', 'hard', 'job', 'nowadays']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:17,892 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,915 : INFO : built Dictionary(37 unique tokens: ['adays', 'get', 'hard', 'impossible', 'like']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:17,941 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:17,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:17,984 : INFO : built Dictionary(49 unique tokens: ['accross', 'arabic', 'article', 'bbc', 'brain']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:17,976 : INFO : built Dictionary(33 unique tokens: ['advice', 'anybody', 'cost', 'duster', 'experience']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:18,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,040 : INFO : built Dictionary(36 unique tokens: ['accord', 'advise', 'altima', 'better', 'buy']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:18,082 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,090 : INFO : built Dictionary(55 unique tokens: ['data', 'disk', 'drive', 'even', 'external']...) from 2 documents (total 75 corpus positions)\n",
      "2019-06-17 11:01:18,121 : INFO : built Dictionary(30 unique tokens: ['better', 'buy', 'car', 'civic', 'corolla']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:18,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,196 : INFO : built Dictionary(47 unique tokens: ['abt', 'also', 'best', 'brand', 'brands']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:18,283 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:18,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,352 : INFO : built Dictionary(36 unique tokens: ['back', 'cadillac', 'car', 'comfortable', 'condition']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:18,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,377 : INFO : built Dictionary(48 unique tokens: ['club', 'girls', 'hard', 'help', 'key']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:18,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,443 : INFO : built Dictionary(36 unique tokens: ['accord', 'advise', 'altima', 'better', 'buy']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:18,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,477 : INFO : built Dictionary(58 unique tokens: ['advice', 'always', 'anyone', 'applied', 'approval']...) from 2 documents (total 74 corpus positions)\n",
      "2019-06-17 11:01:18,514 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,565 : INFO : built Dictionary(34 unique tokens: ['according', 'advise', 'available', 'buy', 'clio']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:18,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,631 : INFO : built Dictionary(47 unique tokens: ['accent', 'also', 'alto', 'anyone', 'basic']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:18,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,693 : INFO : built Dictionary(43 unique tokens: ['confirm', 'department', 'hr', 'informed', 'must']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:18,696 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:18,719 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:18,725 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:18,735 : INFO : built Dictionary(51 unique tokens: ['advice', 'best', 'better', 'buy', 'cars']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:01:18,872 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:19,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,235 : INFO : built Dictionary(39 unique tokens: ['business', 'changes', 'countries', 'current', 'every']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:19,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,325 : INFO : built Dictionary(26 unique tokens: ['back', 'business', 'exchange', 'foreign', 'good']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:19,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,367 : INFO : built Dictionary(28 unique tokens: ['anyone', 'appreciated', 'certain', 'doha', 'elsewhere']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:19,341 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,401 : INFO : built Dictionary(21 unique tokens: ['boy', 'child', 'help', 'need', 'old']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:19,399 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:19,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,421 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,444 : INFO : built Dictionary(50 unique tokens: ['asd', 'assessed', 'assessments', 'autism', 'autistic']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:19,440 : INFO : built Dictionary(32 unique tokens: ['alternative', 'alternatives', 'could', 'dont', 'eating']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:19,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,495 : INFO : built Dictionary(28 unique tokens: ['air', 'al', 'anybody', 'asia', 'canter']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:19,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,537 : INFO : built Dictionary(20 unique tokens: ['anyone', 'available', 'counselling', 'doha', 'good']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:19,551 : INFO : built Dictionary(33 unique tokens: ['climate', 'countries', 'fat', 'food', 'gain']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:19,560 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,591 : INFO : built Dictionary(33 unique tokens: ['anxiety', 'certainly', 'conflict', 'counselors', 'depression']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:19,626 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,621 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:19,647 : INFO : built Dictionary(30 unique tokens: ['banned', 'cant', 'check', 'doha', 'find']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:19,646 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,669 : INFO : built Dictionary(38 unique tokens: ['born', 'children', 'classifieds', 'desai', 'donot']...) from 2 documents (total 49 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:19,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,722 : INFO : built Dictionary(28 unique tokens: ['anyone', 'center', 'curves', 'help', 'hi']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:19,737 : INFO : built Dictionary(45 unique tokens: ['ago', 'anyone', 'best', 'counseling', 'difficult']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:19,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,785 : INFO : built Dictionary(39 unique tokens: ['ago', 'anyone', 'arrived', 'doha', 'everyone']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:19,801 : INFO : built Dictionary(40 unique tokens: ['accommodation', 'allowance', 'anxious', 'contract', 'corp']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:19,821 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:19,836 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,902 : INFO : built Dictionary(35 unique tokens: ['come', 'contact', 'couple', 'designed', 'details']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:19,914 : INFO : built Dictionary(37 unique tokens: ['ahli', 'al', 'anyone', 'clear', 'coverage']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:19,937 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:19,960 : INFO : built Dictionary(35 unique tokens: ['adenoids', 'advice', 'anyone', 'childs', 'doha']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:19,933 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:20,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,038 : INFO : built Dictionary(35 unique tokens: ['bad', 'beauty', 'career', 'child', 'children']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:20,068 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:20,302 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:20,325 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,344 : INFO : built Dictionary(42 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:20,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,400 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,393 : INFO : built Dictionary(43 unique tokens: ['accept', 'advise', 'back', 'come', 'company']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:20,413 : INFO : built Dictionary(36 unique tokens: ['account', 'anyone', 'bank', 'directly', 'employee']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:20,473 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:20,475 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:20,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,492 : INFO : built Dictionary(34 unique tokens: ['anyone', 'anything', 'appropriate', 'comfortable', 'especially']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:20,508 : INFO : built Dictionary(46 unique tokens: ['another', 'change', 'co', 'company', 'contract']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:20,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,552 : INFO : built Dictionary(33 unique tokens: ['address', 'anyone', 'baby', 'contact', 'could']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:20,582 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,604 : INFO : built Dictionary(30 unique tokens: ['advise', 'company', 'dont', 'experts', 'got']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:20,609 : INFO : built Dictionary(33 unique tokens: ['advisable', 'application', 'appreciate', 'canada', 'consultants']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:20,628 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,659 : INFO : built Dictionary(45 unique tokens: ['accept', 'ban', 'cannot', 'clerical', 'company']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:20,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,687 : INFO : built Dictionary(57 unique tokens: ['asd', 'assessed', 'assessments', 'autism', 'autistic']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:20,729 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:20,749 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,777 : INFO : built Dictionary(46 unique tokens: ['according', 'action', 'advise', 'allow', 'back']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:20,789 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:20,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,848 : INFO : built Dictionary(40 unique tokens: ['acceptable', 'anything', 'bit', 'concerned', 'cover']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:20,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,881 : INFO : built Dictionary(47 unique tokens: ['accepted', 'accepting', 'advice', 'ago', 'anymore']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:20,888 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,911 : INFO : built Dictionary(39 unique tokens: ['advance', 'anyone', 'body', 'climate', 'considering']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:20,975 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:20,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,015 : INFO : built Dictionary(29 unique tokens: ['argu', 'country', 'friend', 'gulf', 'kuwait']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:21,013 : INFO : built Dictionary(52 unique tokens: ['actually', 'amazing', 'appreciate', 'courier', 'dear']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:21,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,050 : INFO : built Dictionary(31 unique tokens: ['add', 'best', 'bit', 'choose', 'else']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:21,071 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:21,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,161 : INFO : built Dictionary(36 unique tokens: ['advance', 'anyone', 'ask', 'contract', 'end']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:21,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,251 : INFO : built Dictionary(37 unique tokens: ['advice', 'change', 'cid', 'clearance', 'company']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:21,304 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:21,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,379 : INFO : built Dictionary(45 unique tokens: ['advice', 'back', 'based', 'companies', 'contract']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:21,458 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:21,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:21,815 : INFO : built Dictionary(44 unique tokens: ['already', 'arabic', 'chef', 'doha', 'end']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:21,917 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:21,926 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,948 : INFO : built Dictionary(35 unique tokens: ['available', 'could', 'daily', 'evening', 'find']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:21,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:21,989 : INFO : built Dictionary(36 unique tokens: ['agency', 'application', 'apply', 'applying', 'bayt']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:22,030 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:22,036 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,051 : INFO : built Dictionary(47 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:22,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,069 : INFO : built Dictionary(37 unique tokens: ['ad', 'amount', 'anyone', 'data', 'entry']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:22,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,119 : INFO : built Dictionary(38 unique tokens: ['application', 'apply', 'applying', 'care', 'failed']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:22,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,202 : INFO : built Dictionary(36 unique tokens: ['appreciated', 'august', 'company', 'doha', 'embassy']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:22,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,219 : INFO : built Dictionary(36 unique tokens: ['advised', 'ago', 'bank', 'chance', 'clearance']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:22,264 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,267 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:22,284 : INFO : built Dictionary(37 unique tokens: ['american', 'bachelor', 'bayt', 'com', 'cv']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:22,289 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,308 : INFO : built Dictionary(47 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:22,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,376 : INFO : built Dictionary(44 unique tokens: ['airport', 'alcohol', 'apartment', 'cafeteria', 'close']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:22,390 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,406 : INFO : built Dictionary(39 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:22,419 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:22,425 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,440 : INFO : built Dictionary(18 unique tokens: ['aravind', 'doha', 'experience', 'hi', 'job']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:22,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,467 : INFO : built Dictionary(49 unique tokens: ['actually', 'anybody', 'anyone', 'applied', 'come']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:22,456 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:22,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,479 : INFO : built Dictionary(35 unique tokens: ['b', 'documents', 'doha', 'done', 'ect']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:22,503 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,503 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:22,540 : INFO : built Dictionary(49 unique tokens: ['advance', 'anybody', 'appointment', 'approval', 'ask']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:22,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,679 : INFO : built Dictionary(29 unique tokens: ['aquiring', 'doha', 'easy', 'everyone', 'get']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:22,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,712 : INFO : built Dictionary(46 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:22,759 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,785 : INFO : built Dictionary(49 unique tokens: ['already', 'anyone', 'application', 'apply', 'aside']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:22,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,885 : INFO : built Dictionary(43 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:01:22,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,934 : INFO : built Dictionary(31 unique tokens: ['days', 'finish', 'many', 'permit', 'processing']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:22,952 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:22,946 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:23,014 : INFO : built Dictionary(32 unique tokens: ['change', 'family', 'husband', 'new', 'sponsor']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:23,105 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,108 : INFO : built Dictionary(42 unique tokens: ['apply', 'cost', 'get', 'hello', 'im']...) from 2 documents (total 59 corpus positions)\n",
      "2019-06-17 11:01:23,218 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:23,231 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,253 : INFO : built Dictionary(50 unique tokens: ['advance', 'company', 'duration', 'help', 'hi']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:23,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,335 : INFO : built Dictionary(41 unique tokens: ['able', 'ask', 'clearance', 'get', 'gets']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:23,450 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,493 : INFO : built Dictionary(45 unique tokens: ['advise', 'apply', 'coming', 'currently', 'ever']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:23,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,566 : INFO : built Dictionary(33 unique tokens: ['advise', 'around', 'canada', 'contact', 'daycare']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:23,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,596 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:23,608 : INFO : built Dictionary(30 unique tokens: ['c', 'get', 'hcv', 'hepatitis', 'permit']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:23,614 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,633 : INFO : built Dictionary(51 unique tokens: ['available', 'besides', 'checked', 'days', 'disease']...) from 2 documents (total 65 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:23,637 : INFO : built Dictionary(47 unique tokens: ['admissions', 'admit', 'advance', 'back', 'birla']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:23,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,733 : INFO : built Dictionary(27 unique tokens: ['companies', 'currently', 'daycare', 'employers', 'facility']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:23,761 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,787 : INFO : built Dictionary(47 unique tokens: ['advise', 'airways', 'anybody', 'british', 'children']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:23,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,840 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,847 : INFO : built Dictionary(48 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:01:23,877 : INFO : built Dictionary(40 unique tokens: ['big', 'bored', 'contact', 'daughter', 'doha']...) from 2 documents (total 50 corpus positions)\n",
      "2019-06-17 11:01:23,951 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:23,953 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:23,989 : INFO : built Dictionary(45 unique tokens: ['almost', 'appeal', 'application', 'applied', 'first']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:24,005 : INFO : built Dictionary(40 unique tokens: ['born', 'children', 'classifieds', 'desai', 'donot']...) from 2 documents (total 51 corpus positions)\n",
      "2019-06-17 11:01:24,032 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,051 : INFO : built Dictionary(37 unique tokens: ['afternoon', 'anyone', 'bus', 'child', 'children']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:24,053 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:24,104 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,133 : INFO : built Dictionary(29 unique tokens: ['available', 'baby', 'babycare', 'babysitting', 'doha']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:24,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,208 : INFO : built Dictionary(52 unique tokens: ['asd', 'assessed', 'assessments', 'autism', 'autistic']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:24,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,295 : INFO : built Dictionary(38 unique tokens: ['another', 'baby', 'babysitter', 'come', 'common']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:24,346 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:24,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,538 : INFO : built Dictionary(38 unique tokens: ['alcohol', 'ban', 'brands', 'certain', 'check']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:24,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,577 : INFO : built Dictionary(39 unique tokens: ['anyone', 'apple', 'best', 'bit', 'bought']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:24,635 : INFO : Removed 4 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:24,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,657 : INFO : built Dictionary(31 unique tokens: ['al', 'anyone', 'area', 'bill', 'called']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:24,702 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:24,740 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,747 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,747 : INFO : built Dictionary(48 unique tokens: ['advice', 'allowance', 'apt', 'car', 'cost']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:24,770 : INFO : built Dictionary(46 unique tokens: ['asked', 'base', 'car', 'company', 'could']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:24,789 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:24,809 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,823 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:24,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,819 : INFO : built Dictionary(42 unique tokens: ['allow', 'another', 'apologize', 'conduct', 'confess']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:24,859 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:24,856 : INFO : built Dictionary(40 unique tokens: ['alahly', 'alot', 'also', 'best', 'clinics']...) from 2 documents (total 48 corpus positions)\n",
      "2019-06-17 11:01:24,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,889 : INFO : built Dictionary(26 unique tokens: ['anyway', 'facebook', 'feeling', 'fellow', 'group']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:24,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,912 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,926 : INFO : built Dictionary(24 unique tokens: ['angel', 'brings', 'face', 'hearing', 'little']...) from 2 documents (total 27 corpus positions)\n",
      "2019-06-17 11:01:24,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,919 : INFO : built Dictionary(49 unique tokens: ['apply', 'called', 'card', 'company', 'cost']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:24,962 : INFO : built Dictionary(25 unique tokens: ['enough', 'good', 'housing', 'including', 'k']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:24,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:24,985 : INFO : built Dictionary(31 unique tokens: ['average', 'considering', 'foundation', 'get', 'hi']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:24,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,020 : INFO : built Dictionary(39 unique tokens: ['airways', 'anyone', 'anything', 'commercial', 'currently']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:25,027 : INFO : built Dictionary(46 unique tokens: ['always', 'amazing', 'aside', 'bring', 'celebrate']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:25,056 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,074 : INFO : built Dictionary(28 unique tokens: ['also', 'anyone', 'apply', 'believe', 'free']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:25,077 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:25,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,102 : INFO : built Dictionary(35 unique tokens: ['anyone', 'apply', 'complicated', 'confuses', 'french']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:25,132 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,146 : INFO : built Dictionary(35 unique tokens: ['appreciated', 'blind', 'card', 'cost', 'cover']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:25,178 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:25,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,210 : INFO : built Dictionary(46 unique tokens: ['ahli', 'al', 'babies', 'birth', 'c']...) from 2 documents (total 51 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:25,293 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:25,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,325 : INFO : built Dictionary(31 unique tokens: ['american', 'anyone', 'chiropractor', 'clinic', 'closed']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:25,363 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:25,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,401 : INFO : built Dictionary(38 unique tokens: ['advance', 'baby', 'bcg', 'better', 'child']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:25,438 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:25,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,650 : INFO : built Dictionary(17 unique tokens: ['anyone', 'car', 'company', 'departure', 'drive']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:25,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,697 : INFO : built Dictionary(22 unique tokens: ['anyone', 'fly', 'flying', 'learn', 'lessons']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:25,708 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:25,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,744 : INFO : built Dictionary(55 unique tokens: ['acutally', 'also', 'appreciate', 'authority', 'aviation']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:01:25,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,861 : INFO : built Dictionary(23 unique tokens: ['application', 'credential', 'hmc', 'right', 'stage']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:25,894 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,897 : INFO : built Dictionary(31 unique tokens: ['agency', 'authenticate', 'embassy', 'going', 'issued']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:25,919 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:25,978 : INFO : built Dictionary(28 unique tokens: ['aircraft', 'airways', 'anyone', 'called', 'engineer']...) from 2 documents (total 39 corpus positions)\n",
      "2019-06-17 11:01:26,019 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,028 : INFO : built Dictionary(22 unique tokens: ['agencies', 'everybody', 'hi', 'know', 'qatar']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:26,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,048 : INFO : built Dictionary(27 unique tokens: ['aeroplane', 'air', 'answer', 'befor', 'give']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:26,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,092 : INFO : built Dictionary(26 unique tokens: ['cancel', 'cancellation', 'even', 'guys', 'passport']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:26,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,115 : INFO : built Dictionary(23 unique tokens: ['almost', 'another', 'apply', 'case', 'company']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:26,113 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,131 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,135 : INFO : built Dictionary(49 unique tokens: ['arabic', 'ask', 'back', 'beg', 'cancellation']...) from 2 documents (total 70 corpus positions)\n",
      "2019-06-17 11:01:26,148 : INFO : built Dictionary(12 unique tokens: ['advise', 'help', 'need', 'please', 'sponsorship']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:26,164 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,193 : INFO : built Dictionary(30 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:26,197 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:26,207 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,225 : INFO : built Dictionary(14 unique tokens: ['enlighten', 'hi', 'hw', 'kindly', 'kinds']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:01:26,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,240 : INFO : built Dictionary(36 unique tokens: ['applicable', 'avoid', 'back', 'ban', 'case']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:26,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,273 : INFO : built Dictionary(18 unique tokens: ['apply', 'company', 'got', 'job', 'new']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:26,297 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:26,326 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,330 : INFO : built Dictionary(40 unique tokens: ['another', 'change', 'co', 'company', 'contract']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:26,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,375 : INFO : built Dictionary(23 unique tokens: ['anyone', 'anything', 'companies', 'company', 'damaging']...) from 2 documents (total 34 corpus positions)\n",
      "2019-06-17 11:01:26,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,415 : INFO : built Dictionary(34 unique tokens: ['advance', 'bank', 'big', 'card', 'company']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:26,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,471 : INFO : built Dictionary(31 unique tokens: ['administration', 'agent', 'body', 'business', 'change']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:26,493 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:26,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,762 : INFO : built Dictionary(53 unique tokens: ['come', 'country', 'culture', 'feel', 'idea']...) from 2 documents (total 65 corpus positions)\n",
      "2019-06-17 11:01:26,878 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:26,901 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:26,939 : INFO : built Dictionary(45 unique tokens: ['allowed', 'chicken', 'daily', 'days', 'done']...) from 2 documents (total 53 corpus positions)\n",
      "2019-06-17 11:01:26,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,057 : INFO : built Dictionary(49 unique tokens: ['advice', 'appreciated', 'august', 'comments', 'documents']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:27,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,136 : INFO : built Dictionary(39 unique tokens: ['advance', 'advice', 'civil', 'convert', 'daughter']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:27,138 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,183 : INFO : built Dictionary(60 unique tokens: ['advice', 'come', 'coming', 'comments', 'continue']...) from 2 documents (total 67 corpus positions)\n",
      "2019-06-17 11:01:27,202 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,223 : INFO : built Dictionary(44 unique tokens: ['able', 'ask', 'brought', 'change', 'children']...) from 2 documents (total 73 corpus positions)\n",
      "2019-06-17 11:01:27,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,358 : INFO : built Dictionary(31 unique tokens: ['advice', 'anyone', 'application', 'apply', 'cannot']...) from 2 documents (total 48 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:27,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,388 : INFO : built Dictionary(51 unique tokens: ['anyone', 'apart', 'bring', 'customs', 'expect']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:27,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,425 : INFO : built Dictionary(30 unique tokens: ['appreciate', 'bringing', 'family', 'help', 'limitations']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:27,469 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,490 : INFO : built Dictionary(39 unique tokens: ['anyone', 'apply', 'completed', 'country', 'exit']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:27,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,502 : INFO : built Dictionary(48 unique tokens: ['allowed', 'come', 'doha', 'dresses', 'formals']...) from 2 documents (total 54 corpus positions)\n",
      "2019-06-17 11:01:27,559 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,615 : INFO : built Dictionary(39 unique tokens: ['answer', 'apply', 'applying', 'arrive', 'asap']...) from 2 documents (total 63 corpus positions)\n",
      "2019-06-17 11:01:27,645 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,668 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,663 : INFO : built Dictionary(47 unique tokens: ['advice', 'ang', 'anybody', 'apply', 'appreciated']...) from 2 documents (total 66 corpus positions)\n",
      "2019-06-17 11:01:27,687 : INFO : built Dictionary(34 unique tokens: ['bacon', 'bring', 'friends', 'ok', 'packed']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:27,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,730 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,722 : INFO : built Dictionary(44 unique tokens: ['advisable', 'altima', 'back', 'car', 'earlier']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:27,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,756 : INFO : built Dictionary(50 unique tokens: ['application', 'assistance', 'attest', 'baby', 'birth']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:27,785 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,828 : INFO : built Dictionary(44 unique tokens: ['ago', 'anyone', 'anytime', 'applied', 'banned']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:27,821 : INFO : built Dictionary(36 unique tokens: ['anyone', 'bahrain', 'bit', 'please', 'pm']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:27,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,859 : INFO : built Dictionary(54 unique tokens: ['also', 'anyone', 'august', 'bring', 'car']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:01:27,918 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:27,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:27,934 : INFO : built Dictionary(40 unique tokens: ['advise', 'anyone', 'applying', 'attest', 'attestation']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:27,973 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:27,994 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:28,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,665 : INFO : built Dictionary(50 unique tokens: ['ago', 'believe', 'co', 'difficult', 'expat']...) from 2 documents (total 64 corpus positions)\n",
      "2019-06-17 11:01:28,675 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,696 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,705 : INFO : built Dictionary(35 unique tokens: ['amateur', 'basketball', 'community', 'couple', 'doha']...) from 2 documents (total 45 corpus positions)\n",
      "2019-06-17 11:01:28,728 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,748 : INFO : built Dictionary(19 unique tokens: ['afternoon', 'basketball', 'courts', 'every', 'paid']...) from 2 documents (total 26 corpus positions)\n",
      "2019-06-17 11:01:28,761 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,798 : INFO : built Dictionary(24 unique tokens: ['basketball', 'c', 'enthusiast', 'group', 'know']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:28,798 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,819 : INFO : built Dictionary(46 unique tokens: ['acceptable', 'anything', 'bit', 'concerned', 'cover']...) from 2 documents (total 57 corpus positions)\n",
      "2019-06-17 11:01:28,811 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,876 : INFO : built Dictionary(18 unique tokens: ['anybody', 'basketball', 'knows', 'play', 'playin']...) from 2 documents (total 24 corpus positions)\n",
      "2019-06-17 11:01:28,910 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:28,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,953 : INFO : built Dictionary(35 unique tokens: ['anyone', 'back', 'bowling', 'could', 'days']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:28,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:28,989 : INFO : built Dictionary(50 unique tokens: ['burqa', 'conservatively', 'culture', 'disrespectful', 'dressed']...) from 2 documents (total 60 corpus positions)\n",
      "2019-06-17 11:01:28,997 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,023 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,061 : INFO : built Dictionary(48 unique tokens: ['advance', 'anyone', 'anyways', 'ask', 'basketball']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:29,145 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,153 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,198 : INFO : built Dictionary(48 unique tokens: ['advice', 'also', 'anywhere', 'best', 'bit']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:29,207 : INFO : built Dictionary(41 unique tokens: ['anyone', 'anything', 'appropriate', 'comfortable', 'especially']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:29,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,281 : INFO : built Dictionary(34 unique tokens: ['aloud', 'bring', 'clothes', 'december', 'jeans']...) from 2 documents (total 43 corpus positions)\n",
      "2019-06-17 11:01:29,271 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:29,304 : INFO : built Dictionary(18 unique tokens: ['home', 'room', 'spend', 'time', 'arrange']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:29,329 : INFO : built Dictionary(44 unique tokens: ['abaya', 'arab', 'curious', 'dying', 'hey']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:29,323 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,352 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,356 : INFO : built Dictionary(20 unique tokens: ['compound', 'gardens', 'information', 'someone', 'tell']...) from 2 documents (total 23 corpus positions)\n",
      "2019-06-17 11:01:29,377 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,394 : INFO : built Dictionary(28 unique tokens: ['bacon', 'black', 'breakfest', 'bull', 'call']...) from 2 documents (total 33 corpus positions)\n",
      "2019-06-17 11:01:29,401 : INFO : built Dictionary(46 unique tokens: ['cheers', 'chilly', 'cold', 'coming', 'december']...) from 2 documents (total 56 corpus positions)\n",
      "2019-06-17 11:01:29,419 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:29,477 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:29,497 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,517 : INFO : built Dictionary(57 unique tokens: ['advice', 'come', 'coming', 'comments', 'continue']...) from 2 documents (total 69 corpus positions)\n",
      "2019-06-17 11:01:29,610 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,629 : INFO : built Dictionary(50 unique tokens: ['attitiude', 'attitude', 'bad', 'countries', 'daughters']...) from 2 documents (total 62 corpus positions)\n",
      "2019-06-17 11:01:29,701 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,735 : INFO : built Dictionary(55 unique tokens: ['abaya', 'advise', 'appreciate', 'assuming', 'better']...) from 2 documents (total 68 corpus positions)\n",
      "2019-06-17 11:01:29,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,790 : INFO : built Dictionary(13 unique tokens: ['around', 'buy', 'doha', 'fish', 'fresh']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:29,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,794 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:29,806 : INFO : built Dictionary(16 unique tokens: ['anyone', 'best', 'buy', 'cod', 'could']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:29,840 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,861 : INFO : built Dictionary(30 unique tokens: ['birthday', 'caviar', 'excellent', 'good', 'la']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:29,887 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,926 : INFO : built Dictionary(31 unique tokens: ['ask', 'avoid', 'best', 'cost', 'course']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:29,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:29,997 : INFO : built Dictionary(25 unique tokens: ['also', 'anyone', 'choose', 'cook', 'find']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:30,013 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:30,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,041 : INFO : built Dictionary(23 unique tokens: ['also', 'appreciate', 'cooking', 'doha', 'find']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:30,055 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,077 : INFO : built Dictionary(30 unique tokens: ['apart', 'bar', 'behalf', 'belt', 'celebrate']...) from 2 documents (total 35 corpus positions)\n",
      "2019-06-17 11:01:30,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,116 : INFO : built Dictionary(14 unique tokens: ['community', 'filipino', 'filipinos', 'find', 'many']...) from 2 documents (total 18 corpus positions)\n",
      "2019-06-17 11:01:30,137 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,165 : INFO : built Dictionary(35 unique tokens: ['advance', 'akis', 'al', 'already', 'anybody']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:30,121 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,186 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,193 : INFO : built Dictionary(12 unique tokens: ['eat', 'fish', 'least', 'mercury', 'safe']...) from 2 documents (total 19 corpus positions)\n",
      "2019-06-17 11:01:30,182 : INFO : built Dictionary(12 unique tokens: ['anyone', 'anything', 'attempt', 'east', 'ever']...) from 2 documents (total 30 corpus positions)\n",
      "2019-06-17 11:01:30,208 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:30,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,236 : INFO : built Dictionary(29 unique tokens: ['anyone', 'details', 'directly', 'doha', 'drive']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:30,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,288 : INFO : built Dictionary(32 unique tokens: ['advance', 'apply', 'best', 'decided', 'driving']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:30,360 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:30,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,421 : INFO : built Dictionary(38 unique tokens: ['advice', 'bad', 'bullshit', 'different', 'driving']...) from 2 documents (total 49 corpus positions)\n",
      "2019-06-17 11:01:30,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,456 : INFO : built Dictionary(19 unique tokens: ['car', 'drift', 'hi', 'knows', 'like']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:30,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,484 : INFO : built Dictionary(32 unique tokens: ['answered', 'ask', 'asked', 'babies', 'big']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:30,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,521 : INFO : built Dictionary(17 unique tokens: ['anybody', 'best', 'doha', 'driving', 'school']...) from 2 documents (total 21 corpus positions)\n",
      "2019-06-17 11:01:30,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,569 : INFO : built Dictionary(36 unique tokens: ['ask', 'asked', 'asking', 'common', 'could']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:30,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,614 : INFO : built Dictionary(23 unique tokens: ['arabic', 'course', 'good', 'group', 'lessons']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:30,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,658 : INFO : built Dictionary(30 unique tokens: ['comments', 'driver', 'drivers', 'driving', 'goes']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:30,690 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:30,803 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,849 : INFO : built Dictionary(37 unique tokens: ['account', 'also', 'bank', 'born', 'cheapest']...) from 2 documents (total 52 corpus positions)\n",
      "2019-06-17 11:01:30,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,893 : INFO : built Dictionary(29 unique tokens: ['accnt', 'anybody', 'charges', 'commission', 'electronic']...) from 2 documents (total 40 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-17 11:01:30,914 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:30,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:30,953 : INFO : built Dictionary(49 unique tokens: ['account', 'across', 'aka', 'axis', 'bank']...) from 2 documents (total 61 corpus positions)\n",
      "2019-06-17 11:01:31,029 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:31,061 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,076 : INFO : built Dictionary(49 unique tokens: ['account', 'appeared', 'bank', 'bussiness', 'call']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:31,136 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,165 : INFO : built Dictionary(32 unique tokens: ['accomodation', 'day', 'doha', 'free', 'give']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:31,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,219 : INFO : built Dictionary(11 unique tokens: ['choice', 'decide', 'experts', 'got', 'htc']...) from 2 documents (total 22 corpus positions)\n",
      "2019-06-17 11:01:31,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,242 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,240 : INFO : built Dictionary(25 unique tokens: ['afternoon', 'good', 'guys', 'ideas', 'like']...) from 2 documents (total 28 corpus positions)\n",
      "2019-06-17 11:01:31,258 : INFO : built Dictionary(25 unique tokens: ['baby', 'best', 'childbirth', 'deliver', 'doctors']...) from 2 documents (total 25 corpus positions)\n",
      "2019-06-17 11:01:31,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,279 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,291 : INFO : built Dictionary(27 unique tokens: ['advisable', 'application', 'appreciate', 'canada', 'consultants']...) from 2 documents (total 29 corpus positions)\n",
      "2019-06-17 11:01:31,318 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,305 : INFO : built Dictionary(34 unique tokens: ['anyone', 'country', 'easy', 'evening', 'get']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:31,335 : INFO : built Dictionary(35 unique tokens: ['attitude', 'break', 'change', 'colour', 'could']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:31,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,384 : INFO : built Dictionary(50 unique tokens: ['also', 'apply', 'companies', 'company', 'day']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:31,405 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:31,424 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,442 : INFO : built Dictionary(44 unique tokens: ['area', 'captured', 'cover', 'covering', 'deranged']...) from 2 documents (total 47 corpus positions)\n",
      "2019-06-17 11:01:31,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,494 : INFO : built Dictionary(50 unique tokens: ['around', 'bundle', 'car', 'card', 'care']...) from 2 documents (total 55 corpus positions)\n",
      "2019-06-17 11:01:31,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,529 : INFO : built Dictionary(38 unique tokens: ['advance', 'advise', 'anyone', 'arriving', 'carlton']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:31,552 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,578 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:31,570 : INFO : built Dictionary(34 unique tokens: ['account', 'anyone', 'bank', 'directly', 'employee']...) from 2 documents (total 38 corpus positions)\n",
      "2019-06-17 11:01:31,622 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:31,634 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,661 : INFO : built Dictionary(30 unique tokens: ['agenda', 'americans', 'better', 'care', 'college']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:31,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,701 : INFO : built Dictionary(33 unique tokens: ['achieved', 'alpha', 'anger', 'angry', 'article']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:31,731 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,744 : INFO : built Dictionary(35 unique tokens: ['another', 'best', 'capita', 'congratulations', 'consume']...) from 2 documents (total 37 corpus positions)\n",
      "2019-06-17 11:01:31,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:31,805 : INFO : built Dictionary(36 unique tokens: ['anyone', 'around', 'arrange', 'competitive', 'doha']...) from 2 documents (total 40 corpus positions)\n",
      "2019-06-17 11:01:31,837 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-17 11:01:32,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,133 : INFO : built Dictionary(26 unique tokens: ['anyone', 'anything', 'companies', 'company', 'damaging']...) from 2 documents (total 36 corpus positions)\n",
      "2019-06-17 11:01:32,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,158 : INFO : built Dictionary(24 unique tokens: ['bangalore', 'clearance', 'customs', 'doha', 'door']...) from 2 documents (total 32 corpus positions)\n",
      "2019-06-17 11:01:32,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,193 : INFO : built Dictionary(37 unique tokens: ['accept', 'bank', 'contractor', 'currently', 'doha']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:32,273 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,289 : INFO : built Dictionary(37 unique tokens: ['anybody', 'availabe', 'cargo', 'center', 'cheapest']...) from 2 documents (total 41 corpus positions)\n",
      "2019-06-17 11:01:32,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,339 : INFO : built Dictionary(35 unique tokens: ['advice', 'appreciated', 'august', 'comments', 'documents']...) from 2 documents (total 42 corpus positions)\n",
      "2019-06-17 11:01:32,343 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,361 : INFO : built Dictionary(35 unique tokens: ['advise', 'anyone', 'area', 'back', 'called']...) from 2 documents (total 78 corpus positions)\n",
      "2019-06-17 11:01:32,368 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2019-06-17 11:01:32,385 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,400 : INFO : built Dictionary(38 unique tokens: ['advice', 'anyone', 'april', 'books', 'bringing']...) from 2 documents (total 46 corpus positions)\n",
      "2019-06-17 11:01:32,449 : INFO : built Dictionary(47 unique tokens: ['advice', 'anybody', 'big', 'coming', 'diwali']...) from 2 documents (total 58 corpus positions)\n",
      "2019-06-17 11:01:32,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,501 : INFO : built Dictionary(35 unique tokens: ['able', 'advise', 'amount', 'anything', 'bank']...) from 2 documents (total 44 corpus positions)\n",
      "2019-06-17 11:01:32,523 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-06-17 11:01:32,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "args_list = [\n",
    "    (dataset, technique)\n",
    "    for dataset in (\"2016-test\", \"2017-test\")\n",
    "    for technique in (\"softcossim\", \"wmd-gensim\", \"wmd-relax\", \"cossim\")]\n",
    "with Pool() as pool:\n",
    "    results = pool.map(crossvalidate, args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the pointwise estimates of means and standard variances for MAP scores and elapsed times. Baselines and winners for each year are displayed in bold. We can see that the Soft Cosine Measure gives a strong performance on both the 2016 and the 2017 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "output = []\n",
    "baselines = [\n",
    "    ((\"2016-test\", \"**Winner (UH-PRHLT-primary)**\"), ((76.70, 0), (0, 0))),\n",
    "    ((\"2016-test\", \"**Baseline 1 (IR)**\"), ((74.75, 0), (0, 0))),\n",
    "    ((\"2016-test\", \"**Baseline 2 (random)**\"), ((46.98, 0), (0, 0))),\n",
    "    ((\"2017-test\", \"**Winner (SimBow-primary)**\"), ((47.22, 0), (0, 0))),\n",
    "    ((\"2017-test\", \"**Baseline 1 (IR)**\"), ((41.85, 0), (0, 0))),\n",
    "    ((\"2017-test\", \"**Baseline 2 (random)**\"), ((29.81, 0), (0, 0)))]\n",
    "table_header = [\"Dataset | Strategy | MAP score | Elapsed time (sec)\", \":---|:---|:---|---:\"]\n",
    "for row, ((dataset, technique), ((mean_map_score, mean_duration), (std_map_score, std_duration))) \\\n",
    "        in enumerate(sorted(chain(zip(args_list, results), baselines), key=lambda x: (x[0][0], -x[1][0][0]))):\n",
    "    if row % (len(strategies) + 3) == 0:\n",
    "        output.extend(chain([\"\\n\"], table_header))\n",
    "    map_score = \"%.02f ±%.02f\" % (mean_map_score, std_map_score)\n",
    "    duration = \"%.02f ±%.02f\" % (mean_duration, std_duration) if mean_duration else \"\"\n",
    "    output.append(\"%s|%s|%s|%s\" % (dataset, technique, map_score, duration))\n",
    "\n",
    "display(Markdown('\\n'.join(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Grigori Sidorov et al. *Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model*, 2014. ([link to PDF](http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a7.pdf))\n",
    "2. Delphine Charlet and Geraldine Damnati, SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering, 2017. ([link to PDF](http://www.aclweb.org/anthology/S17-2051))\n",
    "3. Thomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space, 2013. ([link to PDF](https://arxiv.org/pdf/1301.3781.pdf))\n",
    "4. Vít Novotný. *Implementation Notes for the Soft Cosine Measure*, 2018. ([link to PDF](https://arxiv.org/pdf/1808.09407))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
