{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Background\n",
    "\n",
    "As dicussion in this [PR](https://github.com/RaRe-Technologies/gensim/pull/1434), Translation Matrix not only can used to translate the words from one source language to another target lanuage, but also to translate new document vectors back to old model space.\n",
    "\n",
    "For example, if we have trained 15k documents using doc2vec (we called this as model1), and we are going to train new 35k documents using doc2vec(we called this as model2). So we can include those 15k documents as reference documents into the new 35k documents. Then we can get 15k document vectors from model1 and 50k document vectors from model2, but both of the two models have vectors for those 15k documents. We can use those vectors to build a mapping from model1 to model2. Finally, with this relation, we can back-mapping the model2's vector to model1. Therefore, 35k document vectors are learned using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we use the IMDB dataset as example. For more information about this dataset, please refer to [this](http://ai.stanford.edu/~amaas/data/sentiment/). And some of code are borrowed from this [notebook](http://localhost:8888/notebooks/docs/notebooks/doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n",
      "25000 25000 100000 15000 50000\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import namedtuple\n",
    "from gensim import utils\n",
    "\n",
    "def read_sentimentDocs():\n",
    "    SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "    alldocs = []  # will hold all docs in original order\n",
    "    with utils.smart_open('aclImdb/alldata-id.txt', encoding='utf-8') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            words = tokens[1:]\n",
    "            tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "            split = ['train','test','extra','extra'][line_no // 25000]  # 25k train, 25k test, 25k extra\n",
    "            sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no // 12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "            alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "    train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "    test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "    doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "    print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))\n",
    "\n",
    "    return train_docs, test_docs, doc_list\n",
    "\n",
    "train_docs, test_docs, doc_list = read_sentimentDocs()\n",
    "\n",
    "small_corpus = train_docs[:15000]\n",
    "large_corpus = train_docs + test_docs\n",
    "\n",
    "print len(train_docs), len(test_docs), len(doc_list), len(small_corpus), len(large_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for the computer performance limited, didn't run on notebook\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "model1 = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "model2 = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "\n",
    "small_train_docs = train_docs[:15000]\n",
    "# train for small corpus\n",
    "model1.build_vocab(small_train_docs)\n",
    "for epoch in xrange(50):\n",
    "    shuffle(small_train_docs)\n",
    "    model1.train(small_train_docs, total_examples=len(small_train_docs), epochs=1)\n",
    "model.save(\"small_doc_15000_iter50.bin\")\n",
    "\n",
    "train_docs.extend(test_docs)\n",
    "# train for large corpus\n",
    "model2.build_vocab(train_docs)\n",
    "for epoch in xrange(50):\n",
    "    shuffle(train_docs)\n",
    "    model2.train(train_docs, total_examples=len(train_docs), epochs=1)\n",
    "# save the model\n",
    "model2.save(\"large_doc_50000_iter50.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def test_classifier_error(train, train_label, test, test_label):\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(train, train_label)\n",
    "    score = classifier.score(test, test_label)\n",
    "    print \"the classifier score :\", score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors are learned by doc2vec method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the classifier score : 0.83372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83372000000000002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you can change the data folder\n",
    "basedir = \"/home/robotcator/doc2vec\"\n",
    "\n",
    "model2 = Doc2Vec.load(os.path.join(basedir, \"large_doc_50000_iter50.bin\"))\n",
    "m2 = []\n",
    "for i in range(len(large_corpus)):\n",
    "    m2.append(model2.docvecs[large_corpus[i].tags])\n",
    "\n",
    "train_array = np.zeros((25000, 100))\n",
    "train_label = np.zeros((25000, 1))\n",
    "test_array = np.zeros((25000, 100))\n",
    "test_label = np.zeros((25000, 1))\n",
    "\n",
    "for i in range(12500):\n",
    "    train_array[i] = m2[i]\n",
    "    train_label[i] = 1\n",
    "\n",
    "    train_array[i+12500] = m2[i+12500]\n",
    "    train_label[i+12500] = 0\n",
    "\n",
    "    test_array[i] = m2[i+25000]\n",
    "    test_label[i] = 1\n",
    "\n",
    "    test_array[i+12500] = m2[i+37500]\n",
    "    test_label[i+12500] = 0\n",
    "\n",
    "print \"The vectors are learned by doc2vec method\"\n",
    "test_classifier_error(train_array, train_label, test_array, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To evalute those document vector, we use split those 50k document into two part, one for training and the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors are learned by back-mapping method\n",
      "the classifier score : 0.71796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71796000000000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import translation_matrix\n",
    "# you can change the data folder\n",
    "basedir = \"/home/robotcator/doc2vec\"\n",
    "\n",
    "model1 = Doc2Vec.load(os.path.join(basedir, \"small_doc_15000_iter50.bin\"))\n",
    "model2 = Doc2Vec.load(os.path.join(basedir, \"large_doc_50000_iter50.bin\"))\n",
    "\n",
    "l = model1.docvecs.count\n",
    "l2 = model2.docvecs.count\n",
    "m1 = np.array([model1.docvecs[large_corpus[i].tags].flatten() for i in range(l)])\n",
    "\n",
    "# learn the mapping bettween two model\n",
    "model = translation_matrix.BackMappingTranslationMatrix(large_corpus[:15000], model1, model2)\n",
    "model.train(large_corpus[:15000])\n",
    "\n",
    "for i in range(l, l2):\n",
    "    infered_vec = model.infer_vector(model2.docvecs[large_corpus[i].tags])\n",
    "    m1 = np.vstack((m1, infered_vec.flatten()))\n",
    "\n",
    "train_array = np.zeros((25000, 100))\n",
    "train_label = np.zeros((25000, 1))\n",
    "test_array = np.zeros((25000, 100))\n",
    "test_label = np.zeros((25000, 1))\n",
    "\n",
    "# because those document, 25k documents are postive label, 25k documents are negative label\n",
    "for i in range(12500):\n",
    "    train_array[i] = m1[i]\n",
    "    train_label[i] = 1\n",
    "\n",
    "    train_array[i+12500] = m1[i+12500]\n",
    "    train_label[i+12500] = 0\n",
    "\n",
    "    test_array[i] = m1[i+25000]\n",
    "    test_label[i] = 1\n",
    "\n",
    "    test_array[i+12500] = m1[i+37500]\n",
    "    test_label[i+12500] = 0\n",
    "\n",
    "print \"The vectors are learned by back-mapping method\"\n",
    "test_classifier_error(train_array, train_label, test_array, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see that, the vectors learned by back-mapping method performed not bad but still need improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
