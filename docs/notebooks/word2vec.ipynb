{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial\n",
    "\n",
    "In case you missed the buzz, word2vec is a widely featured as a member of the “new wave” of machine learning algorithms based on neural networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow). Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word, with remarkable linear relationships that allow us to do things like vec(“king”) – vec(“man”) + vec(“woman”) =~ vec(“queen”), or vec(“Montreal Canadiens”) – vec(“Montreal”) + vec(“Toronto”) resembles the vector for “Toronto Maple Leafs”.\n",
    "\n",
    "Word2vec is very useful in [automatic text tagging](https://github.com/RaRe-Technologies/movie-plots-by-genre), recommender systems and machine translation.\n",
    "\n",
    "Check out an [online word2vec demo](http://radimrehurek.com/2014/02/word2vec-tutorial/#app) where you can try this vector algebra for yourself. That demo runs `word2vec` on the Google News dataset, of **about 100 billion words**.\n",
    "\n",
    "## This tutorial\n",
    "\n",
    "In this tutorial you will learn how to train and evaluate word2vec models on your business data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Input\n",
    "Starting from the beginning, gensim’s `word2vec` expects a sequence of sentences as its input. Each sentence is a list of words (utf8 strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,111 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,113 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,114 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2017-12-19 17:52:07,115 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,116 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-12-19 17:52:07,117 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-12-19 17:52:07,118 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-12-19 17:52:07,119 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-12-19 17:52:07,120 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-12-19 17:52:07,121 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-12-19 17:52:07,122 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,123 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,130 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-12-19 17:52:07,131 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the input as a Python built-in list is convenient, but can use up a lot of RAM when the input is large.\n",
    "\n",
    "Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some toy data to use with the following example\n",
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f1.txt', './data/f2.txt']\n",
    "\n",
    "for i, fname in enumerate(filenames):\n",
    "    with smart_open.smart_open(fname, 'w') as fout:\n",
    "        for line in sentences[i]:\n",
    "            fout.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['second'], ['sentence'], ['first'], ['sentence']]\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentences('./data/') # a memory-friendly iterator\n",
    "print(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,161 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,162 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,163 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2017-12-19 17:52:07,164 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,165 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-12-19 17:52:07,166 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-12-19 17:52:07,167 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-12-19 17:52:07,169 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-12-19 17:52:07,170 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-12-19 17:52:07,171 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-12-19 17:52:07,172 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,173 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,177 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,179 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-12-19 17:52:07,179 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# generate the Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'second': <gensim.models.keyedvectors.Vocab object at 0x7f058032fc90>, 'sentence': <gensim.models.keyedvectors.Vocab object at 0x7f058032f850>, 'first': <gensim.models.keyedvectors.Vocab object at 0x7f058032f9d0>}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to further preprocess the words from the files — convert to unicode, lowercase, remove numbers, extract named entities… All of this can be done inside the `MySentences` iterator and `word2vec` doesn’t need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
    "\n",
    "**Note to advanced users:** calling `Word2Vec(sentences, iter=1)` will run **two** passes over the sentences iterator. In general it runs `iter+1` passes. By the way, the default value is `iter=5` to comply with Google's word2vec in C language. \n",
    "  1. The first pass collects words and their frequencies to build an internal dictionary tree structure. \n",
    "  2. The second pass trains the neural model.\n",
    "\n",
    "These two passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,193 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,195 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,197 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2017-12-19 17:52:07,198 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,199 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-12-19 17:52:07,200 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-12-19 17:52:07,201 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-12-19 17:52:07,202 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-12-19 17:52:07,203 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-12-19 17:52:07,204 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-12-19 17:52:07,205 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,206 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,210 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-12-19 17:52:07,211 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the same model, making the 2 steps explicit\n",
    "new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training\n",
    "new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     \n",
    "new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       \n",
    "# can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'second': <gensim.models.keyedvectors.Vocab object at 0x7f058032fc90>, 'sentence': <gensim.models.keyedvectors.Vocab object at 0x7f058032f850>, 'first': <gensim.models.keyedvectors.Vocab object at 0x7f058032f9d0>}\n"
     ]
    }
   ],
   "source": [
    "print(new_model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data would be nice\n",
    "For the following examples, we'll use the [Lee Corpus](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor) (which you already have if you've installed gensim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyText object at 0x7f054e29c450>\n"
     ]
    }
   ],
   "source": [
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower().split()\n",
    "\n",
    "sentences = MyText()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "`Word2Vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "### min_count\n",
    "`min_count` is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,247 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,248 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,265 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:52:07,266 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,272 : INFO : min_count=10 retains 806 unique words (7% of original 10186, drops 9380)\n",
      "2017-12-19 17:52:07,273 : INFO : min_count=10 leaves 40964 word corpus (68% of original 59890, drops 18926)\n",
      "2017-12-19 17:52:07,277 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-12-19 17:52:07,279 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2017-12-19 17:52:07,280 : INFO : downsampling leaves estimated 26224 word corpus (64.0% of prior 40964)\n",
      "2017-12-19 17:52:07,281 : INFO : estimated required memory for 806 words and 100 dimensions: 1047800 bytes\n",
      "2017-12-19 17:52:07,284 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,294 : INFO : training model with 3 workers on 806 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,416 : INFO : training on 299450 raw words (130961 effective words) took 0.1s, 1092652 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of min_count=5\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size\n",
    "`size` is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,423 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,424 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,439 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:52:07,440 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,448 : INFO : min_count=5 retains 1723 unique words (16% of original 10186, drops 8463)\n",
      "2017-12-19 17:52:07,449 : INFO : min_count=5 leaves 46858 word corpus (78% of original 59890, drops 13032)\n",
      "2017-12-19 17:52:07,456 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-12-19 17:52:07,457 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-12-19 17:52:07,457 : INFO : downsampling leaves estimated 32849 word corpus (70.1% of prior 46858)\n",
      "2017-12-19 17:52:07,458 : INFO : estimated required memory for 1723 words and 200 dimensions: 3618300 bytes\n",
      "2017-12-19 17:52:07,463 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,486 : INFO : training model with 3 workers on 1723 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,697 : INFO : training on 299450 raw words (164263 effective words) took 0.2s, 786516 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of size=100\n",
    "model = gensim.models.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workers\n",
    "`workers`, the last of the major parameters (full list [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:07,705 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:07,706 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:07,722 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:52:07,723 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:07,774 : INFO : min_count=5 retains 1723 unique words (16% of original 10186, drops 8463)\n",
      "2017-12-19 17:52:07,775 : INFO : min_count=5 leaves 46858 word corpus (78% of original 59890, drops 13032)\n",
      "2017-12-19 17:52:07,781 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-12-19 17:52:07,783 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-12-19 17:52:07,784 : INFO : downsampling leaves estimated 32849 word corpus (70.1% of prior 46858)\n",
      "2017-12-19 17:52:07,785 : INFO : estimated required memory for 1723 words and 100 dimensions: 2239900 bytes\n",
      "2017-12-19 17:52:07,790 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:07,808 : INFO : training model with 4 workers on 1723 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:07,937 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-19 17:52:07,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:07,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:07,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:07,948 : INFO : training on 299450 raw words (164316 effective words) took 0.1s, 1196340 effective words/s\n",
      "2017-12-19 17:52:07,948 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# default value of workers=3 (tutorial says 1...)\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `workers` parameter only has an effect if you have [Cython](http://cython.org/) installed. Without Cython, you’ll only be able to use one core because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) (and `word2vec` training will be [miserably slow](http://rare-technologies.com/word2vec-in-python-part-two-optimizing/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "At its core, `word2vec` model parameters are stored as matrices (NumPy arrays). Each array is **#vocabulary** (controlled by min_count parameter) times **#size** (size parameter) of floats (single precision aka 4 bytes).\n",
    "\n",
    "Three such matrices are held in RAM (work is underway to reduce that number to two, or even one). So if your input contains 100,000 unique words, and you asked for layer `size=200`, the model will require approx. `100,000*200*4*3 bytes = ~229MB`.\n",
    "\n",
    "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would take a few megabytes), but unless your words are extremely loooong strings, memory footprint will be dominated by the three matrices above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "`Word2Vec` training is an unsupervised task, there’s no good way to objectively evaluate the result. Evaluation depends on your end application.\n",
    "\n",
    "Google has released their testing set of about 20,000 syntactic and semantic test examples, following the “A is to B as C is to D” task. It is provided in the 'datasets' folder.\n",
    "\n",
    "For example a syntactic analogy of comparative type is bad:worse;good:?. There are total of 9 types of syntactic comparisons in the dataset like plural nouns and nouns of opposite meaning.\n",
    "\n",
    "The semantic questions contain five types of semantic analogies, such as capital cities (Paris:France;Tokyo:?) or family members (brother:sister;dad:?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim supports the same evaluation set, in exactly the same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:08,009 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-12-19 17:52:08,026 : INFO : family: 0.0% (0/2)\n",
      "2017-12-19 17:52:08,050 : INFO : gram3-comparative: 0.0% (0/12)\n",
      "2017-12-19 17:52:08,061 : INFO : gram4-superlative: 0.0% (0/12)\n",
      "2017-12-19 17:52:08,073 : INFO : gram5-present-participle: 5.0% (1/20)\n",
      "2017-12-19 17:52:08,095 : INFO : gram6-nationality-adjective: 0.0% (0/20)\n",
      "2017-12-19 17:52:08,123 : INFO : gram7-past-tense: 0.0% (0/20)\n",
      "2017-12-19 17:52:08,136 : INFO : gram8-plural: 0.0% (0/12)\n",
      "2017-12-19 17:52:08,145 : INFO : total: 1.0% (1/98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'correct': [], 'incorrect': [], 'section': u'capital-common-countries'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'capital-world'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'currency'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'city-in-state'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'HE', u'SHE', u'HIS', u'HER'),\n",
       "   (u'HIS', u'HER', u'HE', u'SHE')],\n",
       "  'section': u'family'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'gram1-adjective-to-adverb'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'gram2-opposite'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'GOOD', u'BETTER', u'GREAT', u'GREATER'),\n",
       "   (u'GOOD', u'BETTER', u'LONG', u'LONGER'),\n",
       "   (u'GOOD', u'BETTER', u'LOW', u'LOWER'),\n",
       "   (u'GREAT', u'GREATER', u'LONG', u'LONGER'),\n",
       "   (u'GREAT', u'GREATER', u'LOW', u'LOWER'),\n",
       "   (u'GREAT', u'GREATER', u'GOOD', u'BETTER'),\n",
       "   (u'LONG', u'LONGER', u'LOW', u'LOWER'),\n",
       "   (u'LONG', u'LONGER', u'GOOD', u'BETTER'),\n",
       "   (u'LONG', u'LONGER', u'GREAT', u'GREATER'),\n",
       "   (u'LOW', u'LOWER', u'GOOD', u'BETTER'),\n",
       "   (u'LOW', u'LOWER', u'GREAT', u'GREATER'),\n",
       "   (u'LOW', u'LOWER', u'LONG', u'LONGER')],\n",
       "  'section': u'gram3-comparative'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'BIG', u'BIGGEST', u'GOOD', u'BEST'),\n",
       "   (u'BIG', u'BIGGEST', u'GREAT', u'GREATEST'),\n",
       "   (u'BIG', u'BIGGEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GOOD', u'BEST', u'GREAT', u'GREATEST'),\n",
       "   (u'GOOD', u'BEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GOOD', u'BEST', u'BIG', u'BIGGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'BIG', u'BIGGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'GOOD', u'BEST'),\n",
       "   (u'LARGE', u'LARGEST', u'BIG', u'BIGGEST'),\n",
       "   (u'LARGE', u'LARGEST', u'GOOD', u'BEST'),\n",
       "   (u'LARGE', u'LARGEST', u'GREAT', u'GREATEST')],\n",
       "  'section': u'gram4-superlative'},\n",
       " {'correct': [(u'LOOK', u'LOOKING', u'SAY', u'SAYING')],\n",
       "  'incorrect': [(u'GO', u'GOING', u'LOOK', u'LOOKING'),\n",
       "   (u'GO', u'GOING', u'PLAY', u'PLAYING'),\n",
       "   (u'GO', u'GOING', u'RUN', u'RUNNING'),\n",
       "   (u'GO', u'GOING', u'SAY', u'SAYING'),\n",
       "   (u'LOOK', u'LOOKING', u'PLAY', u'PLAYING'),\n",
       "   (u'LOOK', u'LOOKING', u'RUN', u'RUNNING'),\n",
       "   (u'LOOK', u'LOOKING', u'GO', u'GOING'),\n",
       "   (u'PLAY', u'PLAYING', u'RUN', u'RUNNING'),\n",
       "   (u'PLAY', u'PLAYING', u'SAY', u'SAYING'),\n",
       "   (u'PLAY', u'PLAYING', u'GO', u'GOING'),\n",
       "   (u'PLAY', u'PLAYING', u'LOOK', u'LOOKING'),\n",
       "   (u'RUN', u'RUNNING', u'SAY', u'SAYING'),\n",
       "   (u'RUN', u'RUNNING', u'GO', u'GOING'),\n",
       "   (u'RUN', u'RUNNING', u'LOOK', u'LOOKING'),\n",
       "   (u'RUN', u'RUNNING', u'PLAY', u'PLAYING'),\n",
       "   (u'SAY', u'SAYING', u'GO', u'GOING'),\n",
       "   (u'SAY', u'SAYING', u'LOOK', u'LOOKING'),\n",
       "   (u'SAY', u'SAYING', u'PLAY', u'PLAYING'),\n",
       "   (u'SAY', u'SAYING', u'RUN', u'RUNNING')],\n",
       "  'section': u'gram5-present-participle'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'AUSTRALIA', u'AUSTRALIAN', u'FRANCE', u'FRENCH'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'INDIA', u'INDIAN'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'FRANCE', u'FRENCH', u'INDIA', u'INDIAN'),\n",
       "   (u'FRANCE', u'FRENCH', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'FRANCE', u'FRENCH', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'FRANCE', u'FRENCH', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'INDIA', u'INDIAN', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'INDIA', u'INDIAN', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'INDIA', u'INDIAN', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'INDIA', u'INDIAN', u'FRANCE', u'FRENCH'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'FRANCE', u'FRENCH'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'INDIA', u'INDIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'FRANCE', u'FRENCH'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'INDIA', u'INDIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'ISRAEL', u'ISRAELI')],\n",
       "  'section': u'gram6-nationality-adjective'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'GOING', u'WENT', u'PAYING', u'PAID'),\n",
       "   (u'GOING', u'WENT', u'PLAYING', u'PLAYED'),\n",
       "   (u'GOING', u'WENT', u'SAYING', u'SAID'),\n",
       "   (u'GOING', u'WENT', u'TAKING', u'TOOK'),\n",
       "   (u'PAYING', u'PAID', u'PLAYING', u'PLAYED'),\n",
       "   (u'PAYING', u'PAID', u'SAYING', u'SAID'),\n",
       "   (u'PAYING', u'PAID', u'TAKING', u'TOOK'),\n",
       "   (u'PAYING', u'PAID', u'GOING', u'WENT'),\n",
       "   (u'PLAYING', u'PLAYED', u'SAYING', u'SAID'),\n",
       "   (u'PLAYING', u'PLAYED', u'TAKING', u'TOOK'),\n",
       "   (u'PLAYING', u'PLAYED', u'GOING', u'WENT'),\n",
       "   (u'PLAYING', u'PLAYED', u'PAYING', u'PAID'),\n",
       "   (u'SAYING', u'SAID', u'TAKING', u'TOOK'),\n",
       "   (u'SAYING', u'SAID', u'GOING', u'WENT'),\n",
       "   (u'SAYING', u'SAID', u'PAYING', u'PAID'),\n",
       "   (u'SAYING', u'SAID', u'PLAYING', u'PLAYED'),\n",
       "   (u'TAKING', u'TOOK', u'GOING', u'WENT'),\n",
       "   (u'TAKING', u'TOOK', u'PAYING', u'PAID'),\n",
       "   (u'TAKING', u'TOOK', u'PLAYING', u'PLAYED'),\n",
       "   (u'TAKING', u'TOOK', u'SAYING', u'SAID')],\n",
       "  'section': u'gram7-past-tense'},\n",
       " {'correct': [],\n",
       "  'incorrect': [(u'BUILDING', u'BUILDINGS', u'CAR', u'CARS'),\n",
       "   (u'BUILDING', u'BUILDINGS', u'CHILD', u'CHILDREN'),\n",
       "   (u'BUILDING', u'BUILDINGS', u'MAN', u'MEN'),\n",
       "   (u'CAR', u'CARS', u'CHILD', u'CHILDREN'),\n",
       "   (u'CAR', u'CARS', u'MAN', u'MEN'),\n",
       "   (u'CAR', u'CARS', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'CHILD', u'CHILDREN', u'MAN', u'MEN'),\n",
       "   (u'CHILD', u'CHILDREN', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'CHILD', u'CHILDREN', u'CAR', u'CARS'),\n",
       "   (u'MAN', u'MEN', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'MAN', u'MEN', u'CAR', u'CARS'),\n",
       "   (u'MAN', u'MEN', u'CHILD', u'CHILDREN')],\n",
       "  'section': u'gram8-plural'},\n",
       " {'correct': [], 'incorrect': [], 'section': u'gram9-plural-verbs'},\n",
       " {'correct': [(u'LOOK', u'LOOKING', u'SAY', u'SAYING')],\n",
       "  'incorrect': [(u'HE', u'SHE', u'HIS', u'HER'),\n",
       "   (u'HIS', u'HER', u'HE', u'SHE'),\n",
       "   (u'GOOD', u'BETTER', u'GREAT', u'GREATER'),\n",
       "   (u'GOOD', u'BETTER', u'LONG', u'LONGER'),\n",
       "   (u'GOOD', u'BETTER', u'LOW', u'LOWER'),\n",
       "   (u'GREAT', u'GREATER', u'LONG', u'LONGER'),\n",
       "   (u'GREAT', u'GREATER', u'LOW', u'LOWER'),\n",
       "   (u'GREAT', u'GREATER', u'GOOD', u'BETTER'),\n",
       "   (u'LONG', u'LONGER', u'LOW', u'LOWER'),\n",
       "   (u'LONG', u'LONGER', u'GOOD', u'BETTER'),\n",
       "   (u'LONG', u'LONGER', u'GREAT', u'GREATER'),\n",
       "   (u'LOW', u'LOWER', u'GOOD', u'BETTER'),\n",
       "   (u'LOW', u'LOWER', u'GREAT', u'GREATER'),\n",
       "   (u'LOW', u'LOWER', u'LONG', u'LONGER'),\n",
       "   (u'BIG', u'BIGGEST', u'GOOD', u'BEST'),\n",
       "   (u'BIG', u'BIGGEST', u'GREAT', u'GREATEST'),\n",
       "   (u'BIG', u'BIGGEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GOOD', u'BEST', u'GREAT', u'GREATEST'),\n",
       "   (u'GOOD', u'BEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GOOD', u'BEST', u'BIG', u'BIGGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'LARGE', u'LARGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'BIG', u'BIGGEST'),\n",
       "   (u'GREAT', u'GREATEST', u'GOOD', u'BEST'),\n",
       "   (u'LARGE', u'LARGEST', u'BIG', u'BIGGEST'),\n",
       "   (u'LARGE', u'LARGEST', u'GOOD', u'BEST'),\n",
       "   (u'LARGE', u'LARGEST', u'GREAT', u'GREATEST'),\n",
       "   (u'GO', u'GOING', u'LOOK', u'LOOKING'),\n",
       "   (u'GO', u'GOING', u'PLAY', u'PLAYING'),\n",
       "   (u'GO', u'GOING', u'RUN', u'RUNNING'),\n",
       "   (u'GO', u'GOING', u'SAY', u'SAYING'),\n",
       "   (u'LOOK', u'LOOKING', u'PLAY', u'PLAYING'),\n",
       "   (u'LOOK', u'LOOKING', u'RUN', u'RUNNING'),\n",
       "   (u'LOOK', u'LOOKING', u'GO', u'GOING'),\n",
       "   (u'PLAY', u'PLAYING', u'RUN', u'RUNNING'),\n",
       "   (u'PLAY', u'PLAYING', u'SAY', u'SAYING'),\n",
       "   (u'PLAY', u'PLAYING', u'GO', u'GOING'),\n",
       "   (u'PLAY', u'PLAYING', u'LOOK', u'LOOKING'),\n",
       "   (u'RUN', u'RUNNING', u'SAY', u'SAYING'),\n",
       "   (u'RUN', u'RUNNING', u'GO', u'GOING'),\n",
       "   (u'RUN', u'RUNNING', u'LOOK', u'LOOKING'),\n",
       "   (u'RUN', u'RUNNING', u'PLAY', u'PLAYING'),\n",
       "   (u'SAY', u'SAYING', u'GO', u'GOING'),\n",
       "   (u'SAY', u'SAYING', u'LOOK', u'LOOKING'),\n",
       "   (u'SAY', u'SAYING', u'PLAY', u'PLAYING'),\n",
       "   (u'SAY', u'SAYING', u'RUN', u'RUNNING'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'FRANCE', u'FRENCH'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'INDIA', u'INDIAN'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'AUSTRALIA', u'AUSTRALIAN', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'FRANCE', u'FRENCH', u'INDIA', u'INDIAN'),\n",
       "   (u'FRANCE', u'FRENCH', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'FRANCE', u'FRENCH', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'FRANCE', u'FRENCH', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'INDIA', u'INDIAN', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'INDIA', u'INDIAN', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'INDIA', u'INDIAN', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'INDIA', u'INDIAN', u'FRANCE', u'FRENCH'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'SWITZERLAND', u'SWISS'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'FRANCE', u'FRENCH'),\n",
       "   (u'ISRAEL', u'ISRAELI', u'INDIA', u'INDIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'AUSTRALIA', u'AUSTRALIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'FRANCE', u'FRENCH'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'INDIA', u'INDIAN'),\n",
       "   (u'SWITZERLAND', u'SWISS', u'ISRAEL', u'ISRAELI'),\n",
       "   (u'GOING', u'WENT', u'PAYING', u'PAID'),\n",
       "   (u'GOING', u'WENT', u'PLAYING', u'PLAYED'),\n",
       "   (u'GOING', u'WENT', u'SAYING', u'SAID'),\n",
       "   (u'GOING', u'WENT', u'TAKING', u'TOOK'),\n",
       "   (u'PAYING', u'PAID', u'PLAYING', u'PLAYED'),\n",
       "   (u'PAYING', u'PAID', u'SAYING', u'SAID'),\n",
       "   (u'PAYING', u'PAID', u'TAKING', u'TOOK'),\n",
       "   (u'PAYING', u'PAID', u'GOING', u'WENT'),\n",
       "   (u'PLAYING', u'PLAYED', u'SAYING', u'SAID'),\n",
       "   (u'PLAYING', u'PLAYED', u'TAKING', u'TOOK'),\n",
       "   (u'PLAYING', u'PLAYED', u'GOING', u'WENT'),\n",
       "   (u'PLAYING', u'PLAYED', u'PAYING', u'PAID'),\n",
       "   (u'SAYING', u'SAID', u'TAKING', u'TOOK'),\n",
       "   (u'SAYING', u'SAID', u'GOING', u'WENT'),\n",
       "   (u'SAYING', u'SAID', u'PAYING', u'PAID'),\n",
       "   (u'SAYING', u'SAID', u'PLAYING', u'PLAYED'),\n",
       "   (u'TAKING', u'TOOK', u'GOING', u'WENT'),\n",
       "   (u'TAKING', u'TOOK', u'PAYING', u'PAID'),\n",
       "   (u'TAKING', u'TOOK', u'PLAYING', u'PLAYED'),\n",
       "   (u'TAKING', u'TOOK', u'SAYING', u'SAID'),\n",
       "   (u'BUILDING', u'BUILDINGS', u'CAR', u'CARS'),\n",
       "   (u'BUILDING', u'BUILDINGS', u'CHILD', u'CHILDREN'),\n",
       "   (u'BUILDING', u'BUILDINGS', u'MAN', u'MEN'),\n",
       "   (u'CAR', u'CARS', u'CHILD', u'CHILDREN'),\n",
       "   (u'CAR', u'CARS', u'MAN', u'MEN'),\n",
       "   (u'CAR', u'CARS', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'CHILD', u'CHILDREN', u'MAN', u'MEN'),\n",
       "   (u'CHILD', u'CHILDREN', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'CHILD', u'CHILDREN', u'CAR', u'CARS'),\n",
       "   (u'MAN', u'MEN', u'BUILDING', u'BUILDINGS'),\n",
       "   (u'MAN', u'MEN', u'CAR', u'CARS'),\n",
       "   (u'MAN', u'MEN', u'CHILD', u'CHILDREN')],\n",
       "  'section': 'total'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy('./datasets/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `accuracy` takes an \n",
    "[optional parameter](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.accuracy) `restrict_vocab` \n",
    "which limits which test examples are to be considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
    "\n",
    "By default it uses an academic dataset WS-353 but one can create a dataset specific to your business based on it. It contains word pairs together with human-assigned similarity judgments. It measures the relatedness or co-occurrence of two words. For example, 'coast' and 'shore' are very similar as they appear in the same context. At the same time 'clothes' and 'closet' are less similar because they are related but not interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `evaluate_word_pairs` (Method will be removed in 4.0.0, use self.wv.evaluate_word_pairs() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2017-12-19 17:52:08,199 : INFO : Pearson correlation coefficient against /usr/local/lib/python2.7/dist-packages/gensim-3.2.0-py2.7-linux-x86_64.egg/gensim/test/test_data/wordsim353.tsv: 0.1016\n",
      "2017-12-19 17:52:08,202 : INFO : Spearman rank-order correlation coefficient against /usr/local/lib/python2.7/dist-packages/gensim-3.2.0-py2.7-linux-x86_64.egg/gensim/test/test_data/wordsim353.tsv: 0.0829\n",
      "2017-12-19 17:52:08,204 : INFO : Pairs with unknown words ratio: 85.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.10160691441325205, 0.47803354077719229),\n",
       " SpearmanrResult(correlation=0.082860052232988618, pvalue=0.56322041285528901),\n",
       " 85.55240793201133)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, **good performance on Google's or WS-353 test set doesn’t mean word2vec will work well in your application, or vice versa**. It’s always best to evaluate directly on your intended task. For an example of how to use word2vec in a classifier pipeline, see this [tutorial](https://github.com/RaRe-Technologies/movie-plots-by-genre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading models\n",
    "You can store/load models using the standard gensim methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:08,220 : INFO : saving Word2Vec object under /tmp/tmp3Fmm6zgensim_temp, separately None\n",
      "2017-12-19 17:52:08,223 : INFO : not storing attribute syn0norm\n",
      "2017-12-19 17:52:08,223 : INFO : not storing attribute cum_table\n",
      "2017-12-19 17:52:08,236 : INFO : saved /tmp/tmp3Fmm6zgensim_temp\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkstemp\n",
    "\n",
    "fs, temp_path = mkstemp(\"gensim_temp\")  # creates a temp file\n",
    "\n",
    "model.save(temp_path)  # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:08,240 : INFO : loading Word2Vec object from /tmp/tmp3Fmm6zgensim_temp\n",
      "2017-12-19 17:52:08,246 : INFO : loading wv recursively from /tmp/tmp3Fmm6zgensim_temp.wv.* with mmap=None\n",
      "2017-12-19 17:52:08,247 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-12-19 17:52:08,247 : INFO : setting ignored attribute cum_table to None\n",
      "2017-12-19 17:52:08,248 : INFO : loaded /tmp/tmp3Fmm6zgensim_temp\n"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec.load(temp_path)  # open the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which uses pickle internally, optionally `mmap`‘ing the model’s internal large NumPy matrices into virtual memory directly from disk files, for inter-process memory sharing.\n",
    "\n",
    "In addition, you can load models created by the original C tool, both using its text and binary formats:\n",
    "```\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "  # using gzipped/bz2 input works too, no need to unzip:\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online training / Resuming training\n",
    "Advanced users can load a model and continue training it with more sentences and [new vocabulary words](online_w2v_tutorial.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:08,258 : INFO : loading Word2Vec object from /tmp/tmp3Fmm6zgensim_temp\n",
      "2017-12-19 17:52:08,264 : INFO : loading wv recursively from /tmp/tmp3Fmm6zgensim_temp.wv.* with mmap=None\n",
      "2017-12-19 17:52:08,265 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-12-19 17:52:08,266 : INFO : setting ignored attribute cum_table to None\n",
      "2017-12-19 17:52:08,266 : INFO : loaded /tmp/tmp3Fmm6zgensim_temp\n",
      "2017-12-19 17:52:08,271 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:08,271 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:08,272 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences\n",
      "2017-12-19 17:52:08,272 : INFO : Updating model with new vocabulary\n",
      "2017-12-19 17:52:08,273 : INFO : New added 0 unique words (0% of original 13) and increased the count of 0 pre-existing words (0% of original 13)\n",
      "2017-12-19 17:52:08,273 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2017-12-19 17:52:08,274 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2017-12-19 17:52:08,274 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "2017-12-19 17:52:08,275 : INFO : estimated required memory for 1723 words and 100 dimensions: 2239900 bytes\n",
      "2017-12-19 17:52:08,278 : INFO : updating layer weights\n",
      "2017-12-19 17:52:08,279 : INFO : training model with 4 workers on 1723 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:08,282 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-19 17:52:08,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:08,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:08,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:08,284 : INFO : training on 65 raw words (28 effective words) took 0.0s, 11063 effective words/s\n",
      "2017-12-19 17:52:08,284 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(temp_path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "# cleaning up temp\n",
    "os.close(fs)\n",
    "os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tweak the `total_words` parameter to `train()`, depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C tool, `KeyedVectors.load_word2vec_format()`. You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there.\n",
    "\n",
    "## Using the model\n",
    "`Word2Vec` supports several word similarity tasks out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2017-12-19 17:52:08,289 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('longer', 0.9901083707809448)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2017-12-19 17:52:08,299 : WARNING : vectors for words set(['lunch', 'input', 'cat']) are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"input is lunch he sentence cat\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999207180781\n",
      "0.995506030126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('human', 'party'))\n",
    "print(model.similarity('tree', 'murder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the probability distribution for the center word given the context words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('more', 0.0010476575), ('continue', 0.00091407861), ('can', 0.0008993838), ('training', 0.00088255142), ('it', 0.00076750276), ('australia', 0.00075592351), ('there', 0.00074262387), ('could', 0.00073777919), ('or', 0.00073338969), ('which', 0.00073328678)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_output_word(['emergency', 'beacon', 'received']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here don't look good because the training corpus is very small. To get meaningful results one needs to train on 500k+ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the raw output vectors in your application, you can access these either on a word-by-word basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00667319,  0.02240003, -0.02821624, -0.00195638, -0.03252663,\n",
       "       -0.03634753,  0.01395547, -0.09243303, -0.01979843, -0.02785961,\n",
       "        0.02810719, -0.0192453 , -0.0654271 , -0.01889815, -0.01939129,\n",
       "        0.04507412, -0.03254715,  0.00107034,  0.02099092, -0.02853582,\n",
       "       -0.07192255, -0.00532108,  0.06820007,  0.04883457,  0.01651307,\n",
       "        0.02314343, -0.01205105,  0.01732391, -0.0238515 , -0.00896147,\n",
       "       -0.00098032,  0.06890224,  0.00935661, -0.00999498,  0.0349995 ,\n",
       "        0.02084912, -0.05495631,  0.01464141,  0.00810294, -0.06084112,\n",
       "       -0.08123555,  0.01204256, -0.03747954, -0.0300667 ,  0.02757824,\n",
       "       -0.05517297,  0.03925103, -0.00146865, -0.01849323,  0.03568978,\n",
       "        0.025634  ,  0.00453765, -0.03697179,  0.00249616,  0.01047032,\n",
       "       -0.02252927,  0.05656527, -0.02479618,  0.00859999, -0.03645766,\n",
       "       -0.01967132, -0.04699609,  0.02325002, -0.01824303, -0.01137256,\n",
       "       -0.06449074, -0.06765109,  0.03982421,  0.06353555,  0.00541282,\n",
       "       -0.02527811,  0.01693042, -0.05867881,  0.02318909, -0.04790874,\n",
       "       -0.05558173,  0.04097841,  0.00438625,  0.04643733,  0.04305142,\n",
       "       -0.02625137,  0.02748337, -0.01584204,  0.001226  ,  0.01381602,\n",
       "        0.01890058, -0.09808521, -0.07873033,  0.07132247, -0.05206963,\n",
       "       -0.06372968, -0.04240701, -0.04504857, -0.0107055 , -0.02857297,\n",
       "       -0.02668625, -0.0291579 ,  0.0141784 , -0.04410733, -0.00451531], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['tree']  # raw NumPy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…or en-masse as a 2D NumPy matrix from `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Computation\n",
    "\n",
    "The parameter `compute_loss` can be used to toggle computation of loss while training the Word2Vec model. The computed loss is stored in the model attribute `running_training_loss` and can be retrieved using the function `get_latest_training_loss` as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:52:08,334 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:52:08,337 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:52:08,362 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:52:08,363 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:52:08,392 : INFO : min_count=1 retains 10186 unique words (100% of original 10186, drops 0)\n",
      "2017-12-19 17:52:08,393 : INFO : min_count=1 leaves 59890 word corpus (100% of original 59890, drops 0)\n",
      "2017-12-19 17:52:08,427 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2017-12-19 17:52:08,428 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2017-12-19 17:52:08,429 : INFO : downsampling leaves estimated 47231 word corpus (78.9% of prior 59890)\n",
      "2017-12-19 17:52:08,429 : INFO : estimated required memory for 10186 words and 100 dimensions: 13241800 bytes\n",
      "2017-12-19 17:52:08,454 : INFO : resetting layer weights\n",
      "2017-12-19 17:52:08,552 : INFO : training model with 3 workers on 10186 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:52:09,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:52:09,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:52:09,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:52:09,215 : INFO : training on 299450 raw words (235935 effective words) took 0.7s, 357086 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643973.5\n"
     ]
    }
   ],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks to see effect of training loss compuation code on training time\n",
    "\n",
    "We first download and setup the test data used for getting the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-19 17:52:09--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip           100%[===================>]  29.89M   284KB/s    in 1m 54s  \n",
      "\n",
      "2017-12-19 17:54:04 (268 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  text8.zip\n",
      "  inflating: text8                   \n",
      "['/home/markroxor/Documents/gensim/docs/notebooks/../../gensim/test/test_data/lee_background.cor', '/home/markroxor/Documents/gensim/docs/notebooks/text8_1000000', '/home/markroxor/Documents/gensim/docs/notebooks/text8_10000000', '/home/markroxor/Documents/gensim/docs/notebooks/text8_50000000', '/home/markroxor/Documents/gensim/docs/notebooks/text8']\n"
     ]
    }
   ],
   "source": [
    "input_data_files = []\n",
    "\n",
    "def setup_input_data():\n",
    "    # check if test data already present\n",
    "    if os.path.isfile('./text8') is False:\n",
    "\n",
    "        # download and decompress 'text8' corpus\n",
    "        import zipfile\n",
    "        ! wget 'http://mattmahoney.net/dc/text8.zip'\n",
    "        ! unzip 'text8.zip'\n",
    "    \n",
    "        # create 1 MB, 10 MB and 50 MB files\n",
    "        ! head -c1000000 text8 > text8_1000000\n",
    "        ! head -c10000000 text8 > text8_10000000\n",
    "        ! head -c50000000 text8 > text8_50000000\n",
    "                \n",
    "    # add 25 KB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), '../../gensim/test/test_data/lee_background.cor'))\n",
    "\n",
    "    # add 1 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_1000000'))\n",
    "\n",
    "    # add 10 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_10000000'))\n",
    "\n",
    "    # add 50 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_50000000'))\n",
    "\n",
    "    # add 100 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8'))\n",
    "\n",
    "setup_input_data()\n",
    "print(input_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the training time taken for different combinations of input data and model training parameters like `hs` and `sg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/markroxor/Documents/gensim/docs/notebooks/../../gensim/test/test_data/lee_background.cor', '/home/markroxor/Documents/gensim/docs/notebooks/text8_50000000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:05,998 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:06,000 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:06,023 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:06,024 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:06,032 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:06,034 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:06,041 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:06,043 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:06,044 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:06,045 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:06,051 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:06,071 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:06,248 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:06,249 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:06,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:06,254 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 902471 effective words/s\n",
      "2017-12-19 17:54:06,256 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:06,257 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:06,279 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:06,280 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:06,289 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:06,290 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:06,298 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:06,300 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:06,301 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:06,302 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:06,307 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:06,328 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:06,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:06,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:06,505 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:06,506 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 925600 effective words/s\n",
      "2017-12-19 17:54:06,508 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:06,510 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:06,534 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:06,535 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:06,544 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:06,545 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:06,553 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:06,555 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:06,556 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:06,558 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:06,564 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:06,584 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:06,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:06,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:06,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:06,762 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 929715 effective words/s\n",
      "2017-12-19 17:54:06,764 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:06,766 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:06,790 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:06,790 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:06,803 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:06,804 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:06,816 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:06,819 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:06,821 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:06,822 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:06,828 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:06,851 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:07,039 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:07,041 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:07,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:07,044 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 856368 effective words/s\n",
      "2017-12-19 17:54:07,046 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:07,048 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:07,074 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:07,075 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:07,138 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:07,139 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:07,146 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:07,148 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:07,149 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:07,150 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:07,155 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:07,175 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:07,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:07,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:07,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:07,360 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 891395 effective words/s\n",
      "2017-12-19 17:54:07,362 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:07,362 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:07,381 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:07,382 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:07,389 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:07,390 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:07,398 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:07,399 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:07,400 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:07,401 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:07,407 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:07,433 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:07,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:07,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:07,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:07,615 : INFO : training on 299450 raw words (162889 effective words) took 0.2s, 909238 effective words/s\n",
      "2017-12-19 17:54:07,617 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:07,618 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:07,641 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:07,642 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:07,652 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:07,653 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:07,662 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:07,663 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:07,664 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:07,665 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:07,668 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:07,710 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:07,714 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:07,735 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:08,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:08,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:08,022 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:08,024 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 568580 effective words/s\n",
      "2017-12-19 17:54:08,025 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:08,027 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:08,047 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:08,048 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:08,055 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:08,058 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:08,065 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:08,067 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:08,067 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:08,068 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:08,071 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:08,115 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:08,119 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:08,140 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:08,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:08,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:08,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:08,429 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 569757 effective words/s\n",
      "2017-12-19 17:54:08,432 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:08,434 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:08,459 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:08,460 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:08,469 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:08,470 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:08,477 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:08,478 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:08,479 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:08,479 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:08,481 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:08,525 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:08,530 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:08,552 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:08,816 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:08,828 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:08,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:08,831 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 590929 effective words/s\n",
      "2017-12-19 17:54:08,833 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:08,834 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:08,856 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:08,857 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:08,864 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:08,865 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:08,873 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:08,874 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:08,875 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:08,876 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:08,879 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:08,919 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:08,923 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:08,943 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:09,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:09,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:09,224 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:09,225 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 582841 effective words/s\n",
      "2017-12-19 17:54:09,229 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:09,230 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:09,254 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:09,255 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:09,265 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:09,266 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:09,273 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:09,275 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:09,276 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:09,277 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:09,280 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:09,323 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:09,328 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:09,347 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:09,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:09,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:09,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:09,608 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 631759 effective words/s\n",
      "2017-12-19 17:54:09,611 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:09,612 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:09,635 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:09,636 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:09,645 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:09,646 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:09,653 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:09,655 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:09,656 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:09,657 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:09,659 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:09,702 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:09,707 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:09,734 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:09,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:09,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:09,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:09,996 : INFO : training on 299450 raw words (162889 effective words) took 0.3s, 628003 effective words/s\n",
      "2017-12-19 17:54:09,999 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:10,001 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:10,022 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:10,023 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:10,031 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:10,032 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:10,040 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:10,041 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:10,042 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:10,043 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:10,048 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:10,070 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:10,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:10,513 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:10,517 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:10,518 : INFO : training on 299450 raw words (162889 effective words) took 0.4s, 365839 effective words/s\n",
      "2017-12-19 17:54:10,521 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:10,522 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:10,545 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:10,546 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:10,556 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:10,557 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:10,565 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:10,567 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:10,568 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:10,569 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:10,576 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:10,596 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:11,000 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:11,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:11,022 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:11,022 : INFO : training on 299450 raw words (162889 effective words) took 0.4s, 383776 effective words/s\n",
      "2017-12-19 17:54:11,024 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:11,025 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:11,049 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:11,049 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:11,057 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:11,059 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:11,065 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:11,067 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:11,068 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:11,069 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:11,074 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:11,094 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:11,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:11,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:11,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:11,556 : INFO : training on 299450 raw words (162889 effective words) took 0.5s, 354427 effective words/s\n",
      "2017-12-19 17:54:11,558 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:11,560 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:11,586 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:11,587 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:11,595 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:11,596 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:11,604 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:11,605 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:11,607 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:11,608 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:11,614 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:11,634 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:12,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:12,068 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:12,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:12,077 : INFO : training on 299450 raw words (162889 effective words) took 0.4s, 368472 effective words/s\n",
      "2017-12-19 17:54:12,079 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:12,080 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:12,103 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:12,104 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:12,111 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:12,112 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:12,119 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:12,121 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:12,122 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:12,123 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:12,128 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:12,148 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:12,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:12,620 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:12,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:12,625 : INFO : training on 299450 raw words (162889 effective words) took 0.5s, 343027 effective words/s\n",
      "2017-12-19 17:54:12,627 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:12,629 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:12,657 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:12,658 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:12,667 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:12,667 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:12,676 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:12,677 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:12,678 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:12,680 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "2017-12-19 17:54:12,686 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:12,706 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:13,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:13,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:13,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:13,155 : INFO : training on 299450 raw words (162889 effective words) took 0.4s, 363770 effective words/s\n",
      "2017-12-19 17:54:13,158 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:13,159 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:13,185 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:13,186 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:13,194 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:13,195 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:13,202 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:13,204 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:13,205 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:13,205 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:13,209 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:13,251 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:13,255 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:13,275 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:14,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:14,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:14,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:14,193 : INFO : training on 299450 raw words (162889 effective words) took 0.9s, 178065 effective words/s\n",
      "2017-12-19 17:54:14,195 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:14,197 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:14,225 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:14,225 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:14,233 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:14,234 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:14,242 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:14,244 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:14,245 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:14,246 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:14,249 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:14,292 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:14,297 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:14,317 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:15,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:15,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:15,198 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:15,199 : INFO : training on 299450 raw words (162889 effective words) took 0.9s, 185065 effective words/s\n",
      "2017-12-19 17:54:15,202 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:15,204 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:15,229 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:15,229 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:15,238 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:15,239 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:15,248 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:15,249 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:15,251 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:15,252 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:15,255 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:15,296 : INFO : built huffman tree with maximum node depth 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:15,300 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:15,320 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:16,090 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:16,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:16,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:16,132 : INFO : training on 299450 raw words (162889 effective words) took 0.8s, 201209 effective words/s\n",
      "2017-12-19 17:54:16,135 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:16,137 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:16,159 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:16,160 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:16,213 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:16,214 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:16,221 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:16,223 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:16,224 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:16,225 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:16,227 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:16,269 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:16,273 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:16,290 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:17,157 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:17,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:17,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:17,204 : INFO : training on 299450 raw words (162889 effective words) took 0.9s, 178830 effective words/s\n",
      "2017-12-19 17:54:17,207 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:17,208 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:17,233 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:17,234 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:17,242 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:17,243 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:17,251 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:17,253 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:17,254 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:17,255 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:17,258 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:17,301 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:17,306 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:17,327 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:18,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:18,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:18,232 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:18,234 : INFO : training on 299450 raw words (162889 effective words) took 0.9s, 180208 effective words/s\n",
      "2017-12-19 17:54:18,237 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:18,238 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:18,263 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2017-12-19 17:54:18,264 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:18,281 : INFO : min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "2017-12-19 17:54:18,282 : INFO : min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "2017-12-19 17:54:18,288 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2017-12-19 17:54:18,289 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-12-19 17:54:18,289 : INFO : downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "2017-12-19 17:54:18,290 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
      "2017-12-19 17:54:18,292 : INFO : constructing a huffman tree from 1762 words\n",
      "2017-12-19 17:54:18,333 : INFO : built huffman tree with maximum node depth 13\n",
      "2017-12-19 17:54:18,341 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:18,364 : INFO : training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:19,194 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:19,204 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:19,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:19,242 : INFO : training on 299450 raw words (162889 effective words) took 0.9s, 186016 effective words/s\n",
      "2017-12-19 17:54:19,245 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:19,873 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:21,585 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:54:21,586 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:21,822 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:54:21,823 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:54:21,986 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:54:22,025 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:54:22,026 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:54:22,027 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:54:22,206 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:22,662 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:23,667 : INFO : PROGRESS: at 1.32% examples, 409429 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:24,674 : INFO : PROGRESS: at 4.88% examples, 746600 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:54:25,682 : INFO : PROGRESS: at 8.41% examples, 859485 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:26,688 : INFO : PROGRESS: at 11.76% examples, 905906 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:27,687 : INFO : PROGRESS: at 15.15% examples, 936073 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:28,692 : INFO : PROGRESS: at 18.73% examples, 965364 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:29,766 : INFO : PROGRESS: at 19.98% examples, 873489 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:54:30,766 : INFO : PROGRESS: at 23.44% examples, 896402 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:54:31,771 : INFO : PROGRESS: at 27.02% examples, 919067 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:32,775 : INFO : PROGRESS: at 30.72% examples, 941776 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:33,778 : INFO : PROGRESS: at 34.44% examples, 961693 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:54:34,784 : INFO : PROGRESS: at 38.09% examples, 975410 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:36,010 : INFO : PROGRESS: at 39.98% examples, 929991 words/s, in_qsize 1, out_qsize 0\n",
      "2017-12-19 17:54:37,009 : INFO : PROGRESS: at 43.65% examples, 943434 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:54:38,014 : INFO : PROGRESS: at 47.11% examples, 951425 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:39,032 : INFO : PROGRESS: at 50.55% examples, 957948 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:54:40,033 : INFO : PROGRESS: at 54.02% examples, 964969 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:41,034 : INFO : PROGRESS: at 57.53% examples, 971771 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:42,438 : INFO : PROGRESS: at 60.00% examples, 941501 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:43,445 : INFO : PROGRESS: at 63.67% examples, 950267 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:44,450 : INFO : PROGRESS: at 67.33% examples, 958209 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:45,458 : INFO : PROGRESS: at 70.98% examples, 965787 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:46,464 : INFO : PROGRESS: at 74.63% examples, 973044 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:47,469 : INFO : PROGRESS: at 78.14% examples, 977500 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:48,712 : INFO : PROGRESS: at 80.00% examples, 952960 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:49,725 : INFO : PROGRESS: at 83.49% examples, 956897 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:50,732 : INFO : PROGRESS: at 86.95% examples, 960817 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:54:51,730 : INFO : PROGRESS: at 90.34% examples, 964032 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:54:52,741 : INFO : PROGRESS: at 93.64% examples, 966112 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:54:53,738 : INFO : PROGRESS: at 96.98% examples, 968425 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:54,742 : INFO : PROGRESS: at 99.88% examples, 966224 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:54:54,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:54:54,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:54:54,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:54:54,765 : INFO : training on 42426485 raw words (31026416 effective words) took 32.1s, 966553 effective words/s\n",
      "2017-12-19 17:54:54,768 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:54:55,322 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:54:57,209 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:54:57,210 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:54:57,470 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:54:57,471 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:54:57,636 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:54:57,674 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:54:57,675 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:54:57,676 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:54:57,869 : INFO : resetting layer weights\n",
      "2017-12-19 17:54:58,324 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:54:59,338 : INFO : PROGRESS: at 1.20% examples, 372960 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:00,356 : INFO : PROGRESS: at 4.50% examples, 683097 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:01,352 : INFO : PROGRESS: at 8.08% examples, 823181 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:02,366 : INFO : PROGRESS: at 11.28% examples, 864925 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:03,370 : INFO : PROGRESS: at 14.56% examples, 896208 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:04,375 : INFO : PROGRESS: at 17.76% examples, 911506 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:05,741 : INFO : PROGRESS: at 19.98% examples, 837041 words/s, in_qsize 3, out_qsize 0\n",
      "2017-12-19 17:55:06,737 : INFO : PROGRESS: at 23.30% examples, 858445 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:07,738 : INFO : PROGRESS: at 26.64% examples, 876730 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:08,745 : INFO : PROGRESS: at 29.75% examples, 884840 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:09,752 : INFO : PROGRESS: at 32.58% examples, 884332 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:10,765 : INFO : PROGRESS: at 35.69% examples, 891093 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:55:11,757 : INFO : PROGRESS: at 39.20% examples, 905798 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:55:12,768 : INFO : PROGRESS: at 40.12% examples, 861985 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:13,771 : INFO : PROGRESS: at 43.30% examples, 869200 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:14,773 : INFO : PROGRESS: at 47.00% examples, 885633 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:15,783 : INFO : PROGRESS: at 50.62% examples, 899045 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:55:16,787 : INFO : PROGRESS: at 54.04% examples, 908130 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:55:17,795 : INFO : PROGRESS: at 57.41% examples, 914860 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:55:19,207 : INFO : PROGRESS: at 60.00% examples, 891404 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:20,213 : INFO : PROGRESS: at 63.27% examples, 896628 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:55:21,210 : INFO : PROGRESS: at 66.78% examples, 904625 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:22,215 : INFO : PROGRESS: at 70.06% examples, 909180 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:23,215 : INFO : PROGRESS: at 73.43% examples, 915093 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:24,236 : INFO : PROGRESS: at 76.73% examples, 918839 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:25,288 : INFO : PROGRESS: at 79.79% examples, 918156 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:55:26,288 : INFO : PROGRESS: at 81.46% examples, 903755 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:27,289 : INFO : PROGRESS: at 84.85% examples, 908390 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:28,300 : INFO : PROGRESS: at 88.55% examples, 915913 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:55:29,305 : INFO : PROGRESS: at 92.16% examples, 922619 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:30,317 : INFO : PROGRESS: at 95.81% examples, 929182 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:31,314 : INFO : PROGRESS: at 99.53% examples, 936028 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:31,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:55:31,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:55:31,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:55:31,612 : INFO : training on 42426485 raw words (31019271 effective words) took 33.3s, 931936 effective words/s\n",
      "2017-12-19 17:55:31,639 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:55:32,187 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:55:33,959 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:55:33,961 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:55:34,236 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:55:34,237 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:55:34,399 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:55:34,436 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:55:34,437 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:55:34,437 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:55:34,618 : INFO : resetting layer weights\n",
      "2017-12-19 17:55:35,045 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:55:36,054 : INFO : PROGRESS: at 1.37% examples, 424277 words/s, in_qsize 5, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:55:37,053 : INFO : PROGRESS: at 4.85% examples, 743756 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:38,055 : INFO : PROGRESS: at 8.36% examples, 857029 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:39,062 : INFO : PROGRESS: at 12.08% examples, 933378 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:40,065 : INFO : PROGRESS: at 15.59% examples, 964371 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:41,067 : INFO : PROGRESS: at 19.22% examples, 991157 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:42,073 : INFO : PROGRESS: at 20.33% examples, 897913 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:43,076 : INFO : PROGRESS: at 24.00% examples, 926179 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:44,082 : INFO : PROGRESS: at 27.73% examples, 950395 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:55:45,089 : INFO : PROGRESS: at 31.35% examples, 967921 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:46,090 : INFO : PROGRESS: at 35.01% examples, 983584 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:47,096 : INFO : PROGRESS: at 38.66% examples, 995623 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:48,207 : INFO : PROGRESS: at 39.95% examples, 942298 words/s, in_qsize 1, out_qsize 1\n",
      "2017-12-19 17:55:49,210 : INFO : PROGRESS: at 43.49% examples, 951909 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:50,216 : INFO : PROGRESS: at 46.27% examples, 945228 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:51,223 : INFO : PROGRESS: at 49.82% examples, 955081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:52,218 : INFO : PROGRESS: at 53.29% examples, 962596 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:53,221 : INFO : PROGRESS: at 56.68% examples, 967599 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:54,354 : INFO : PROGRESS: at 59.79% examples, 960857 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:55:55,359 : INFO : PROGRESS: at 61.15% examples, 934129 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:56,360 : INFO : PROGRESS: at 64.66% examples, 940611 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:55:57,366 : INFO : PROGRESS: at 67.96% examples, 944137 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 17:55:58,367 : INFO : PROGRESS: at 71.31% examples, 948457 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:55:59,377 : INFO : PROGRESS: at 74.77% examples, 953660 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:00,379 : INFO : PROGRESS: at 78.14% examples, 957243 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:01,592 : INFO : PROGRESS: at 79.98% examples, 934999 words/s, in_qsize 0, out_qsize 0\n",
      "2017-12-19 17:56:02,595 : INFO : PROGRESS: at 83.23% examples, 937166 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:03,597 : INFO : PROGRESS: at 86.78% examples, 942633 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:04,597 : INFO : PROGRESS: at 90.39% examples, 948601 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:05,602 : INFO : PROGRESS: at 93.95% examples, 953859 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:06,607 : INFO : PROGRESS: at 97.55% examples, 958980 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:07,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:56:07,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:56:07,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:56:07,434 : INFO : training on 42426485 raw words (31023269 effective words) took 32.4s, 957898 effective words/s\n",
      "2017-12-19 17:56:07,460 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:56:07,994 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:56:09,759 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:56:09,760 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:56:10,021 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:56:10,022 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:56:10,193 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:56:10,233 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:56:10,234 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:56:10,236 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:56:10,431 : INFO : resetting layer weights\n",
      "2017-12-19 17:56:10,864 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:56:11,872 : INFO : PROGRESS: at 1.32% examples, 408124 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:12,874 : INFO : PROGRESS: at 4.81% examples, 735728 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:13,891 : INFO : PROGRESS: at 8.46% examples, 861846 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:14,909 : INFO : PROGRESS: at 12.13% examples, 930719 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:56:15,913 : INFO : PROGRESS: at 15.92% examples, 979190 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:16,927 : INFO : PROGRESS: at 19.58% examples, 1003453 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:56:17,926 : INFO : PROGRESS: at 20.64% examples, 906929 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:18,935 : INFO : PROGRESS: at 24.31% examples, 933737 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:19,931 : INFO : PROGRESS: at 28.01% examples, 956711 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:20,935 : INFO : PROGRESS: at 31.59% examples, 972504 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:21,937 : INFO : PROGRESS: at 35.43% examples, 992806 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:22,942 : INFO : PROGRESS: at 39.08% examples, 1004222 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:23,948 : INFO : PROGRESS: at 40.05% examples, 950128 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 17:56:24,950 : INFO : PROGRESS: at 43.70% examples, 961855 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:25,958 : INFO : PROGRESS: at 47.54% examples, 976163 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:26,966 : INFO : PROGRESS: at 51.24% examples, 986754 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:56:27,967 : INFO : PROGRESS: at 54.70% examples, 992375 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:28,976 : INFO : PROGRESS: at 58.28% examples, 998814 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:30,125 : INFO : PROGRESS: at 59.98% examples, 966549 words/s, in_qsize 3, out_qsize 0\n",
      "2017-12-19 17:56:31,128 : INFO : PROGRESS: at 63.39% examples, 970377 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:56:32,128 : INFO : PROGRESS: at 67.11% examples, 978593 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:56:33,136 : INFO : PROGRESS: at 70.62% examples, 983372 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:34,137 : INFO : PROGRESS: at 74.30% examples, 990580 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:35,147 : INFO : PROGRESS: at 77.97% examples, 996403 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:36,390 : INFO : PROGRESS: at 79.98% examples, 972668 words/s, in_qsize 1, out_qsize 0\n",
      "2017-12-19 17:56:37,385 : INFO : PROGRESS: at 83.42% examples, 975537 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:38,386 : INFO : PROGRESS: at 87.11% examples, 981549 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:39,391 : INFO : PROGRESS: at 90.88% examples, 988103 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:40,401 : INFO : PROGRESS: at 94.51% examples, 993029 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:41,409 : INFO : PROGRESS: at 98.14% examples, 996914 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:42,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:56:42,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:56:42,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:56:42,077 : INFO : training on 42426485 raw words (31023748 effective words) took 31.2s, 994016 effective words/s\n",
      "2017-12-19 17:56:42,105 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:56:42,675 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:56:44,441 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:56:44,442 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:56:44,706 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:56:44,707 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:56:44,862 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:56:44,899 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:56:44,901 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:56:44,902 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:56:45,088 : INFO : resetting layer weights\n",
      "2017-12-19 17:56:45,524 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:56:46,529 : INFO : PROGRESS: at 1.32% examples, 409988 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:47,533 : INFO : PROGRESS: at 4.73% examples, 726045 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:48,545 : INFO : PROGRESS: at 8.46% examples, 865983 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:56:49,545 : INFO : PROGRESS: at 12.06% examples, 930341 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:50,550 : INFO : PROGRESS: at 15.52% examples, 959279 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:56:51,553 : INFO : PROGRESS: at 19.01% examples, 979427 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:52,571 : INFO : PROGRESS: at 20.02% examples, 882106 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:56:53,574 : INFO : PROGRESS: at 23.49% examples, 904351 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:54,581 : INFO : PROGRESS: at 26.90% examples, 920280 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:55,584 : INFO : PROGRESS: at 30.44% examples, 937894 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:56,584 : INFO : PROGRESS: at 33.83% examples, 949170 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:57,590 : INFO : PROGRESS: at 37.20% examples, 956898 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:56:59,109 : INFO : PROGRESS: at 40.00% examples, 913698 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:00,115 : INFO : PROGRESS: at 43.46% examples, 923574 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:01,117 : INFO : PROGRESS: at 46.95% examples, 933251 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:02,121 : INFO : PROGRESS: at 50.39% examples, 941322 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:03,123 : INFO : PROGRESS: at 53.90% examples, 950216 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:04,128 : INFO : PROGRESS: at 57.22% examples, 954419 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:57:05,635 : INFO : PROGRESS: at 60.00% examples, 926011 words/s, in_qsize 4, out_qsize 0\n",
      "2017-12-19 17:57:06,632 : INFO : PROGRESS: at 63.39% examples, 931353 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:07,636 : INFO : PROGRESS: at 66.88% examples, 937722 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:08,643 : INFO : PROGRESS: at 70.34% examples, 943694 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:57:09,651 : INFO : PROGRESS: at 73.66% examples, 947235 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:57:10,656 : INFO : PROGRESS: at 77.34% examples, 954857 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:12,150 : INFO : PROGRESS: at 79.98% examples, 932207 words/s, in_qsize 1, out_qsize 0\n",
      "2017-12-19 17:57:13,150 : INFO : PROGRESS: at 83.58% examples, 938290 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:14,156 : INFO : PROGRESS: at 87.37% examples, 946235 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:15,166 : INFO : PROGRESS: at 90.93% examples, 951401 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:16,170 : INFO : PROGRESS: at 94.46% examples, 956393 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:17,173 : INFO : PROGRESS: at 98.12% examples, 961887 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:17,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:57:17,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:57:17,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:57:17,901 : INFO : training on 42426485 raw words (31021799 effective words) took 32.4s, 958243 effective words/s\n",
      "2017-12-19 17:57:17,930 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:57:18,487 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:57:20,271 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:57:20,272 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:57:20,531 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:57:20,533 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:57:20,689 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:57:20,724 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:57:20,726 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:57:20,727 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 17:57:20,910 : INFO : resetting layer weights\n",
      "2017-12-19 17:57:21,336 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:57:22,344 : INFO : PROGRESS: at 1.44% examples, 444621 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:23,346 : INFO : PROGRESS: at 5.02% examples, 768680 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:24,349 : INFO : PROGRESS: at 8.67% examples, 888130 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:25,356 : INFO : PROGRESS: at 12.39% examples, 955660 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:26,357 : INFO : PROGRESS: at 16.02% examples, 990648 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:27,362 : INFO : PROGRESS: at 19.72% examples, 1016219 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:28,363 : INFO : PROGRESS: at 20.59% examples, 909520 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:29,364 : INFO : PROGRESS: at 24.36% examples, 939774 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:30,366 : INFO : PROGRESS: at 28.03% examples, 961501 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:31,371 : INFO : PROGRESS: at 31.52% examples, 973873 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:32,379 : INFO : PROGRESS: at 35.15% examples, 987706 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:33,382 : INFO : PROGRESS: at 38.87% examples, 1001508 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:34,455 : INFO : PROGRESS: at 39.98% examples, 946195 words/s, in_qsize 3, out_qsize 0\n",
      "2017-12-19 17:57:35,454 : INFO : PROGRESS: at 43.65% examples, 958728 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:36,464 : INFO : PROGRESS: at 47.21% examples, 967652 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:57:37,465 : INFO : PROGRESS: at 50.72% examples, 975030 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:38,478 : INFO : PROGRESS: at 54.35% examples, 984114 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:57:39,478 : INFO : PROGRESS: at 57.93% examples, 990782 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:40,852 : INFO : PROGRESS: at 59.98% examples, 954564 words/s, in_qsize 0, out_qsize 0\n",
      "2017-12-19 17:57:41,835 : INFO : PROGRESS: at 63.46% examples, 960037 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:42,839 : INFO : PROGRESS: at 66.93% examples, 964910 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:43,851 : INFO : PROGRESS: at 70.58% examples, 972318 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:44,849 : INFO : PROGRESS: at 74.13% examples, 978178 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:45,852 : INFO : PROGRESS: at 77.79% examples, 984424 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:57:47,207 : INFO : PROGRESS: at 80.00% examples, 959538 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:48,203 : INFO : PROGRESS: at 83.75% examples, 966624 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:57:49,212 : INFO : PROGRESS: at 87.26% examples, 970541 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:50,229 : INFO : PROGRESS: at 90.77% examples, 974227 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:57:51,236 : INFO : PROGRESS: at 94.42% examples, 979726 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:57:52,240 : INFO : PROGRESS: at 98.19% examples, 985752 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:52,947 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:57:52,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:57:52,954 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:57:52,955 : INFO : training on 42426485 raw words (31020819 effective words) took 31.6s, 981202 effective words/s\n",
      "2017-12-19 17:57:52,983 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:57:53,536 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:57:55,313 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:57:55,315 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:57:55,490 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:57:55,491 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:57:55,653 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:57:55,686 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:57:55,688 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:57:55,689 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 17:57:55,765 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 17:57:57,154 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 17:57:57,274 : INFO : resetting layer weights\n",
      "2017-12-19 17:57:57,714 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:57:58,722 : INFO : PROGRESS: at 0.68% examples, 212041 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:57:59,740 : INFO : PROGRESS: at 2.50% examples, 379840 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:58:00,752 : INFO : PROGRESS: at 4.26% examples, 431540 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:01,777 : INFO : PROGRESS: at 6.08% examples, 460975 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:58:02,797 : INFO : PROGRESS: at 7.87% examples, 476958 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:03,816 : INFO : PROGRESS: at 9.80% examples, 496171 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:58:04,827 : INFO : PROGRESS: at 11.78% examples, 512619 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:05,848 : INFO : PROGRESS: at 13.71% examples, 522852 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:06,857 : INFO : PROGRESS: at 15.67% examples, 531680 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:58:07,867 : INFO : PROGRESS: at 17.64% examples, 539628 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:08,876 : INFO : PROGRESS: at 19.53% examples, 543194 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:58:09,884 : INFO : PROGRESS: at 20.12% examples, 512996 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:10,887 : INFO : PROGRESS: at 22.05% examples, 519289 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:58:11,893 : INFO : PROGRESS: at 24.10% examples, 526526 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:12,901 : INFO : PROGRESS: at 26.08% examples, 531754 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:13,903 : INFO : PROGRESS: at 28.03% examples, 536256 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:14,928 : INFO : PROGRESS: at 30.08% examples, 541657 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:58:15,924 : INFO : PROGRESS: at 31.92% examples, 543531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:16,933 : INFO : PROGRESS: at 33.90% examples, 547210 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:17,936 : INFO : PROGRESS: at 35.88% examples, 550507 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:18,948 : INFO : PROGRESS: at 37.79% examples, 552172 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:19,953 : INFO : PROGRESS: at 39.69% examples, 553955 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:20,964 : INFO : PROGRESS: at 40.21% examples, 536670 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:21,974 : INFO : PROGRESS: at 42.19% examples, 539504 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:22,979 : INFO : PROGRESS: at 44.10% examples, 541086 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:23,990 : INFO : PROGRESS: at 46.08% examples, 543502 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:24,993 : INFO : PROGRESS: at 48.03% examples, 545711 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:26,003 : INFO : PROGRESS: at 49.87% examples, 546652 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:26,997 : INFO : PROGRESS: at 51.83% examples, 548882 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:28,010 : INFO : PROGRESS: at 53.85% examples, 551497 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:29,011 : INFO : PROGRESS: at 55.88% examples, 553981 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:30,017 : INFO : PROGRESS: at 57.83% examples, 555515 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:31,153 : INFO : PROGRESS: at 59.81% examples, 555176 words/s, in_qsize 4, out_qsize 0\n",
      "2017-12-19 17:58:32,150 : INFO : PROGRESS: at 60.61% examples, 546152 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:33,152 : INFO : PROGRESS: at 62.52% examples, 547176 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:34,160 : INFO : PROGRESS: at 64.38% examples, 547700 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:58:35,170 : INFO : PROGRESS: at 66.43% examples, 549857 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:36,183 : INFO : PROGRESS: at 68.32% examples, 550642 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 17:58:37,183 : INFO : PROGRESS: at 70.22% examples, 551701 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:38,190 : INFO : PROGRESS: at 72.23% examples, 553489 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:39,199 : INFO : PROGRESS: at 74.06% examples, 553910 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:40,230 : INFO : PROGRESS: at 75.92% examples, 554091 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:58:41,240 : INFO : PROGRESS: at 77.93% examples, 555525 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:42,269 : INFO : PROGRESS: at 79.74% examples, 555368 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 17:58:43,275 : INFO : PROGRESS: at 80.33% examples, 547056 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:44,293 : INFO : PROGRESS: at 82.19% examples, 547496 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:58:45,287 : INFO : PROGRESS: at 83.91% examples, 547009 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:46,294 : INFO : PROGRESS: at 85.70% examples, 547048 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:47,307 : INFO : PROGRESS: at 87.49% examples, 547054 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:48,323 : INFO : PROGRESS: at 89.21% examples, 546624 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:49,329 : INFO : PROGRESS: at 91.21% examples, 548123 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:50,343 : INFO : PROGRESS: at 93.10% examples, 548781 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:51,360 : INFO : PROGRESS: at 95.03% examples, 549613 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:58:52,369 : INFO : PROGRESS: at 97.03% examples, 550913 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:58:53,369 : INFO : PROGRESS: at 99.06% examples, 552248 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:58:54,011 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:58:54,031 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:58:54,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:58:54,041 : INFO : training on 42426485 raw words (31023859 effective words) took 56.3s, 550807 effective words/s\n",
      "2017-12-19 17:58:54,070 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:58:54,626 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:58:56,565 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:58:56,566 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:58:56,821 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:58:56,822 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:58:56,987 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:58:57,019 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 17:58:57,021 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:58:57,022 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 17:58:57,093 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 17:58:58,590 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 17:58:58,704 : INFO : resetting layer weights\n",
      "2017-12-19 17:58:59,149 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 17:59:00,166 : INFO : PROGRESS: at 0.64% examples, 197185 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:59:01,178 : INFO : PROGRESS: at 2.43% examples, 369333 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:02,183 : INFO : PROGRESS: at 4.22% examples, 427709 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:03,186 : INFO : PROGRESS: at 5.98% examples, 456664 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:04,190 : INFO : PROGRESS: at 7.75% examples, 474193 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:05,209 : INFO : PROGRESS: at 9.54% examples, 486867 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 17:59:06,215 : INFO : PROGRESS: at 11.45% examples, 501928 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:07,231 : INFO : PROGRESS: at 13.33% examples, 511953 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:08,244 : INFO : PROGRESS: at 15.15% examples, 517469 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:09,244 : INFO : PROGRESS: at 17.03% examples, 523865 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:10,264 : INFO : PROGRESS: at 19.03% examples, 531838 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:11,415 : INFO : PROGRESS: at 19.91% examples, 504377 words/s, in_qsize 3, out_qsize 0\n",
      "2017-12-19 17:59:12,410 : INFO : PROGRESS: at 21.93% examples, 513227 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:13,413 : INFO : PROGRESS: at 23.91% examples, 519434 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:14,417 : INFO : PROGRESS: at 25.77% examples, 522874 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:15,424 : INFO : PROGRESS: at 27.70% examples, 527251 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:16,434 : INFO : PROGRESS: at 29.47% examples, 528345 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:17,446 : INFO : PROGRESS: at 31.31% examples, 530468 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:18,453 : INFO : PROGRESS: at 33.17% examples, 533017 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:19,482 : INFO : PROGRESS: at 34.94% examples, 533375 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 17:59:20,479 : INFO : PROGRESS: at 36.75% examples, 534629 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:21,484 : INFO : PROGRESS: at 38.80% examples, 539140 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:22,771 : INFO : PROGRESS: at 39.91% examples, 524379 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 17:59:23,784 : INFO : PROGRESS: at 41.91% examples, 527796 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:24,792 : INFO : PROGRESS: at 43.82% examples, 529766 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:25,797 : INFO : PROGRESS: at 45.65% examples, 531038 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:26,817 : INFO : PROGRESS: at 47.73% examples, 534631 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:27,822 : INFO : PROGRESS: at 49.61% examples, 536443 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:28,847 : INFO : PROGRESS: at 51.66% examples, 539484 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:29,850 : INFO : PROGRESS: at 53.69% examples, 542537 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:30,861 : INFO : PROGRESS: at 55.59% examples, 543962 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:31,870 : INFO : PROGRESS: at 57.41% examples, 544523 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:32,875 : INFO : PROGRESS: at 59.39% examples, 546540 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:33,877 : INFO : PROGRESS: at 60.02% examples, 536286 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:34,885 : INFO : PROGRESS: at 62.00% examples, 538295 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:35,896 : INFO : PROGRESS: at 64.00% examples, 540121 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:36,914 : INFO : PROGRESS: at 66.08% examples, 542480 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:37,917 : INFO : PROGRESS: at 68.03% examples, 544073 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:38,923 : INFO : PROGRESS: at 70.11% examples, 546645 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:39,946 : INFO : PROGRESS: at 71.94% examples, 547014 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:40,963 : INFO : PROGRESS: at 73.92% examples, 548537 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:41,963 : INFO : PROGRESS: at 75.78% examples, 549233 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:42,966 : INFO : PROGRESS: at 77.71% examples, 550342 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:44,187 : INFO : PROGRESS: at 79.79% examples, 549830 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 17:59:45,206 : INFO : PROGRESS: at 80.61% examples, 543087 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:46,251 : INFO : PROGRESS: at 82.45% examples, 543076 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 17:59:47,247 : INFO : PROGRESS: at 84.50% examples, 544781 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:48,252 : INFO : PROGRESS: at 86.48% examples, 546107 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:49,264 : INFO : PROGRESS: at 88.46% examples, 547407 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:50,272 : INFO : PROGRESS: at 90.41% examples, 548509 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:51,291 : INFO : PROGRESS: at 92.25% examples, 548826 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 17:59:52,304 : INFO : PROGRESS: at 94.28% examples, 550370 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:53,312 : INFO : PROGRESS: at 96.14% examples, 550730 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 17:59:54,333 : INFO : PROGRESS: at 98.02% examples, 551219 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 17:59:55,446 : INFO : PROGRESS: at 99.79% examples, 550067 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 17:59:55,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 17:59:55,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 17:59:55,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 17:59:55,537 : INFO : training on 42426485 raw words (31023864 effective words) took 56.4s, 550228 effective words/s\n",
      "2017-12-19 17:59:55,588 : INFO : collecting all words and their counts\n",
      "2017-12-19 17:59:56,134 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 17:59:57,875 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 17:59:57,877 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 17:59:58,056 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 17:59:58,058 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 17:59:58,216 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 17:59:58,260 : INFO : sample=0.001 downsamples 38 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 17:59:58,262 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 17:59:58,263 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 17:59:58,341 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 17:59:59,718 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 17:59:59,828 : INFO : resetting layer weights\n",
      "2017-12-19 18:00:00,257 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:00:01,269 : INFO : PROGRESS: at 0.80% examples, 247696 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:02,287 : INFO : PROGRESS: at 2.64% examples, 400000 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:03,303 : INFO : PROGRESS: at 4.57% examples, 461285 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:04,312 : INFO : PROGRESS: at 6.31% examples, 479609 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:05,322 : INFO : PROGRESS: at 8.08% examples, 491920 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:06,332 : INFO : PROGRESS: at 9.82% examples, 499800 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:07,344 : INFO : PROGRESS: at 11.52% examples, 503933 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:00:08,339 : INFO : PROGRESS: at 13.38% examples, 513629 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:09,343 : INFO : PROGRESS: at 15.10% examples, 515915 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:10,355 : INFO : PROGRESS: at 16.91% examples, 520253 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:00:11,360 : INFO : PROGRESS: at 18.73% examples, 523751 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:12,758 : INFO : PROGRESS: at 19.91% examples, 494686 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:13,753 : INFO : PROGRESS: at 21.70% examples, 498952 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:14,785 : INFO : PROGRESS: at 23.56% examples, 502763 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:00:15,783 : INFO : PROGRESS: at 25.23% examples, 503323 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:16,799 : INFO : PROGRESS: at 26.90% examples, 503913 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:00:17,802 : INFO : PROGRESS: at 28.65% examples, 505682 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:18,808 : INFO : PROGRESS: at 30.39% examples, 507680 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:00:19,817 : INFO : PROGRESS: at 32.13% examples, 509396 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:20,831 : INFO : PROGRESS: at 33.85% examples, 510507 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:21,837 : INFO : PROGRESS: at 35.69% examples, 513316 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:22,852 : INFO : PROGRESS: at 37.46% examples, 514377 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:23,858 : INFO : PROGRESS: at 39.41% examples, 518228 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:24,870 : INFO : PROGRESS: at 39.95% examples, 503770 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:25,882 : INFO : PROGRESS: at 41.91% examples, 507357 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:26,889 : INFO : PROGRESS: at 43.75% examples, 509204 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:27,890 : INFO : PROGRESS: at 45.72% examples, 512855 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:28,905 : INFO : PROGRESS: at 47.68% examples, 515811 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:29,944 : INFO : PROGRESS: at 49.56% examples, 517582 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:00:30,966 : INFO : PROGRESS: at 51.31% examples, 518071 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:32,005 : INFO : PROGRESS: at 53.10% examples, 518895 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:00:33,044 : INFO : PROGRESS: at 54.89% examples, 519416 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:00:34,056 : INFO : PROGRESS: at 56.56% examples, 519160 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:00:35,059 : INFO : PROGRESS: at 58.26% examples, 519379 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:36,158 : INFO : PROGRESS: at 59.79% examples, 516730 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:00:37,174 : INFO : PROGRESS: at 60.57% examples, 508987 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:38,179 : INFO : PROGRESS: at 62.43% examples, 510538 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:39,187 : INFO : PROGRESS: at 64.24% examples, 511594 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:40,214 : INFO : PROGRESS: at 66.08% examples, 512622 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:41,223 : INFO : PROGRESS: at 67.89% examples, 513784 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:42,221 : INFO : PROGRESS: at 69.63% examples, 514511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:43,245 : INFO : PROGRESS: at 71.35% examples, 514719 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:44,282 : INFO : PROGRESS: at 73.10% examples, 515011 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:00:45,293 : INFO : PROGRESS: at 74.84% examples, 515562 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:46,294 : INFO : PROGRESS: at 76.68% examples, 516717 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:47,313 : INFO : PROGRESS: at 78.47% examples, 517440 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:00:48,833 : INFO : PROGRESS: at 79.88% examples, 510358 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:00:49,841 : INFO : PROGRESS: at 81.70% examples, 511154 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:50,863 : INFO : PROGRESS: at 83.49% examples, 511656 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:00:51,883 : INFO : PROGRESS: at 85.18% examples, 511603 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:52,890 : INFO : PROGRESS: at 86.95% examples, 512208 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:53,899 : INFO : PROGRESS: at 88.76% examples, 513046 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:54,902 : INFO : PROGRESS: at 90.48% examples, 513485 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:55,902 : INFO : PROGRESS: at 92.53% examples, 515805 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:56,925 : INFO : PROGRESS: at 94.20% examples, 515782 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:00:57,920 : INFO : PROGRESS: at 96.23% examples, 517739 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:00:58,946 : INFO : PROGRESS: at 98.19% examples, 519048 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:00:59,952 : INFO : PROGRESS: at 99.79% examples, 518647 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:00,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:01:00,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:01:00,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:01:00,059 : INFO : training on 42426485 raw words (31021814 effective words) took 59.8s, 518762 effective words/s\n",
      "2017-12-19 18:01:00,115 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:01:00,679 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:01:02,445 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:01:02,446 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:01:02,694 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:01:02,695 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:01:02,854 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:01:02,889 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:01:02,890 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:01:02,890 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:01:02,963 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:01:04,338 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:01:04,451 : INFO : resetting layer weights\n",
      "2017-12-19 18:01:04,885 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:01:05,902 : INFO : PROGRESS: at 0.71% examples, 217692 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:01:06,908 : INFO : PROGRESS: at 2.80% examples, 426289 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:07,918 : INFO : PROGRESS: at 4.69% examples, 475081 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:08,925 : INFO : PROGRESS: at 6.64% examples, 506472 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:09,932 : INFO : PROGRESS: at 8.65% examples, 528360 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:10,935 : INFO : PROGRESS: at 10.53% examples, 538256 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:11,939 : INFO : PROGRESS: at 12.60% examples, 553827 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:12,950 : INFO : PROGRESS: at 14.49% examples, 557650 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:13,969 : INFO : PROGRESS: at 16.54% examples, 564921 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:14,972 : INFO : PROGRESS: at 18.52% examples, 569796 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:01:16,491 : INFO : PROGRESS: at 19.91% examples, 532632 words/s, in_qsize 1, out_qsize 2\n",
      "2017-12-19 18:01:17,487 : INFO : PROGRESS: at 21.60% examples, 531849 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:18,515 : INFO : PROGRESS: at 23.37% examples, 531197 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:19,526 : INFO : PROGRESS: at 25.09% examples, 530671 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:20,535 : INFO : PROGRESS: at 26.90% examples, 532302 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:21,545 : INFO : PROGRESS: at 28.74% examples, 534446 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:22,572 : INFO : PROGRESS: at 30.46% examples, 533886 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:01:23,583 : INFO : PROGRESS: at 32.39% examples, 537216 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:24,592 : INFO : PROGRESS: at 34.30% examples, 540052 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:25,600 : INFO : PROGRESS: at 36.23% examples, 542687 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:26,601 : INFO : PROGRESS: at 38.26% examples, 546676 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:27,606 : INFO : PROGRESS: at 39.88% examples, 544758 words/s, in_qsize 1, out_qsize 0\n",
      "2017-12-19 18:01:28,612 : INFO : PROGRESS: at 40.85% examples, 534349 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:29,609 : INFO : PROGRESS: at 42.69% examples, 535316 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:30,617 : INFO : PROGRESS: at 44.71% examples, 538670 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:31,617 : INFO : PROGRESS: at 46.81% examples, 542646 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:32,621 : INFO : PROGRESS: at 48.79% examples, 545123 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:33,622 : INFO : PROGRESS: at 50.81% examples, 548196 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:01:34,626 : INFO : PROGRESS: at 52.72% examples, 549795 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:35,633 : INFO : PROGRESS: at 54.44% examples, 549329 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:36,640 : INFO : PROGRESS: at 56.28% examples, 549816 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:37,649 : INFO : PROGRESS: at 58.02% examples, 549420 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:38,834 : INFO : PROGRESS: at 59.79% examples, 546633 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:01:39,829 : INFO : PROGRESS: at 60.64% examples, 538366 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:40,837 : INFO : PROGRESS: at 62.33% examples, 537756 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:41,843 : INFO : PROGRESS: at 64.22% examples, 538721 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:42,884 : INFO : PROGRESS: at 65.96% examples, 538126 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:01:43,905 : INFO : PROGRESS: at 67.94% examples, 539809 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:01:44,912 : INFO : PROGRESS: at 69.73% examples, 540163 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:45,924 : INFO : PROGRESS: at 71.59% examples, 541011 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:46,925 : INFO : PROGRESS: at 73.50% examples, 542349 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:47,929 : INFO : PROGRESS: at 75.22% examples, 542179 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:48,932 : INFO : PROGRESS: at 77.10% examples, 543095 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:49,961 : INFO : PROGRESS: at 79.10% examples, 544511 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:01:51,219 : INFO : PROGRESS: at 79.86% examples, 534942 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:01:52,218 : INFO : PROGRESS: at 81.88% examples, 536780 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:01:53,224 : INFO : PROGRESS: at 83.75% examples, 537285 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:54,224 : INFO : PROGRESS: at 85.72% examples, 538767 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:55,226 : INFO : PROGRESS: at 87.80% examples, 540787 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:56,251 : INFO : PROGRESS: at 89.68% examples, 541457 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:01:57,272 : INFO : PROGRESS: at 91.31% examples, 540603 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:01:58,283 : INFO : PROGRESS: at 93.12% examples, 541069 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:01:59,286 : INFO : PROGRESS: at 94.89% examples, 541185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:00,293 : INFO : PROGRESS: at 96.84% examples, 542350 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:01,288 : INFO : PROGRESS: at 98.61% examples, 542467 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:02,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:02:02,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:02:02,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:02:02,268 : INFO : training on 42426485 raw words (31023895 effective words) took 57.4s, 540677 effective words/s\n",
      "2017-12-19 18:02:02,321 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:02:02,909 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:02:04,808 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:02:04,809 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:02:05,076 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:02:05,078 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:02:05,247 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:02:05,285 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:02:05,287 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:02:05,288 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:02:05,376 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:02:06,875 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:02:06,992 : INFO : resetting layer weights\n",
      "2017-12-19 18:02:07,463 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:02:08,470 : INFO : PROGRESS: at 0.61% examples, 190401 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:02:09,509 : INFO : PROGRESS: at 2.52% examples, 380663 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:02:10,513 : INFO : PROGRESS: at 4.15% examples, 418887 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:02:11,529 : INFO : PROGRESS: at 6.20% examples, 469164 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:12,543 : INFO : PROGRESS: at 8.13% examples, 493131 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:13,545 : INFO : PROGRESS: at 10.04% examples, 509852 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:14,556 : INFO : PROGRESS: at 11.94% examples, 521788 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:15,568 : INFO : PROGRESS: at 13.57% examples, 519715 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:02:16,573 : INFO : PROGRESS: at 15.22% examples, 518950 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:02:17,591 : INFO : PROGRESS: at 17.15% examples, 525708 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:02:18,608 : INFO : PROGRESS: at 19.15% examples, 533561 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:19,843 : INFO : PROGRESS: at 19.88% examples, 499421 words/s, in_qsize 5, out_qsize 3\n",
      "2017-12-19 18:02:20,826 : INFO : PROGRESS: at 21.86% examples, 507636 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:02:21,845 : INFO : PROGRESS: at 23.65% examples, 509832 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:02:22,844 : INFO : PROGRESS: at 25.37% examples, 510923 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:23,845 : INFO : PROGRESS: at 27.28% examples, 515793 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:02:24,851 : INFO : PROGRESS: at 29.16% examples, 519533 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:25,862 : INFO : PROGRESS: at 30.98% examples, 521847 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:26,881 : INFO : PROGRESS: at 32.96% examples, 526407 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:27,896 : INFO : PROGRESS: at 35.05% examples, 532341 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:28,904 : INFO : PROGRESS: at 37.10% examples, 537149 words/s, in_qsize 4, out_qsize 0\n",
      "2017-12-19 18:02:29,905 : INFO : PROGRESS: at 39.18% examples, 541887 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:31,057 : INFO : PROGRESS: at 39.91% examples, 525149 words/s, in_qsize 3, out_qsize 1\n",
      "2017-12-19 18:02:32,049 : INFO : PROGRESS: at 41.79% examples, 527371 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:33,060 : INFO : PROGRESS: at 43.56% examples, 527561 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:02:34,090 : INFO : PROGRESS: at 45.23% examples, 526643 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:02:35,097 : INFO : PROGRESS: at 47.00% examples, 527131 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:36,108 : INFO : PROGRESS: at 48.88% examples, 528871 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:37,123 : INFO : PROGRESS: at 50.74% examples, 530560 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:38,143 : INFO : PROGRESS: at 52.51% examples, 530893 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:39,146 : INFO : PROGRESS: at 54.32% examples, 532024 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:40,166 : INFO : PROGRESS: at 56.09% examples, 532187 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:41,190 : INFO : PROGRESS: at 57.86% examples, 532313 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:02:42,194 : INFO : PROGRESS: at 59.62% examples, 532734 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:43,210 : INFO : PROGRESS: at 60.02% examples, 520993 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:02:44,230 : INFO : PROGRESS: at 61.77% examples, 521224 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:45,233 : INFO : PROGRESS: at 63.46% examples, 521059 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:46,258 : INFO : PROGRESS: at 65.35% examples, 522264 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:47,271 : INFO : PROGRESS: at 67.14% examples, 522901 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:48,277 : INFO : PROGRESS: at 68.72% examples, 521994 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:49,285 : INFO : PROGRESS: at 70.48% examples, 522646 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:50,290 : INFO : PROGRESS: at 72.27% examples, 523479 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:51,325 : INFO : PROGRESS: at 73.92% examples, 522917 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:52,333 : INFO : PROGRESS: at 75.76% examples, 523885 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:53,340 : INFO : PROGRESS: at 77.79% examples, 526091 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:02:54,531 : INFO : PROGRESS: at 79.76% examples, 525856 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:55,534 : INFO : PROGRESS: at 80.59% examples, 520166 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:56,541 : INFO : PROGRESS: at 82.57% examples, 521824 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:57,545 : INFO : PROGRESS: at 84.43% examples, 522739 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:02:58,572 : INFO : PROGRESS: at 86.29% examples, 523509 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:02:59,585 : INFO : PROGRESS: at 88.29% examples, 525302 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:03:00,588 : INFO : PROGRESS: at 90.08% examples, 525887 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:01,591 : INFO : PROGRESS: at 92.04% examples, 527496 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:02,598 : INFO : PROGRESS: at 93.97% examples, 528786 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:03,602 : INFO : PROGRESS: at 95.90% examples, 530018 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:04,623 : INFO : PROGRESS: at 97.67% examples, 530135 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:05,635 : INFO : PROGRESS: at 99.39% examples, 530124 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:06,141 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:03:06,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:03:06,157 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:03:06,159 : INFO : training on 42426485 raw words (31024048 effective words) took 58.7s, 528584 effective words/s\n",
      "2017-12-19 18:03:06,209 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:03:06,769 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:03:08,547 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:03:08,548 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:03:08,797 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:03:08,798 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:03:08,956 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:03:08,990 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:03:08,991 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:03:08,993 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:03:09,062 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:03:10,442 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:03:10,560 : INFO : resetting layer weights\n",
      "2017-12-19 18:03:10,998 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:03:12,003 : INFO : PROGRESS: at 0.61% examples, 190845 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:13,035 : INFO : PROGRESS: at 2.33% examples, 354815 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:14,050 : INFO : PROGRESS: at 4.05% examples, 409168 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:03:15,047 : INFO : PROGRESS: at 5.94% examples, 451178 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:16,070 : INFO : PROGRESS: at 7.87% examples, 478004 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:17,089 : INFO : PROGRESS: at 9.59% examples, 486511 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:18,101 : INFO : PROGRESS: at 11.43% examples, 497803 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:19,102 : INFO : PROGRESS: at 13.40% examples, 512882 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:20,104 : INFO : PROGRESS: at 15.34% examples, 522536 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:21,109 : INFO : PROGRESS: at 17.31% examples, 531511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:22,110 : INFO : PROGRESS: at 19.32% examples, 539661 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:03:23,135 : INFO : PROGRESS: at 19.88% examples, 508545 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:03:24,148 : INFO : PROGRESS: at 21.86% examples, 515736 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:03:25,153 : INFO : PROGRESS: at 23.70% examples, 518615 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:26,159 : INFO : PROGRESS: at 25.84% examples, 527777 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:03:27,170 : INFO : PROGRESS: at 27.80% examples, 532288 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:03:28,175 : INFO : PROGRESS: at 29.75% examples, 536686 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:29,206 : INFO : PROGRESS: at 31.78% examples, 541281 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:30,208 : INFO : PROGRESS: at 33.85% examples, 546713 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:31,213 : INFO : PROGRESS: at 35.78% examples, 549226 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:32,215 : INFO : PROGRESS: at 37.81% examples, 552971 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:33,313 : INFO : PROGRESS: at 39.79% examples, 553325 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:03:34,328 : INFO : PROGRESS: at 40.64% examples, 540440 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:35,329 : INFO : PROGRESS: at 42.71% examples, 544236 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:36,330 : INFO : PROGRESS: at 44.73% examples, 547320 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:37,339 : INFO : PROGRESS: at 46.78% examples, 550400 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:38,351 : INFO : PROGRESS: at 48.88% examples, 553794 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:39,390 : INFO : PROGRESS: at 50.88% examples, 555611 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:03:40,391 : INFO : PROGRESS: at 52.77% examples, 556803 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:41,404 : INFO : PROGRESS: at 54.75% examples, 558649 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:42,416 : INFO : PROGRESS: at 56.82% examples, 561069 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:43,419 : INFO : PROGRESS: at 58.85% examples, 563172 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:44,690 : INFO : PROGRESS: at 59.86% examples, 551284 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:45,693 : INFO : PROGRESS: at 61.81% examples, 552710 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:46,693 : INFO : PROGRESS: at 63.84% examples, 554539 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:47,724 : INFO : PROGRESS: at 65.80% examples, 555468 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:48,739 : INFO : PROGRESS: at 67.77% examples, 556677 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:49,751 : INFO : PROGRESS: at 69.66% examples, 557336 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:50,757 : INFO : PROGRESS: at 71.59% examples, 558410 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:51,770 : INFO : PROGRESS: at 73.52% examples, 559391 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:52,783 : INFO : PROGRESS: at 75.52% examples, 560817 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:03:53,801 : INFO : PROGRESS: at 77.46% examples, 561409 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:54,824 : INFO : PROGRESS: at 79.43% examples, 562376 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:55,872 : INFO : PROGRESS: at 79.93% examples, 552748 words/s, in_qsize 0, out_qsize 2\n",
      "2017-12-19 18:03:56,870 : INFO : PROGRESS: at 81.60% examples, 551880 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:57,877 : INFO : PROGRESS: at 83.53% examples, 552586 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:03:58,880 : INFO : PROGRESS: at 85.56% examples, 554078 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:03:59,896 : INFO : PROGRESS: at 87.54% examples, 555142 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:00,911 : INFO : PROGRESS: at 89.63% examples, 556888 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:01,911 : INFO : PROGRESS: at 91.57% examples, 557791 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:02,918 : INFO : PROGRESS: at 93.55% examples, 558927 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:03,926 : INFO : PROGRESS: at 95.52% examples, 559914 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:04,943 : INFO : PROGRESS: at 97.29% examples, 559548 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:05,971 : INFO : PROGRESS: at 99.18% examples, 559729 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:06,644 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:04:06,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:04:06,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:04:06,659 : INFO : training on 42426485 raw words (31021957 effective words) took 55.7s, 557366 effective words/s\n",
      "2017-12-19 18:04:06,710 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:04:07,302 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:04:09,226 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:04:09,227 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:04:09,480 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:04:09,481 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:04:09,627 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:04:09,661 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:04:09,662 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:04:09,663 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:04:09,844 : INFO : resetting layer weights\n",
      "2017-12-19 18:04:10,311 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:04:11,327 : INFO : PROGRESS: at 0.40% examples, 124386 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:12,342 : INFO : PROGRESS: at 1.48% examples, 227348 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:13,342 : INFO : PROGRESS: at 2.47% examples, 251444 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:14,349 : INFO : PROGRESS: at 3.51% examples, 267345 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:15,360 : INFO : PROGRESS: at 4.52% examples, 275425 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:04:16,360 : INFO : PROGRESS: at 5.49% examples, 279268 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:17,381 : INFO : PROGRESS: at 6.36% examples, 276995 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:18,388 : INFO : PROGRESS: at 7.35% examples, 280430 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:19,410 : INFO : PROGRESS: at 8.32% examples, 281862 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:04:20,441 : INFO : PROGRESS: at 9.26% examples, 282121 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:21,449 : INFO : PROGRESS: at 10.22% examples, 283678 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:22,495 : INFO : PROGRESS: at 11.21% examples, 284786 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:23,526 : INFO : PROGRESS: at 12.16% examples, 284942 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:24,530 : INFO : PROGRESS: at 13.07% examples, 285063 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:25,532 : INFO : PROGRESS: at 13.99% examples, 285194 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:26,545 : INFO : PROGRESS: at 14.96% examples, 285933 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:27,548 : INFO : PROGRESS: at 15.88% examples, 285839 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:28,561 : INFO : PROGRESS: at 16.91% examples, 287585 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:29,564 : INFO : PROGRESS: at 17.90% examples, 288605 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:30,605 : INFO : PROGRESS: at 18.92% examples, 289327 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:31,621 : INFO : PROGRESS: at 19.79% examples, 288244 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:04:32,632 : INFO : PROGRESS: at 20.24% examples, 281316 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:33,633 : INFO : PROGRESS: at 21.25% examples, 282763 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:34,655 : INFO : PROGRESS: at 22.33% examples, 284438 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:35,662 : INFO : PROGRESS: at 23.46% examples, 286723 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:36,679 : INFO : PROGRESS: at 24.57% examples, 288535 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:04:37,686 : INFO : PROGRESS: at 25.65% examples, 290239 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:38,694 : INFO : PROGRESS: at 26.64% examples, 290677 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:39,705 : INFO : PROGRESS: at 27.73% examples, 292107 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:40,712 : INFO : PROGRESS: at 28.88% examples, 294206 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:41,741 : INFO : PROGRESS: at 30.04% examples, 296055 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:42,756 : INFO : PROGRESS: at 31.02% examples, 296415 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:04:43,755 : INFO : PROGRESS: at 32.04% examples, 297031 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:44,759 : INFO : PROGRESS: at 33.05% examples, 297586 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:45,774 : INFO : PROGRESS: at 34.06% examples, 298001 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:46,824 : INFO : PROGRESS: at 35.12% examples, 298465 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:47,865 : INFO : PROGRESS: at 36.21% examples, 299135 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:48,867 : INFO : PROGRESS: at 37.22% examples, 299568 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:04:49,921 : INFO : PROGRESS: at 38.23% examples, 299511 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:50,953 : INFO : PROGRESS: at 39.29% examples, 300054 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:52,196 : INFO : PROGRESS: at 39.86% examples, 295386 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:04:53,209 : INFO : PROGRESS: at 40.87% examples, 295621 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:54,237 : INFO : PROGRESS: at 41.86% examples, 295632 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:04:55,247 : INFO : PROGRESS: at 42.92% examples, 296125 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:04:56,252 : INFO : PROGRESS: at 43.96% examples, 296591 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:57,261 : INFO : PROGRESS: at 45.02% examples, 297170 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:04:58,301 : INFO : PROGRESS: at 46.05% examples, 297395 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:04:59,319 : INFO : PROGRESS: at 47.09% examples, 297794 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:00,332 : INFO : PROGRESS: at 48.08% examples, 297860 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:05:01,344 : INFO : PROGRESS: at 49.12% examples, 298281 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:02,387 : INFO : PROGRESS: at 50.18% examples, 298665 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:03,429 : INFO : PROGRESS: at 51.24% examples, 299078 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:04,431 : INFO : PROGRESS: at 52.30% examples, 299679 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:05,442 : INFO : PROGRESS: at 53.31% examples, 299968 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:06,455 : INFO : PROGRESS: at 54.32% examples, 300185 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:07,473 : INFO : PROGRESS: at 55.38% examples, 300581 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:08,501 : INFO : PROGRESS: at 56.44% examples, 300921 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:09,539 : INFO : PROGRESS: at 57.46% examples, 300991 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:10,562 : INFO : PROGRESS: at 58.52% examples, 301339 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:11,583 : INFO : PROGRESS: at 59.58% examples, 301688 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:12,634 : INFO : PROGRESS: at 59.86% examples, 298065 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:05:13,645 : INFO : PROGRESS: at 60.85% examples, 298116 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:05:14,664 : INFO : PROGRESS: at 61.79% examples, 297879 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:05:15,693 : INFO : PROGRESS: at 62.85% examples, 298080 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:16,703 : INFO : PROGRESS: at 63.86% examples, 298248 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:17,722 : INFO : PROGRESS: at 64.97% examples, 298799 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:18,723 : INFO : PROGRESS: at 66.03% examples, 299212 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:19,725 : INFO : PROGRESS: at 67.07% examples, 299528 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:20,788 : INFO : PROGRESS: at 68.15% examples, 299804 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:05:21,815 : INFO : PROGRESS: at 69.14% examples, 299767 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:22,861 : INFO : PROGRESS: at 70.08% examples, 299524 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:05:23,862 : INFO : PROGRESS: at 71.10% examples, 299754 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:24,896 : INFO : PROGRESS: at 72.04% examples, 299569 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:25,903 : INFO : PROGRESS: at 73.00% examples, 299574 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:26,950 : INFO : PROGRESS: at 74.02% examples, 299624 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:27,950 : INFO : PROGRESS: at 75.03% examples, 299825 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:28,962 : INFO : PROGRESS: at 76.07% examples, 300057 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:29,989 : INFO : PROGRESS: at 77.10% examples, 300236 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:31,022 : INFO : PROGRESS: at 78.14% examples, 300382 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:32,028 : INFO : PROGRESS: at 79.13% examples, 300451 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:33,509 : INFO : PROGRESS: at 79.86% examples, 297891 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:05:34,533 : INFO : PROGRESS: at 80.90% examples, 298028 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:05:35,551 : INFO : PROGRESS: at 81.96% examples, 298278 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:36,561 : INFO : PROGRESS: at 82.94% examples, 298254 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:37,619 : INFO : PROGRESS: at 83.98% examples, 298335 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:05:38,625 : INFO : PROGRESS: at 85.02% examples, 298528 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:39,654 : INFO : PROGRESS: at 86.10% examples, 298837 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:40,691 : INFO : PROGRESS: at 87.14% examples, 298955 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:41,717 : INFO : PROGRESS: at 88.24% examples, 299345 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:42,721 : INFO : PROGRESS: at 89.33% examples, 299761 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:43,723 : INFO : PROGRESS: at 90.44% examples, 300250 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:44,730 : INFO : PROGRESS: at 91.57% examples, 300799 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:45,740 : INFO : PROGRESS: at 92.70% examples, 301348 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:46,768 : INFO : PROGRESS: at 93.83% examples, 301795 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:47,798 : INFO : PROGRESS: at 94.91% examples, 302075 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:48,833 : INFO : PROGRESS: at 95.95% examples, 302161 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:05:49,840 : INFO : PROGRESS: at 97.08% examples, 302637 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:50,845 : INFO : PROGRESS: at 98.14% examples, 302885 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:51,879 : INFO : PROGRESS: at 99.20% examples, 303065 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:52,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:05:52,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:05:52,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:05:52,750 : INFO : training on 42426485 raw words (31025177 effective words) took 102.4s, 302876 effective words/s\n",
      "2017-12-19 18:05:52,802 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:05:53,398 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:05:55,187 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:05:55,189 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:05:55,538 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:05:55,540 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:05:55,706 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:05:55,741 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:05:55,743 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:05:55,744 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:05:55,924 : INFO : resetting layer weights\n",
      "2017-12-19 18:05:56,357 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:05:57,361 : INFO : PROGRESS: at 0.42% examples, 133111 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:05:58,375 : INFO : PROGRESS: at 1.39% examples, 214517 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:05:59,398 : INFO : PROGRESS: at 2.43% examples, 246097 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:00,412 : INFO : PROGRESS: at 3.42% examples, 259116 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:01,466 : INFO : PROGRESS: at 4.45% examples, 267827 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:06:02,469 : INFO : PROGRESS: at 5.56% examples, 280019 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:03,497 : INFO : PROGRESS: at 6.67% examples, 287527 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:04,507 : INFO : PROGRESS: at 7.77% examples, 293945 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:05,515 : INFO : PROGRESS: at 8.90% examples, 299835 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:06,539 : INFO : PROGRESS: at 10.01% examples, 303702 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:07,578 : INFO : PROGRESS: at 11.10% examples, 305922 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:08,591 : INFO : PROGRESS: at 12.20% examples, 308966 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:09,593 : INFO : PROGRESS: at 13.31% examples, 311787 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:10,604 : INFO : PROGRESS: at 14.46% examples, 315071 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:11,643 : INFO : PROGRESS: at 15.62% examples, 317025 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:12,660 : INFO : PROGRESS: at 16.73% examples, 318330 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:13,704 : INFO : PROGRESS: at 17.79% examples, 318163 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:14,719 : INFO : PROGRESS: at 18.78% examples, 317395 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:15,888 : INFO : PROGRESS: at 19.79% examples, 314496 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:06:16,894 : INFO : PROGRESS: at 20.21% examples, 305390 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:17,905 : INFO : PROGRESS: at 21.20% examples, 305421 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:06:18,911 : INFO : PROGRESS: at 22.31% examples, 306750 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:06:19,909 : INFO : PROGRESS: at 23.44% examples, 308300 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:20,921 : INFO : PROGRESS: at 24.59% examples, 310006 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:21,941 : INFO : PROGRESS: at 25.68% examples, 310810 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:22,946 : INFO : PROGRESS: at 26.69% examples, 310806 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:23,955 : INFO : PROGRESS: at 27.77% examples, 311634 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:24,957 : INFO : PROGRESS: at 28.90% examples, 312967 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:25,965 : INFO : PROGRESS: at 29.94% examples, 313287 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:06:26,982 : INFO : PROGRESS: at 30.93% examples, 312996 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:27,993 : INFO : PROGRESS: at 31.99% examples, 313544 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:29,007 : INFO : PROGRESS: at 32.93% examples, 312821 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:30,028 : INFO : PROGRESS: at 34.04% examples, 313637 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:31,033 : INFO : PROGRESS: at 35.17% examples, 314708 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:32,036 : INFO : PROGRESS: at 36.28% examples, 315461 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:33,039 : INFO : PROGRESS: at 37.34% examples, 315853 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:34,066 : INFO : PROGRESS: at 38.42% examples, 316197 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:35,081 : INFO : PROGRESS: at 39.58% examples, 317133 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:36,089 : INFO : PROGRESS: at 39.91% examples, 311687 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:37,138 : INFO : PROGRESS: at 40.99% examples, 311864 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:38,139 : INFO : PROGRESS: at 42.03% examples, 312019 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:39,150 : INFO : PROGRESS: at 43.11% examples, 312308 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:40,161 : INFO : PROGRESS: at 44.17% examples, 312538 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:41,165 : INFO : PROGRESS: at 45.28% examples, 313153 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:42,171 : INFO : PROGRESS: at 46.31% examples, 313256 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:43,216 : INFO : PROGRESS: at 47.40% examples, 313459 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:06:44,244 : INFO : PROGRESS: at 48.46% examples, 313581 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:45,288 : INFO : PROGRESS: at 49.42% examples, 313080 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:46,340 : INFO : PROGRESS: at 50.44% examples, 312813 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:47,346 : INFO : PROGRESS: at 51.45% examples, 312856 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:48,361 : INFO : PROGRESS: at 52.53% examples, 313287 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:49,373 : INFO : PROGRESS: at 53.64% examples, 313840 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:50,393 : INFO : PROGRESS: at 54.72% examples, 314183 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:51,410 : INFO : PROGRESS: at 55.81% examples, 314485 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:52,417 : INFO : PROGRESS: at 56.84% examples, 314572 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:53,464 : INFO : PROGRESS: at 57.97% examples, 314974 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:54,475 : INFO : PROGRESS: at 59.08% examples, 315408 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:06:55,987 : INFO : PROGRESS: at 59.86% examples, 311604 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:06:56,966 : INFO : PROGRESS: at 60.94% examples, 311950 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:57,988 : INFO : PROGRESS: at 62.05% examples, 312306 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:06:59,036 : INFO : PROGRESS: at 63.18% examples, 312580 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:07:00,077 : INFO : PROGRESS: at 64.38% examples, 313230 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:07:01,102 : INFO : PROGRESS: at 65.44% examples, 313357 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:02,117 : INFO : PROGRESS: at 66.57% examples, 313825 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:03,130 : INFO : PROGRESS: at 67.63% examples, 313981 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:04,141 : INFO : PROGRESS: at 68.74% examples, 314387 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:05,145 : INFO : PROGRESS: at 69.87% examples, 314923 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:06,146 : INFO : PROGRESS: at 70.93% examples, 315152 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:07,169 : INFO : PROGRESS: at 71.90% examples, 314899 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:08,175 : INFO : PROGRESS: at 72.98% examples, 315204 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:09,192 : INFO : PROGRESS: at 74.13% examples, 315778 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:10,209 : INFO : PROGRESS: at 75.27% examples, 316195 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:07:11,211 : INFO : PROGRESS: at 76.33% examples, 316341 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:12,211 : INFO : PROGRESS: at 77.39% examples, 316521 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:13,236 : INFO : PROGRESS: at 78.45% examples, 316587 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:14,243 : INFO : PROGRESS: at 79.41% examples, 316349 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:15,384 : INFO : PROGRESS: at 79.84% examples, 313488 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:07:16,380 : INFO : PROGRESS: at 80.82% examples, 313371 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:17,389 : INFO : PROGRESS: at 81.88% examples, 313514 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:07:18,407 : INFO : PROGRESS: at 82.94% examples, 313510 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:19,412 : INFO : PROGRESS: at 84.00% examples, 313642 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:20,426 : INFO : PROGRESS: at 85.11% examples, 313915 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:21,441 : INFO : PROGRESS: at 86.22% examples, 314191 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:22,447 : INFO : PROGRESS: at 87.30% examples, 314429 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:23,450 : INFO : PROGRESS: at 88.43% examples, 314826 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:24,451 : INFO : PROGRESS: at 89.56% examples, 315278 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:25,464 : INFO : PROGRESS: at 90.72% examples, 315722 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:26,468 : INFO : PROGRESS: at 91.80% examples, 315973 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:27,478 : INFO : PROGRESS: at 92.93% examples, 316357 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:28,490 : INFO : PROGRESS: at 93.95% examples, 316340 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:07:29,498 : INFO : PROGRESS: at 95.03% examples, 316533 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:30,532 : INFO : PROGRESS: at 96.04% examples, 316396 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:07:31,546 : INFO : PROGRESS: at 97.13% examples, 316569 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:32,548 : INFO : PROGRESS: at 98.23% examples, 316843 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:33,557 : INFO : PROGRESS: at 99.34% examples, 317099 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:34,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:07:34,254 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:07:34,272 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:07:34,274 : INFO : training on 42426485 raw words (31022416 effective words) took 97.9s, 316835 effective words/s\n",
      "2017-12-19 18:07:34,300 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:07:34,877 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:07:36,628 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:07:36,630 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:07:36,817 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:07:36,819 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:07:36,979 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:07:37,012 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:07:37,014 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:07:37,015 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:07:37,188 : INFO : resetting layer weights\n",
      "2017-12-19 18:07:37,627 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:07:38,643 : INFO : PROGRESS: at 0.40% examples, 124163 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:39,653 : INFO : PROGRESS: at 1.48% examples, 228321 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:07:40,666 : INFO : PROGRESS: at 2.64% examples, 266834 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:41,680 : INFO : PROGRESS: at 3.77% examples, 286003 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:42,686 : INFO : PROGRESS: at 4.90% examples, 297822 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:43,708 : INFO : PROGRESS: at 6.03% examples, 305163 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:44,740 : INFO : PROGRESS: at 7.16% examples, 310185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:45,763 : INFO : PROGRESS: at 8.29% examples, 314161 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:46,786 : INFO : PROGRESS: at 9.31% examples, 313647 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:07:47,800 : INFO : PROGRESS: at 10.29% examples, 312746 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:48,807 : INFO : PROGRESS: at 11.35% examples, 314251 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:49,845 : INFO : PROGRESS: at 12.46% examples, 316012 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:50,866 : INFO : PROGRESS: at 13.57% examples, 317832 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:51,867 : INFO : PROGRESS: at 14.63% examples, 318809 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:52,877 : INFO : PROGRESS: at 15.78% examples, 321142 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:53,891 : INFO : PROGRESS: at 16.84% examples, 321342 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:07:54,912 : INFO : PROGRESS: at 17.97% examples, 322736 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:07:55,916 : INFO : PROGRESS: at 19.03% examples, 323026 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:07:57,378 : INFO : PROGRESS: at 19.84% examples, 311858 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:07:58,384 : INFO : PROGRESS: at 20.97% examples, 313499 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:07:59,387 : INFO : PROGRESS: at 22.07% examples, 314689 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:08:00,407 : INFO : PROGRESS: at 23.20% examples, 315575 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:01,434 : INFO : PROGRESS: at 24.36% examples, 316804 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:02,441 : INFO : PROGRESS: at 25.49% examples, 318083 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:03,455 : INFO : PROGRESS: at 26.60% examples, 318825 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:04,465 : INFO : PROGRESS: at 27.68% examples, 319361 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:05,478 : INFO : PROGRESS: at 28.81% examples, 320351 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:06,489 : INFO : PROGRESS: at 29.89% examples, 320864 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:07,513 : INFO : PROGRESS: at 31.02% examples, 321745 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:08:08,513 : INFO : PROGRESS: at 32.08% examples, 322092 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:09,514 : INFO : PROGRESS: at 33.17% examples, 322631 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:10,524 : INFO : PROGRESS: at 34.28% examples, 323251 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:11,529 : INFO : PROGRESS: at 35.34% examples, 323355 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:12,532 : INFO : PROGRESS: at 36.40% examples, 323508 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:13,538 : INFO : PROGRESS: at 37.48% examples, 323824 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:14,553 : INFO : PROGRESS: at 38.59% examples, 324302 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:08:15,589 : INFO : PROGRESS: at 39.69% examples, 324477 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:16,611 : INFO : PROGRESS: at 40.02% examples, 318520 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:08:17,634 : INFO : PROGRESS: at 41.11% examples, 318810 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:18,643 : INFO : PROGRESS: at 42.19% examples, 319060 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:19,654 : INFO : PROGRESS: at 43.18% examples, 318537 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:20,682 : INFO : PROGRESS: at 44.17% examples, 317985 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:08:21,691 : INFO : PROGRESS: at 45.14% examples, 317463 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:22,698 : INFO : PROGRESS: at 46.15% examples, 317298 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:23,724 : INFO : PROGRESS: at 47.23% examples, 317539 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:24,734 : INFO : PROGRESS: at 48.22% examples, 317255 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:08:25,742 : INFO : PROGRESS: at 49.14% examples, 316577 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:08:26,754 : INFO : PROGRESS: at 50.13% examples, 316281 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:27,791 : INFO : PROGRESS: at 51.19% examples, 316389 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:28,794 : INFO : PROGRESS: at 52.18% examples, 316249 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:29,807 : INFO : PROGRESS: at 53.22% examples, 316337 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:30,823 : INFO : PROGRESS: at 54.23% examples, 316259 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:31,823 : INFO : PROGRESS: at 55.27% examples, 316377 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:32,829 : INFO : PROGRESS: at 56.28% examples, 316279 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:33,848 : INFO : PROGRESS: at 57.34% examples, 316423 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:34,865 : INFO : PROGRESS: at 58.42% examples, 316677 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:35,881 : INFO : PROGRESS: at 59.55% examples, 317186 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:36,944 : INFO : PROGRESS: at 59.93% examples, 313487 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:37,962 : INFO : PROGRESS: at 60.97% examples, 313499 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:38,980 : INFO : PROGRESS: at 62.00% examples, 313491 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:39,984 : INFO : PROGRESS: at 63.06% examples, 313584 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:40,985 : INFO : PROGRESS: at 64.08% examples, 313551 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:41,993 : INFO : PROGRESS: at 65.09% examples, 313518 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:08:43,016 : INFO : PROGRESS: at 66.10% examples, 313367 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:08:44,017 : INFO : PROGRESS: at 67.14% examples, 313500 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:45,034 : INFO : PROGRESS: at 68.20% examples, 313631 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:46,051 : INFO : PROGRESS: at 69.23% examples, 313700 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:47,058 : INFO : PROGRESS: at 70.22% examples, 313608 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:48,073 : INFO : PROGRESS: at 71.21% examples, 313496 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:49,117 : INFO : PROGRESS: at 72.25% examples, 313460 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:50,122 : INFO : PROGRESS: at 73.26% examples, 313498 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:08:51,158 : INFO : PROGRESS: at 74.30% examples, 313498 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:52,160 : INFO : PROGRESS: at 75.34% examples, 313591 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:53,164 : INFO : PROGRESS: at 76.37% examples, 313686 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:54,169 : INFO : PROGRESS: at 77.39% examples, 313689 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:55,179 : INFO : PROGRESS: at 78.37% examples, 313568 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:56,199 : INFO : PROGRESS: at 79.41% examples, 313601 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:08:57,338 : INFO : PROGRESS: at 79.86% examples, 310893 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:08:58,344 : INFO : PROGRESS: at 80.90% examples, 310950 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:08:59,353 : INFO : PROGRESS: at 81.93% examples, 311012 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:00,356 : INFO : PROGRESS: at 82.97% examples, 311035 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:09:01,406 : INFO : PROGRESS: at 84.05% examples, 311112 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:09:02,411 : INFO : PROGRESS: at 85.11% examples, 311272 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:09:03,440 : INFO : PROGRESS: at 86.15% examples, 311272 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:04,470 : INFO : PROGRESS: at 87.18% examples, 311305 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:09:05,472 : INFO : PROGRESS: at 88.22% examples, 311378 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:06,513 : INFO : PROGRESS: at 89.28% examples, 311455 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:07,515 : INFO : PROGRESS: at 90.32% examples, 311592 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:08,553 : INFO : PROGRESS: at 91.35% examples, 311608 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:09,622 : INFO : PROGRESS: at 92.44% examples, 311696 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:09:10,622 : INFO : PROGRESS: at 93.45% examples, 311740 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:11,664 : INFO : PROGRESS: at 94.53% examples, 311904 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:12,698 : INFO : PROGRESS: at 95.59% examples, 311955 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:09:13,721 : INFO : PROGRESS: at 96.63% examples, 311981 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:14,725 : INFO : PROGRESS: at 97.69% examples, 312148 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:15,727 : INFO : PROGRESS: at 98.70% examples, 312183 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:16,761 : INFO : PROGRESS: at 99.76% examples, 312251 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:17,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:09:17,117 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:09:17,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:09:17,133 : INFO : training on 42426485 raw words (31023840 effective words) took 99.5s, 311786 effective words/s\n",
      "2017-12-19 18:09:17,157 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:09:17,739 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:09:19,490 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:09:19,491 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:09:19,730 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:09:19,731 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:09:19,898 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:09:19,935 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:09:19,936 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:09:19,936 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:09:20,108 : INFO : resetting layer weights\n",
      "2017-12-19 18:09:20,550 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:09:21,564 : INFO : PROGRESS: at 0.40% examples, 124511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:22,579 : INFO : PROGRESS: at 1.51% examples, 230917 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:23,598 : INFO : PROGRESS: at 2.66% examples, 268390 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:24,612 : INFO : PROGRESS: at 3.84% examples, 290664 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:25,612 : INFO : PROGRESS: at 4.99% examples, 303359 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:26,623 : INFO : PROGRESS: at 6.15% examples, 311593 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:27,646 : INFO : PROGRESS: at 7.26% examples, 315182 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:28,656 : INFO : PROGRESS: at 8.43% examples, 320838 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:29,658 : INFO : PROGRESS: at 9.52% examples, 322946 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:30,674 : INFO : PROGRESS: at 10.65% examples, 325276 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:09:31,695 : INFO : PROGRESS: at 11.73% examples, 325943 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:32,734 : INFO : PROGRESS: at 12.86% examples, 327236 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:33,742 : INFO : PROGRESS: at 13.92% examples, 327491 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:34,756 : INFO : PROGRESS: at 15.05% examples, 328879 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:35,759 : INFO : PROGRESS: at 16.14% examples, 329261 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:36,821 : INFO : PROGRESS: at 17.22% examples, 328561 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:37,823 : INFO : PROGRESS: at 18.33% examples, 329369 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:38,877 : INFO : PROGRESS: at 19.36% examples, 328026 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:40,055 : INFO : PROGRESS: at 19.84% examples, 315867 words/s, in_qsize 4, out_qsize 0\n",
      "2017-12-19 18:09:41,087 : INFO : PROGRESS: at 21.01% examples, 317592 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:42,090 : INFO : PROGRESS: at 22.17% examples, 319240 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:43,094 : INFO : PROGRESS: at 23.35% examples, 320906 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:44,131 : INFO : PROGRESS: at 24.45% examples, 321128 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:45,135 : INFO : PROGRESS: at 25.61% examples, 322637 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:46,162 : INFO : PROGRESS: at 26.76% examples, 323562 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:47,172 : INFO : PROGRESS: at 27.94% examples, 324977 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:48,175 : INFO : PROGRESS: at 28.95% examples, 324572 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:49,190 : INFO : PROGRESS: at 30.04% examples, 324915 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:09:50,194 : INFO : PROGRESS: at 31.17% examples, 325899 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:51,203 : INFO : PROGRESS: at 32.32% examples, 326979 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:52,205 : INFO : PROGRESS: at 33.38% examples, 327099 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:53,216 : INFO : PROGRESS: at 34.51% examples, 327831 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:54,217 : INFO : PROGRESS: at 35.64% examples, 328459 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:55,237 : INFO : PROGRESS: at 36.73% examples, 328541 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:09:56,240 : INFO : PROGRESS: at 37.83% examples, 328918 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:09:57,260 : INFO : PROGRESS: at 38.99% examples, 329565 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:09:58,699 : INFO : PROGRESS: at 39.86% examples, 324245 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:09:59,717 : INFO : PROGRESS: at 41.01% examples, 324916 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:00,745 : INFO : PROGRESS: at 42.14% examples, 325288 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:10:01,766 : INFO : PROGRESS: at 43.30% examples, 325718 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:10:02,798 : INFO : PROGRESS: at 44.45% examples, 326072 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:03,810 : INFO : PROGRESS: at 45.58% examples, 326575 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:04,829 : INFO : PROGRESS: at 46.74% examples, 327093 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:05,851 : INFO : PROGRESS: at 47.84% examples, 327281 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:06,883 : INFO : PROGRESS: at 49.00% examples, 327770 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:10:07,879 : INFO : PROGRESS: at 50.13% examples, 328306 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:08,906 : INFO : PROGRESS: at 51.28% examples, 328836 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:09,915 : INFO : PROGRESS: at 52.44% examples, 329436 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:10,923 : INFO : PROGRESS: at 53.57% examples, 329871 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:11,928 : INFO : PROGRESS: at 54.70% examples, 330315 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:12,982 : INFO : PROGRESS: at 55.88% examples, 330626 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:14,006 : INFO : PROGRESS: at 57.01% examples, 330856 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:15,010 : INFO : PROGRESS: at 58.16% examples, 331350 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:10:16,034 : INFO : PROGRESS: at 59.32% examples, 331711 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:17,195 : INFO : PROGRESS: at 59.86% examples, 327883 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:10:18,210 : INFO : PROGRESS: at 60.99% examples, 328167 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:19,229 : INFO : PROGRESS: at 62.17% examples, 328627 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:20,255 : INFO : PROGRESS: at 63.32% examples, 328845 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:21,257 : INFO : PROGRESS: at 64.48% examples, 329233 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:22,270 : INFO : PROGRESS: at 65.63% examples, 329650 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:23,279 : INFO : PROGRESS: at 66.71% examples, 329719 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:10:24,274 : INFO : PROGRESS: at 67.82% examples, 329914 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:25,284 : INFO : PROGRESS: at 68.95% examples, 330176 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:26,291 : INFO : PROGRESS: at 70.06% examples, 330408 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:10:27,292 : INFO : PROGRESS: at 71.19% examples, 330759 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:28,316 : INFO : PROGRESS: at 72.37% examples, 331204 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:29,334 : INFO : PROGRESS: at 73.45% examples, 331243 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:30,363 : INFO : PROGRESS: at 74.58% examples, 331443 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:31,375 : INFO : PROGRESS: at 75.76% examples, 331850 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:32,401 : INFO : PROGRESS: at 76.89% examples, 332002 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:33,402 : INFO : PROGRESS: at 78.07% examples, 332461 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:34,420 : INFO : PROGRESS: at 79.18% examples, 332544 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:10:35,725 : INFO : PROGRESS: at 79.84% examples, 329539 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:10:36,723 : INFO : PROGRESS: at 80.97% examples, 329774 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:37,761 : INFO : PROGRESS: at 82.14% examples, 330019 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:38,776 : INFO : PROGRESS: at 83.35% examples, 330406 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:39,783 : INFO : PROGRESS: at 84.48% examples, 330569 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:40,803 : INFO : PROGRESS: at 85.65% examples, 330927 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:41,838 : INFO : PROGRESS: at 86.86% examples, 331279 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:42,880 : INFO : PROGRESS: at 88.03% examples, 331508 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:43,887 : INFO : PROGRESS: at 89.19% examples, 331820 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:44,922 : INFO : PROGRESS: at 90.37% examples, 332131 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:45,933 : INFO : PROGRESS: at 91.50% examples, 332345 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:46,934 : INFO : PROGRESS: at 92.65% examples, 332688 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:47,937 : INFO : PROGRESS: at 93.80% examples, 333010 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:48,952 : INFO : PROGRESS: at 94.98% examples, 333337 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:49,957 : INFO : PROGRESS: at 96.09% examples, 333435 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:50,962 : INFO : PROGRESS: at 97.27% examples, 333786 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:10:51,969 : INFO : PROGRESS: at 98.37% examples, 333883 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:10:52,965 : INFO : PROGRESS: at 99.48% examples, 333996 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:10:53,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:10:53,612 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:10:53,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:10:53,631 : INFO : training on 42426485 raw words (31022421 effective words) took 93.1s, 333294 effective words/s\n",
      "2017-12-19 18:10:53,661 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:10:54,225 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:10:56,014 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:10:56,015 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:10:56,272 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:10:56,273 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:10:56,444 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:10:56,477 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:10:56,478 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:10:56,479 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:10:56,656 : INFO : resetting layer weights\n",
      "2017-12-19 18:10:57,110 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:10:58,125 : INFO : PROGRESS: at 0.40% examples, 124429 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:10:59,154 : INFO : PROGRESS: at 1.48% examples, 225716 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:00,161 : INFO : PROGRESS: at 2.52% examples, 254309 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:01,186 : INFO : PROGRESS: at 3.53% examples, 266596 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:02,198 : INFO : PROGRESS: at 4.64% examples, 280194 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:03,223 : INFO : PROGRESS: at 5.77% examples, 290522 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:04,247 : INFO : PROGRESS: at 6.86% examples, 295818 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:05,276 : INFO : PROGRESS: at 7.96% examples, 300417 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:06,292 : INFO : PROGRESS: at 9.09% examples, 305532 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:07,309 : INFO : PROGRESS: at 10.13% examples, 306976 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:11:08,307 : INFO : PROGRESS: at 11.19% examples, 309222 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:09,319 : INFO : PROGRESS: at 12.27% examples, 311412 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:10,330 : INFO : PROGRESS: at 13.40% examples, 314392 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:11,344 : INFO : PROGRESS: at 14.49% examples, 315880 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:12,358 : INFO : PROGRESS: at 15.62% examples, 317791 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:13,384 : INFO : PROGRESS: at 16.63% examples, 317062 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:14,407 : INFO : PROGRESS: at 17.76% examples, 318627 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:15,431 : INFO : PROGRESS: at 18.80% examples, 318484 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:16,520 : INFO : PROGRESS: at 19.79% examples, 316581 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:11:17,553 : INFO : PROGRESS: at 20.28% examples, 307862 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:11:18,577 : INFO : PROGRESS: at 21.41% examples, 309483 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:19,603 : INFO : PROGRESS: at 22.54% examples, 310626 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:20,617 : INFO : PROGRESS: at 23.58% examples, 310731 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:21,650 : INFO : PROGRESS: at 24.64% examples, 310928 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:22,696 : INFO : PROGRESS: at 25.61% examples, 309937 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:23,713 : INFO : PROGRESS: at 26.53% examples, 308773 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:11:24,718 : INFO : PROGRESS: at 27.59% examples, 309376 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:25,745 : INFO : PROGRESS: at 28.74% examples, 310824 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:11:26,744 : INFO : PROGRESS: at 29.78% examples, 311264 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:27,746 : INFO : PROGRESS: at 30.81% examples, 311657 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:28,793 : INFO : PROGRESS: at 31.92% examples, 312343 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:29,838 : INFO : PROGRESS: at 32.96% examples, 312260 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:30,840 : INFO : PROGRESS: at 34.11% examples, 313709 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:31,852 : INFO : PROGRESS: at 35.19% examples, 314298 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:32,856 : INFO : PROGRESS: at 36.33% examples, 315256 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:33,864 : INFO : PROGRESS: at 37.39% examples, 315626 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:34,883 : INFO : PROGRESS: at 38.49% examples, 316181 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:35,892 : INFO : PROGRESS: at 39.62% examples, 317036 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:36,900 : INFO : PROGRESS: at 39.93% examples, 311394 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:37,926 : INFO : PROGRESS: at 40.97% examples, 311405 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:38,934 : INFO : PROGRESS: at 42.03% examples, 311709 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:39,980 : INFO : PROGRESS: at 43.09% examples, 311585 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:40,992 : INFO : PROGRESS: at 44.17% examples, 311996 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:42,020 : INFO : PROGRESS: at 45.18% examples, 311809 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:43,025 : INFO : PROGRESS: at 46.22% examples, 311950 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:44,056 : INFO : PROGRESS: at 47.23% examples, 311839 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:11:45,054 : INFO : PROGRESS: at 48.24% examples, 311824 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:11:46,059 : INFO : PROGRESS: at 49.28% examples, 312042 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:47,079 : INFO : PROGRESS: at 50.27% examples, 311853 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:48,079 : INFO : PROGRESS: at 51.28% examples, 311965 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:49,091 : INFO : PROGRESS: at 52.25% examples, 311715 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:11:50,126 : INFO : PROGRESS: at 53.24% examples, 311487 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:11:51,177 : INFO : PROGRESS: at 54.25% examples, 311301 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:11:52,187 : INFO : PROGRESS: at 55.29% examples, 311451 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:53,207 : INFO : PROGRESS: at 56.35% examples, 311632 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:54,223 : INFO : PROGRESS: at 57.36% examples, 311622 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:55,243 : INFO : PROGRESS: at 58.40% examples, 311690 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:56,257 : INFO : PROGRESS: at 59.43% examples, 311793 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:11:57,360 : INFO : PROGRESS: at 59.84% examples, 308198 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:11:58,361 : INFO : PROGRESS: at 60.87% examples, 308344 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:11:59,373 : INFO : PROGRESS: at 61.91% examples, 308446 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:00,388 : INFO : PROGRESS: at 62.99% examples, 308684 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:01,393 : INFO : PROGRESS: at 64.03% examples, 308816 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:02,410 : INFO : PROGRESS: at 65.02% examples, 308675 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:03,415 : INFO : PROGRESS: at 66.05% examples, 308818 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:12:04,441 : INFO : PROGRESS: at 67.04% examples, 308680 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:05,450 : INFO : PROGRESS: at 68.06% examples, 308732 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:06,448 : INFO : PROGRESS: at 69.07% examples, 308802 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:07,485 : INFO : PROGRESS: at 70.11% examples, 308881 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:12:08,533 : INFO : PROGRESS: at 71.14% examples, 308894 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:09,554 : INFO : PROGRESS: at 72.18% examples, 309021 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:10,615 : INFO : PROGRESS: at 73.24% examples, 309085 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:11,640 : INFO : PROGRESS: at 74.25% examples, 309085 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:12,663 : INFO : PROGRESS: at 75.27% examples, 309081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:13,664 : INFO : PROGRESS: at 76.25% examples, 309032 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:12:14,677 : INFO : PROGRESS: at 77.29% examples, 309162 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:15,679 : INFO : PROGRESS: at 78.30% examples, 309225 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:16,711 : INFO : PROGRESS: at 79.36% examples, 309355 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:17,901 : INFO : PROGRESS: at 79.86% examples, 306738 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:12:18,929 : INFO : PROGRESS: at 80.94% examples, 306941 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:19,982 : INFO : PROGRESS: at 82.07% examples, 307232 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:21,009 : INFO : PROGRESS: at 83.20% examples, 307564 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:22,013 : INFO : PROGRESS: at 84.36% examples, 308088 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:23,045 : INFO : PROGRESS: at 85.47% examples, 308383 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:24,046 : INFO : PROGRESS: at 86.60% examples, 308845 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:25,054 : INFO : PROGRESS: at 87.75% examples, 309389 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:26,050 : INFO : PROGRESS: at 88.86% examples, 309766 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:27,062 : INFO : PROGRESS: at 89.96% examples, 310125 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:28,073 : INFO : PROGRESS: at 91.12% examples, 310665 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:29,081 : INFO : PROGRESS: at 92.25% examples, 311118 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:30,104 : INFO : PROGRESS: at 93.38% examples, 311500 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:12:31,127 : INFO : PROGRESS: at 94.53% examples, 311969 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:32,180 : INFO : PROGRESS: at 95.74% examples, 312422 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:33,182 : INFO : PROGRESS: at 96.87% examples, 312816 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:34,184 : INFO : PROGRESS: at 97.97% examples, 313144 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:35,216 : INFO : PROGRESS: at 98.96% examples, 312981 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:12:36,278 : INFO : PROGRESS: at 99.86% examples, 312438 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:36,353 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:12:36,357 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:12:36,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:12:36,374 : INFO : training on 42426485 raw words (31023205 effective words) took 99.3s, 312540 effective words/s\n",
      "2017-12-19 18:12:36,401 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:12:36,958 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:12:38,685 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:12:38,686 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:12:39,032 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:12:39,033 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:12:39,191 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:12:39,222 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:12:39,224 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:12:39,225 : INFO : estimated required memory for 48753 words and 100 dimensions: 63378900 bytes\n",
      "2017-12-19 18:12:39,405 : INFO : resetting layer weights\n",
      "2017-12-19 18:12:39,833 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:12:40,874 : INFO : PROGRESS: at 0.45% examples, 135258 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:41,874 : INFO : PROGRESS: at 1.58% examples, 240330 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:42,920 : INFO : PROGRESS: at 2.66% examples, 265073 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:43,942 : INFO : PROGRESS: at 3.77% examples, 282190 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:44,970 : INFO : PROGRESS: at 4.85% examples, 290482 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:46,004 : INFO : PROGRESS: at 5.96% examples, 297187 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:47,005 : INFO : PROGRESS: at 6.97% examples, 299541 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:48,014 : INFO : PROGRESS: at 8.03% examples, 302544 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:49,049 : INFO : PROGRESS: at 9.09% examples, 304399 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:50,052 : INFO : PROGRESS: at 10.15% examples, 306965 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:51,076 : INFO : PROGRESS: at 11.19% examples, 307961 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:52,084 : INFO : PROGRESS: at 12.23% examples, 309124 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:53,099 : INFO : PROGRESS: at 13.26% examples, 309992 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:54,160 : INFO : PROGRESS: at 14.32% examples, 310322 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:12:55,176 : INFO : PROGRESS: at 15.38% examples, 311100 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:56,180 : INFO : PROGRESS: at 16.47% examples, 312546 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:57,208 : INFO : PROGRESS: at 17.53% examples, 313046 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:12:58,218 : INFO : PROGRESS: at 18.59% examples, 313811 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:12:59,260 : INFO : PROGRESS: at 19.65% examples, 314018 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:13:00,263 : INFO : PROGRESS: at 19.98% examples, 303374 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:01,299 : INFO : PROGRESS: at 21.08% examples, 304865 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:02,308 : INFO : PROGRESS: at 22.26% examples, 307162 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:03,311 : INFO : PROGRESS: at 23.42% examples, 309018 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:04,315 : INFO : PROGRESS: at 24.55% examples, 310484 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:05,329 : INFO : PROGRESS: at 25.65% examples, 311643 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:06,358 : INFO : PROGRESS: at 26.78% examples, 312760 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:07,363 : INFO : PROGRESS: at 27.87% examples, 313469 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:08,372 : INFO : PROGRESS: at 29.05% examples, 315217 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:09,415 : INFO : PROGRESS: at 30.04% examples, 314562 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:10,444 : INFO : PROGRESS: at 31.05% examples, 314395 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:11,461 : INFO : PROGRESS: at 32.06% examples, 314331 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:12,503 : INFO : PROGRESS: at 33.22% examples, 315366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:13,510 : INFO : PROGRESS: at 34.28% examples, 315779 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:13:14,532 : INFO : PROGRESS: at 35.29% examples, 315560 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:15,546 : INFO : PROGRESS: at 36.33% examples, 315585 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:16,559 : INFO : PROGRESS: at 37.31% examples, 315277 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:13:17,567 : INFO : PROGRESS: at 38.49% examples, 316537 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:18,575 : INFO : PROGRESS: at 39.43% examples, 315855 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:19,770 : INFO : PROGRESS: at 39.86% examples, 309849 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:13:20,765 : INFO : PROGRESS: at 40.97% examples, 310534 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:21,770 : INFO : PROGRESS: at 42.05% examples, 311047 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:22,791 : INFO : PROGRESS: at 43.13% examples, 311296 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:23,858 : INFO : PROGRESS: at 44.24% examples, 311484 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:24,882 : INFO : PROGRESS: at 45.37% examples, 312142 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:25,919 : INFO : PROGRESS: at 46.50% examples, 312724 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:26,964 : INFO : PROGRESS: at 47.70% examples, 313660 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:27,976 : INFO : PROGRESS: at 48.76% examples, 313898 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:29,010 : INFO : PROGRESS: at 49.82% examples, 314048 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:30,018 : INFO : PROGRESS: at 50.81% examples, 313935 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:13:31,015 : INFO : PROGRESS: at 51.78% examples, 313701 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:32,016 : INFO : PROGRESS: at 52.82% examples, 313904 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:33,018 : INFO : PROGRESS: at 53.97% examples, 314805 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:34,034 : INFO : PROGRESS: at 55.05% examples, 315124 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:35,062 : INFO : PROGRESS: at 56.16% examples, 315499 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:36,060 : INFO : PROGRESS: at 57.24% examples, 315883 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:37,069 : INFO : PROGRESS: at 58.35% examples, 316310 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:13:38,075 : INFO : PROGRESS: at 59.48% examples, 316917 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:13:39,192 : INFO : PROGRESS: at 59.84% examples, 312813 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:13:40,192 : INFO : PROGRESS: at 60.92% examples, 313131 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:41,206 : INFO : PROGRESS: at 61.96% examples, 313158 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:42,208 : INFO : PROGRESS: at 63.04% examples, 313391 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:43,211 : INFO : PROGRESS: at 64.03% examples, 313229 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:44,214 : INFO : PROGRESS: at 65.09% examples, 313422 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:45,254 : INFO : PROGRESS: at 66.22% examples, 313780 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:46,278 : INFO : PROGRESS: at 67.30% examples, 314011 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:47,297 : INFO : PROGRESS: at 68.36% examples, 314153 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:13:48,319 : INFO : PROGRESS: at 69.47% examples, 314515 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:49,331 : INFO : PROGRESS: at 70.44% examples, 314266 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:50,390 : INFO : PROGRESS: at 71.50% examples, 314254 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:13:51,392 : INFO : PROGRESS: at 72.63% examples, 314806 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:52,400 : INFO : PROGRESS: at 73.73% examples, 315218 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:53,433 : INFO : PROGRESS: at 74.84% examples, 315496 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:54,476 : INFO : PROGRESS: at 75.97% examples, 315785 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:55,495 : INFO : PROGRESS: at 77.03% examples, 315888 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:13:56,522 : INFO : PROGRESS: at 78.14% examples, 316144 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:57,532 : INFO : PROGRESS: at 79.20% examples, 316279 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:13:58,880 : INFO : PROGRESS: at 79.86% examples, 313514 words/s, in_qsize 2, out_qsize 2\n",
      "2017-12-19 18:13:59,894 : INFO : PROGRESS: at 80.92% examples, 313594 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:00,959 : INFO : PROGRESS: at 82.00% examples, 313639 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:14:01,956 : INFO : PROGRESS: at 83.09% examples, 313779 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:02,960 : INFO : PROGRESS: at 84.19% examples, 314082 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:03,980 : INFO : PROGRESS: at 85.18% examples, 313899 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:04,981 : INFO : PROGRESS: at 86.22% examples, 313968 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:06,006 : INFO : PROGRESS: at 87.23% examples, 313888 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:07,015 : INFO : PROGRESS: at 88.27% examples, 313927 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:08,022 : INFO : PROGRESS: at 89.35% examples, 314186 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:09,027 : INFO : PROGRESS: at 90.37% examples, 314196 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:10,041 : INFO : PROGRESS: at 91.45% examples, 314423 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:11,055 : INFO : PROGRESS: at 92.58% examples, 314815 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:12,091 : INFO : PROGRESS: at 93.71% examples, 315120 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:13,098 : INFO : PROGRESS: at 94.79% examples, 315350 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:14,109 : INFO : PROGRESS: at 95.85% examples, 315449 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:15,148 : INFO : PROGRESS: at 96.96% examples, 315614 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:16,159 : INFO : PROGRESS: at 98.04% examples, 315796 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:17,192 : INFO : PROGRESS: at 99.10% examples, 315836 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:18,149 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:14:18,167 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:14:18,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:14:18,175 : INFO : training on 42426485 raw words (31023283 effective words) took 98.3s, 315472 effective words/s\n",
      "2017-12-19 18:14:18,201 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:14:18,774 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:14:20,545 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:14:20,547 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:14:20,707 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:14:20,709 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:14:20,873 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:14:20,906 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:14:20,907 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:14:20,908 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:14:20,978 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:14:22,397 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:14:22,513 : INFO : resetting layer weights\n",
      "2017-12-19 18:14:22,949 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:14:24,003 : INFO : PROGRESS: at 0.16% examples, 49448 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:14:25,039 : INFO : PROGRESS: at 0.66% examples, 98665 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:26,055 : INFO : PROGRESS: at 1.15% examples, 116158 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:27,064 : INFO : PROGRESS: at 1.63% examples, 122752 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:28,089 : INFO : PROGRESS: at 2.12% examples, 127705 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:29,210 : INFO : PROGRESS: at 2.66% examples, 130597 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:30,235 : INFO : PROGRESS: at 3.18% examples, 134233 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:31,285 : INFO : PROGRESS: at 3.70% examples, 136406 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:32,308 : INFO : PROGRESS: at 4.19% examples, 137690 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:33,366 : INFO : PROGRESS: at 4.71% examples, 138928 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:14:34,381 : INFO : PROGRESS: at 5.23% examples, 140658 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:35,420 : INFO : PROGRESS: at 5.72% examples, 141251 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:36,446 : INFO : PROGRESS: at 6.22% examples, 141790 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:37,543 : INFO : PROGRESS: at 6.78% examples, 143120 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:38,572 : INFO : PROGRESS: at 7.33% examples, 144471 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:39,670 : INFO : PROGRESS: at 7.87% examples, 144948 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:40,738 : INFO : PROGRESS: at 8.39% examples, 145302 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:41,779 : INFO : PROGRESS: at 8.88% examples, 145429 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:14:42,833 : INFO : PROGRESS: at 9.40% examples, 145931 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:43,836 : INFO : PROGRESS: at 9.89% examples, 146285 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:44,853 : INFO : PROGRESS: at 10.39% examples, 146608 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:45,884 : INFO : PROGRESS: at 10.86% examples, 146453 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:46,935 : INFO : PROGRESS: at 11.35% examples, 146464 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:47,943 : INFO : PROGRESS: at 11.85% examples, 146795 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:48,952 : INFO : PROGRESS: at 12.34% examples, 147058 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:49,986 : INFO : PROGRESS: at 12.84% examples, 147145 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:51,024 : INFO : PROGRESS: at 13.31% examples, 146972 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:52,036 : INFO : PROGRESS: at 13.80% examples, 147184 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:53,044 : INFO : PROGRESS: at 14.28% examples, 147162 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:54,049 : INFO : PROGRESS: at 14.79% examples, 147607 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:55,051 : INFO : PROGRESS: at 15.27% examples, 147560 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:14:56,072 : INFO : PROGRESS: at 15.76% examples, 147642 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:14:57,071 : INFO : PROGRESS: at 16.25% examples, 147786 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:58,095 : INFO : PROGRESS: at 16.75% examples, 147856 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:14:59,099 : INFO : PROGRESS: at 17.27% examples, 148239 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:00,129 : INFO : PROGRESS: at 17.76% examples, 148221 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:01,151 : INFO : PROGRESS: at 18.28% examples, 148505 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:02,165 : INFO : PROGRESS: at 18.75% examples, 148404 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:03,182 : INFO : PROGRESS: at 19.22% examples, 148284 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:04,202 : INFO : PROGRESS: at 19.74% examples, 148528 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:05,219 : INFO : PROGRESS: at 19.95% examples, 146513 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:06,245 : INFO : PROGRESS: at 20.45% examples, 146546 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:07,361 : INFO : PROGRESS: at 20.97% examples, 146512 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:15:08,361 : INFO : PROGRESS: at 21.48% examples, 146764 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:15:09,403 : INFO : PROGRESS: at 22.07% examples, 147370 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:10,411 : INFO : PROGRESS: at 22.61% examples, 147635 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:11,411 : INFO : PROGRESS: at 23.16% examples, 148033 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:12,436 : INFO : PROGRESS: at 23.75% examples, 148624 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:13,513 : INFO : PROGRESS: at 24.33% examples, 149014 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:14,534 : INFO : PROGRESS: at 24.90% examples, 149471 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:15,593 : INFO : PROGRESS: at 25.49% examples, 149923 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:16,628 : INFO : PROGRESS: at 26.08% examples, 150399 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:17,630 : INFO : PROGRESS: at 26.64% examples, 150854 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:18,662 : INFO : PROGRESS: at 27.26% examples, 151500 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:19,698 : INFO : PROGRESS: at 27.84% examples, 151922 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:20,737 : INFO : PROGRESS: at 28.39% examples, 152102 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:21,744 : INFO : PROGRESS: at 28.93% examples, 152370 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:22,764 : INFO : PROGRESS: at 29.47% examples, 152641 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:23,780 : INFO : PROGRESS: at 30.01% examples, 152831 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:24,827 : INFO : PROGRESS: at 30.60% examples, 153235 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:25,874 : INFO : PROGRESS: at 31.17% examples, 153514 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:26,895 : INFO : PROGRESS: at 31.73% examples, 153826 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:27,935 : INFO : PROGRESS: at 32.30% examples, 154114 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:15:28,937 : INFO : PROGRESS: at 32.84% examples, 154323 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:29,961 : INFO : PROGRESS: at 33.43% examples, 154734 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:31,009 : INFO : PROGRESS: at 33.99% examples, 154944 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:32,009 : INFO : PROGRESS: at 34.53% examples, 155165 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:33,052 : INFO : PROGRESS: at 35.10% examples, 155341 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:34,060 : INFO : PROGRESS: at 35.64% examples, 155498 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:35,113 : INFO : PROGRESS: at 36.23% examples, 155760 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:36,158 : INFO : PROGRESS: at 36.82% examples, 156040 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:15:37,214 : INFO : PROGRESS: at 37.39% examples, 156195 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:38,224 : INFO : PROGRESS: at 37.93% examples, 156339 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:39,270 : INFO : PROGRESS: at 38.49% examples, 156501 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:40,274 : INFO : PROGRESS: at 39.10% examples, 156919 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:41,276 : INFO : PROGRESS: at 39.62% examples, 156972 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:42,330 : INFO : PROGRESS: at 39.86% examples, 155845 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:15:43,357 : INFO : PROGRESS: at 40.42% examples, 155983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:44,382 : INFO : PROGRESS: at 40.99% examples, 156175 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:45,423 : INFO : PROGRESS: at 41.55% examples, 156311 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:46,429 : INFO : PROGRESS: at 42.17% examples, 156680 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:15:47,487 : INFO : PROGRESS: at 42.76% examples, 156807 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:15:48,494 : INFO : PROGRESS: at 43.27% examples, 156828 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:49,502 : INFO : PROGRESS: at 43.84% examples, 157012 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:50,548 : INFO : PROGRESS: at 44.41% examples, 157102 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:51,583 : INFO : PROGRESS: at 44.99% examples, 157327 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:52,619 : INFO : PROGRESS: at 45.56% examples, 157470 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:53,672 : INFO : PROGRESS: at 46.15% examples, 157633 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:54,683 : INFO : PROGRESS: at 46.71% examples, 157804 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:55,698 : INFO : PROGRESS: at 47.28% examples, 157974 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:56,731 : INFO : PROGRESS: at 47.89% examples, 158242 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:15:57,766 : INFO : PROGRESS: at 48.46% examples, 158374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:58,768 : INFO : PROGRESS: at 49.02% examples, 158547 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:15:59,827 : INFO : PROGRESS: at 49.59% examples, 158671 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:00,857 : INFO : PROGRESS: at 50.20% examples, 158933 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:01,887 : INFO : PROGRESS: at 50.69% examples, 158845 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:02,918 : INFO : PROGRESS: at 51.26% examples, 158983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:03,944 : INFO : PROGRESS: at 51.83% examples, 159124 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:04,959 : INFO : PROGRESS: at 52.32% examples, 159061 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:05,988 : INFO : PROGRESS: at 52.89% examples, 159187 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:07,010 : INFO : PROGRESS: at 53.45% examples, 159327 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:08,037 : INFO : PROGRESS: at 53.99% examples, 159390 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:09,077 : INFO : PROGRESS: at 54.58% examples, 159570 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:10,110 : INFO : PROGRESS: at 55.19% examples, 159797 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:11,160 : INFO : PROGRESS: at 55.78% examples, 159927 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:12,195 : INFO : PROGRESS: at 56.37% examples, 160083 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:13,251 : INFO : PROGRESS: at 56.94% examples, 160144 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:14,292 : INFO : PROGRESS: at 57.48% examples, 160157 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:15,356 : INFO : PROGRESS: at 58.07% examples, 160274 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:16,385 : INFO : PROGRESS: at 58.63% examples, 160373 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:17,386 : INFO : PROGRESS: at 59.20% examples, 160504 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:18,399 : INFO : PROGRESS: at 59.76% examples, 160619 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:19,402 : INFO : PROGRESS: at 60.00% examples, 159836 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:20,403 : INFO : PROGRESS: at 60.57% examples, 159977 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:21,449 : INFO : PROGRESS: at 61.13% examples, 160054 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:22,500 : INFO : PROGRESS: at 61.70% examples, 160098 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:23,532 : INFO : PROGRESS: at 62.29% examples, 160210 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:24,569 : INFO : PROGRESS: at 62.90% examples, 160364 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:25,572 : INFO : PROGRESS: at 63.49% examples, 160533 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:26,582 : INFO : PROGRESS: at 64.00% examples, 160508 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:27,583 : INFO : PROGRESS: at 64.55% examples, 160548 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:16:28,595 : INFO : PROGRESS: at 65.16% examples, 160770 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:29,617 : INFO : PROGRESS: at 65.75% examples, 160910 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:30,678 : INFO : PROGRESS: at 66.36% examples, 161053 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:31,717 : INFO : PROGRESS: at 66.93% examples, 161125 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:16:32,775 : INFO : PROGRESS: at 67.47% examples, 161100 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:33,806 : INFO : PROGRESS: at 68.03% examples, 161160 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:34,853 : INFO : PROGRESS: at 68.60% examples, 161220 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:35,888 : INFO : PROGRESS: at 69.16% examples, 161294 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:36,946 : INFO : PROGRESS: at 69.66% examples, 161185 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:37,964 : INFO : PROGRESS: at 70.27% examples, 161375 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:38,966 : INFO : PROGRESS: at 70.84% examples, 161489 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:39,974 : INFO : PROGRESS: at 71.35% examples, 161488 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:40,981 : INFO : PROGRESS: at 71.92% examples, 161600 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:41,989 : INFO : PROGRESS: at 72.49% examples, 161699 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:42,991 : INFO : PROGRESS: at 73.07% examples, 161861 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:44,081 : INFO : PROGRESS: at 73.69% examples, 161970 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:45,100 : INFO : PROGRESS: at 74.25% examples, 162063 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:16:46,159 : INFO : PROGRESS: at 74.82% examples, 162089 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:47,180 : INFO : PROGRESS: at 75.43% examples, 162253 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:48,288 : INFO : PROGRESS: at 76.02% examples, 162274 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:49,297 : INFO : PROGRESS: at 76.58% examples, 162347 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:50,343 : INFO : PROGRESS: at 77.15% examples, 162400 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:51,352 : INFO : PROGRESS: at 77.76% examples, 162567 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:52,355 : INFO : PROGRESS: at 78.33% examples, 162658 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:53,374 : INFO : PROGRESS: at 78.89% examples, 162729 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:54,378 : INFO : PROGRESS: at 79.41% examples, 162710 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:55,846 : INFO : PROGRESS: at 79.86% examples, 162062 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:16:56,881 : INFO : PROGRESS: at 80.45% examples, 162146 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:16:57,895 : INFO : PROGRESS: at 81.01% examples, 162218 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:58,941 : INFO : PROGRESS: at 81.60% examples, 162292 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:16:59,953 : INFO : PROGRESS: at 82.14% examples, 162303 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:00,964 : INFO : PROGRESS: at 82.73% examples, 162379 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:01,980 : INFO : PROGRESS: at 83.30% examples, 162440 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:03,070 : INFO : PROGRESS: at 83.82% examples, 162332 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:04,079 : INFO : PROGRESS: at 84.38% examples, 162382 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:05,101 : INFO : PROGRESS: at 84.99% examples, 162531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:06,122 : INFO : PROGRESS: at 85.56% examples, 162594 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:07,126 : INFO : PROGRESS: at 86.10% examples, 162611 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:17:08,140 : INFO : PROGRESS: at 86.64% examples, 162636 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:17:09,158 : INFO : PROGRESS: at 87.16% examples, 162605 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:17:10,168 : INFO : PROGRESS: at 87.73% examples, 162667 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:17:11,274 : INFO : PROGRESS: at 88.29% examples, 162638 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:12,275 : INFO : PROGRESS: at 88.86% examples, 162715 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:13,320 : INFO : PROGRESS: at 89.42% examples, 162765 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:14,327 : INFO : PROGRESS: at 90.04% examples, 162919 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:15,327 : INFO : PROGRESS: at 90.55% examples, 162916 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:16,360 : INFO : PROGRESS: at 91.12% examples, 162969 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:17,397 : INFO : PROGRESS: at 91.68% examples, 163013 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:18,444 : INFO : PROGRESS: at 92.25% examples, 163051 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:19,448 : INFO : PROGRESS: at 92.82% examples, 163127 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:20,468 : INFO : PROGRESS: at 93.40% examples, 163231 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:21,471 : INFO : PROGRESS: at 93.92% examples, 163224 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:22,488 : INFO : PROGRESS: at 94.49% examples, 163288 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:23,542 : INFO : PROGRESS: at 95.10% examples, 163384 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:24,573 : INFO : PROGRESS: at 95.71% examples, 163499 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:25,616 : INFO : PROGRESS: at 96.28% examples, 163524 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:26,624 : INFO : PROGRESS: at 96.84% examples, 163586 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:27,653 : INFO : PROGRESS: at 97.43% examples, 163663 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:28,662 : INFO : PROGRESS: at 98.02% examples, 163763 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:29,674 : INFO : PROGRESS: at 98.59% examples, 163815 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:30,703 : INFO : PROGRESS: at 99.15% examples, 163853 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:31,728 : INFO : PROGRESS: at 99.72% examples, 163896 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:32,240 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:17:32,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:17:32,292 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:17:32,293 : INFO : training on 42426485 raw words (31023857 effective words) took 189.3s, 163851 effective words/s\n",
      "2017-12-19 18:17:32,319 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:17:32,861 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:17:34,604 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:17:34,605 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:17:34,849 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:17:34,850 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:17:35,006 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:17:35,038 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:17:35,039 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:17:35,040 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:17:35,110 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:17:36,664 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:17:36,780 : INFO : resetting layer weights\n",
      "2017-12-19 18:17:37,211 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:17:38,226 : INFO : PROGRESS: at 0.16% examples, 51561 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:39,259 : INFO : PROGRESS: at 0.73% examples, 111555 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:40,275 : INFO : PROGRESS: at 1.27% examples, 129607 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:41,351 : INFO : PROGRESS: at 1.84% examples, 137692 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:17:42,375 : INFO : PROGRESS: at 2.40% examples, 143631 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:43,405 : INFO : PROGRESS: at 2.99% examples, 148579 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:44,417 : INFO : PROGRESS: at 3.53% examples, 150846 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:45,437 : INFO : PROGRESS: at 4.12% examples, 154113 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:46,446 : INFO : PROGRESS: at 4.66% examples, 155284 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:17:47,453 : INFO : PROGRESS: at 5.23% examples, 157105 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:48,455 : INFO : PROGRESS: at 5.80% examples, 158648 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:49,544 : INFO : PROGRESS: at 6.38% examples, 159406 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:50,558 : INFO : PROGRESS: at 6.93% examples, 159882 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:51,579 : INFO : PROGRESS: at 7.44% examples, 159700 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:52,594 : INFO : PROGRESS: at 8.01% examples, 160498 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:53,646 : INFO : PROGRESS: at 8.57% examples, 160941 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:54,701 : INFO : PROGRESS: at 9.16% examples, 161724 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:55,712 : INFO : PROGRESS: at 9.73% examples, 162522 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:56,786 : INFO : PROGRESS: at 10.29% examples, 162589 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:57,836 : INFO : PROGRESS: at 10.86% examples, 162887 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:17:58,879 : INFO : PROGRESS: at 11.43% examples, 163255 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:17:59,907 : INFO : PROGRESS: at 11.99% examples, 163696 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:00,942 : INFO : PROGRESS: at 12.56% examples, 164010 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:01,972 : INFO : PROGRESS: at 13.10% examples, 164053 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:02,988 : INFO : PROGRESS: at 13.66% examples, 164444 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:03,993 : INFO : PROGRESS: at 14.23% examples, 164884 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:05,057 : INFO : PROGRESS: at 14.79% examples, 164922 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:06,107 : INFO : PROGRESS: at 15.38% examples, 165214 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:07,142 : INFO : PROGRESS: at 15.95% examples, 165379 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:08,187 : INFO : PROGRESS: at 16.51% examples, 165416 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:09,206 : INFO : PROGRESS: at 17.08% examples, 165695 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:10,237 : INFO : PROGRESS: at 17.64% examples, 165808 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:11,272 : INFO : PROGRESS: at 18.21% examples, 165958 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:12,297 : INFO : PROGRESS: at 18.80% examples, 166343 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:13,362 : INFO : PROGRESS: at 19.34% examples, 166097 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:14,906 : INFO : PROGRESS: at 19.84% examples, 163428 words/s, in_qsize 3, out_qsize 2\n",
      "2017-12-19 18:18:15,905 : INFO : PROGRESS: at 20.42% examples, 163837 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:16,916 : INFO : PROGRESS: at 20.99% examples, 164080 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:17,964 : INFO : PROGRESS: at 21.55% examples, 164125 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:19,004 : INFO : PROGRESS: at 22.14% examples, 164366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:20,060 : INFO : PROGRESS: at 22.76% examples, 164611 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:18:21,075 : INFO : PROGRESS: at 23.32% examples, 164755 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:18:22,099 : INFO : PROGRESS: at 23.86% examples, 164703 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:23,129 : INFO : PROGRESS: at 24.48% examples, 165069 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:24,140 : INFO : PROGRESS: at 25.02% examples, 165100 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:25,143 : INFO : PROGRESS: at 25.61% examples, 165473 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:26,181 : INFO : PROGRESS: at 26.20% examples, 165652 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:27,211 : INFO : PROGRESS: at 26.76% examples, 165741 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:28,241 : INFO : PROGRESS: at 27.28% examples, 165558 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:29,255 : INFO : PROGRESS: at 27.87% examples, 165812 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:30,312 : INFO : PROGRESS: at 28.39% examples, 165541 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:31,326 : INFO : PROGRESS: at 28.98% examples, 165812 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:32,332 : INFO : PROGRESS: at 29.54% examples, 166051 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:33,351 : INFO : PROGRESS: at 30.08% examples, 165999 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:34,356 : INFO : PROGRESS: at 30.65% examples, 166204 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:35,376 : INFO : PROGRESS: at 31.19% examples, 166212 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:36,383 : INFO : PROGRESS: at 31.76% examples, 166366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:37,410 : INFO : PROGRESS: at 32.32% examples, 166487 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:38,454 : INFO : PROGRESS: at 32.89% examples, 166531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:39,461 : INFO : PROGRESS: at 33.47% examples, 166802 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:40,492 : INFO : PROGRESS: at 34.02% examples, 166769 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:41,526 : INFO : PROGRESS: at 34.58% examples, 166854 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:18:42,608 : INFO : PROGRESS: at 35.19% examples, 166995 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:43,608 : INFO : PROGRESS: at 35.76% examples, 167107 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:44,626 : INFO : PROGRESS: at 36.35% examples, 167287 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:45,690 : INFO : PROGRESS: at 36.89% examples, 167150 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:46,701 : INFO : PROGRESS: at 37.50% examples, 167459 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:47,746 : INFO : PROGRESS: at 38.07% examples, 167468 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:48,789 : INFO : PROGRESS: at 38.63% examples, 167489 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:49,802 : INFO : PROGRESS: at 39.22% examples, 167672 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:50,936 : INFO : PROGRESS: at 39.79% examples, 167484 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:18:51,950 : INFO : PROGRESS: at 40.07% examples, 166346 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:18:53,032 : INFO : PROGRESS: at 40.61% examples, 166199 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:54,043 : INFO : PROGRESS: at 41.18% examples, 166306 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:55,108 : INFO : PROGRESS: at 41.74% examples, 166257 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:18:56,112 : INFO : PROGRESS: at 42.31% examples, 166315 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:57,137 : INFO : PROGRESS: at 42.90% examples, 166407 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:58,162 : INFO : PROGRESS: at 43.42% examples, 166281 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:18:59,190 : INFO : PROGRESS: at 43.98% examples, 166308 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:00,204 : INFO : PROGRESS: at 44.59% examples, 166537 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:01,284 : INFO : PROGRESS: at 45.14% examples, 166410 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:19:02,310 : INFO : PROGRESS: at 45.70% examples, 166454 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:03,311 : INFO : PROGRESS: at 46.29% examples, 166616 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:19:04,347 : INFO : PROGRESS: at 46.86% examples, 166654 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:05,404 : INFO : PROGRESS: at 47.47% examples, 166813 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:06,442 : INFO : PROGRESS: at 48.08% examples, 166984 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:07,496 : INFO : PROGRESS: at 48.65% examples, 166983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:08,535 : INFO : PROGRESS: at 49.21% examples, 167027 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:19:09,583 : INFO : PROGRESS: at 49.78% examples, 167047 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:10,589 : INFO : PROGRESS: at 50.32% examples, 167057 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:11,591 : INFO : PROGRESS: at 50.86% examples, 167081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:12,614 : INFO : PROGRESS: at 51.43% examples, 167152 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:13,704 : INFO : PROGRESS: at 52.01% examples, 167182 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:14,715 : INFO : PROGRESS: at 52.58% examples, 167260 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:15,750 : INFO : PROGRESS: at 53.14% examples, 167304 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:19:16,767 : INFO : PROGRESS: at 53.71% examples, 167374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:17,801 : INFO : PROGRESS: at 54.28% examples, 167411 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:18,839 : INFO : PROGRESS: at 54.84% examples, 167438 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:19,888 : INFO : PROGRESS: at 55.41% examples, 167427 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:20,889 : INFO : PROGRESS: at 55.92% examples, 167368 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:21,900 : INFO : PROGRESS: at 56.47% examples, 167347 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:22,938 : INFO : PROGRESS: at 57.01% examples, 167306 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:23,972 : INFO : PROGRESS: at 57.57% examples, 167326 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:24,996 : INFO : PROGRESS: at 58.09% examples, 167233 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:26,053 : INFO : PROGRESS: at 58.66% examples, 167232 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:19:27,058 : INFO : PROGRESS: at 59.22% examples, 167302 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:28,196 : INFO : PROGRESS: at 59.79% examples, 167174 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:19:29,270 : INFO : PROGRESS: at 60.09% examples, 166393 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:30,340 : INFO : PROGRESS: at 60.59% examples, 166180 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:31,391 : INFO : PROGRESS: at 61.08% examples, 166000 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:19:32,394 : INFO : PROGRESS: at 61.60% examples, 165938 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:33,408 : INFO : PROGRESS: at 62.07% examples, 165736 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:34,451 : INFO : PROGRESS: at 62.59% examples, 165577 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:35,492 : INFO : PROGRESS: at 63.09% examples, 165406 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:36,590 : INFO : PROGRESS: at 63.60% examples, 165218 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:19:37,634 : INFO : PROGRESS: at 64.17% examples, 165229 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:38,655 : INFO : PROGRESS: at 64.73% examples, 165269 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:39,671 : INFO : PROGRESS: at 65.25% examples, 165210 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:40,757 : INFO : PROGRESS: at 65.87% examples, 165285 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:41,809 : INFO : PROGRESS: at 66.43% examples, 165297 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:42,859 : INFO : PROGRESS: at 67.00% examples, 165316 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:43,886 : INFO : PROGRESS: at 67.56% examples, 165358 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:19:44,926 : INFO : PROGRESS: at 68.13% examples, 165374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:45,940 : INFO : PROGRESS: at 68.65% examples, 165326 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:46,947 : INFO : PROGRESS: at 69.23% examples, 165464 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:47,951 : INFO : PROGRESS: at 69.78% examples, 165494 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:19:48,984 : INFO : PROGRESS: at 70.34% examples, 165538 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:50,027 : INFO : PROGRESS: at 70.91% examples, 165563 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:51,071 : INFO : PROGRESS: at 71.47% examples, 165600 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:52,088 : INFO : PROGRESS: at 72.04% examples, 165671 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:53,125 : INFO : PROGRESS: at 72.60% examples, 165706 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:54,169 : INFO : PROGRESS: at 73.17% examples, 165737 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:55,171 : INFO : PROGRESS: at 73.76% examples, 165869 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:56,239 : INFO : PROGRESS: at 74.35% examples, 165925 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:57,262 : INFO : PROGRESS: at 74.96% examples, 166070 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:19:58,311 : INFO : PROGRESS: at 75.55% examples, 166127 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:19:59,322 : INFO : PROGRESS: at 76.11% examples, 166180 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:00,372 : INFO : PROGRESS: at 76.73% examples, 166288 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:01,437 : INFO : PROGRESS: at 77.29% examples, 166288 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:02,485 : INFO : PROGRESS: at 77.86% examples, 166295 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:03,507 : INFO : PROGRESS: at 78.35% examples, 166185 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:04,518 : INFO : PROGRESS: at 78.92% examples, 166241 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:05,521 : INFO : PROGRESS: at 79.46% examples, 166252 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:06,864 : INFO : PROGRESS: at 79.84% examples, 165545 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:20:07,911 : INFO : PROGRESS: at 80.42% examples, 165595 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:08,925 : INFO : PROGRESS: at 81.01% examples, 165692 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:09,961 : INFO : PROGRESS: at 81.60% examples, 165752 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:10,968 : INFO : PROGRESS: at 82.21% examples, 165885 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:11,985 : INFO : PROGRESS: at 82.80% examples, 165934 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:12,986 : INFO : PROGRESS: at 83.32% examples, 165896 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:14,006 : INFO : PROGRESS: at 83.89% examples, 165926 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:15,094 : INFO : PROGRESS: at 84.48% examples, 165915 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:16,121 : INFO : PROGRESS: at 85.02% examples, 165906 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:17,149 : INFO : PROGRESS: at 85.49% examples, 165755 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:18,207 : INFO : PROGRESS: at 85.96% examples, 165565 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:20:19,234 : INFO : PROGRESS: at 86.48% examples, 165506 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:20,271 : INFO : PROGRESS: at 86.97% examples, 165404 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:21,277 : INFO : PROGRESS: at 87.56% examples, 165493 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:22,310 : INFO : PROGRESS: at 88.08% examples, 165426 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:23,330 : INFO : PROGRESS: at 88.60% examples, 165383 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:24,333 : INFO : PROGRESS: at 89.16% examples, 165444 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:25,366 : INFO : PROGRESS: at 89.63% examples, 165317 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:26,422 : INFO : PROGRESS: at 90.13% examples, 165185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:27,452 : INFO : PROGRESS: at 90.69% examples, 165235 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:28,457 : INFO : PROGRESS: at 91.26% examples, 165296 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:29,503 : INFO : PROGRESS: at 91.78% examples, 165232 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:30,523 : INFO : PROGRESS: at 92.27% examples, 165157 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:31,528 : INFO : PROGRESS: at 92.82% examples, 165177 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:32,544 : INFO : PROGRESS: at 93.33% examples, 165149 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:20:33,571 : INFO : PROGRESS: at 93.90% examples, 165191 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:34,613 : INFO : PROGRESS: at 94.46% examples, 165220 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:35,674 : INFO : PROGRESS: at 95.03% examples, 165215 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:36,697 : INFO : PROGRESS: at 95.59% examples, 165250 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:37,767 : INFO : PROGRESS: at 96.16% examples, 165242 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:38,769 : INFO : PROGRESS: at 96.68% examples, 165216 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:39,826 : INFO : PROGRESS: at 97.22% examples, 165191 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:40,893 : INFO : PROGRESS: at 97.79% examples, 165180 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:41,911 : INFO : PROGRESS: at 98.35% examples, 165223 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:42,929 : INFO : PROGRESS: at 98.96% examples, 165347 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:43,944 : INFO : PROGRESS: at 99.48% examples, 165311 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:44,951 : INFO : PROGRESS: at 99.93% examples, 165166 words/s, in_qsize 3, out_qsize 0\n",
      "2017-12-19 18:20:44,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:20:44,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:20:45,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:20:45,008 : INFO : training on 42426485 raw words (31026328 effective words) took 187.8s, 165214 effective words/s\n",
      "2017-12-19 18:20:45,061 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:20:45,621 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:20:47,337 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:20:47,338 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:20:47,526 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:20:47,527 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:20:47,680 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:20:47,714 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:20:47,715 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:20:47,716 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:20:47,787 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:20:49,127 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:20:49,244 : INFO : resetting layer weights\n",
      "2017-12-19 18:20:49,696 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:20:50,791 : INFO : PROGRESS: at 0.24% examples, 68320 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:51,822 : INFO : PROGRESS: at 0.80% examples, 117769 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:52,823 : INFO : PROGRESS: at 1.34% examples, 133657 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:53,838 : INFO : PROGRESS: at 1.93% examples, 144505 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:20:54,882 : INFO : PROGRESS: at 2.52% examples, 149522 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:55,932 : INFO : PROGRESS: at 3.09% examples, 152069 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:56,986 : INFO : PROGRESS: at 3.72% examples, 156965 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:20:57,997 : INFO : PROGRESS: at 4.29% examples, 158713 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:20:59,028 : INFO : PROGRESS: at 4.85% examples, 159893 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:00,037 : INFO : PROGRESS: at 5.37% examples, 159770 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:01,096 : INFO : PROGRESS: at 5.91% examples, 159568 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:02,113 : INFO : PROGRESS: at 6.50% examples, 161186 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:03,135 : INFO : PROGRESS: at 7.09% examples, 162568 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:04,206 : INFO : PROGRESS: at 7.63% examples, 162106 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:21:05,220 : INFO : PROGRESS: at 8.17% examples, 162269 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:21:06,260 : INFO : PROGRESS: at 8.74% examples, 162739 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:07,292 : INFO : PROGRESS: at 9.31% examples, 163286 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:08,296 : INFO : PROGRESS: at 9.87% examples, 163937 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:09,313 : INFO : PROGRESS: at 10.37% examples, 163368 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:10,375 : INFO : PROGRESS: at 10.93% examples, 163497 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:11,407 : INFO : PROGRESS: at 11.50% examples, 163923 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:12,419 : INFO : PROGRESS: at 12.06% examples, 164438 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:13,460 : INFO : PROGRESS: at 12.65% examples, 164981 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:14,498 : INFO : PROGRESS: at 13.22% examples, 165222 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:15,514 : INFO : PROGRESS: at 13.78% examples, 165585 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:16,560 : INFO : PROGRESS: at 14.35% examples, 165736 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:21:17,583 : INFO : PROGRESS: at 14.86% examples, 165450 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:18,586 : INFO : PROGRESS: at 15.41% examples, 165496 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:19,631 : INFO : PROGRESS: at 15.97% examples, 165585 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:20,645 : INFO : PROGRESS: at 16.54% examples, 165792 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:21,674 : INFO : PROGRESS: at 17.10% examples, 165998 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:22,683 : INFO : PROGRESS: at 17.69% examples, 166444 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:23,692 : INFO : PROGRESS: at 18.21% examples, 166257 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:24,780 : INFO : PROGRESS: at 18.85% examples, 166756 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:25,795 : INFO : PROGRESS: at 19.46% examples, 167337 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:27,175 : INFO : PROGRESS: at 19.84% examples, 164319 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:21:28,194 : INFO : PROGRESS: at 20.40% examples, 164473 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:29,268 : INFO : PROGRESS: at 20.99% examples, 164627 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:30,336 : INFO : PROGRESS: at 21.55% examples, 164571 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:31,353 : INFO : PROGRESS: at 22.14% examples, 164894 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:32,357 : INFO : PROGRESS: at 22.71% examples, 164966 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:33,359 : INFO : PROGRESS: at 23.27% examples, 165186 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:34,376 : INFO : PROGRESS: at 23.84% examples, 165314 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:35,416 : INFO : PROGRESS: at 24.41% examples, 165317 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:21:36,432 : INFO : PROGRESS: at 24.97% examples, 165481 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:37,436 : INFO : PROGRESS: at 25.54% examples, 165678 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:38,484 : INFO : PROGRESS: at 26.12% examples, 165819 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:39,547 : INFO : PROGRESS: at 26.71% examples, 165952 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:21:40,559 : INFO : PROGRESS: at 27.28% examples, 166104 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:41,567 : INFO : PROGRESS: at 27.84% examples, 166230 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:42,576 : INFO : PROGRESS: at 28.46% examples, 166658 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:43,616 : INFO : PROGRESS: at 29.05% examples, 166828 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:44,667 : INFO : PROGRESS: at 29.61% examples, 166929 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:45,692 : INFO : PROGRESS: at 30.18% examples, 166965 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:46,715 : INFO : PROGRESS: at 30.74% examples, 167083 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:47,782 : INFO : PROGRESS: at 31.31% examples, 167074 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:48,809 : INFO : PROGRESS: at 31.87% examples, 167184 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:49,852 : INFO : PROGRESS: at 32.46% examples, 167338 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:50,864 : INFO : PROGRESS: at 33.05% examples, 167588 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:51,870 : INFO : PROGRESS: at 33.62% examples, 167719 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:52,895 : INFO : PROGRESS: at 34.18% examples, 167809 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:53,915 : INFO : PROGRESS: at 34.77% examples, 168011 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:54,971 : INFO : PROGRESS: at 35.36% examples, 168069 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:55,976 : INFO : PROGRESS: at 35.90% examples, 168070 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:57,011 : INFO : PROGRESS: at 36.49% examples, 168185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:21:58,032 : INFO : PROGRESS: at 37.06% examples, 168261 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:21:59,076 : INFO : PROGRESS: at 37.62% examples, 168247 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:00,087 : INFO : PROGRESS: at 38.16% examples, 168231 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:01,150 : INFO : PROGRESS: at 38.73% examples, 168195 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:02,187 : INFO : PROGRESS: at 39.29% examples, 168212 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:03,213 : INFO : PROGRESS: at 39.79% examples, 167957 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:04,239 : INFO : PROGRESS: at 40.09% examples, 166886 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:05,254 : INFO : PROGRESS: at 40.66% examples, 166972 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:06,293 : INFO : PROGRESS: at 41.18% examples, 166824 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:07,320 : INFO : PROGRESS: at 41.77% examples, 166942 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:08,358 : INFO : PROGRESS: at 42.36% examples, 167006 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:09,399 : INFO : PROGRESS: at 42.94% examples, 167058 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:10,455 : INFO : PROGRESS: at 43.53% examples, 167118 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:11,463 : INFO : PROGRESS: at 44.10% examples, 167180 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:12,494 : INFO : PROGRESS: at 44.66% examples, 167206 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:22:13,496 : INFO : PROGRESS: at 45.25% examples, 167370 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:14,507 : INFO : PROGRESS: at 45.80% examples, 167352 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:22:15,520 : INFO : PROGRESS: at 46.41% examples, 167580 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:16,530 : INFO : PROGRESS: at 46.95% examples, 167572 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:22:17,613 : INFO : PROGRESS: at 47.56% examples, 167665 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:18,624 : INFO : PROGRESS: at 48.17% examples, 167885 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:19,663 : INFO : PROGRESS: at 48.76% examples, 167988 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:20,683 : INFO : PROGRESS: at 49.33% examples, 168056 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:21,719 : INFO : PROGRESS: at 49.87% examples, 167994 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:22,740 : INFO : PROGRESS: at 50.41% examples, 167983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:23,779 : INFO : PROGRESS: at 50.98% examples, 168004 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:24,797 : INFO : PROGRESS: at 51.57% examples, 168147 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:25,831 : INFO : PROGRESS: at 52.16% examples, 168266 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:26,843 : INFO : PROGRESS: at 52.67% examples, 168182 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:27,895 : INFO : PROGRESS: at 53.24% examples, 168186 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:28,928 : INFO : PROGRESS: at 53.85% examples, 168368 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:30,008 : INFO : PROGRESS: at 54.42% examples, 168322 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:31,029 : INFO : PROGRESS: at 55.03% examples, 168492 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:32,044 : INFO : PROGRESS: at 55.59% examples, 168532 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:33,048 : INFO : PROGRESS: at 56.16% examples, 168593 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:34,085 : INFO : PROGRESS: at 56.75% examples, 168671 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:22:35,111 : INFO : PROGRESS: at 57.36% examples, 168850 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:36,126 : INFO : PROGRESS: at 57.93% examples, 168883 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:37,153 : INFO : PROGRESS: at 58.49% examples, 168901 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:38,160 : INFO : PROGRESS: at 59.03% examples, 168888 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:39,177 : INFO : PROGRESS: at 59.58% examples, 168859 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:40,269 : INFO : PROGRESS: at 59.84% examples, 167925 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:22:41,282 : INFO : PROGRESS: at 60.40% examples, 167955 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:42,340 : INFO : PROGRESS: at 60.97% examples, 167935 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:43,378 : INFO : PROGRESS: at 61.53% examples, 167933 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:44,449 : INFO : PROGRESS: at 62.17% examples, 168064 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:45,452 : INFO : PROGRESS: at 62.78% examples, 168189 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:46,468 : INFO : PROGRESS: at 63.30% examples, 168098 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:47,472 : INFO : PROGRESS: at 63.89% examples, 168199 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:48,491 : INFO : PROGRESS: at 64.50% examples, 168327 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:49,509 : INFO : PROGRESS: at 65.06% examples, 168364 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:50,513 : INFO : PROGRESS: at 65.61% examples, 168366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:51,626 : INFO : PROGRESS: at 66.22% examples, 168367 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:22:52,632 : INFO : PROGRESS: at 66.78% examples, 168415 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:53,649 : INFO : PROGRESS: at 67.35% examples, 168455 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:54,677 : INFO : PROGRESS: at 67.94% examples, 168515 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:55,689 : INFO : PROGRESS: at 68.46% examples, 168455 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:22:56,724 : INFO : PROGRESS: at 69.02% examples, 168454 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:57,804 : INFO : PROGRESS: at 69.59% examples, 168438 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:58,807 : INFO : PROGRESS: at 70.11% examples, 168359 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:22:59,829 : INFO : PROGRESS: at 70.65% examples, 168347 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:23:00,864 : INFO : PROGRESS: at 71.19% examples, 168319 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:01,881 : INFO : PROGRESS: at 71.80% examples, 168471 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:02,946 : INFO : PROGRESS: at 72.39% examples, 168511 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:03,946 : INFO : PROGRESS: at 72.96% examples, 168575 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:04,973 : INFO : PROGRESS: at 73.52% examples, 168605 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:06,025 : INFO : PROGRESS: at 74.11% examples, 168658 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:07,068 : INFO : PROGRESS: at 74.70% examples, 168718 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:08,076 : INFO : PROGRESS: at 75.31% examples, 168859 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:09,120 : INFO : PROGRESS: at 75.83% examples, 168760 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:10,129 : INFO : PROGRESS: at 76.40% examples, 168781 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:23:11,164 : INFO : PROGRESS: at 76.96% examples, 168800 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:12,170 : INFO : PROGRESS: at 77.50% examples, 168783 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:13,178 : INFO : PROGRESS: at 78.02% examples, 168727 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:14,230 : INFO : PROGRESS: at 78.56% examples, 168660 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:15,254 : INFO : PROGRESS: at 79.13% examples, 168681 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:16,290 : INFO : PROGRESS: at 79.72% examples, 168742 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:17,301 : INFO : PROGRESS: at 79.91% examples, 167980 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:18,335 : INFO : PROGRESS: at 80.47% examples, 167982 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:19,342 : INFO : PROGRESS: at 81.04% examples, 168025 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:20,387 : INFO : PROGRESS: at 81.60% examples, 168011 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:21,395 : INFO : PROGRESS: at 82.21% examples, 168126 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:22,398 : INFO : PROGRESS: at 82.80% examples, 168180 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:23,397 : INFO : PROGRESS: at 83.37% examples, 168221 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:24,419 : INFO : PROGRESS: at 83.93% examples, 168232 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:25,420 : INFO : PROGRESS: at 84.52% examples, 168305 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:26,442 : INFO : PROGRESS: at 85.09% examples, 168329 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:27,476 : INFO : PROGRESS: at 85.68% examples, 168386 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:28,485 : INFO : PROGRESS: at 86.27% examples, 168452 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:29,487 : INFO : PROGRESS: at 86.83% examples, 168493 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:30,529 : INFO : PROGRESS: at 87.42% examples, 168540 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:31,534 : INFO : PROGRESS: at 87.99% examples, 168568 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:32,547 : INFO : PROGRESS: at 88.55% examples, 168603 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:33,572 : INFO : PROGRESS: at 89.09% examples, 168577 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:34,573 : INFO : PROGRESS: at 89.63% examples, 168595 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:35,589 : INFO : PROGRESS: at 90.20% examples, 168616 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:36,602 : INFO : PROGRESS: at 90.77% examples, 168658 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:37,603 : INFO : PROGRESS: at 91.33% examples, 168708 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:38,609 : INFO : PROGRESS: at 91.87% examples, 168714 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:23:39,693 : INFO : PROGRESS: at 92.44% examples, 168674 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:40,731 : INFO : PROGRESS: at 93.00% examples, 168686 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:41,782 : INFO : PROGRESS: at 93.57% examples, 168692 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:23:42,806 : INFO : PROGRESS: at 94.13% examples, 168712 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:43,875 : INFO : PROGRESS: at 94.70% examples, 168689 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:44,911 : INFO : PROGRESS: at 95.27% examples, 168695 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:45,954 : INFO : PROGRESS: at 95.83% examples, 168694 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:46,991 : INFO : PROGRESS: at 96.40% examples, 168685 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:23:48,063 : INFO : PROGRESS: at 96.96% examples, 168660 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:49,125 : INFO : PROGRESS: at 97.48% examples, 168557 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:50,203 : INFO : PROGRESS: at 98.04% examples, 168523 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:23:51,253 : INFO : PROGRESS: at 98.63% examples, 168560 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:23:52,265 : INFO : PROGRESS: at 99.18% examples, 168548 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:23:53,454 : INFO : PROGRESS: at 99.79% examples, 168494 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:23:53,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:23:53,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:23:53,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:23:53,824 : INFO : training on 42426485 raw words (31023903 effective words) took 184.1s, 168494 effective words/s\n",
      "2017-12-19 18:23:53,876 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:23:54,427 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:23:56,145 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:23:56,146 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:23:56,383 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:23:56,385 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:23:56,545 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:23:56,577 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:23:56,578 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:23:56,579 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:23:56,648 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:23:57,992 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:23:58,105 : INFO : resetting layer weights\n",
      "2017-12-19 18:23:58,532 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:23:59,542 : INFO : PROGRESS: at 0.19% examples, 58961 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:00,598 : INFO : PROGRESS: at 0.73% examples, 110495 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:01,612 : INFO : PROGRESS: at 1.30% examples, 131007 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:02,620 : INFO : PROGRESS: at 1.88% examples, 143021 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:03,639 : INFO : PROGRESS: at 2.47% examples, 149142 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:04,663 : INFO : PROGRESS: at 3.06% examples, 153519 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:05,682 : INFO : PROGRESS: at 3.63% examples, 155995 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:06,683 : INFO : PROGRESS: at 4.19% examples, 158110 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:07,696 : INFO : PROGRESS: at 4.78% examples, 160399 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:08,728 : INFO : PROGRESS: at 5.37% examples, 162031 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:09,754 : INFO : PROGRESS: at 5.96% examples, 163388 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:10,757 : INFO : PROGRESS: at 6.50% examples, 163691 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:11,795 : INFO : PROGRESS: at 7.11% examples, 165251 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:12,854 : INFO : PROGRESS: at 7.70% examples, 165717 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:13,907 : INFO : PROGRESS: at 8.34% examples, 167223 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:14,902 : INFO : PROGRESS: at 8.90% examples, 167718 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:15,928 : INFO : PROGRESS: at 9.45% examples, 167713 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:16,999 : INFO : PROGRESS: at 10.01% examples, 167419 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:18,008 : INFO : PROGRESS: at 10.60% examples, 168243 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:19,039 : INFO : PROGRESS: at 11.19% examples, 168824 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:20,069 : INFO : PROGRESS: at 11.80% examples, 169635 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:21,147 : INFO : PROGRESS: at 12.39% examples, 169731 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:22,157 : INFO : PROGRESS: at 12.98% examples, 170269 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:23,170 : INFO : PROGRESS: at 13.55% examples, 170468 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:24,195 : INFO : PROGRESS: at 14.13% examples, 170860 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:25,209 : INFO : PROGRESS: at 14.70% examples, 170980 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:26,218 : INFO : PROGRESS: at 15.27% examples, 171098 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:27,230 : INFO : PROGRESS: at 15.88% examples, 171667 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:28,271 : INFO : PROGRESS: at 16.47% examples, 171774 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:29,345 : INFO : PROGRESS: at 17.03% examples, 171524 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:24:30,406 : INFO : PROGRESS: at 17.60% examples, 171292 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:31,428 : INFO : PROGRESS: at 18.16% examples, 171340 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:32,459 : INFO : PROGRESS: at 18.73% examples, 171321 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:33,500 : INFO : PROGRESS: at 19.27% examples, 171040 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:34,518 : INFO : PROGRESS: at 19.79% examples, 170673 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:35,520 : INFO : PROGRESS: at 20.12% examples, 168754 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:36,540 : INFO : PROGRESS: at 20.68% examples, 168846 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:24:37,560 : INFO : PROGRESS: at 21.30% examples, 169296 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:38,592 : INFO : PROGRESS: at 21.84% examples, 169082 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:39,617 : INFO : PROGRESS: at 22.45% examples, 169368 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:40,656 : INFO : PROGRESS: at 23.02% examples, 169284 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:41,704 : INFO : PROGRESS: at 23.60% examples, 169358 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:42,710 : INFO : PROGRESS: at 24.19% examples, 169607 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:43,737 : INFO : PROGRESS: at 24.81% examples, 169930 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:44,738 : INFO : PROGRESS: at 25.37% examples, 170024 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:45,819 : INFO : PROGRESS: at 25.96% examples, 169972 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:46,869 : INFO : PROGRESS: at 26.60% examples, 170355 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:47,917 : INFO : PROGRESS: at 27.21% examples, 170598 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:48,933 : INFO : PROGRESS: at 27.80% examples, 170768 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:24:49,966 : INFO : PROGRESS: at 28.36% examples, 170731 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:24:51,055 : INFO : PROGRESS: at 28.95% examples, 170667 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:52,086 : INFO : PROGRESS: at 29.52% examples, 170740 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:53,106 : INFO : PROGRESS: at 30.08% examples, 170735 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:24:54,153 : INFO : PROGRESS: at 30.65% examples, 170719 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:55,294 : INFO : PROGRESS: at 31.28% examples, 170807 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:24:56,377 : INFO : PROGRESS: at 31.92% examples, 171071 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:57,385 : INFO : PROGRESS: at 32.53% examples, 171392 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:24:58,445 : INFO : PROGRESS: at 33.12% examples, 171461 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:24:59,522 : INFO : PROGRESS: at 33.76% examples, 171676 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:00,578 : INFO : PROGRESS: at 34.32% examples, 171610 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:01,606 : INFO : PROGRESS: at 34.89% examples, 171604 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:25:02,641 : INFO : PROGRESS: at 35.48% examples, 171668 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:03,718 : INFO : PROGRESS: at 36.09% examples, 171751 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:04,730 : INFO : PROGRESS: at 36.68% examples, 171883 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:05,737 : INFO : PROGRESS: at 37.27% examples, 172054 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:06,782 : INFO : PROGRESS: at 37.86% examples, 172088 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:07,820 : INFO : PROGRESS: at 38.37% examples, 171834 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:08,833 : INFO : PROGRESS: at 38.94% examples, 171872 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:09,870 : INFO : PROGRESS: at 39.46% examples, 171629 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:11,258 : INFO : PROGRESS: at 39.86% examples, 170080 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:25:12,304 : INFO : PROGRESS: at 40.49% examples, 170305 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:13,321 : INFO : PROGRESS: at 41.08% examples, 170431 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:14,358 : INFO : PROGRESS: at 41.60% examples, 170199 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:25:15,359 : INFO : PROGRESS: at 42.17% examples, 170238 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:16,383 : INFO : PROGRESS: at 42.78% examples, 170347 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:17,410 : INFO : PROGRESS: at 43.37% examples, 170431 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:18,415 : INFO : PROGRESS: at 43.96% examples, 170547 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:19,420 : INFO : PROGRESS: at 44.52% examples, 170565 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:20,464 : INFO : PROGRESS: at 45.09% examples, 170547 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:25:21,483 : INFO : PROGRESS: at 45.70% examples, 170734 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:22,519 : INFO : PROGRESS: at 46.22% examples, 170521 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:23,546 : INFO : PROGRESS: at 46.78% examples, 170521 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:24,591 : INFO : PROGRESS: at 47.35% examples, 170497 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:25:25,619 : INFO : PROGRESS: at 47.92% examples, 170477 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:26,643 : INFO : PROGRESS: at 48.48% examples, 170501 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:27,651 : INFO : PROGRESS: at 49.05% examples, 170539 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:28,739 : INFO : PROGRESS: at 49.63% examples, 170556 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:29,748 : INFO : PROGRESS: at 50.25% examples, 170759 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:30,766 : INFO : PROGRESS: at 50.77% examples, 170625 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:31,768 : INFO : PROGRESS: at 51.33% examples, 170694 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:32,782 : INFO : PROGRESS: at 51.90% examples, 170749 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:33,835 : INFO : PROGRESS: at 52.49% examples, 170792 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:34,849 : INFO : PROGRESS: at 53.10% examples, 170986 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:35,893 : INFO : PROGRESS: at 53.69% examples, 171051 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:36,895 : INFO : PROGRESS: at 54.25% examples, 171098 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:37,918 : INFO : PROGRESS: at 54.84% examples, 171187 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:38,938 : INFO : PROGRESS: at 55.45% examples, 171330 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:39,996 : INFO : PROGRESS: at 56.04% examples, 171346 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:41,019 : INFO : PROGRESS: at 56.61% examples, 171341 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:42,076 : INFO : PROGRESS: at 57.17% examples, 171305 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:43,143 : INFO : PROGRESS: at 57.71% examples, 171154 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:25:44,153 : INFO : PROGRESS: at 58.28% examples, 171191 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:25:45,188 : INFO : PROGRESS: at 58.85% examples, 171180 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:46,214 : INFO : PROGRESS: at 59.41% examples, 171178 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:47,629 : INFO : PROGRESS: at 59.86% examples, 170251 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:25:48,634 : INFO : PROGRESS: at 60.47% examples, 170395 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:49,665 : INFO : PROGRESS: at 61.04% examples, 170396 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:50,706 : INFO : PROGRESS: at 61.63% examples, 170426 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:51,707 : INFO : PROGRESS: at 62.21% examples, 170501 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:52,723 : INFO : PROGRESS: at 62.85% examples, 170652 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:53,772 : INFO : PROGRESS: at 63.44% examples, 170675 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:54,782 : INFO : PROGRESS: at 64.03% examples, 170753 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:55,848 : INFO : PROGRESS: at 64.64% examples, 170802 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:25:56,871 : INFO : PROGRESS: at 65.21% examples, 170806 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:57,874 : INFO : PROGRESS: at 65.72% examples, 170719 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:25:58,877 : INFO : PROGRESS: at 66.29% examples, 170740 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:25:59,901 : INFO : PROGRESS: at 66.90% examples, 170872 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:00,904 : INFO : PROGRESS: at 67.42% examples, 170784 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:01,950 : INFO : PROGRESS: at 67.99% examples, 170746 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:02,966 : INFO : PROGRESS: at 68.55% examples, 170768 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:04,047 : INFO : PROGRESS: at 69.14% examples, 170759 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:05,086 : INFO : PROGRESS: at 69.71% examples, 170765 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:06,086 : INFO : PROGRESS: at 70.27% examples, 170801 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:07,095 : INFO : PROGRESS: at 70.81% examples, 170782 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:08,116 : INFO : PROGRESS: at 71.33% examples, 170690 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:09,125 : INFO : PROGRESS: at 71.90% examples, 170737 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:10,188 : INFO : PROGRESS: at 72.46% examples, 170697 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:11,200 : INFO : PROGRESS: at 72.98% examples, 170617 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:12,212 : INFO : PROGRESS: at 73.57% examples, 170709 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:13,225 : INFO : PROGRESS: at 74.16% examples, 170796 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:26:14,297 : INFO : PROGRESS: at 74.75% examples, 170802 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:15,308 : INFO : PROGRESS: at 75.36% examples, 170922 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:16,375 : INFO : PROGRESS: at 75.95% examples, 170929 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:17,420 : INFO : PROGRESS: at 76.51% examples, 170900 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:26:18,519 : INFO : PROGRESS: at 77.15% examples, 170983 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:19,533 : INFO : PROGRESS: at 77.76% examples, 171090 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:20,558 : INFO : PROGRESS: at 78.28% examples, 170998 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:21,578 : INFO : PROGRESS: at 78.87% examples, 171059 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:22,628 : INFO : PROGRESS: at 79.43% examples, 171032 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:24,011 : INFO : PROGRESS: at 79.84% examples, 170267 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:26:25,013 : INFO : PROGRESS: at 80.42% examples, 170334 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:26,019 : INFO : PROGRESS: at 81.01% examples, 170413 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:27,043 : INFO : PROGRESS: at 81.58% examples, 170408 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:28,052 : INFO : PROGRESS: at 82.14% examples, 170419 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:29,059 : INFO : PROGRESS: at 82.71% examples, 170398 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:30,083 : INFO : PROGRESS: at 83.30% examples, 170449 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:31,171 : INFO : PROGRESS: at 83.89% examples, 170416 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:32,178 : INFO : PROGRESS: at 84.50% examples, 170513 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:33,206 : INFO : PROGRESS: at 85.02% examples, 170421 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:26:34,216 : INFO : PROGRESS: at 85.58% examples, 170447 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:35,281 : INFO : PROGRESS: at 86.22% examples, 170534 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:36,294 : INFO : PROGRESS: at 86.78% examples, 170548 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:37,336 : INFO : PROGRESS: at 87.40% examples, 170630 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:38,367 : INFO : PROGRESS: at 87.96% examples, 170618 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:39,402 : INFO : PROGRESS: at 88.50% examples, 170569 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:40,421 : INFO : PROGRESS: at 89.07% examples, 170580 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:41,450 : INFO : PROGRESS: at 89.66% examples, 170649 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:42,463 : INFO : PROGRESS: at 90.22% examples, 170663 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:43,465 : INFO : PROGRESS: at 90.79% examples, 170702 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:44,510 : INFO : PROGRESS: at 91.38% examples, 170739 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:45,525 : INFO : PROGRESS: at 91.94% examples, 170769 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:46,576 : INFO : PROGRESS: at 92.53% examples, 170796 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:47,588 : INFO : PROGRESS: at 93.10% examples, 170829 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:48,594 : INFO : PROGRESS: at 93.64% examples, 170809 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:49,597 : INFO : PROGRESS: at 94.18% examples, 170800 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:50,618 : INFO : PROGRESS: at 94.77% examples, 170857 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:51,668 : INFO : PROGRESS: at 95.36% examples, 170868 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:52,675 : INFO : PROGRESS: at 95.92% examples, 170894 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:26:53,691 : INFO : PROGRESS: at 96.47% examples, 170856 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:54,740 : INFO : PROGRESS: at 97.01% examples, 170799 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:55,771 : INFO : PROGRESS: at 97.60% examples, 170834 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:26:56,865 : INFO : PROGRESS: at 98.12% examples, 170693 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:57,865 : INFO : PROGRESS: at 98.73% examples, 170806 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:26:58,870 : INFO : PROGRESS: at 99.27% examples, 170787 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:26:59,980 : INFO : PROGRESS: at 99.79% examples, 170630 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-19 18:27:00,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:27:00,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:27:00,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-19 18:27:00,318 : INFO : training on 42426485 raw words (31022319 effective words) took 181.8s, 170656 effective words/s\n",
      "2017-12-19 18:27:00,369 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:27:00,913 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:27:02,658 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:27:02,660 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:27:02,908 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:27:02,909 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:27:03,069 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:27:03,101 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:27:03,102 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:27:03,103 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:27:03,176 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:27:04,621 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:27:04,735 : INFO : resetting layer weights\n",
      "2017-12-19 18:27:05,163 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:27:06,186 : INFO : PROGRESS: at 0.16% examples, 50933 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:07,219 : INFO : PROGRESS: at 0.75% examples, 114705 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:08,308 : INFO : PROGRESS: at 1.32% examples, 130531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:09,361 : INFO : PROGRESS: at 1.96% examples, 144397 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:10,402 : INFO : PROGRESS: at 2.59% examples, 152071 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:11,416 : INFO : PROGRESS: at 3.16% examples, 155211 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:27:12,428 : INFO : PROGRESS: at 3.75% examples, 158508 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:13,457 : INFO : PROGRESS: at 4.33% examples, 160500 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:14,510 : INFO : PROGRESS: at 4.90% examples, 161175 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:15,562 : INFO : PROGRESS: at 5.47% examples, 161719 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:16,575 : INFO : PROGRESS: at 6.03% examples, 162586 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:17,603 : INFO : PROGRESS: at 6.57% examples, 162636 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:18,641 : INFO : PROGRESS: at 7.14% examples, 163169 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:27:19,729 : INFO : PROGRESS: at 7.77% examples, 164493 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:20,768 : INFO : PROGRESS: at 8.34% examples, 164719 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:21,776 : INFO : PROGRESS: at 8.93% examples, 165705 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:22,814 : INFO : PROGRESS: at 9.49% examples, 166153 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:27:23,867 : INFO : PROGRESS: at 10.11% examples, 166878 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:24,886 : INFO : PROGRESS: at 10.67% examples, 167272 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:27:25,941 : INFO : PROGRESS: at 11.24% examples, 167336 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:27:27,004 : INFO : PROGRESS: at 11.80% examples, 167281 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:28,030 : INFO : PROGRESS: at 12.37% examples, 167546 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:29,063 : INFO : PROGRESS: at 12.91% examples, 167401 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:27:30,072 : INFO : PROGRESS: at 13.45% examples, 167452 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:31,100 : INFO : PROGRESS: at 13.99% examples, 167366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:32,102 : INFO : PROGRESS: at 14.58% examples, 167996 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:33,107 : INFO : PROGRESS: at 15.17% examples, 168469 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:34,108 : INFO : PROGRESS: at 15.71% examples, 168430 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:35,108 : INFO : PROGRESS: at 16.28% examples, 168646 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:36,158 : INFO : PROGRESS: at 16.89% examples, 169090 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:37,167 : INFO : PROGRESS: at 17.48% examples, 169463 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:38,235 : INFO : PROGRESS: at 18.07% examples, 169541 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:39,244 : INFO : PROGRESS: at 18.63% examples, 169700 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:40,309 : INFO : PROGRESS: at 19.18% examples, 169351 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:41,320 : INFO : PROGRESS: at 19.76% examples, 169678 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:42,347 : INFO : PROGRESS: at 20.00% examples, 166868 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:43,355 : INFO : PROGRESS: at 20.57% examples, 167089 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:44,357 : INFO : PROGRESS: at 21.15% examples, 167498 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:45,383 : INFO : PROGRESS: at 21.74% examples, 167716 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:46,434 : INFO : PROGRESS: at 22.38% examples, 168111 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:47,502 : INFO : PROGRESS: at 23.02% examples, 168437 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:48,506 : INFO : PROGRESS: at 23.63% examples, 168873 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:49,510 : INFO : PROGRESS: at 24.19% examples, 168964 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:50,520 : INFO : PROGRESS: at 24.81% examples, 169364 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:51,551 : INFO : PROGRESS: at 25.35% examples, 169201 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:52,570 : INFO : PROGRESS: at 25.89% examples, 169082 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:53,630 : INFO : PROGRESS: at 26.53% examples, 169448 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:54,660 : INFO : PROGRESS: at 27.14% examples, 169774 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:55,664 : INFO : PROGRESS: at 27.70% examples, 169856 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:56,690 : INFO : PROGRESS: at 28.29% examples, 170005 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:27:57,736 : INFO : PROGRESS: at 28.81% examples, 169680 words/s, in_qsize 4, out_qsize 1\n",
      "2017-12-19 18:27:58,750 : INFO : PROGRESS: at 29.38% examples, 169792 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:27:59,759 : INFO : PROGRESS: at 29.94% examples, 169870 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:00,780 : INFO : PROGRESS: at 30.53% examples, 170080 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:01,799 : INFO : PROGRESS: at 31.10% examples, 170153 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:02,833 : INFO : PROGRESS: at 31.71% examples, 170423 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:03,853 : INFO : PROGRESS: at 32.25% examples, 170366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:04,862 : INFO : PROGRESS: at 32.86% examples, 170699 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:05,974 : INFO : PROGRESS: at 33.45% examples, 170616 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:06,984 : INFO : PROGRESS: at 34.02% examples, 170695 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:08,015 : INFO : PROGRESS: at 34.58% examples, 170723 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:09,103 : INFO : PROGRESS: at 35.17% examples, 170669 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:10,135 : INFO : PROGRESS: at 35.71% examples, 170531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:11,154 : INFO : PROGRESS: at 36.25% examples, 170445 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:12,197 : INFO : PROGRESS: at 36.75% examples, 170089 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:13,217 : INFO : PROGRESS: at 37.24% examples, 169822 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:14,239 : INFO : PROGRESS: at 37.81% examples, 169830 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:15,296 : INFO : PROGRESS: at 38.33% examples, 169573 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:16,330 : INFO : PROGRESS: at 38.89% examples, 169585 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:17,357 : INFO : PROGRESS: at 39.43% examples, 169503 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:18,717 : INFO : PROGRESS: at 39.86% examples, 168183 words/s, in_qsize 6, out_qsize 2\n",
      "2017-12-19 18:28:19,725 : INFO : PROGRESS: at 40.40% examples, 168121 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:20,739 : INFO : PROGRESS: at 41.01% examples, 168381 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:21,774 : INFO : PROGRESS: at 41.60% examples, 168470 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:22,815 : INFO : PROGRESS: at 42.17% examples, 168444 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:23,914 : INFO : PROGRESS: at 42.80% examples, 168507 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:28:24,919 : INFO : PROGRESS: at 43.42% examples, 168757 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:25,927 : INFO : PROGRESS: at 43.96% examples, 168704 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:26,995 : INFO : PROGRESS: at 44.52% examples, 168610 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:28,011 : INFO : PROGRESS: at 45.09% examples, 168663 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:29,076 : INFO : PROGRESS: at 45.63% examples, 168536 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:30,140 : INFO : PROGRESS: at 46.20% examples, 168465 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:31,157 : INFO : PROGRESS: at 46.76% examples, 168511 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:32,174 : INFO : PROGRESS: at 47.33% examples, 168562 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:33,240 : INFO : PROGRESS: at 47.96% examples, 168745 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:34,247 : INFO : PROGRESS: at 48.55% examples, 168909 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:35,319 : INFO : PROGRESS: at 49.09% examples, 168756 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:36,383 : INFO : PROGRESS: at 49.63% examples, 168673 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:37,385 : INFO : PROGRESS: at 50.20% examples, 168733 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:38,401 : INFO : PROGRESS: at 50.77% examples, 168800 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:39,449 : INFO : PROGRESS: at 51.31% examples, 168725 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:40,449 : INFO : PROGRESS: at 51.87% examples, 168824 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:41,453 : INFO : PROGRESS: at 52.46% examples, 168972 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:42,460 : INFO : PROGRESS: at 52.98% examples, 168886 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:43,462 : INFO : PROGRESS: at 53.50% examples, 168819 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:44,508 : INFO : PROGRESS: at 54.04% examples, 168748 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:45,521 : INFO : PROGRESS: at 54.61% examples, 168814 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:46,557 : INFO : PROGRESS: at 55.19% examples, 168886 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:47,607 : INFO : PROGRESS: at 55.81% examples, 169001 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:28:48,626 : INFO : PROGRESS: at 56.40% examples, 169092 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:49,635 : INFO : PROGRESS: at 56.96% examples, 169148 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:50,680 : INFO : PROGRESS: at 57.53% examples, 169137 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:51,715 : INFO : PROGRESS: at 58.07% examples, 169076 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:28:52,733 : INFO : PROGRESS: at 58.66% examples, 169183 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:53,751 : INFO : PROGRESS: at 59.20% examples, 169145 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:54,793 : INFO : PROGRESS: at 59.76% examples, 169140 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:55,839 : INFO : PROGRESS: at 60.00% examples, 168172 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:56,843 : INFO : PROGRESS: at 60.57% examples, 168242 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:57,880 : INFO : PROGRESS: at 61.15% examples, 168325 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:28:58,885 : INFO : PROGRESS: at 61.77% examples, 168486 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:28:59,909 : INFO : PROGRESS: at 62.38% examples, 168596 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:00,958 : INFO : PROGRESS: at 62.97% examples, 168610 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:02,019 : INFO : PROGRESS: at 63.60% examples, 168749 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:03,063 : INFO : PROGRESS: at 64.19% examples, 168793 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:04,086 : INFO : PROGRESS: at 64.81% examples, 168928 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:05,093 : INFO : PROGRESS: at 65.37% examples, 168969 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:06,112 : INFO : PROGRESS: at 65.94% examples, 168984 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:07,124 : INFO : PROGRESS: at 66.50% examples, 169017 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:08,168 : INFO : PROGRESS: at 67.11% examples, 169138 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:09,207 : INFO : PROGRESS: at 67.70% examples, 169185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:10,229 : INFO : PROGRESS: at 68.29% examples, 169259 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:11,249 : INFO : PROGRESS: at 68.88% examples, 169340 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:12,279 : INFO : PROGRESS: at 69.47% examples, 169434 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:13,356 : INFO : PROGRESS: at 70.04% examples, 169368 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:14,372 : INFO : PROGRESS: at 70.60% examples, 169411 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:29:15,402 : INFO : PROGRESS: at 71.17% examples, 169438 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:16,427 : INFO : PROGRESS: at 71.76% examples, 169514 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:17,429 : INFO : PROGRESS: at 72.27% examples, 169463 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:18,432 : INFO : PROGRESS: at 72.86% examples, 169570 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:19,492 : INFO : PROGRESS: at 73.47% examples, 169664 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:20,497 : INFO : PROGRESS: at 74.09% examples, 169821 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:21,521 : INFO : PROGRESS: at 74.68% examples, 169898 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:22,529 : INFO : PROGRESS: at 75.24% examples, 169932 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:23,544 : INFO : PROGRESS: at 75.81% examples, 169947 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:24,564 : INFO : PROGRESS: at 76.37% examples, 169958 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:25,571 : INFO : PROGRESS: at 76.94% examples, 169994 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:26,575 : INFO : PROGRESS: at 77.48% examples, 169975 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:27,580 : INFO : PROGRESS: at 78.07% examples, 170064 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:28,594 : INFO : PROGRESS: at 78.59% examples, 169987 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:29,603 : INFO : PROGRESS: at 79.18% examples, 170069 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:30,606 : INFO : PROGRESS: at 79.74% examples, 170106 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:31,626 : INFO : PROGRESS: at 80.00% examples, 169447 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:32,644 : INFO : PROGRESS: at 80.59% examples, 169525 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:33,696 : INFO : PROGRESS: at 81.18% examples, 169563 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:34,712 : INFO : PROGRESS: at 81.79% examples, 169662 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:35,740 : INFO : PROGRESS: at 82.38% examples, 169688 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:36,747 : INFO : PROGRESS: at 82.94% examples, 169688 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:37,754 : INFO : PROGRESS: at 83.53% examples, 169756 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:38,755 : INFO : PROGRESS: at 84.05% examples, 169689 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:39,761 : INFO : PROGRESS: at 84.66% examples, 169797 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:40,808 : INFO : PROGRESS: at 85.28% examples, 169873 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:41,817 : INFO : PROGRESS: at 85.87% examples, 169937 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:42,837 : INFO : PROGRESS: at 86.45% examples, 169996 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:43,852 : INFO : PROGRESS: at 87.00% examples, 169976 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:29:44,912 : INFO : PROGRESS: at 87.61% examples, 170027 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:45,947 : INFO : PROGRESS: at 88.17% examples, 170018 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:46,951 : INFO : PROGRESS: at 88.79% examples, 170143 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:47,971 : INFO : PROGRESS: at 89.35% examples, 170168 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:48,975 : INFO : PROGRESS: at 89.94% examples, 170243 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:50,021 : INFO : PROGRESS: at 90.51% examples, 170240 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:51,094 : INFO : PROGRESS: at 91.07% examples, 170210 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:52,116 : INFO : PROGRESS: at 91.64% examples, 170226 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:53,129 : INFO : PROGRESS: at 92.16% examples, 170172 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:29:54,135 : INFO : PROGRESS: at 92.72% examples, 170206 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:55,193 : INFO : PROGRESS: at 93.31% examples, 170233 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:29:56,222 : INFO : PROGRESS: at 93.90% examples, 170290 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:57,222 : INFO : PROGRESS: at 94.42% examples, 170243 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:58,230 : INFO : PROGRESS: at 95.01% examples, 170304 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:29:59,232 : INFO : PROGRESS: at 95.55% examples, 170290 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:00,249 : INFO : PROGRESS: at 96.09% examples, 170263 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:01,280 : INFO : PROGRESS: at 96.65% examples, 170260 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:02,300 : INFO : PROGRESS: at 97.22% examples, 170281 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:03,344 : INFO : PROGRESS: at 97.83% examples, 170345 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:04,393 : INFO : PROGRESS: at 98.42% examples, 170369 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:05,473 : INFO : PROGRESS: at 98.99% examples, 170327 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:06,530 : INFO : PROGRESS: at 99.55% examples, 170300 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:07,458 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-19 18:30:07,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-19 18:30:07,511 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:30:07,512 : INFO : training on 42426485 raw words (31022350 effective words) took 182.3s, 170129 effective words/s\n",
      "2017-12-19 18:30:07,563 : INFO : collecting all words and their counts\n",
      "2017-12-19 18:30:08,106 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-19 18:30:09,833 : INFO : collected 171140 word types from a corpus of 8485297 raw words and 849 sentences\n",
      "2017-12-19 18:30:09,834 : INFO : Loading a fresh vocabulary\n",
      "2017-12-19 18:30:10,091 : INFO : min_count=5 retains 48753 unique words (28% of original 171140, drops 122387)\n",
      "2017-12-19 18:30:10,092 : INFO : min_count=5 leaves 8292974 word corpus (97% of original 8485297, drops 192323)\n",
      "2017-12-19 18:30:10,260 : INFO : deleting the raw counts dictionary of 171140 items\n",
      "2017-12-19 18:30:10,293 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-12-19 18:30:10,294 : INFO : downsampling leaves estimated 6205111 word corpus (74.8% of prior 8292974)\n",
      "2017-12-19 18:30:10,295 : INFO : estimated required memory for 48753 words and 100 dimensions: 92630700 bytes\n",
      "2017-12-19 18:30:10,368 : INFO : constructing a huffman tree from 48753 words\n",
      "2017-12-19 18:30:11,779 : INFO : built huffman tree with maximum node depth 21\n",
      "2017-12-19 18:30:11,880 : INFO : resetting layer weights\n",
      "2017-12-19 18:30:12,309 : INFO : training model with 3 workers on 48753 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2017-12-19 18:30:13,324 : INFO : PROGRESS: at 0.19% examples, 58662 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:14,364 : INFO : PROGRESS: at 0.75% examples, 114756 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:15,382 : INFO : PROGRESS: at 1.30% examples, 131269 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:16,386 : INFO : PROGRESS: at 1.88% examples, 143372 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:17,410 : INFO : PROGRESS: at 2.50% examples, 150729 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:18,426 : INFO : PROGRESS: at 3.06% examples, 153886 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:19,435 : INFO : PROGRESS: at 3.65% examples, 157568 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:20,445 : INFO : PROGRESS: at 4.24% examples, 160197 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:21,484 : INFO : PROGRESS: at 4.83% examples, 161781 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:22,509 : INFO : PROGRESS: at 5.44% examples, 164161 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:23,537 : INFO : PROGRESS: at 6.01% examples, 164619 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:24,586 : INFO : PROGRESS: at 6.55% examples, 164192 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:25,595 : INFO : PROGRESS: at 7.09% examples, 164399 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:26,596 : INFO : PROGRESS: at 7.63% examples, 164572 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:27,611 : INFO : PROGRESS: at 8.24% examples, 166046 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:28,612 : INFO : PROGRESS: at 8.83% examples, 167083 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:29,624 : INFO : PROGRESS: at 9.40% examples, 167589 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:30,650 : INFO : PROGRESS: at 9.94% examples, 167381 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:31,703 : INFO : PROGRESS: at 10.51% examples, 167460 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:32,751 : INFO : PROGRESS: at 11.07% examples, 167546 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:33,771 : INFO : PROGRESS: at 11.64% examples, 167805 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:34,791 : INFO : PROGRESS: at 12.20% examples, 168113 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:35,833 : INFO : PROGRESS: at 12.77% examples, 168175 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:36,876 : INFO : PROGRESS: at 13.31% examples, 167957 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:37,919 : INFO : PROGRESS: at 13.88% examples, 168052 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:38,943 : INFO : PROGRESS: at 14.46% examples, 168520 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:40,004 : INFO : PROGRESS: at 15.03% examples, 168367 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:41,073 : INFO : PROGRESS: at 15.67% examples, 168974 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:42,106 : INFO : PROGRESS: at 16.23% examples, 168992 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:43,110 : INFO : PROGRESS: at 16.84% examples, 169674 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:44,174 : INFO : PROGRESS: at 17.43% examples, 169736 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:45,229 : INFO : PROGRESS: at 18.00% examples, 169700 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:30:46,247 : INFO : PROGRESS: at 18.54% examples, 169524 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:47,266 : INFO : PROGRESS: at 19.15% examples, 170042 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:48,309 : INFO : PROGRESS: at 19.72% examples, 170005 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:49,344 : INFO : PROGRESS: at 20.00% examples, 167527 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:50,367 : INFO : PROGRESS: at 20.59% examples, 167853 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:51,468 : INFO : PROGRESS: at 21.18% examples, 167820 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:52,474 : INFO : PROGRESS: at 21.74% examples, 167931 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:53,505 : INFO : PROGRESS: at 22.33% examples, 168059 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:30:54,525 : INFO : PROGRESS: at 22.94% examples, 168374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:55,531 : INFO : PROGRESS: at 23.56% examples, 168819 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:56,562 : INFO : PROGRESS: at 24.17% examples, 169138 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:30:57,586 : INFO : PROGRESS: at 24.76% examples, 169318 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:58,590 : INFO : PROGRESS: at 25.35% examples, 169583 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:30:59,608 : INFO : PROGRESS: at 25.91% examples, 169613 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:00,643 : INFO : PROGRESS: at 26.45% examples, 169459 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:01,660 : INFO : PROGRESS: at 27.04% examples, 169679 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:02,696 : INFO : PROGRESS: at 27.66% examples, 169951 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:03,704 : INFO : PROGRESS: at 28.20% examples, 169877 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:04,707 : INFO : PROGRESS: at 28.79% examples, 170126 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:05,722 : INFO : PROGRESS: at 29.35% examples, 170221 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:06,750 : INFO : PROGRESS: at 29.94% examples, 170370 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:07,753 : INFO : PROGRESS: at 30.46% examples, 170234 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:08,799 : INFO : PROGRESS: at 31.02% examples, 170212 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:09,820 : INFO : PROGRESS: at 31.61% examples, 170394 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:10,858 : INFO : PROGRESS: at 32.13% examples, 170158 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:11,878 : INFO : PROGRESS: at 32.72% examples, 170339 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:12,892 : INFO : PROGRESS: at 33.29% examples, 170426 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:13,888 : INFO : PROGRESS: at 33.83% examples, 170414 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:14,939 : INFO : PROGRESS: at 34.37% examples, 170278 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:15,971 : INFO : PROGRESS: at 34.89% examples, 170034 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:17,031 : INFO : PROGRESS: at 35.50% examples, 170167 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:18,052 : INFO : PROGRESS: at 36.02% examples, 169980 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:19,126 : INFO : PROGRESS: at 36.58% examples, 169863 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:20,148 : INFO : PROGRESS: at 37.20% examples, 170140 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-19 18:31:21,254 : INFO : PROGRESS: at 37.79% examples, 170041 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:22,257 : INFO : PROGRESS: at 38.35% examples, 170121 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:23,274 : INFO : PROGRESS: at 38.92% examples, 170179 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:24,289 : INFO : PROGRESS: at 39.51% examples, 170309 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:25,589 : INFO : PROGRESS: at 39.86% examples, 168805 words/s, in_qsize 4, out_qsize 2\n",
      "2017-12-19 18:31:26,631 : INFO : PROGRESS: at 40.42% examples, 168757 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:27,643 : INFO : PROGRESS: at 40.97% examples, 168720 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:28,646 : INFO : PROGRESS: at 41.58% examples, 168973 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:29,688 : INFO : PROGRESS: at 42.17% examples, 169047 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:30,686 : INFO : PROGRESS: at 42.76% examples, 169121 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:31:31,744 : INFO : PROGRESS: at 43.37% examples, 169252 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:32,804 : INFO : PROGRESS: at 43.96% examples, 169270 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:33,812 : INFO : PROGRESS: at 44.57% examples, 169476 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:34,824 : INFO : PROGRESS: at 45.14% examples, 169528 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:35,824 : INFO : PROGRESS: at 45.65% examples, 169425 words/s, in_qsize 5, out_qsize 1\n",
      "2017-12-19 18:31:36,826 : INFO : PROGRESS: at 46.24% examples, 169552 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:37,898 : INFO : PROGRESS: at 46.86% examples, 169649 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:31:38,916 : INFO : PROGRESS: at 47.42% examples, 169682 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:39,959 : INFO : PROGRESS: at 47.99% examples, 169649 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:40,987 : INFO : PROGRESS: at 48.55% examples, 169670 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:42,010 : INFO : PROGRESS: at 49.14% examples, 169779 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:43,024 : INFO : PROGRESS: at 49.68% examples, 169770 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:44,040 : INFO : PROGRESS: at 50.20% examples, 169634 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:45,041 : INFO : PROGRESS: at 50.81% examples, 169876 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:46,091 : INFO : PROGRESS: at 51.35% examples, 169787 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:47,133 : INFO : PROGRESS: at 51.94% examples, 169879 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:48,211 : INFO : PROGRESS: at 52.56% examples, 169960 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:49,245 : INFO : PROGRESS: at 53.12% examples, 169975 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:50,254 : INFO : PROGRESS: at 53.69% examples, 170030 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:51,284 : INFO : PROGRESS: at 54.28% examples, 170124 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:52,292 : INFO : PROGRESS: at 54.86% examples, 170247 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:53,313 : INFO : PROGRESS: at 55.45% examples, 170323 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:54,340 : INFO : PROGRESS: at 56.02% examples, 170332 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:55,355 : INFO : PROGRESS: at 56.56% examples, 170277 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:56,363 : INFO : PROGRESS: at 57.13% examples, 170331 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:57,411 : INFO : PROGRESS: at 57.71% examples, 170364 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:31:58,515 : INFO : PROGRESS: at 58.35% examples, 170460 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:31:59,608 : INFO : PROGRESS: at 58.99% examples, 170571 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:32:00,611 : INFO : PROGRESS: at 59.55% examples, 170608 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:32:01,760 : INFO : PROGRESS: at 59.86% examples, 169692 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-19 18:32:02,863 : INFO : PROGRESS: at 60.45% examples, 169634 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:32:03,919 : INFO : PROGRESS: at 61.04% examples, 169670 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:32:04,950 : INFO : PROGRESS: at 61.63% examples, 169727 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-19 18:32:05,952 : INFO : PROGRESS: at 62.17% examples, 169686 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-19 18:32:06,958 : INFO : PROGRESS: at 62.78% examples, 169796 words/s, in_qsize 6, out_qsize 0\n"
     ]
    }
   ],
   "source": [
    "# using 25 KB and 50 MB files only for generating o/p -> comment next line for using all 5 test files\n",
    "input_data_files = [input_data_files[0], input_data_files[-2]]\n",
    "print(input_data_files)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_time_values = []\n",
    "seed_val = 42\n",
    "sg_values = [0, 1]\n",
    "hs_values = [0, 1]\n",
    "\n",
    "for data_file in input_data_files:\n",
    "    data = gensim.models.word2vec.LineSentence(data_file) \n",
    "    for sg_val in sg_values:\n",
    "        for hs_val in hs_values:\n",
    "            for loss_flag in [True, False]:\n",
    "                time_taken_list = []\n",
    "                for i in range(3):\n",
    "                    start_time = time.time()\n",
    "                    w2v_model = gensim.models.Word2Vec(data, compute_loss=loss_flag, sg=sg_val, hs=hs_val, seed=seed_val) \n",
    "                    time_taken_list.append(time.time() - start_time)\n",
    "\n",
    "                time_taken_list = np.array(time_taken_list)\n",
    "                time_mean = np.mean(time_taken_list)\n",
    "                time_std = np.std(time_taken_list)\n",
    "                train_time_values.append({'train_data': data_file, 'compute_loss': loss_flag, 'sg': sg_val, 'hs': hs_val, 'mean': time_mean, 'std': time_std})\n",
    "\n",
    "train_times_table = pd.DataFrame(train_time_values)\n",
    "train_times_table = train_times_table.sort_values(by=['train_data', 'sg', 'hs', 'compute_loss'], ascending=[False, False, True, False])\n",
    "print(train_times_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Word2Vec \"model to dict\" method to production pipeline\n",
    "Suppose, we still want more performance improvement in production. \n",
    "One good way is to cache all the similar words in a dictionary.\n",
    "So that next time when we get the similar query word, we'll search it first in the dict.\n",
    "And if it's a hit then we will show the result directly from the dictionary.\n",
    "otherwise we will query the word and then cache it so that it doesn't miss next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similars_precalc = {word : model.wv.most_similar(word) for word in model.wv.index2word}\n",
    "print(most_similars_precalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with and without caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for time being lets take 4 words randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "words = ['voted','few','their','around']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for word in words:\n",
    "    result = model.wv.most_similar(word)\n",
    "    print(result)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for word in words:\n",
    "    if 'voted' in most_similars_precalc:\n",
    "        result = most_similars_precalc[word]\n",
    "        print(result)\n",
    "    else:\n",
    "        result = model.wv.most_similar(word)\n",
    "        most_similars_precalc[word] = result\n",
    "        print(result)\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly you can see the improvement but this difference will be even larger when we take more words in the consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embeddings made by the model can be visualised by reducing dimensionality of the words to 2 dimensions using tSNE.\n",
    "\n",
    "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
    "\n",
    "Example: Semantic- words like cat, dog, cow, etc. have a tendency to lie close by\n",
    "         Syntactic- words like run, running or cut, cutting lie close together.\n",
    "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
    "\n",
    "Additional dependencies : \n",
    "- sklearn\n",
    "- numpy\n",
    "- plotly\n",
    "\n",
    "The function below can be used to plot the embeddings in an ipython notebook.\n",
    "It requires the model as the necessary parameter. If you don't have the model, you can load it by\n",
    "\n",
    "`model = gensim.models.Word2Vec.load('path/to/model')`\n",
    "\n",
    "If you don't want to plot inside a notebook, set the `plot_in_notebook` parameter to `False`.\n",
    "\n",
    "Note: the model used for the visualisation is trained on a small corpus. Thus some of the relations might not be so clear\n",
    "\n",
    "Beware : This sort dimension reduction comes at the cost of loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def reduce_dimensions(model, plot_in_notebook = True):\n",
    "\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = []        # positions in vector space\n",
    "    labels = []         # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    logging.info('starting tSNE dimensionality reduction. This may take some time.')\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "        \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        mode='text',\n",
    "        text=labels\n",
    "        )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    logging.info('All done. Plotting.')\n",
    "    \n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dimensions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we learned how to train word2vec models on your custom data and also how to evaluate it. Hope that you too will find this popular tool useful in your Machine Learning tasks!\n",
    "\n",
    "## Links\n",
    "\n",
    "\n",
    "Full `word2vec` API docs [here](http://radimrehurek.com/gensim/models/word2vec.html); get [gensim](http://radimrehurek.com/gensim/) here. Original C toolkit and `word2vec` papers by Google [here](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
