{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using Gensim's API for downloading corpuses/models\n",
    "Let's start by importing the api module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets download the text8 corpus and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 20:00:53,429 : INFO : Creating /home/chaitali/gensim-data/text8\n",
      "2017-09-30 20:00:53,431 : INFO : Creation of /home/chaitali/gensim-data/text8 successful.\n",
      "2017-09-30 20:00:53,433 : INFO : Downloading text8\n",
      "2017-09-30 20:04:46,938 : INFO : text8 downloaded\n",
      "2017-09-30 20:04:46,951 : INFO : Extracting files from /home/chaitali/gensim-data/text8\n",
      "2017-09-30 20:04:48,888 : INFO : text8 installed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the corpus has been installed, let's create a word2vec model of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 20:04:59,672 : INFO : collecting all words and their counts\n",
      "2017-09-30 20:04:59,677 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-30 20:05:06,425 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-09-30 20:05:06,426 : INFO : Loading a fresh vocabulary\n",
      "2017-09-30 20:05:06,711 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-09-30 20:05:06,711 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-09-30 20:05:06,925 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-09-30 20:05:06,935 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-09-30 20:05:06,936 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-09-30 20:05:06,938 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-09-30 20:05:07,267 : INFO : resetting layer weights\n",
      "2017-09-30 20:05:08,240 : INFO : training model with 3 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-30 20:05:09,250 : INFO : PROGRESS: at 1.09% examples, 677048 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:10,260 : INFO : PROGRESS: at 2.22% examples, 682662 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:05:11,266 : INFO : PROGRESS: at 3.35% examples, 688976 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:12,281 : INFO : PROGRESS: at 4.47% examples, 688776 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:13,283 : INFO : PROGRESS: at 5.58% examples, 692300 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:14,291 : INFO : PROGRESS: at 6.71% examples, 695154 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:15,298 : INFO : PROGRESS: at 7.83% examples, 695477 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:16,302 : INFO : PROGRESS: at 8.94% examples, 694913 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:17,320 : INFO : PROGRESS: at 10.04% examples, 693561 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:18,334 : INFO : PROGRESS: at 11.17% examples, 694297 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:19,349 : INFO : PROGRESS: at 12.28% examples, 693175 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:20,357 : INFO : PROGRESS: at 13.39% examples, 693211 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:21,369 : INFO : PROGRESS: at 14.52% examples, 693794 words/s, in_qsize 4, out_qsize 0\n",
      "2017-09-30 20:05:22,377 : INFO : PROGRESS: at 15.57% examples, 689525 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:23,377 : INFO : PROGRESS: at 16.68% examples, 689973 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:24,391 : INFO : PROGRESS: at 17.72% examples, 686717 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:25,398 : INFO : PROGRESS: at 18.86% examples, 687798 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:05:26,409 : INFO : PROGRESS: at 20.00% examples, 688486 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:27,411 : INFO : PROGRESS: at 21.15% examples, 689967 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:28,424 : INFO : PROGRESS: at 22.27% examples, 689469 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:29,432 : INFO : PROGRESS: at 23.41% examples, 690341 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:30,440 : INFO : PROGRESS: at 24.53% examples, 690470 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:31,454 : INFO : PROGRESS: at 25.64% examples, 690757 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:32,460 : INFO : PROGRESS: at 26.76% examples, 691272 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:33,468 : INFO : PROGRESS: at 27.89% examples, 691773 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:34,481 : INFO : PROGRESS: at 29.02% examples, 692058 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:35,491 : INFO : PROGRESS: at 30.14% examples, 692208 words/s, in_qsize 4, out_qsize 0\n",
      "2017-09-30 20:05:36,501 : INFO : PROGRESS: at 31.23% examples, 691812 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:37,507 : INFO : PROGRESS: at 32.35% examples, 691913 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:38,527 : INFO : PROGRESS: at 33.47% examples, 691941 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:05:39,534 : INFO : PROGRESS: at 34.59% examples, 692104 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:40,535 : INFO : PROGRESS: at 35.73% examples, 692232 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:41,542 : INFO : PROGRESS: at 36.83% examples, 691803 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:42,549 : INFO : PROGRESS: at 37.95% examples, 692107 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:43,561 : INFO : PROGRESS: at 39.08% examples, 692095 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:44,572 : INFO : PROGRESS: at 40.21% examples, 692168 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:45,574 : INFO : PROGRESS: at 41.35% examples, 692460 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:46,586 : INFO : PROGRESS: at 42.46% examples, 692126 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:47,597 : INFO : PROGRESS: at 43.61% examples, 692641 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:48,599 : INFO : PROGRESS: at 44.71% examples, 692672 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:49,609 : INFO : PROGRESS: at 45.81% examples, 692414 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:50,613 : INFO : PROGRESS: at 46.89% examples, 692196 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:05:51,614 : INFO : PROGRESS: at 47.98% examples, 692082 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:05:52,615 : INFO : PROGRESS: at 49.08% examples, 691919 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:05:53,622 : INFO : PROGRESS: at 50.18% examples, 691886 words/s, in_qsize 5, out_qsize 2\n",
      "2017-09-30 20:05:54,631 : INFO : PROGRESS: at 51.30% examples, 691985 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:55,631 : INFO : PROGRESS: at 52.39% examples, 691836 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:56,644 : INFO : PROGRESS: at 53.50% examples, 691643 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:57,650 : INFO : PROGRESS: at 54.61% examples, 691789 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:58,651 : INFO : PROGRESS: at 55.72% examples, 691433 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:05:59,651 : INFO : PROGRESS: at 56.81% examples, 691258 words/s, in_qsize 2, out_qsize 1\n",
      "2017-09-30 20:06:00,662 : INFO : PROGRESS: at 57.93% examples, 691247 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:01,666 : INFO : PROGRESS: at 59.05% examples, 691236 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:02,674 : INFO : PROGRESS: at 60.16% examples, 691214 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:03,692 : INFO : PROGRESS: at 61.31% examples, 691243 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:06:04,693 : INFO : PROGRESS: at 62.41% examples, 691156 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:05,706 : INFO : PROGRESS: at 63.54% examples, 691252 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:06,726 : INFO : PROGRESS: at 64.66% examples, 691169 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:07,742 : INFO : PROGRESS: at 65.77% examples, 691212 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:08,747 : INFO : PROGRESS: at 66.88% examples, 691305 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:09,752 : INFO : PROGRESS: at 67.98% examples, 691292 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:10,761 : INFO : PROGRESS: at 69.09% examples, 691231 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:06:11,767 : INFO : PROGRESS: at 70.21% examples, 691345 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:12,776 : INFO : PROGRESS: at 71.30% examples, 691192 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:13,779 : INFO : PROGRESS: at 72.39% examples, 691076 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:06:14,783 : INFO : PROGRESS: at 73.50% examples, 691043 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:15,799 : INFO : PROGRESS: at 74.60% examples, 690947 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:16,804 : INFO : PROGRESS: at 75.72% examples, 690751 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:17,823 : INFO : PROGRESS: at 76.83% examples, 690552 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 20:06:18,828 : INFO : PROGRESS: at 77.95% examples, 690734 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:19,838 : INFO : PROGRESS: at 79.07% examples, 690668 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:20,845 : INFO : PROGRESS: at 80.20% examples, 690773 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:21,848 : INFO : PROGRESS: at 81.33% examples, 690841 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:22,853 : INFO : PROGRESS: at 82.43% examples, 690748 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:23,861 : INFO : PROGRESS: at 83.55% examples, 690766 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:24,868 : INFO : PROGRESS: at 84.67% examples, 690835 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-30 20:06:25,870 : INFO : PROGRESS: at 85.77% examples, 690917 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:26,876 : INFO : PROGRESS: at 86.88% examples, 690986 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-30 20:06:27,879 : INFO : PROGRESS: at 88.00% examples, 691091 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:28,896 : INFO : PROGRESS: at 89.12% examples, 691157 words/s, in_qsize 5, out_qsize 1\n",
      "2017-09-30 20:06:29,900 : INFO : PROGRESS: at 90.22% examples, 691067 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:30,901 : INFO : PROGRESS: at 91.31% examples, 691025 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:31,903 : INFO : PROGRESS: at 92.42% examples, 691030 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:32,915 : INFO : PROGRESS: at 93.52% examples, 690949 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:33,927 : INFO : PROGRESS: at 94.64% examples, 690994 words/s, in_qsize 4, out_qsize 0\n",
      "2017-09-30 20:06:34,932 : INFO : PROGRESS: at 95.76% examples, 690838 words/s, in_qsize 5, out_qsize 1\n",
      "2017-09-30 20:06:35,949 : INFO : PROGRESS: at 96.86% examples, 690691 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:36,955 : INFO : PROGRESS: at 97.99% examples, 690835 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:37,971 : INFO : PROGRESS: at 99.13% examples, 690908 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-30 20:06:38,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-30 20:06:38,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-30 20:06:38,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-30 20:06:38,760 : INFO : training on 85026035 raw words (62532240 effective words) took 90.5s, 690828 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word2vec model, let's find words that are similar to 'tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 20:08:12,509 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('trees', 0.7073001861572266),\n",
       " ('bark', 0.7032904028892517),\n",
       " ('leaf', 0.6881209015846252),\n",
       " ('bird', 0.6044381260871887),\n",
       " ('flower', 0.6009336709976196),\n",
       " ('fruit', 0.597153902053833),\n",
       " ('avl', 0.5837888717651367),\n",
       " ('cactus', 0.5712562799453735),\n",
       " ('bee', 0.5658263564109802),\n",
       " ('garden', 0.565678596496582)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the API to download many corpuses and models. You can get the list of all the models and corpuses that are provided, by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": {\n",
      "        \"Google_News_word2vec\": {\n",
      "            \"desc\": \"Google has published pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\",\n",
      "            \"filename\": \"GoogleNews-vectors-negative300.bin.gz\",\n",
      "            \"checksum\": \"4fa963d128fe65ec8cd5dd4d9377f8ed\"\n",
      "        },\n",
      "        \"fasttext_eng_model\": {\n",
      "            \"desc\": \"fastText is a library for efficient learning of word representations and sentence classification.These vectors for english language in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.\",\n",
      "            \"filename\": \"wiki.en.vec\",\n",
      "            \"checksum\": \"2de532213d7fa8b937263337c6e9deeb\"\n",
      "        },\n",
      "        \"glove_common_crawl_42B\": {\n",
      "            \"desc\": \"This model is trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.42B.300d.zip\",\n",
      "            \"checksum\": \"d6f41a6e9e5bf905d349a01b5216826a\"\n",
      "        },\n",
      "        \"glove_common_crawl_840B\": {\n",
      "            \"desc\": \"This model is trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.840B.300d.zip\",\n",
      "            \"checksum\": \"72f02c239743c750eaea8747839e4852\"\n",
      "        },\n",
      "        \"glove_wiki_gigaword_300d\": {\n",
      "            \"desc\": \" This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 300d vectors).GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.6B.300d.txt\",\n",
      "            \"checksum\": \"e0c1af43ab57753d11da2fa642c3ff82\"\n",
      "        },\n",
      "        \"glove_wiki_gigaword_200d\": {\n",
      "            \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 200d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.6B.200d.txt\",\n",
      "            \"checksum\": \"c4e58068e16be476b115699f94fa82cb\"\n",
      "        },\n",
      "        \"glove_wiki_gigaword_100d\": {\n",
      "            \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 100d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.6B.100d.txt\",\n",
      "            \"checksum\": \"7067a76b2adc0e92a1f71e2919382c95\"\n",
      "        },\n",
      "        \"glove_wiki_gigaword_50d\": {\n",
      "            \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 50d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.6B.50d.txt\",\n",
      "            \"checksum\": \"44d71eb1db9485d9c8a605a5ed560d8c\"\n",
      "        },\n",
      "        \"glove_twitter_200d\": {\n",
      "            \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.twitter.27B.200d.txt\",\n",
      "            \"checksum\": \"91b40581d04e2ff5306d2f0452e34f72\"\n",
      "        },\n",
      "        \"glove_twitter_100d\": {\n",
      "            \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 100d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.twitter.27B.100d.txt\",\n",
      "            \"checksum\": \"2825c182e4ac2afd8d2dede8445919ab\"\n",
      "        },\n",
      "        \"glove_twitter_50d\": {\n",
      "            \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 50d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.twitter.27B.50d.txt\",\n",
      "            \"checksum\": \"9842275a894ebdfb60b270877bb8f60c\"\n",
      "        },\n",
      "        \"glove_twitter_25d\": {\n",
      "            \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 25d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "            \"filename\": \"glove.twitter.27B.25d.txt\",\n",
      "            \"checksum\": \"9802ffec313d8612bf790d1aa4d37ddd\"\n",
      "        }\n",
      "    },\n",
      "    \"corpus\": {\n",
      "        \"text8\": {\n",
      "            \"desc\": \"Wikipedia English corpus\",\n",
      "            \"filename\": \"text8\",\n",
      "            \"checksum\": \"5d703f1842fb1ca55bf86f2e2552012c\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_list = api.info()\n",
    "print(json.dumps(data_list, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get detailed information about the model/corpus, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n"
     ]
    }
   ],
   "source": [
    "glove_common_crawl_info = api.info('glove_common_crawl_42B')\n",
    "print(glove_common_crawl_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you do not want to load the corpus/model to memory. You would just want to get the path to the corpus/model. For that, use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text8_path = api.load('text8', return_path=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
