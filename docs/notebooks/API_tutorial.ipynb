{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using Gensim's API for downloading corpuses/models\n",
    "Let's start by importing the api module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets download the text8 corpus and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19 17:40:13,699 :gensim.api :INFO : Creating /home/chaitali/gensim-data/text8\n",
      "2017-09-19 17:40:13,707 :gensim.api :INFO : Creation of /home/chaitali/gensim-data/text8 successful.\n",
      "2017-09-19 17:40:13,713 :gensim.api :INFO : Downloading text8\n",
      "2017-09-19 17:46:24,545 :gensim.api :INFO : text8 downloaded\n",
      "2017-09-19 17:46:24,560 :gensim.api :INFO : Extracting files from /home/chaitali/gensim-data/text8\n",
      "2017-09-19 17:46:26,676 :gensim.api :INFO : text8 installed\n"
     ]
    }
   ],
   "source": [
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the corpus has been installed, let's create a word2vec model of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19 17:46:39,030 :gensim.models.word2vec :INFO : collecting all words and their counts\n",
      "2017-09-19 17:46:39,037 :gensim.models.word2vec :INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-19 17:46:46,104 :gensim.models.word2vec :INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-09-19 17:46:46,105 :gensim.models.word2vec :INFO : Loading a fresh vocabulary\n",
      "2017-09-19 17:46:46,393 :gensim.models.word2vec :INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-09-19 17:46:46,394 :gensim.models.word2vec :INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-09-19 17:46:46,607 :gensim.models.word2vec :INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-09-19 17:46:46,618 :gensim.models.word2vec :INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-09-19 17:46:46,620 :gensim.models.word2vec :INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-09-19 17:46:46,621 :gensim.models.word2vec :INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-09-19 17:46:46,946 :gensim.models.word2vec :INFO : resetting layer weights\n",
      "2017-09-19 17:46:48,052 :gensim.models.word2vec :INFO : training model with 3 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-19 17:46:49,058 :gensim.models.word2vec :INFO : PROGRESS: at 1.14% examples, 707464 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:50,070 :gensim.models.word2vec :INFO : PROGRESS: at 2.33% examples, 716418 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:51,081 :gensim.models.word2vec :INFO : PROGRESS: at 3.50% examples, 720064 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:52,084 :gensim.models.word2vec :INFO : PROGRESS: at 4.68% examples, 724069 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:53,087 :gensim.models.word2vec :INFO : PROGRESS: at 5.83% examples, 724165 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:54,090 :gensim.models.word2vec :INFO : PROGRESS: at 7.00% examples, 726206 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:46:55,094 :gensim.models.word2vec :INFO : PROGRESS: at 8.15% examples, 725286 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:56,096 :gensim.models.word2vec :INFO : PROGRESS: at 9.30% examples, 724925 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:46:57,114 :gensim.models.word2vec :INFO : PROGRESS: at 10.18% examples, 704674 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:58,130 :gensim.models.word2vec :INFO : PROGRESS: at 11.37% examples, 707549 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:46:59,136 :gensim.models.word2vec :INFO : PROGRESS: at 12.30% examples, 696048 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:00,140 :gensim.models.word2vec :INFO : PROGRESS: at 13.47% examples, 699081 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:01,150 :gensim.models.word2vec :INFO : PROGRESS: at 14.63% examples, 700492 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:02,151 :gensim.models.word2vec :INFO : PROGRESS: at 15.79% examples, 701182 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:03,156 :gensim.models.word2vec :INFO : PROGRESS: at 16.95% examples, 702555 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:04,161 :gensim.models.word2vec :INFO : PROGRESS: at 18.13% examples, 704501 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:05,162 :gensim.models.word2vec :INFO : PROGRESS: at 19.04% examples, 696025 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:47:06,176 :gensim.models.word2vec :INFO : PROGRESS: at 19.96% examples, 689054 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:07,183 :gensim.models.word2vec :INFO : PROGRESS: at 21.15% examples, 691295 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:08,185 :gensim.models.word2vec :INFO : PROGRESS: at 22.30% examples, 692243 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:09,192 :gensim.models.word2vec :INFO : PROGRESS: at 23.47% examples, 693696 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:10,211 :gensim.models.word2vec :INFO : PROGRESS: at 24.64% examples, 695112 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:11,219 :gensim.models.word2vec :INFO : PROGRESS: at 25.77% examples, 695601 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:12,237 :gensim.models.word2vec :INFO : PROGRESS: at 26.90% examples, 695914 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:13,241 :gensim.models.word2vec :INFO : PROGRESS: at 28.09% examples, 697735 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:14,260 :gensim.models.word2vec :INFO : PROGRESS: at 29.25% examples, 698509 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:15,260 :gensim.models.word2vec :INFO : PROGRESS: at 30.36% examples, 698320 words/s, in_qsize 5, out_qsize 2\n",
      "2017-09-19 17:47:16,273 :gensim.models.word2vec :INFO : PROGRESS: at 31.15% examples, 690867 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:17,293 :gensim.models.word2vec :INFO : PROGRESS: at 31.72% examples, 679085 words/s, in_qsize 6, out_qsize 1\n",
      "2017-09-19 17:47:18,311 :gensim.models.word2vec :INFO : PROGRESS: at 32.31% examples, 668474 words/s, in_qsize 5, out_qsize 1\n",
      "2017-09-19 17:47:19,318 :gensim.models.word2vec :INFO : PROGRESS: at 32.98% examples, 660311 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:20,320 :gensim.models.word2vec :INFO : PROGRESS: at 34.05% examples, 660626 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:21,328 :gensim.models.word2vec :INFO : PROGRESS: at 35.19% examples, 661837 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:22,350 :gensim.models.word2vec :INFO : PROGRESS: at 36.01% examples, 656869 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:47:23,358 :gensim.models.word2vec :INFO : PROGRESS: at 36.74% examples, 651028 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:24,365 :gensim.models.word2vec :INFO : PROGRESS: at 37.44% examples, 644886 words/s, in_qsize 3, out_qsize 1\n",
      "2017-09-19 17:47:25,381 :gensim.models.word2vec :INFO : PROGRESS: at 38.11% examples, 638630 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:26,384 :gensim.models.word2vec :INFO : PROGRESS: at 38.73% examples, 631930 words/s, in_qsize 4, out_qsize 0\n",
      "2017-09-19 17:47:27,386 :gensim.models.word2vec :INFO : PROGRESS: at 39.56% examples, 629025 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:47:28,403 :gensim.models.word2vec :INFO : PROGRESS: at 40.40% examples, 626116 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:29,412 :gensim.models.word2vec :INFO : PROGRESS: at 41.48% examples, 626962 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:30,422 :gensim.models.word2vec :INFO : PROGRESS: at 42.68% examples, 629634 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:31,439 :gensim.models.word2vec :INFO : PROGRESS: at 43.62% examples, 628434 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:32,451 :gensim.models.word2vec :INFO : PROGRESS: at 44.30% examples, 623710 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:33,494 :gensim.models.word2vec :INFO : PROGRESS: at 45.24% examples, 622439 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:34,500 :gensim.models.word2vec :INFO : PROGRESS: at 46.14% examples, 621141 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:35,501 :gensim.models.word2vec :INFO : PROGRESS: at 47.16% examples, 621680 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:36,502 :gensim.models.word2vec :INFO : PROGRESS: at 47.96% examples, 619190 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:37,507 :gensim.models.word2vec :INFO : PROGRESS: at 49.01% examples, 619861 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:38,516 :gensim.models.word2vec :INFO : PROGRESS: at 49.65% examples, 615540 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:39,538 :gensim.models.word2vec :INFO : PROGRESS: at 50.32% examples, 611462 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:40,554 :gensim.models.word2vec :INFO : PROGRESS: at 51.06% examples, 608521 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:41,559 :gensim.models.word2vec :INFO : PROGRESS: at 52.02% examples, 608259 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:42,575 :gensim.models.word2vec :INFO : PROGRESS: at 52.82% examples, 606121 words/s, in_qsize 5, out_qsize 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19 17:47:43,592 :gensim.models.word2vec :INFO : PROGRESS: at 53.52% examples, 602970 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:44,647 :gensim.models.word2vec :INFO : PROGRESS: at 54.27% examples, 600118 words/s, in_qsize 6, out_qsize 1\n",
      "2017-09-19 17:47:45,650 :gensim.models.word2vec :INFO : PROGRESS: at 55.16% examples, 599088 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:46,652 :gensim.models.word2vec :INFO : PROGRESS: at 56.32% examples, 601102 words/s, in_qsize 4, out_qsize 0\n",
      "2017-09-19 17:47:47,668 :gensim.models.word2vec :INFO : PROGRESS: at 57.50% examples, 603159 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:48,671 :gensim.models.word2vec :INFO : PROGRESS: at 58.66% examples, 605176 words/s, in_qsize 5, out_qsize 1\n",
      "2017-09-19 17:47:49,677 :gensim.models.word2vec :INFO : PROGRESS: at 59.84% examples, 607140 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:50,686 :gensim.models.word2vec :INFO : PROGRESS: at 61.02% examples, 609189 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:51,686 :gensim.models.word2vec :INFO : PROGRESS: at 61.96% examples, 608709 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:47:52,688 :gensim.models.word2vec :INFO : PROGRESS: at 62.94% examples, 608671 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:53,697 :gensim.models.word2vec :INFO : PROGRESS: at 63.97% examples, 609169 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:54,707 :gensim.models.word2vec :INFO : PROGRESS: at 65.16% examples, 611179 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:55,708 :gensim.models.word2vec :INFO : PROGRESS: at 66.31% examples, 612923 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:56,709 :gensim.models.word2vec :INFO : PROGRESS: at 67.30% examples, 613086 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:57,718 :gensim.models.word2vec :INFO : PROGRESS: at 68.48% examples, 614770 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:47:58,725 :gensim.models.word2vec :INFO : PROGRESS: at 69.42% examples, 614374 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:47:59,730 :gensim.models.word2vec :INFO : PROGRESS: at 70.35% examples, 613877 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:00,753 :gensim.models.word2vec :INFO : PROGRESS: at 71.09% examples, 611666 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:48:01,767 :gensim.models.word2vec :INFO : PROGRESS: at 72.06% examples, 611557 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:02,774 :gensim.models.word2vec :INFO : PROGRESS: at 72.92% examples, 610505 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:03,778 :gensim.models.word2vec :INFO : PROGRESS: at 73.80% examples, 609723 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:04,784 :gensim.models.word2vec :INFO : PROGRESS: at 74.81% examples, 610019 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:48:05,787 :gensim.models.word2vec :INFO : PROGRESS: at 75.87% examples, 610421 words/s, in_qsize 3, out_qsize 2\n",
      "2017-09-19 17:48:06,795 :gensim.models.word2vec :INFO : PROGRESS: at 76.87% examples, 610533 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:07,808 :gensim.models.word2vec :INFO : PROGRESS: at 78.06% examples, 612109 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:08,815 :gensim.models.word2vec :INFO : PROGRESS: at 79.07% examples, 612233 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:48:09,823 :gensim.models.word2vec :INFO : PROGRESS: at 80.27% examples, 613807 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:10,833 :gensim.models.word2vec :INFO : PROGRESS: at 81.26% examples, 613696 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:11,842 :gensim.models.word2vec :INFO : PROGRESS: at 82.45% examples, 615110 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:48:12,851 :gensim.models.word2vec :INFO : PROGRESS: at 83.61% examples, 616375 words/s, in_qsize 6, out_qsize 1\n",
      "2017-09-19 17:48:13,853 :gensim.models.word2vec :INFO : PROGRESS: at 84.77% examples, 617712 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:14,879 :gensim.models.word2vec :INFO : PROGRESS: at 85.67% examples, 616893 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:15,890 :gensim.models.word2vec :INFO : PROGRESS: at 86.64% examples, 616837 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:16,906 :gensim.models.word2vec :INFO : PROGRESS: at 87.69% examples, 617193 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:48:17,915 :gensim.models.word2vec :INFO : PROGRESS: at 88.79% examples, 617970 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:18,917 :gensim.models.word2vec :INFO : PROGRESS: at 89.78% examples, 618000 words/s, in_qsize 6, out_qsize 0\n",
      "2017-09-19 17:48:19,918 :gensim.models.word2vec :INFO : PROGRESS: at 90.91% examples, 618999 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:20,922 :gensim.models.word2vec :INFO : PROGRESS: at 91.96% examples, 619363 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:21,936 :gensim.models.word2vec :INFO : PROGRESS: at 92.85% examples, 618640 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:22,943 :gensim.models.word2vec :INFO : PROGRESS: at 93.84% examples, 618597 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:23,958 :gensim.models.word2vec :INFO : PROGRESS: at 94.85% examples, 618687 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:24,974 :gensim.models.word2vec :INFO : PROGRESS: at 95.90% examples, 618756 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:48:25,978 :gensim.models.word2vec :INFO : PROGRESS: at 96.93% examples, 619000 words/s, in_qsize 4, out_qsize 1\n",
      "2017-09-19 17:48:26,980 :gensim.models.word2vec :INFO : PROGRESS: at 97.93% examples, 619074 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:27,982 :gensim.models.word2vec :INFO : PROGRESS: at 98.99% examples, 619431 words/s, in_qsize 5, out_qsize 0\n",
      "2017-09-19 17:48:28,823 :gensim.models.word2vec :INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-19 17:48:28,830 :gensim.models.word2vec :INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-19 17:48:28,836 :gensim.models.word2vec :INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-19 17:48:28,837 :gensim.models.word2vec :INFO : training on 85026035 raw words (62526300 effective words) took 100.8s, 620401 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word2vec model, let's find words that are similar to 'tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19 17:48:45,837 :gensim.models.keyedvectors :INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('leaf', 0.7284336090087891),\n",
       " ('trees', 0.7024068236351013),\n",
       " ('bark', 0.6984879970550537),\n",
       " ('fruit', 0.623538613319397),\n",
       " ('flower', 0.6177238821983337),\n",
       " ('nest', 0.6133654713630676),\n",
       " ('garden', 0.5962027311325073),\n",
       " ('avl', 0.5909914374351501),\n",
       " ('cave', 0.5902420282363892),\n",
       " ('pond', 0.5827507972717285)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the API to download many corpuses and models. You can get the list of all the models and corpuses that are provided, by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gensim\": {\n",
      "        \"model\": {\n",
      "            \"Google_News_word2vec\": {\n",
      "                \"desc\": \"Google has published pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\",\n",
      "                \"filename\": \"GoogleNews-vectors-negative300.bin.gz\",\n",
      "                \"checksum\": \"4fa963d128fe65ec8cd5dd4d9377f8ed\"\n",
      "            },\n",
      "            \"fasttext_eng_model\": {\n",
      "                \"desc\": \"fastText is a library for efficient learning of word representations and sentence classification.These vectors for english language in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.\",\n",
      "                \"filename\": \"wiki.en.vec\",\n",
      "                \"checksum\": \"2de532213d7fa8b937263337c6e9deeb\"\n",
      "            },\n",
      "            \"glove_common_crawl_42B\": {\n",
      "                \"desc\": \"This model is trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.42B.300d.zip\",\n",
      "                \"checksum\": \"d6f41a6e9e5bf905d349a01b5216826a\"\n",
      "            },\n",
      "            \"glove_common_crawl_840B\": {\n",
      "                \"desc\": \"This model is trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.840B.300d.zip\",\n",
      "                \"checksum\": \"72f02c239743c750eaea8747839e4852\"\n",
      "            },\n",
      "            \"glove_wiki_gigaword_300d\": {\n",
      "                \"desc\": \" This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 300d vectors).GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.6B.300d.txt\",\n",
      "                \"checksum\": \"e0c1af43ab57753d11da2fa642c3ff82\"\n",
      "            },\n",
      "            \"glove_wiki_gigaword_200d\": {\n",
      "                \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 200d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.6B.200d.txt\",\n",
      "                \"checksum\": \"c4e58068e16be476b115699f94fa82cb\"\n",
      "            },\n",
      "            \"glove_wiki_gigaword_100d\": {\n",
      "                \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 100d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.6B.100d.txt\",\n",
      "                \"checksum\": \"7067a76b2adc0e92a1f71e2919382c95\"\n",
      "            },\n",
      "            \"glove_wiki_gigaword_50d\": {\n",
      "                \"desc\": \"This model is trained on Wikipedia 2014 + Gigaword 56B tokens, 400K vocab, uncased, 50d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.6B.50d.txt\",\n",
      "                \"checksum\": \"44d71eb1db9485d9c8a605a5ed560d8c\"\n",
      "            },\n",
      "            \"glove_twitter_200d\": {\n",
      "                \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.twitter.27B.200d.txt\",\n",
      "                \"checksum\": \"91b40581d04e2ff5306d2f0452e34f72\"\n",
      "            },\n",
      "            \"glove_twitter_100d\": {\n",
      "                \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 100d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.twitter.27B.100d.txt\",\n",
      "                \"checksum\": \"2825c182e4ac2afd8d2dede8445919ab\"\n",
      "            },\n",
      "            \"glove_twitter_50d\": {\n",
      "                \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 50d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.twitter.27B.50d.txt\",\n",
      "                \"checksum\": \"9842275a894ebdfb60b270877bb8f60c\"\n",
      "            },\n",
      "            \"glove_twitter_25d\": {\n",
      "                \"desc\": \"This model is trained on twitter(2B tweets, 27B tokens, 1.2M vocab, uncased, 25d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\",\n",
      "                \"filename\": \"glove.twitter.27B.25d.txt\",\n",
      "                \"checksum\": \"9802ffec313d8612bf790d1aa4d37ddd\"\n",
      "            }\n",
      "        },\n",
      "        \"corpus\": {\n",
      "            \"text8\": {\n",
      "                \"desc\": \"Wikipedia English corpus\",\n",
      "                \"filename\": \"text8\",\n",
      "                \"checksum\": \"5d703f1842fb1ca55bf86f2e2552012c\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset_list = api.info()\n",
    "print(json.dumps(dataset_list, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get detailed information about the model/corpus, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19 21:56:42,071 :gensim.api :INFO : This model is trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "api.info('glove_common_crawl_42B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you do not want to load the corpus/model to memory. You would just want to get the path to the corpus/model. For that, use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text8_path = api.load('text8', return_path=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
