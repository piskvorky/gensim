{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using Gensim's API for downloading corpuses/models\n",
    "Let's start by importing the api module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets download the text8 corpus. For that, you have to use the load function. If the dataset has been already downloaded, then corpus path will be returned. Otherwise,  dataset will be downloaded and then path will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 00:54:48,792 : INFO : Downloading text8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.7/31.7MB downloaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 00:55:11,503 : INFO : text8 downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to corpus:  /home/chaitali/gensim-data/text8/text8\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "corpus_path = api.load('text8')\n",
    "print(\"Path to corpus: \", corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the corpus has been downloaded and extracted, let's create a word2vec model of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 00:55:47,594 : INFO : collecting all words and their counts\n",
      "2017-10-25 00:55:47,600 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-25 00:55:55,687 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-10-25 00:55:55,688 : INFO : Loading a fresh vocabulary\n",
      "2017-10-25 00:55:55,968 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-10-25 00:55:55,969 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-10-25 00:55:56,172 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-10-25 00:55:56,181 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-10-25 00:55:56,181 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-10-25 00:55:56,182 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-10-25 00:55:56,474 : INFO : resetting layer weights\n",
      "2017-10-25 00:55:57,350 : INFO : training model with 3 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-10-25 00:55:58,358 : INFO : PROGRESS: at 1.19% examples, 736000 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:55:59,368 : INFO : PROGRESS: at 2.06% examples, 633832 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:00,373 : INFO : PROGRESS: at 3.25% examples, 667998 words/s, in_qsize 5, out_qsize 1\n",
      "2017-10-25 00:56:01,375 : INFO : PROGRESS: at 4.44% examples, 688247 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:02,376 : INFO : PROGRESS: at 5.62% examples, 699262 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:03,376 : INFO : PROGRESS: at 6.81% examples, 708000 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:04,403 : INFO : PROGRESS: at 7.70% examples, 684594 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:05,407 : INFO : PROGRESS: at 8.34% examples, 648645 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:06,415 : INFO : PROGRESS: at 9.54% examples, 659722 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:07,427 : INFO : PROGRESS: at 10.72% examples, 667502 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:08,434 : INFO : PROGRESS: at 11.91% examples, 674170 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:09,441 : INFO : PROGRESS: at 13.12% examples, 680713 words/s, in_qsize 3, out_qsize 0\n",
      "2017-10-25 00:56:10,443 : INFO : PROGRESS: at 14.30% examples, 685139 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:11,455 : INFO : PROGRESS: at 15.51% examples, 688569 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:12,455 : INFO : PROGRESS: at 16.71% examples, 692452 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:13,456 : INFO : PROGRESS: at 17.90% examples, 695587 words/s, in_qsize 4, out_qsize 0\n",
      "2017-10-25 00:56:14,456 : INFO : PROGRESS: at 19.07% examples, 697658 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:15,460 : INFO : PROGRESS: at 20.27% examples, 700206 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:16,477 : INFO : PROGRESS: at 21.50% examples, 702899 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:17,495 : INFO : PROGRESS: at 22.69% examples, 704046 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:18,499 : INFO : PROGRESS: at 23.90% examples, 706366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:19,502 : INFO : PROGRESS: at 25.08% examples, 707804 words/s, in_qsize 5, out_qsize 1\n",
      "2017-10-25 00:56:20,514 : INFO : PROGRESS: at 26.27% examples, 709358 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:21,519 : INFO : PROGRESS: at 27.45% examples, 710918 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:22,536 : INFO : PROGRESS: at 28.67% examples, 712387 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:23,537 : INFO : PROGRESS: at 29.85% examples, 713647 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:24,537 : INFO : PROGRESS: at 31.02% examples, 714298 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:25,542 : INFO : PROGRESS: at 32.16% examples, 714256 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:26,551 : INFO : PROGRESS: at 33.33% examples, 714777 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:27,555 : INFO : PROGRESS: at 34.51% examples, 715477 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-25 00:56:28,560 : INFO : PROGRESS: at 35.72% examples, 716204 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:29,571 : INFO : PROGRESS: at 36.88% examples, 716264 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:30,579 : INFO : PROGRESS: at 38.08% examples, 717179 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:31,581 : INFO : PROGRESS: at 39.24% examples, 717091 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:32,593 : INFO : PROGRESS: at 40.19% examples, 713299 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:33,593 : INFO : PROGRESS: at 40.87% examples, 705355 words/s, in_qsize 6, out_qsize 1\n",
      "2017-10-25 00:56:34,596 : INFO : PROGRESS: at 41.92% examples, 703678 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:35,628 : INFO : PROGRESS: at 42.99% examples, 702102 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:36,633 : INFO : PROGRESS: at 43.97% examples, 699841 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:37,647 : INFO : PROGRESS: at 45.14% examples, 700441 words/s, in_qsize 5, out_qsize 1\n",
      "2017-10-25 00:56:38,665 : INFO : PROGRESS: at 45.90% examples, 694902 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:39,667 : INFO : PROGRESS: at 46.57% examples, 688487 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-25 00:56:40,673 : INFO : PROGRESS: at 47.30% examples, 683151 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:41,675 : INFO : PROGRESS: at 48.01% examples, 677667 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:42,688 : INFO : PROGRESS: at 49.16% examples, 678471 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:43,695 : INFO : PROGRESS: at 50.29% examples, 679025 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:44,708 : INFO : PROGRESS: at 51.49% examples, 680405 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:45,710 : INFO : PROGRESS: at 52.66% examples, 681577 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:46,724 : INFO : PROGRESS: at 53.85% examples, 682664 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:47,753 : INFO : PROGRESS: at 54.65% examples, 678686 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:48,768 : INFO : PROGRESS: at 55.43% examples, 674446 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:49,784 : INFO : PROGRESS: at 56.37% examples, 672526 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:50,791 : INFO : PROGRESS: at 57.52% examples, 673311 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:51,796 : INFO : PROGRESS: at 58.65% examples, 673821 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:52,801 : INFO : PROGRESS: at 59.78% examples, 674243 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:56:53,825 : INFO : PROGRESS: at 60.74% examples, 672693 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:54,829 : INFO : PROGRESS: at 61.89% examples, 673317 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:55,830 : INFO : PROGRESS: at 63.09% examples, 674568 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:56,837 : INFO : PROGRESS: at 64.24% examples, 675262 words/s, in_qsize 4, out_qsize 0\n",
      "2017-10-25 00:56:57,838 : INFO : PROGRESS: at 65.33% examples, 675366 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:58,851 : INFO : PROGRESS: at 66.35% examples, 674781 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:56:59,864 : INFO : PROGRESS: at 67.03% examples, 670749 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:00,874 : INFO : PROGRESS: at 68.14% examples, 670995 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:57:01,877 : INFO : PROGRESS: at 69.29% examples, 671790 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:02,885 : INFO : PROGRESS: at 70.46% examples, 672715 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:03,887 : INFO : PROGRESS: at 71.63% examples, 673593 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:57:04,896 : INFO : PROGRESS: at 72.82% examples, 674561 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:05,902 : INFO : PROGRESS: at 74.02% examples, 675632 words/s, in_qsize 6, out_qsize 1\n",
      "2017-10-25 00:57:06,903 : INFO : PROGRESS: at 75.17% examples, 676178 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 00:57:07,903 : INFO : PROGRESS: at 76.34% examples, 676871 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:08,911 : INFO : PROGRESS: at 77.54% examples, 677805 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:09,915 : INFO : PROGRESS: at 78.72% examples, 678531 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:10,923 : INFO : PROGRESS: at 79.89% examples, 679198 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:57:11,929 : INFO : PROGRESS: at 81.09% examples, 680040 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:12,933 : INFO : PROGRESS: at 82.29% examples, 680809 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:13,939 : INFO : PROGRESS: at 83.47% examples, 681462 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:14,960 : INFO : PROGRESS: at 84.68% examples, 682292 words/s, in_qsize 6, out_qsize 1\n",
      "2017-10-25 00:57:15,970 : INFO : PROGRESS: at 85.88% examples, 683163 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:16,985 : INFO : PROGRESS: at 87.04% examples, 683714 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:17,992 : INFO : PROGRESS: at 88.25% examples, 684584 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-25 00:57:19,000 : INFO : PROGRESS: at 89.41% examples, 685015 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:20,006 : INFO : PROGRESS: at 90.58% examples, 685623 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:21,012 : INFO : PROGRESS: at 91.75% examples, 686097 words/s, in_qsize 4, out_qsize 1\n",
      "2017-10-25 00:57:22,015 : INFO : PROGRESS: at 92.89% examples, 686426 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:23,016 : INFO : PROGRESS: at 94.07% examples, 687092 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:24,019 : INFO : PROGRESS: at 95.27% examples, 687694 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:25,026 : INFO : PROGRESS: at 96.39% examples, 687666 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:26,035 : INFO : PROGRESS: at 97.50% examples, 687629 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:27,044 : INFO : PROGRESS: at 98.61% examples, 687673 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-25 00:57:28,052 : INFO : PROGRESS: at 99.75% examples, 687836 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-25 00:57:28,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-25 00:57:28,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-25 00:57:28,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-25 00:57:28,276 : INFO : training on 85026035 raw words (62539321 effective words) took 90.9s, 687833 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "corpus = Text8Corpus(corpus_path)\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word2vec model, let's find words that are similar to 'tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 01:00:28,412 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('trees', 0.6827899217605591),\n",
       " ('leaf', 0.6613928079605103),\n",
       " ('avl', 0.6607560515403748),\n",
       " ('bark', 0.6449544429779053),\n",
       " ('bird', 0.6373804807662964),\n",
       " ('flower', 0.6225261688232422),\n",
       " ('fruit', 0.6008641719818115),\n",
       " ('grass', 0.5877920985221863),\n",
       " ('cave', 0.5793893933296204),\n",
       " ('cactus', 0.5789991021156311)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the API to download many corpora and models. You can get the list of all the models and corpora that are provided, by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"corpora\": {\n",
      "        \"text8\": {\n",
      "            \"description\": \"Cleaned small sample from wikipedia\",\n",
      "            \"checksum\": \"f407f5aed497fc3b0fb33b98c4f9d855\",\n",
      "            \"file_name\": \"text8\"\n",
      "        },\n",
      "        \"fake-news\": {\n",
      "            \"description\": \"It contains text and metadata scraped from 244 websites tagged as 'bullshit' here by the BS Detector Chrome Extension by Daniel Sieradski.\",\n",
      "            \"checksum\": \"a61f985190ba361defdfc3fef616b9cd\",\n",
      "            \"file_name\": \"fake.csv\",\n",
      "            \"source\": \"Kaggle\"\n",
      "        }\n",
      "    },\n",
      "    \"models\": {\n",
      "        \"glove-wiki-gigaword-50\": {\n",
      "            \"description\": \"Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"parameters\": \"dimension = 50\",\n",
      "            \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`\",\n",
      "            \"papers\": \"https://nlp.stanford.edu/pubs/glove.pdf\",\n",
      "            \"checksum\": \"b269a9c8b16c3178d3d0651fd68c3fe9\",\n",
      "            \"file_name\": \"glove-wiki-gigaword-50.txt\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_list = api.info()\n",
    "print(json.dumps(data_list, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get detailed information about the model/corpus, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': \"It contains text and metadata scraped from 244 websites tagged as 'bullshit' here by the BS Detector Chrome Extension by Daniel Sieradski.\", 'checksum': 'a61f985190ba361defdfc3fef616b9cd', 'file_name': 'fake.csv', 'source': 'Kaggle'}\n"
     ]
    }
   ],
   "source": [
    "fake_news_info = api.info('fake-news')\n",
    "print(fake_news_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you do not want to load the model to memory. You would just want to get the path to the model. For that, use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chaitali/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.txt\n"
     ]
    }
   ],
   "source": [
    "print(api.load('glove-wiki-gigaword-50', return_path=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load the model to memory, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plastic', 0.7942505478858948),\n",
       " ('metal', 0.770871639251709),\n",
       " ('walls', 0.7700636386871338),\n",
       " ('marble', 0.7638523578643799),\n",
       " ('wood', 0.7624280452728271),\n",
       " ('ceramic', 0.7602593302726746),\n",
       " ('pieces', 0.7589111924171448),\n",
       " ('stained', 0.7528817057609558),\n",
       " ('tile', 0.748193621635437),\n",
       " ('furniture', 0.7463858723640442)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "model.most_similar(\"glass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In corpora, the corpus is never loaded to memory. Always,(i.e it doesn't matter if return_path is True or false) the path to the dataset file/folder will be returned and then you will hve to load it yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
