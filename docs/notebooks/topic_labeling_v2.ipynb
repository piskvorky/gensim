{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels pool preparation**\n",
    "\n",
    "Labels pool created from a raw Wikipedia dump\n",
    "Wikipedia portals (article with title starting from Portal:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 146
      },
      {
       "item_id": 26898
      },
      {
       "item_id": 26899
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7277787,
     "status": "ok",
     "timestamp": 1522652208775,
     "user": {
      "displayName": "Arie Genkin",
      "photoUrl": "//lh4.googleusercontent.com/-nRWkZzpaNBo/AAAAAAAAAAI/AAAAAAAADHE/K1FmMgoGSkc/s50-c-k-no/photo.jpg",
      "userId": "100233373637533805591"
     },
     "user_tz": -180
    },
    "id": "SSYHe7wtspN5",
    "outputId": "64d7be3e-eae1-43d9-c0d2-4a2ea96ade7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-02 04:55:31--  https://dumps.wikimedia.org/enwiki/20180301/enwiki-20180301-pages-articles.xml.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.11, 2620:0:861:1:208:80:154:11\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14833155655 (14G) [application/octet-stream]\n",
      "Saving to: ‘enwiki-20180301-pages-articles.xml.bz2’\n",
      "\n",
      "-20180301-pages-art   0%[                    ]  77.41M  1.98MB/s    eta 1h 57m enwiki-20180301-pag 100%[===================>]  13.81G  1.94MB/s    in 2h 1m   \n",
      "\n",
      "2018-04-02 06:56:46 (1.94 MB/s) - ‘enwiki-20180301-pages-articles.xml.bz2’ saved [14833155655/14833155655]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download raw Wikipedia dump\n",
    "!wget --continue https://dumps.wikimedia.org/enwiki/20180301/enwiki-20180301-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lWK5wjq8vNxH"
   },
   "outputs": [],
   "source": [
    "# create labels pool\n",
    "# from Wikipedia Portal titles\n",
    "import bz2\n",
    "import gensim\n",
    "\n",
    "# wikipedia dump\n",
    "f = bz2.BZ2File('enwiki-20180301-pages-articles.xml.bz2')\n",
    "\n",
    "# create iterator traversing on pages\n",
    "pages = gensim.corpora.wikicorpus.extract_pages(f,('0',))\n",
    "\n",
    "# extract page titles starting from Portal: and prune the titles\n",
    "ns = 'Portal:'\n",
    "labels = [title[len(ns):].split('/')[0].lower().strip('\\n') for title,_ in pages if title.startswith(ns)]\n",
    "\n",
    "# remove duplicates\n",
    "labels = list(set(labels))\n",
    "\n",
    "# save Labels to a file\n",
    "with open('labels.txt', 'w', encoding='utf-8') as labels_file:\n",
    "    for label in labels:\n",
    "        labels_file.write(label + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build special word embeddings, as unified embedding space for words ANS labels from Label pool**\n",
    "\n",
    "Labels from label pool converted to Phrases, i.e. joined by _ delimiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SlO1YLMX_S7d"
   },
   "outputs": [],
   "source": [
    "# load preprocessed Wikipedia texts as a preparation for word embeddings\n",
    "import gensim.downloader\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "data = gensim.downloader.load(\"wiki-english-20171001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DDqoO37NpJrj"
   },
   "outputs": [],
   "source": [
    "# Prepare two temporary lists\n",
    "# - list of multi-word labels, to replace these labels in the texts\n",
    "# - list of delimeter (_) joined labels, to search closest embeddings within this list\n",
    "long_labels = []\n",
    "labels_vocab = []\n",
    "with open('labels.txt') as labels_file:\n",
    "    for line in labels_file:\n",
    "        label = line.strip('\\n\\r').decode('utf-8')\n",
    "      \n",
    "        if len(label.split(' '))>1:\n",
    "            long_labels.append(label)\n",
    "      \n",
    "        labels_vocab.append('_'.join(label.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sGQYLjN6jQlr"
   },
   "outputs": [],
   "source": [
    "# iterator replacing labels in articles with corresponding phrases\n",
    "#\n",
    "# traverse articles\n",
    "#     in each article, find labels from the label pool, and replace them with label phrases\n",
    "#     yield processed articles\n",
    "class LabelsToPhrases(object):\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def __iter__(self):\n",
    "        for article in self.corpus:\n",
    "\n",
    "            # conctatenate all texts related to an article - article title, section titles, and section texts\n",
    "            text = article['title'] + ' '.join([ ' ' + section_title + section_text for section_title, section_text in zip(article['section_titles'], article['section_texts'])])\n",
    "\n",
    "            # clean-up: remove multiple blanks, lower()\n",
    "            text = ' '.join(text.lower().split())\n",
    "\n",
    "            # find in an articel text all instances of labels from the labels pool\n",
    "            # and replace these by corresponding phrases, i.e. one token with words separated by a delimiter\n",
    "            for label in long_labels:\n",
    "                text = text.replace( label, '_'.join(label.split()))\n",
    "\n",
    "            # clean up the resulting text\n",
    "            yield preprocess_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2N0gCgHQpG8C"
   },
   "outputs": [],
   "source": [
    "# callback to save w2v model on end of each training iteration\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    \"Callback to save model after every epoch\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "        output_path = 'epoch{}.model'.format(self.epoch)\n",
    "        print(\"Save model to {}\".format(output_path))\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1\n",
    "\n",
    "epoch_saver = EpochSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6cybU6xP2Upg"
   },
   "outputs": [],
   "source": [
    "# build w2v model including labels\n",
    "# Approach #1 - straightforward\n",
    "# TAKES TOO LONG, NEVER ENDS\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "embedding = gensim.models.Word2Vec( LabelsToPhrases(data), min_count=1, workers=cpu_count(), callbacks=[epoch_saver])   \n",
    "word_vectors = embedding.wv\n",
    "\n",
    "# save Embedding model\n",
    "word_vectors.save('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "s3uJsTjsGAdE"
   },
   "outputs": [],
   "source": [
    "# build w2v model including labels\n",
    "# Approach #2 - load vocab from a pre-trained w2v model\n",
    "# PROBLEM: reset_from raises 'Word2VecKeyedVectors' object has no attribute 'vocabulary'\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "ref_w2v = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "embedding = gensim.models.Word2Vec(min_count=1, workers=cpu_count(), callbacks=[epoch_saver])  \n",
    "embedding.reset_from(ref_w2v)\n",
    "embedding.build_vocab( [labels_vocab], update=True )  \n",
    "embedding.train( LabelsToPhrases(data) )  \n",
    "\n",
    "word_vectors = embedding.wv\n",
    "\n",
    "# save Embedding model\n",
    "word_vectors.save('embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7q4bXsGX0Iul"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def phrases(sents):\n",
    "    for sent in sents:\n",
    "        yield '_'.join(sent)\n",
    "\n",
    "topn = 10\n",
    "topic_words = ['venezuela', 'equador', 'colombia', 'brazil']\n",
    "topic_vector = reduce(lambda a,b: a+b, map(word_vectors.get_vector, topic_words)) / len(topic_words)\n",
    "\n",
    "dists = distances( topic_vector, other_entities=phrases(labels))\n",
    "\n",
    "for index in np.argsort(dists)[:topn]:\n",
    "    print( labels[index] )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "topiclabeling.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
