{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Model Selection Using Topic Coherence\n",
    "\n",
    "This notebook will perform topic modeling on the 20 Newsgroups corpus using LDA. We will perform model selection (over the number of topics) using topic coherence as our evaluation metric. This will showcase some of the features of the topic coherence pipeline implemented in `gensim`. In particular, we will see several features of the `CoherenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from gensim.corpora import TextCorpus, MmCorpus\n",
    "from gensim import utils, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import deaccent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Dataset\n",
    "\n",
    "The 20 Newsgroups dataset uses a hierarchical directory structure to store the articles. The structure looks something like this:\n",
    "```\n",
    "20news-18828/\n",
    "|-- alt.atheism\n",
    "|   |-- 49960\n",
    "|   |-- 51060\n",
    "|   |-- 51119\n",
    "|-- comp.graphics\n",
    "|   |-- 37261\n",
    "|   |-- 37913\n",
    "|   |-- 37914\n",
    "|-- comp.os.ms-windows.misc\n",
    "|   |-- 10000\n",
    "|   |-- 10001\n",
    "|   |-- 10002\n",
    "```\n",
    "\n",
    "The files are in the newsgroup markup format, which includes some headers, quoting of previous messages in the thread, and possibly PGP signature blocks. The message body itself is raw text, which requires preprocessing. The code immediately below is an adaptation of [an active PR](https://github.com/RaRe-Technologies/gensim/pull/1388) for parsing hierarchical directory structures into corpora. The code just below that builds on this basic corpus parser to handle the newsgroup-specific text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDirectoryCorpus(TextCorpus):\n",
    "    \"\"\"Read documents recursively from a directory,\n",
    "    where each file is interpreted as a plain text document.\n",
    "    \"\"\"\n",
    "    \n",
    "    def iter_filepaths(self):\n",
    "        \"\"\"Lazily yield paths to each file in the directory structure within the specified\n",
    "        range of depths. If a filename pattern to match was given, further filter to only\n",
    "        those filenames that match.\n",
    "        \"\"\"\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input):\n",
    "            for name in filenames:\n",
    "                yield os.path.join(dirpath, name)\n",
    "                \n",
    "    def getstream(self):\n",
    "        for path in self.iter_filepaths():\n",
    "            with utils.smart_open(path) as f:\n",
    "                doc_content = f.read()\n",
    "            yield doc_content\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = deaccent(\n",
    "            lower_to_unicode(\n",
    "                strip_multiple_whitespaces(text)))\n",
    "        tokens = simple_tokenize(text)\n",
    "        return remove_short(\n",
    "            remove_stopwords(tokens))\n",
    "        \n",
    "    def get_texts(self):\n",
    "        \"\"\"Iterate over the collection, yielding one document at a time. A document\n",
    "        is a sequence of words (strings) that can be fed into `Dictionary.doc2bow`.\n",
    "        Override this function to match your input (parse input files, do any\n",
    "        text preprocessing, lowercasing, tokenizing etc.). There will be no further\n",
    "        preprocessing of the words coming out of this function.\n",
    "        \"\"\"\n",
    "        lines = self.getstream()\n",
    "        if self.metadata:\n",
    "            for lineno, line in enumerate(lines):\n",
    "                yield self.preprocess_text(line), (lineno,)\n",
    "        else:\n",
    "            for line in lines:\n",
    "                yield self.preprocess_text(line)\n",
    "\n",
    "    \n",
    "def remove_stopwords(tokens, stopwords=STOPWORDS):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def remove_short(tokens, minsize=3):\n",
    "    return [token for token in tokens if len(token) >= minsize]\n",
    "\n",
    "def lower_to_unicode(text):\n",
    "    return utils.to_unicode(text.lower(), 'ascii', 'ignore')\n",
    "\n",
    "RE_WHITESPACE = re.compile(r\"(\\s)+\", re.UNICODE)\n",
    "def strip_multiple_whitespaces(text):\n",
    "    return RE_WHITESPACE.sub(\" \", text)\n",
    "\n",
    "PAT_ALPHABETIC = re.compile('(((?![\\d])\\w)+)', re.UNICODE)\n",
    "def simple_tokenize(text):\n",
    "    for match in PAT_ALPHABETIC.finditer(text):\n",
    "        yield match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NewsgroupCorpus(TextDirectoryCorpus):\n",
    "    \"\"\"Parse 20 Newsgroups dataset.\"\"\"\n",
    "\n",
    "    def extract_body(self, text):\n",
    "        return strip_newsgroup_header(\n",
    "            strip_newsgroup_footer(\n",
    "                strip_newsgroup_quoting(text)))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        body = self.extract_body(text)\n",
    "        return super(NewsgroupCorpus, self).preprocess_text(body)\n",
    "\n",
    "\n",
    "def strip_newsgroup_header(text):\n",
    "    \"\"\"Given text in \"news\" format, strip the headers, by removing everything\n",
    "    before the first blank line.\n",
    "    \"\"\"\n",
    "    _before, _blankline, after = text.partition('\\n\\n')\n",
    "    return after\n",
    "\n",
    "\n",
    "_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n",
    "                       r'|^In article|^Quoted from|^\\||^>)')\n",
    "def strip_newsgroup_quoting(text):\n",
    "    \"\"\"Given text in \"news\" format, strip lines beginning with the quote\n",
    "    characters > or |, plus lines that often introduce a quoted section\n",
    "    (for example, because they contain the string 'writes:'.)\n",
    "    \"\"\"\n",
    "    good_lines = [line for line in text.split('\\n')\n",
    "                  if not _QUOTE_RE.search(line)]\n",
    "    return '\\n'.join(good_lines)\n",
    "\n",
    "\n",
    "_PGP_SIG_BEGIN = \"-----BEGIN PGP SIGNATURE-----\"\n",
    "def strip_newsgroup_footer(text):\n",
    "    \"\"\"Given text in \"news\" format, attempt to remove a signature block.\"\"\"\n",
    "    try:\n",
    "        return text[:text.index(_PGP_SIG_BEGIN)]\n",
    "    except ValueError:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Now that we have defined the necessary code for parsing the dataset, let's load it up and serialize it into Matrix Market format. We'll do this because we want to train LDA on it with several different parameter settings, and this will allow us to avoid repeating the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace data_path with path to your own copy of the corpus.\n",
    "# You can download it from here: http://qwone.com/~jason/20Newsgroups/\n",
    "# I'm using the original, called: 20news-19997.tar.gz\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "data_dir = os.path.join(home, 'workshop', 'nlp', 'data')\n",
    "data_path = os.path.join(data_dir, '20_newsgroups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n",
      "Dictionary(24595 unique tokens: [u'woods', u'hanging', u'woody', u'localized', u'gaa']...)\n",
      "CPU times: user 21.7 s, sys: 2.88 s, total: 24.6 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = NewsgroupCorpus(data_path)\n",
    "corpus.dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "dictionary = corpus.dictionary\n",
    "print(len(corpus))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 842 ms, total: 23.6 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mm_path = os.path.join(data_dir, '20_newsgroups.mm')\n",
    "MmCorpus.serialize(mm_path, corpus, id2word=dictionary)\n",
    "mm_corpus = MmCorpus(mm_path)  # load back in to use for LDA training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "\n",
    "Our goal is to determine which number of topics produces the most coherent topics for the 20 Newsgroups corpus. The corpus is roughly 20,000 documents. If we used 100 topics and the documents were evenly distributed among topics, we'd have clusters of 200 documents. This seems like a reasonable upper bound. In this case, the corpus actually has categories, defined by the first-level directory structure. This can be seen in the directory structure shown above, and three examples are: `alt.atheism`, `comp.graphics`, and `comp.os.ms-windows.misc`. There are 20 of these (hence the name of the dataset), so we'll use 20 as our lower bound for the number of topics.\n",
    "\n",
    "One could argue that we already know the model should have 20 topics. I'll argue there may be additional categorizations within each newsgroup and we might hope to capture those by using more topics. We'll step by increments of 10 from 20 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA(k=20)\n",
      "Training LDA(k=30)\n",
      "Training LDA(k=40)\n",
      "Training LDA(k=50)\n",
      "Training LDA(k=60)\n",
      "Training LDA(k=70)\n",
      "Training LDA(k=80)\n",
      "Training LDA(k=90)\n",
      "Training LDA(k=100)\n",
      "CPU times: user 20min 38s, sys: 3min 16s, total: 23min 55s\n",
      "Wall time: 23min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trained_models = {}\n",
    "for num_topics in range(20, 101, 10):\n",
    "    print(\"Training LDA(k=%d)\" % num_topics)\n",
    "    lda = models.LdaMulticore(\n",
    "        mm_corpus, id2word=dictionary, num_topics=num_topics, workers=4,\n",
    "        passes=10, iterations=100, random_state=42, eval_every=None,\n",
    "        alpha='asymmetric',  # shown to be better than symmetric in most cases\n",
    "        decay=0.5, offset=64  # best params from Hoffman paper\n",
    "    )\n",
    "    trained_models[num_topics] = lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Using Coherence\n",
    "\n",
    "Now we get to the heart of this notebook. In this section, we'll evaluate each of our LDA models using topic coherence. Coherence is a measure of how interpretable the topics are to humans. It is based on the representation of topics as the top-N most probable words for a particular topic. More specifically, given the topic-term matrix for LDA, we sort each topic from highest to lowest term weights and then select the first N terms.\n",
    "\n",
    "Coherence essentially measures how similar these words are to each other. There are various methods for doing this, most of which have been explored in the paper [\"Exploring the Space of Topic Coherence Measures\"](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf). The authors performed a comparative analysis of various methods, correlating them to human judgements. The method named \"c_v\" coherence was found to be the most highly correlated. This and several of the other methods have been implemented in `gensim.models.CoherenceModel`. We will use this to perform our evaluations.\n",
    "\n",
    "The \"c_v\" coherence method makes an expensive pass over the corpus, accumulating term occurrence and co-occurrence counts. It only accumulates counts for the terms in the lists of top-N terms for each topic. In order to ensure we only need to make one pass, we'll construct a \"super topic\" from the top-N lists of each of the models. This will consist of a single topic with all the relevant terms from all the models. We choose 20 as N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant terms: 2714\n"
     ]
    }
   ],
   "source": [
    "# Build topic listings from each model.\n",
    "import itertools\n",
    "from gensim import matutils\n",
    "\n",
    "\n",
    "def top_topics(lda, num_words=20):\n",
    "    str_topics = []\n",
    "    for topic in lda.state.get_lambda():\n",
    "        topic = topic / topic.sum()  # normalize to probability distribution\n",
    "        bestn = matutils.argsort(topic, topn=num_words, reverse=True)\n",
    "        beststr = [lda.id2word[_id] for _id in bestn]\n",
    "        str_topics.append(beststr)\n",
    "    return str_topics\n",
    "\n",
    "\n",
    "model_topics = {}\n",
    "super_topic = set()\n",
    "for num_topics, model in trained_models.items():\n",
    "    topics_as_topn_terms = top_topics(model)\n",
    "    model_topics[num_topics] = topics_as_topn_terms\n",
    "    super_topic.update(itertools.chain.from_iterable(topics_as_topn_terms))\n",
    "    \n",
    "print(\"Number of relevant terms: %d\" % len(super_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 3.23 s, total: 42.4 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now estimate the probabilities for the CoherenceModel\n",
    "\n",
    "cm = models.CoherenceModel(\n",
    "    topics=[super_topic], topn=len(super_topic), texts=corpus.get_texts(),\n",
    "    dictionary=dictionary, coherence='c_v')\n",
    "accumulator = cm.estimate_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 16 µs, total: 34 µs\n",
      "Wall time: 30 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_coherence(cm, model_topics):\n",
    "    \"\"\"Perform the coherence evaluation for each of the models.\n",
    "\n",
    "    Since we have already precomputed the probabilities, this simply\n",
    "    involves using the accumulated stats in the `CoherenceModel` to\n",
    "    perform the evaluations, which should be pretty quick.\n",
    "\n",
    "    Args:\n",
    "        cm (CoherenceModel): coherence model to evaluate coherences with. Should\n",
    "            have already estimated probabilities.\n",
    "        model_topics (dict): mapping from `num_topics` to the list of lists of\n",
    "            top-N words for the model trained with that number of topics.\n",
    "\n",
    "    Returns:\n",
    "        dict: mapping from `num_topics` to tuple of `(avg_topic_coherences, avg_coherence)`.\n",
    "            These are the coherence values per topic and the overall model coherence.\n",
    "    \"\"\"\n",
    "    coherences = {}\n",
    "    for num_topics, topics in model_topics.items():\n",
    "        cm.topics = topics\n",
    "\n",
    "        # We evaluate at various values of N and average them. This is a more robust,\n",
    "        # according to: http://people.eng.unimelb.edu.au/tbaldwin/pubs/naacl2016.pdf\n",
    "        coherence_at_n = {}\n",
    "        for n in (20, 15, 10, 5):\n",
    "            cm.topn = n\n",
    "            topic_coherences = cm.get_coherence_per_topic()\n",
    "\n",
    "            # Let's record the coherences for each topic, as well as the aggregated\n",
    "            # coherence across all of the topics.\n",
    "            coherence_at_n[n] = (topic_coherences, cm.aggregate_measures(topic_coherences))\n",
    "\n",
    "        topic_coherences, avg_coherences = zip(*coherence_at_n.values())\n",
    "        avg_topic_coherences = np.vstack(topic_coherences).mean(0)\n",
    "        avg_coherence = np.mean(avg_coherences)\n",
    "        print(\"Avg coherence for num_topics=%d: %.5f\" % (num_topics, avg_coherence))\n",
    "        coherences[num_topics] = (avg_topic_coherences, avg_coherence)\n",
    "\n",
    "    return coherences\n",
    "\n",
    "\n",
    "def print_coherence_rankings(coherences):\n",
    "    avg_coherence = \\\n",
    "        [(num_topics, avg_coherence)\n",
    "         for num_topics, (_, avg_coherence) in coherences.items()]\n",
    "    ranked = sorted(avg_coherence, key=lambda tup: tup[1], reverse=True)\n",
    "    print(\"Ranked by average '%s' coherence:\\n\" % cm.coherence)\n",
    "    for item in ranked:\n",
    "        print(\"num_topics=%d:\\t%.4f\" % item)\n",
    "    print(\"\\nBest: %d\" % ranked[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg coherence for num_topics=100: 0.53087\n",
      "Avg coherence for num_topics=70: 0.51614\n",
      "Avg coherence for num_topics=40: 0.54629\n",
      "Avg coherence for num_topics=80: 0.53581\n",
      "Avg coherence for num_topics=50: 0.54383\n",
      "Avg coherence for num_topics=20: 0.53597\n",
      "Avg coherence for num_topics=90: 0.51484\n",
      "Avg coherence for num_topics=60: 0.52619\n",
      "Avg coherence for num_topics=30: 0.56122\n",
      "Ranked by average 'c_v' coherence:\n",
      "\n",
      "num_topics=30:\t0.5612\n",
      "num_topics=40:\t0.5463\n",
      "num_topics=50:\t0.5438\n",
      "num_topics=20:\t0.5360\n",
      "num_topics=80:\t0.5358\n",
      "num_topics=100:\t0.5309\n",
      "num_topics=60:\t0.5262\n",
      "num_topics=70:\t0.5161\n",
      "num_topics=90:\t0.5148\n",
      "\n",
      "Best: 30\n"
     ]
    }
   ],
   "source": [
    "coherences = eval_coherence(cm, model_topics)\n",
    "print_coherence_rankings(coherences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results so Far\n",
    "\n",
    "So far in this notebook, we have used `gensim`'s `CoherenceModel` to perform model selection over the number of topics for LDA. We found that for the 20 Newsgroups corpus, 30 topics is best. We showcased the ability of the coherence pipeline to evaluate individual topic coherence as well as aggregated model coherence. We also demonstrated how to avoid repeated passes over the corpus, estimating the term similarity probabilities for all relevant terms just once. Topic coherence is a powerful alternative to evaluation using perplexity on a held-out document set. It is appropriate to use whenever the objective of the topic modeling is to present the topics as top-N lists for human consumption.\n",
    "\n",
    "Note that coherence calculations are generally much more accurate when a larger reference corpus is used to estimate the probabilities. In this case, we used the same corpus as for our modeling, which is relatively small at only 20,000 documents. A better reference corpus is the full Wikipedia corpus. The motivated explorer of this notebook is encouraged to download that corpus (see [Experiments on the English Wikipedia](https://radimrehurek.com/gensim/wiki.html)) and use it for probability estimation.\n",
    "\n",
    "Next we'll look at another method of coherence evaluation using distributional word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluating Coherence with Word2Vec\n",
    "\n",
    "The fact that \"c_v\" coherence uses distributional semantics to evalaute word similarity motivates the use of Word2Vec for coherence evaluation. This idea is explored further in an appendix at the end of the notebook. The `CoherenceModel` implemented in `gensim` also supports this, so let's look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 794 ms, total: 21.3 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cm = models.CoherenceModel(\n",
    "    topics=[super_topic], topn=len(super_topic), texts=corpus.get_texts(),\n",
    "    dictionary=dictionary, coherence='c_w2v')\n",
    "cm.estimate_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg coherence for num_topics=100: 0.31072\n",
      "Avg coherence for num_topics=70: 0.31160\n",
      "Avg coherence for num_topics=40: 0.30917\n",
      "Avg coherence for num_topics=80: 0.31221\n",
      "Avg coherence for num_topics=50: 0.30868\n",
      "Avg coherence for num_topics=20: 0.31732\n",
      "Avg coherence for num_topics=90: 0.31081\n",
      "Avg coherence for num_topics=60: 0.31131\n",
      "Avg coherence for num_topics=30: 0.30768\n",
      "Ranked by average 'c_w2v' coherence:\n",
      "\n",
      "num_topics=20:\t0.3173\n",
      "num_topics=80:\t0.3122\n",
      "num_topics=70:\t0.3116\n",
      "num_topics=60:\t0.3113\n",
      "num_topics=90:\t0.3108\n",
      "num_topics=100:\t0.3107\n",
      "num_topics=40:\t0.3092\n",
      "num_topics=50:\t0.3087\n",
      "num_topics=30:\t0.3077\n",
      "\n",
      "Best: 20\n"
     ]
    }
   ],
   "source": [
    "coherences = eval_coherence(cm, model_topics)\n",
    "print_coherence_rankings(coherences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pre-trained word vectors for coherence evaluation.\n",
    "\n",
    "Whoa! These results are completely different from those of the \"c_v\" method, and \"c_w2v\" is saying the model we thought was best is actually the worst! So what happened here?\n",
    "\n",
    "The same note must be made for Word2Vec (\"c_w2v\") that we made for \"c_v\": results are more accurate when a larger reference corpus is used. Except for \"c_w2v\", this is actually _way, way_ more important. Distributional word embedding techniques such as Word2Vec are fitting a probability distribution with a large number of parameters, and doing that takes a lot of data.\n",
    "\n",
    "Luckily, there are a variety of pre-trained word vectors [freely available for download](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/). Below we demonstrate using word vectors trained on ~100 billion words from Google News, [available at this link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing). Note that this file is 1.5G, so downloading it can take quite some time. It is also quite slow to load and ends up occupying about 3.35G in memory (this load time is included in the timing below). There is no need to use such a large set of word vectors for this evaluation; this one is just readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 4.4 s, total: 3min 3s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models_dir = os.path.join(home, 'workshop', 'nlp', 'models')\n",
    "vectors_path = os.path.join(models_dir, 'GoogleNews-vectors-negative300.bin.gz')\n",
    "keyed_vectors = models.KeyedVectors.load_word2vec_format(vectors_path, binary=True)\n",
    "\n",
    "cm = models.CoherenceModel(\n",
    "    topics=[super_topic], texts=corpus.get_texts(),\n",
    "    dictionary=dictionary, coherence='c_w2v',\n",
    "    keyed_vectors=keyed_vectors)\n",
    "cm.estimate_probabilities()  # still need to estimate_probabilities, but corpus is not scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg coherence for num_topics=100: 0.49062\n",
      "Avg coherence for num_topics=70: 0.49919\n",
      "Avg coherence for num_topics=40: 0.50496\n",
      "Avg coherence for num_topics=80: 0.50293\n",
      "Avg coherence for num_topics=50: 0.51106\n",
      "Avg coherence for num_topics=20: 0.50269\n",
      "Avg coherence for num_topics=90: 0.48956\n",
      "Avg coherence for num_topics=60: 0.49603\n",
      "Avg coherence for num_topics=30: 0.51147\n",
      "Ranked by average 'c_w2v' coherence:\n",
      "\n",
      "num_topics=30:\t0.5115\n",
      "num_topics=50:\t0.5111\n",
      "num_topics=40:\t0.5050\n",
      "num_topics=80:\t0.5029\n",
      "num_topics=20:\t0.5027\n",
      "num_topics=70:\t0.4992\n",
      "num_topics=60:\t0.4960\n",
      "num_topics=100:\t0.4906\n",
      "num_topics=90:\t0.4896\n",
      "\n",
      "Best: 30\n"
     ]
    }
   ],
   "source": [
    "coherences = eval_coherence(cm, model_topics)\n",
    "print_coherence_rankings(coherences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like we've now restored order\n",
    "\n",
    "The \"c_w2v\" evalution is now agreeing with \"c_v\" on the best model, and the rest of the ordering is generally quite similar. Note that the \"c_w2v\" values should not be compared directly to those produced by the \"c_v\" method. Only the ranking of models is comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Why Word2Vec for Coherence?\n",
    "\n",
    "The \"c_v\" coherence method drags a sliding window across all documents in the corpus to accumulate co-occurrence statistics. Similarity is calculated using normalized pointwise mutual information (PMI) values estimated from these statistics. More specifically, each word is represented by a vector of its NPMI with every other word in its top-N topic list. These vectors are then used to compute (cosine) similarity between words. The restriction to the other words in the top-N list was found to produce better results than using the entire vocabulary and other methods of reducing the vocabulary (see section 3.2.2 of http://www.aclweb.org/anthology/W13-0102).\n",
    "\n",
    "The fact that a reduced space is superior for these metrics indicates there is noise getting in the way. The \"c_v\" method can be seen as constructing an NPMI matrix between words. The vector of NPMI values for a particular word can then be looked up by indexing the row or column corresponding to that word's `Dictionary` ID. The reduction to the \"topic word space\" can then be achieved by using a mask to select out the top-N topic words. If we are constructing an NPMI matrix between words, then discarding some elements to reduce noise, why not factorize the matrix instead? Dimensionality reduction techniques such as SVD do a great job of reducing noise along with dimensionality, while also providing a compressed representation to work with.\n",
    "\n",
    "[Recent work](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) has shown that Word2Vec (trained with Skip-Gram Negative Sampling (SGNS)) is actually implicitly factorizing a PMI matrix shifted by a positive constant. [A subsequent paper](http://dl.acm.org/citation.cfm?id=2914720) compared Word2Vec to a few different PMI-based metrics and showed that it found coherence values that correlated more strongly with human judgements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
