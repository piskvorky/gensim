{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Model Selection Using Topic Coherence\n",
    "\n",
    "This notebook will perform topic modeling on the 20 Newsgroups corpus using LDA. We will perform model selection (over the number of topics) using topic coherence as our evaluation metric. This will showcase some of the features of the topic coherence pipeline implemented in `gensim`. In particular, we will see several features of the `CoherenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from gensim.corpora import TextCorpus, MmCorpus\n",
    "from gensim import utils, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import deaccent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Dataset\n",
    "\n",
    "The 20 Newsgroups dataset uses a hierarchical directory structure to store the articles. The structure looks something like this:\n",
    "```\n",
    "20news-18828/\n",
    "|-- alt.atheism\n",
    "|   |-- 49960\n",
    "|   |-- 51060\n",
    "|   |-- 51119\n",
    "|-- comp.graphics\n",
    "|   |-- 37261\n",
    "|   |-- 37913\n",
    "|   |-- 37914\n",
    "|-- comp.os.ms-windows.misc\n",
    "|   |-- 10000\n",
    "|   |-- 10001\n",
    "|   |-- 10002\n",
    "```\n",
    "\n",
    "The files are in the newsgroup markup format, which includes some headers, quoting of previous messages in the thread, and possibly PGP signature blocks. The message body itself is raw text, which requires preprocessing. The code immediately below is an adaptation of [an active PR](https://github.com/RaRe-Technologies/gensim/pull/1388) for parsing hierarchical directory structures into corpora. The code just below that builds on this basic corpus parser to handle the newsgroup-specific text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDirectoryCorpus(TextCorpus):\n",
    "    \"\"\"Read documents recursively from a directory,\n",
    "    where each file is interpreted as a plain text document.\n",
    "    \"\"\"\n",
    "    \n",
    "    def iter_filepaths(self):\n",
    "        \"\"\"Lazily yield paths to each file in the directory structure within the specified\n",
    "        range of depths. If a filename pattern to match was given, further filter to only\n",
    "        those filenames that match.\n",
    "        \"\"\"\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input):\n",
    "            for name in filenames:\n",
    "                yield os.path.join(dirpath, name)\n",
    "                \n",
    "    def getstream(self):\n",
    "        for path in self.iter_filepaths():\n",
    "            with utils.smart_open(path) as f:\n",
    "                doc_content = f.read()\n",
    "            yield doc_content\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = deaccent(\n",
    "            lower_to_unicode(\n",
    "                strip_multiple_whitespaces(text)))\n",
    "        tokens = simple_tokenize(text)\n",
    "        return remove_short(\n",
    "            remove_stopwords(tokens))\n",
    "        \n",
    "    def get_texts(self):\n",
    "        \"\"\"Iterate over the collection, yielding one document at a time. A document\n",
    "        is a sequence of words (strings) that can be fed into `Dictionary.doc2bow`.\n",
    "        Override this function to match your input (parse input files, do any\n",
    "        text preprocessing, lowercasing, tokenizing etc.). There will be no further\n",
    "        preprocessing of the words coming out of this function.\n",
    "        \"\"\"\n",
    "        lines = self.getstream()\n",
    "        if self.metadata:\n",
    "            for lineno, line in enumerate(lines):\n",
    "                yield self.preprocess_text(line), (lineno,)\n",
    "        else:\n",
    "            for line in lines:\n",
    "                yield self.preprocess_text(line)\n",
    "\n",
    "    \n",
    "def remove_stopwords(tokens, stopwords=STOPWORDS):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def remove_short(tokens, minsize=3):\n",
    "    return [token for token in tokens if len(token) >= minsize]\n",
    "\n",
    "def lower_to_unicode(text):\n",
    "    return utils.to_unicode(text.lower(), 'ascii', 'ignore')\n",
    "\n",
    "RE_WHITESPACE = re.compile(r\"(\\s)+\", re.UNICODE)\n",
    "def strip_multiple_whitespaces(text):\n",
    "    return RE_WHITESPACE.sub(\" \", text)\n",
    "\n",
    "PAT_ALPHABETIC = re.compile('(((?![\\d])\\w)+)', re.UNICODE)\n",
    "def simple_tokenize(text):\n",
    "    for match in PAT_ALPHABETIC.finditer(text):\n",
    "        yield match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NewsgroupCorpus(TextDirectoryCorpus):\n",
    "    \"\"\"Parse 20 Newsgroups dataset.\"\"\"\n",
    "\n",
    "    def extract_body(self, text):\n",
    "        return strip_newsgroup_header(\n",
    "            strip_newsgroup_footer(\n",
    "                strip_newsgroup_quoting(text)))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        body = self.extract_body(text)\n",
    "        return super(NewsgroupCorpus, self).preprocess_text(body)\n",
    "\n",
    "\n",
    "def strip_newsgroup_header(text):\n",
    "    \"\"\"Given text in \"news\" format, strip the headers, by removing everything\n",
    "    before the first blank line.\n",
    "    \"\"\"\n",
    "    _before, _blankline, after = text.partition('\\n\\n')\n",
    "    return after\n",
    "\n",
    "\n",
    "_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n",
    "                       r'|^In article|^Quoted from|^\\||^>)')\n",
    "def strip_newsgroup_quoting(text):\n",
    "    \"\"\"Given text in \"news\" format, strip lines beginning with the quote\n",
    "    characters > or |, plus lines that often introduce a quoted section\n",
    "    (for example, because they contain the string 'writes:'.)\n",
    "    \"\"\"\n",
    "    good_lines = [line for line in text.split('\\n')\n",
    "                  if not _QUOTE_RE.search(line)]\n",
    "    return '\\n'.join(good_lines)\n",
    "\n",
    "\n",
    "_PGP_SIG_BEGIN = \"-----BEGIN PGP SIGNATURE-----\"\n",
    "def strip_newsgroup_footer(text):\n",
    "    \"\"\"Given text in \"news\" format, attempt to remove a signature block.\"\"\"\n",
    "    try:\n",
    "        return text[:text.index(_PGP_SIG_BEGIN)]\n",
    "    except ValueError:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Now that we have defined the necessary code for parsing the dataset, let's load it up and serialize it into Matrix Market format. We'll do this because we want to train LDA on it with several different parameter settings, and this will allow us to avoid repeating the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace data_path with path to your own copy of the corpus.\n",
    "# You can download it from here: http://qwone.com/~jason/20Newsgroups/\n",
    "# I'm using the original, called: 20news-19997.tar.gz\n",
    "\n",
    "home = os.path.expanduser('~')\n",
    "data_dir = os.path.join(home, 'workshop', 'nlp', 'data')\n",
    "data_path = os.path.join(data_dir, '20_newsgroups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n",
      "Dictionary(107980 unique tokens: [u'jbwn', u'porkification', u'sowell', u'sonja', u'luanch']...)\n",
      "CPU times: user 38.3 s, sys: 2.43 s, total: 40.7 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = NewsgroupCorpus(data_path)\n",
    "dictionary = corpus.dictionary\n",
    "print(len(corpus))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 2.76 s, total: 28.7 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mm_path = os.path.join(data_dir, '20_newsgroups.mm')\n",
    "MmCorpus.serialize(mm_path, corpus, id2word=dictionary)\n",
    "mm_corpus = MmCorpus(mm_path)  # load back in to use for LDA training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "\n",
    "Our goal is to determine which number of topics produces the most coherent topics for the 20 Newsgroups corpus. The corpus is roughly 20,000 documents. If we used 100 topics and the documents were evenly distributed among topics, we'd have clusters of 200 documents. This seems like a reasonable upper bound. In this case, the corpus actually has categories, defined by the first-level directory structure. This can be seen in the directory structure shown above, and three examples are: `alt.atheism`, `comp.graphics`, and `comp.os.ms-windows.misc`. There are 20 of these (hence the name of the dataset), so we'll use 20 as our lower bound for the number of topics.\n",
    "\n",
    "One could argue that we already know the model should have 20 topics. I'll argue there may be additional categorizations within each newsgroup and we might hope to capture those by using more topics. We'll step by increments of 10 from 20 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA(k=20)\n",
      "Training LDA(k=30)\n",
      "Training LDA(k=40)\n",
      "Training LDA(k=50)\n",
      "Training LDA(k=60)\n",
      "Training LDA(k=70)\n",
      "Training LDA(k=80)\n",
      "Training LDA(k=90)\n",
      "Training LDA(k=100)\n",
      "CPU times: user 1h 27min 7s, sys: 7min 54s, total: 1h 35min 2s\n",
      "Wall time: 1h 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trained_models = {}\n",
    "for num_topics in range(20, 101, 10):\n",
    "    print(\"Training LDA(k=%d)\" % num_topics)\n",
    "    lda = models.LdaMulticore(\n",
    "        mm_corpus, id2word=dictionary, num_topics=num_topics, workers=4,\n",
    "        passes=10, iterations=200, random_state=42,\n",
    "        alpha='asymmetric',  # shown to be better than symmetric in most cases\n",
    "        decay=0.5, offset=64  # best params from Hoffman paper\n",
    "    )\n",
    "    trained_models[num_topics] = lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Using Coherence\n",
    "\n",
    "Now we get to the heart of this notebook. In this section, we'll evaluate each of our LDA models using topic coherence. Coherence is a measure of how interpretable the topics are to humans. It is based on the representation of topics as the top-N most probable words for a particular topic. More specifically, given the topic-term matrix for LDA, we sort each topic from highest to lowest term weights and then select the first N terms.\n",
    "\n",
    "Coherence essentially measures how similar these words are to each other. There are various methods for doing this, most of which have been explored in the paper [\"Exploring the Space of Topic Coherence Measures\"](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf). The authors performed a comparative analysis of various methods, correlating them to human judgements. The method named \"c_v\" coherence was found to be the most highly correlated. This and several of the other methods have been implemented in `gensim.models.CoherenceModel`. We will use this to perform our evaluations.\n",
    "\n",
    "The \"c_v\" coherence method makes an expensive pass over the corpus, accumulating term occurrence and co-occurrence counts. It only accumulates counts for the terms in the lists of top-N terms for each topic. In order to ensure we only need to make one pass, we'll construct a \"super topic\" from the top-N lists of each of the models. This will consist of a single topic with all the relevant terms from all the models. We choose 20 as N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant terms: 3517\n"
     ]
    }
   ],
   "source": [
    "# Build topic listings from each model.\n",
    "import itertools\n",
    "from gensim import matutils\n",
    "\n",
    "\n",
    "def top_topics(lda, num_words=20):\n",
    "    str_topics = []\n",
    "    for topic in lda.state.get_lambda():\n",
    "        topic = topic / topic.sum()  # normalize to probability distribution\n",
    "        bestn = matutils.argsort(topic, topn=num_words, reverse=True)\n",
    "        beststr = [lda.id2word[_id] for _id in bestn]\n",
    "        str_topics.append(beststr)\n",
    "    return str_topics\n",
    "\n",
    "\n",
    "model_topics = {}\n",
    "super_topic = set()\n",
    "for num_topics, model in trained_models.items():\n",
    "    topics_as_topn_terms = top_topics(model)\n",
    "    model_topics[num_topics] = topics_as_topn_terms\n",
    "    super_topic.update(itertools.chain.from_iterable(topics_as_topn_terms))\n",
    "    \n",
    "print(\"Number of relevant terms: %d\" % len(super_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 s, sys: 3.1 s, total: 37.1 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now estimate the probabilities for the CoherenceModel\n",
    "\n",
    "cm = models.CoherenceModel(\n",
    "    topics=[super_topic], texts=corpus.get_texts(),\n",
    "    dictionary=dictionary, coherence='c_v')\n",
    "cm.estimate_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg coherence for num_topics=100: 0.48958\n",
      "Avg coherence for num_topics=70: 0.50393\n",
      "Avg coherence for num_topics=40: 0.51029\n",
      "Avg coherence for num_topics=80: 0.51147\n",
      "Avg coherence for num_topics=50: 0.51582\n",
      "Avg coherence for num_topics=20: 0.49602\n",
      "Avg coherence for num_topics=90: 0.47067\n",
      "Avg coherence for num_topics=60: 0.48913\n",
      "Avg coherence for num_topics=30: 0.48709\n",
      "CPU times: user 2min 39s, sys: 524 ms, total: 2min 39s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "# Next we perform the coherence evaluation for each of the models.\n",
    "# Since we have already precomputed the probabilities, this simply\n",
    "# involves using the accumulated stats in the `CoherenceModel` to\n",
    "# perform the evaluations, which should be pretty quick.\n",
    "\n",
    "coherences = {}\n",
    "for num_topics, topics in model_topics.items():\n",
    "    cm.topics = topics\n",
    "\n",
    "    # We evaluate at various values of N and average them. This is a more robust,\n",
    "    # according to: http://people.eng.unimelb.edu.au/tbaldwin/pubs/naacl2016.pdf\n",
    "    coherence_at_n = {}\n",
    "    for n in (20, 15, 10, 5):\n",
    "        cm.topn = n\n",
    "        topic_coherences = cm.get_coherence_per_topic()\n",
    "        \n",
    "        # Let's record the coherences for each topic, as well as the aggregated\n",
    "        # coherence across all of the topics.\n",
    "        coherence_at_n[n] = (topic_coherences, cm.aggregate_measures(topic_coherences))\n",
    "        \n",
    "    topic_coherences, avg_coherences = zip(*coherence_at_n.values())\n",
    "    avg_topic_coherences = np.vstack(topic_coherences).mean(0)\n",
    "    avg_coherence = np.mean(avg_coherences)\n",
    "    print(\"Avg coherence for num_topics=%d: %.5f\" % (num_topics, avg_coherence))\n",
    "    coherences[num_topics] = (avg_topic_coherences, avg_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked by average 'c_v' coherence:\n",
      "\n",
      "num_topics=50:\t0.5158\n",
      "num_topics=80:\t0.5115\n",
      "num_topics=40:\t0.5103\n",
      "num_topics=70:\t0.5039\n",
      "num_topics=20:\t0.4960\n",
      "num_topics=100:\t0.4896\n",
      "num_topics=60:\t0.4891\n",
      "num_topics=30:\t0.4871\n",
      "num_topics=90:\t0.4707\n",
      "\n",
      "Best: 50\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence rankings\n",
    "\n",
    "avg_coherence = \\\n",
    "    [(num_topics, avg_coherence)\n",
    "     for num_topics, (_, avg_coherence) in coherences.items()]\n",
    "ranked = sorted(avg_coherence, key=lambda tup: tup[1], reverse=True)\n",
    "print(\"Ranked by average '%s' coherence:\\n\" % cm.coherence)\n",
    "for item in ranked:\n",
    "    print(\"num_topics=%d:\\t%.4f\" % item)\n",
    "print(\"\\nBest: %d\" % ranked[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we used `gensim`'s `CoherenceModel` to perform model selection over the number of topics for LDA. We found that for the 20 Newsgroups corpus, 50 topics is best. We showcased the ability of the coherence pipeline to evaluate individual topic coherence as well as aggregated model coherence. We also demonstrated how to avoid repeated passes over the corpus, estimating the term similarity probabilities for all relevant terms just once. Topic coherence is a powerful alternative to evaluation using perplexity on a held-out document set. It is appropriate to use whenever the objective of the topic modeling is to present the topics as top-N lists for human consumption.\n",
    "\n",
    "Note that coherence calculations are generally much more accurate when a larger reference corpus is used to estimate the probabilities. In this case, we used the same corpus as for our modeling, which is relatively small at only 20 documents. A better reference corpus is the full Wikipedia corpus. The motivated explorer of this notebook is encouraged to download that corpus (see [Experiments on the English Wikipedia](https://radimrehurek.com/gensim/wiki.html)) and use it for probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
