{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sent2Vec via Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about using sent2vec model in Gensim. Here, we'll learn to work with the sent2vec library for training sentence-embedding models, saving & loading them and performing similarity operations. This notebook also contains a comparison of the gensim implementation with the [original c++ implementation](https://github.com/epfml/sent2vec), Gensim's Doc2Vec and Gensim's FastText. All the evaluation scripts used in the notebook can be found [here](https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download evaluation scripts\n",
    "! wget https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b/raw/7a970c7480474516ee41ba7a60d37092819822ea/eval_classification.py\n",
    "! wget https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b/raw/7a970c7480474516ee41ba7a60d37092819822ea/eval_sick.py\n",
    "! wget https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b/raw/7a970c7480474516ee41ba7a60d37092819822ea/eval_trec.py\n",
    "! wget https://gist.github.com/prerna135/9b5eb55054d29c1495460b75fc061c6b/raw/7a970c7480474516ee41ba7a60d37092819822ea/dataset_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sent2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec delivers numerical representations (features) for short texts or sentences, which can be used as input to any machine learning task later on. Think of it as an unsupervised version of FastText, and an extension of word2vec (CBOW) to sentences. The method uses a simple but efficient unsupervised objective to train distributed representations of sentences. The algorithm outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings, see the [paper](https://arxiv.org/abs/1703.02507) for more details.\n",
    "\n",
    "The sentence embedding is defined as the average of the source word embeddings of its constituent words. This model is furthermore augmented by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model. All models are trained with the same hyperparameters for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.sent2vec import Sent2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.utils import tokenize\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import eval_sick\n",
    "import eval_classification\n",
    "import eval_trec\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = data_dir + 'lee_background.cor'\n",
    "lee_data = []\n",
    "with open(lee_train_file) as f1, open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if line not in ['\\n', '\\r\\n']:\n",
    "            line = re.split('\\.|\\?|\\n', line.strip())\n",
    "            for sentence in line:\n",
    "                if len(sentence) > 1:\n",
    "                    sentence = tokenize(sentence)\n",
    "                    lee_data.append(list(sentence))\n",
    "                    f2.write(' '.join(lee_data[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hundreds', u'of', u'people', u'have', u'been', u'forced', u'to', u'vacate', u'their', u'homes', u'in', u'the', u'Southern', u'Highlands', u'of', u'New', u'South', u'Wales', u'as', u'strong', u'winds', u'today', u'pushed', u'a', u'huge', u'bushfire', u'towards', u'the', u'town', u'of', u'Hill', u'Top'] \n",
      "\n",
      "[u'A', u'new', u'blaze', u'near', u'Goulburn', u'south', u'west', u'of', u'Sydney', u'has', u'forced', u'the', u'closure', u'of', u'the', u'Hume', u'Highway'] \n",
      "\n",
      "[u'At', u'about', u'pm', u'AEDT', u'a', u'marked', u'deterioration', u'in', u'the', u'weather', u'as', u'a', u'storm', u'cell', u'moved', u'east', u'across', u'the', u'Blue', u'Mountains', u'forced', u'authorities', u'to', u'make', u'a', u'decision', u'to', u'evacuate', u'people', u'from', u'homes', u'in', u'outlying', u'streets', u'at', u'Hill', u'Top', u'in', u'the', u'New', u'South', u'Wales', u'southern', u'highlands'] \n",
      "\n",
      "[u'An', u'estimated', u'residents', u'have', u'left', u'their', u'homes', u'for', u'nearby', u'Mittagong'] \n",
      "\n",
      "[u'The', u'New', u'South', u'Wales', u'Rural', u'Fire', u'Service', u'says', u'the', u'weather', u'conditions', u'which', u'caused', u'the', u'fire', u'to', u'burn', u'in', u'a', u'finger', u'formation', u'have', u'now', u'eased', u'and', u'about', u'fire', u'units', u'in', u'and', u'around', u'Hill', u'Top', u'are', u'optimistic', u'of', u'defending', u'all', u'properties'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample training data\n",
    "for sentence in lee_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gensim implementation of sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 0.06 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 1531, tokens read: 60302\n",
      "INFO:gensim.models.sent2vec:training model with 4 workers on 1531 vocabulary and 100 features\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 8.28% words, 79298 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 17.39% words, 81742 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 27.32% words, 86469 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 38.92% words, 92702 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 48.86% words, 92947 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 59.62% words, 94653 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 71.22% words, 95012 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 82.82% words, 97058 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 91.92% words, 95278 words/s\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.sent2vec:training on 1206040 raw words (983720 effective words) took 10.1s, 97335 effective words/s\n",
      "CPU times: user 22 s, sys: 7.11 s, total: 29.1 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model\n",
    "% time sent2vec_model = Sent2Vec(lee_data, size=100, epochs=20, seed=42, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent2Vec supports the following parameters:\n",
    "\n",
    "- sentences : For larger corpora (like the Toronto corpus), consider an iterable that streams the sentences directly from disk/network.\n",
    "- size : Dimensionality of the feature vectors. Default 100\n",
    "- lr : Initial learning rate. Default 0.2\n",
    "- seed : For the random number generator for reproducible reasons. Default 42\n",
    "- min_count : Ignore all words with total frequency lower than this. Default 5\n",
    "- max_vocab_size : Limit RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Default is 30000000.\n",
    "- t : Threshold for configuring which higher-frequency words are randomly downsampled; default is 1e-3, useful range is (0, 1e-5).\n",
    "- loss_type : Default is 'ns', negative sampling will be used.\n",
    "- neg : Specifies how many \"noise words\" should be drawn (usually between 5-20). Default is 10.\n",
    "- epochs : Number of iterations (epochs) over the corpus. Default is 5.\n",
    "- lr_update_rate : Change the rate of updates for the learning rate. Default is 100.\n",
    "- word_ngrams : Max length of word ngram. Default is 2.\n",
    "- bucket : Number of hash buckets for vocabulary. Default is 2000000.\n",
    "- minn : Min length of char ngrams. Default is 3.\n",
    "- maxn : Max length of char ngrams. Default is 6.\n",
    "- dropout_k : Number of ngrams dropped when training a sent2vec model. Default is 2.\n",
    "- batch_words : Target size (in words) for batches of examples passed to worker threads (and thus cython routines). Default is 10000. (Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "- workers : Use this many worker threads to train the model (=faster training with multicore machines). Default is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59427662,  0.45323249,  0.50265895,  0.4995847 ,  0.34286592,\n",
       "        0.50089866,  0.61660693,  0.66634651,  0.49990433,  0.64750528,\n",
       "        0.52456549,  0.57985228,  0.37021146,  0.60212792,  0.43293218,\n",
       "        0.41538005,  0.69427354,  0.65900928,  0.70971879,  0.42145939,\n",
       "        0.56558073,  0.61602969,  0.28584392,  0.5740314 ,  0.58066796,\n",
       "        0.44665115,  0.5402053 ,  0.61129872,  0.52463576,  0.48204397,\n",
       "        0.51067452,  0.52571809,  0.461759  ,  0.45117879,  0.4421997 ,\n",
       "        0.54885694,  0.63369278,  0.46641079,  0.19771745,  0.60596168,\n",
       "        0.76673974,  0.45278565,  0.65774093,  0.21516863,  0.29132966,\n",
       "        0.69450265,  0.41453836,  0.58605764,  0.37342349,  0.33855072,\n",
       "        0.42545663,  0.4449666 ,  0.37623366,  0.41264582,  0.80266036,\n",
       "        0.45889603,  0.4402174 ,  0.65778087,  0.42700231,  0.5813403 ,\n",
       "        0.31794493,  0.37576896,  0.48747489,  0.45650574,  0.39796694,\n",
       "        0.45184011,  0.52442752,  0.34692425,  0.41412414,  0.46049121,\n",
       "        0.16680804,  0.58984551,  0.51408591,  0.42969991,  0.56487781,\n",
       "        0.50728092,  0.67968616,  0.37515504,  0.51621761,  0.3365712 ,\n",
       "        0.57871515,  0.49983472,  0.59300976,  0.72827086,  0.71840471,\n",
       "        0.54974145,  0.51782431,  0.38546004,  0.36161306,  0.68303751,\n",
       "        0.49273053,  0.49579573,  0.40947119,  0.32662868,  0.57133121,\n",
       "        0.36875874,  0.39087458,  0.37304583,  0.52278825,  0.40981476])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sentence vector\n",
    "sent2vec_model.sentence_vectors(['This', 'is', 'an', 'awesome', 'gift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96255669226718132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print cosine similarity between two sentences\n",
    "sent2vec_model.similarity(['This', 'is', 'an', 'awesome', 'gift'], ['This', 'present', 'is', 'great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the load and save methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Sent2Vec object under s2v1, separately None\n",
      "INFO:gensim.utils:storing np array 'wi' to s2v1.wi.npy\n",
      "INFO:gensim.utils:saved s2v1\n"
     ]
    }
   ],
   "source": [
    "# Save trained sent2vec model\n",
    "sent2vec_model.save('s2v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Sent2Vec object from s2v1\n",
      "INFO:gensim.utils:loading wi from s2v1.wi.npy with mmap=None\n",
      "INFO:gensim.utils:loaded s2v1\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained sent2vec model\n",
    "loaded_model = s2v.load('s2v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised similarity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised evaluation of the the learnt sentence embeddings is performed using the sentence cosine similarity, on the [SICK 2014](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools) datasets. These similarity scores are compared to the gold-standard human judgements using [Pearsonâ€™s correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs. We use the code provided by [Kiros et al., 2015](https://github.com/ryankiros/skip-thoughts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.422729545743\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.431216972239\n",
      "Test Spearman: 0.430858902788\n",
      "Test MSE: 0.830003650035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.34222598,  3.36105921,  3.34240926, ...,  3.06700658,\n",
       "        2.60424432,  3.44811346])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(loaded_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Downstream Supervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings are evaluated for various supervised classification tasks. We evaluate classification of movie review sentiment (MR) (Pang & Lee, 2005), subjectivity classification (SUBJ)(Pang & Lee, 2004) and question type classification (TREC) (Voorhees, 2002). To classify, we use the code provided by [(Kiros et al., 2015)](https://github.com/ryankiros/skip-thoughts). Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the [MR and SUBJ](https://www.cs.cornell.edu/people/pabo/movie-review-data/) datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the [TREC dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/), the accuracy is computed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.70099999999999996]\n",
      "[0.70099999999999996, 0.71799999999999997]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998, 0.72399999999999998]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998, 0.72399999999999998, 0.70199999999999996]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998, 0.72399999999999998, 0.70199999999999996, 0.72399999999999998]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998, 0.72399999999999998, 0.70199999999999996, 0.72399999999999998, 0.70099999999999996]\n",
      "[0.70099999999999996, 0.71799999999999997, 0.71199999999999997, 0.72799999999999998, 0.72299999999999998, 0.72399999999999998, 0.70199999999999996, 0.72399999999999998, 0.70099999999999996, 0.72999999999999998]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.70099999999999996,\n",
       " 0.71799999999999997,\n",
       " 0.71199999999999997,\n",
       " 0.72799999999999998,\n",
       " 0.72299999999999998,\n",
       " 0.72399999999999998,\n",
       " 0.70199999999999996,\n",
       " 0.72399999999999998,\n",
       " 0.70099999999999996,\n",
       " 0.72999999999999998]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=loaded_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57357075913776945]\n",
      "[0.57357075913776945, 0.55576382380506095]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857, 0.54596622889305813]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857, 0.54596622889305813, 0.60787992495309573]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857, 0.54596622889305813, 0.60787992495309573, 0.6163227016885553]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857, 0.54596622889305813, 0.60787992495309573, 0.6163227016885553, 0.60037523452157604]\n",
      "[0.57357075913776945, 0.55576382380506095, 0.57973733583489684, 0.57035647279549717, 0.53658536585365857, 0.54596622889305813, 0.60787992495309573, 0.6163227016885553, 0.60037523452157604, 0.58348968105065668]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57357075913776945,\n",
       " 0.55576382380506095,\n",
       " 0.57973733583489684,\n",
       " 0.57035647279549717,\n",
       " 0.53658536585365857,\n",
       " 0.54596622889305813,\n",
       " 0.60787992495309573,\n",
       " 0.6163227016885553,\n",
       " 0.60037523452157604,\n",
       " 0.58348968105065668]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=loaded_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.586\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(model=loaded_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of original c++ implementation of sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build and train c++ implementation of sent2vec, use the following commands. This will produce object files for all the classes as well as the main binary sent2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/sent2vec\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/epfml/sent2vec.git\n",
    "% cd sent2vec\n",
    "! make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  1837\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 92875  lr: 0.000000  loss: 3.107460  eta: 0h0m oss: 6.382276  eta: 0h3m 0h0m 28.8%  words/sec/thread: 41455  lr: 0.142491  loss: 3.573717  eta: 0h0m 0.111096  loss: 3.478478  eta: 0h0m m 69885  lr: 0.079403  loss: 3.358491  eta: 0h0m 73.3%  words/sec/thread: 78577  lr: 0.053372  loss: 3.262215  eta: 0h0m 0.043828  loss: 3.228405  eta: 0h0m 84.7%  words/sec/thread: 85207  lr: 0.030507  loss: 3.187402  eta: 0h0m   lr: 0.008810  loss: 3.128377  eta: 0h0m \n",
      "\n",
      "real\t0m13.490s\n",
      "user\t0m16.215s\n",
      "sys\t0m2.013s\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "! time ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 20 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 4 -t 0.0001 -dropout_k 2 -bucket 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.419326924957\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.415730692175\n",
      "Test Spearman: 0.422953082413\n",
      "Test MSE: 0.842070843724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.00608191,  3.19365489,  3.4313475 , ...,  3.29148397,\n",
       "        2.62857256,  3.05053893])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% cd ..\n",
    "eval_sick.evaluate(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.78700000000000003]\n",
      "[0.78700000000000003, 0.79900000000000004]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003, 0.76200000000000001]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003, 0.76200000000000001, 0.78600000000000003]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003, 0.76200000000000001, 0.78600000000000003, 0.79300000000000004]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003, 0.76200000000000001, 0.78600000000000003, 0.79300000000000004, 0.76100000000000001]\n",
      "[0.78700000000000003, 0.79900000000000004, 0.79000000000000004, 0.78200000000000003, 0.78300000000000003, 0.76200000000000001, 0.78600000000000003, 0.79300000000000004, 0.76100000000000001, 0.79200000000000004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78700000000000003,\n",
       " 0.79900000000000004,\n",
       " 0.79000000000000004,\n",
       " 0.78200000000000003,\n",
       " 0.78300000000000003,\n",
       " 0.76200000000000001,\n",
       " 0.78600000000000003,\n",
       " 0.79300000000000004,\n",
       " 0.76100000000000001,\n",
       " 0.79200000000000004]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57825679475164016]\n",
      "[0.57825679475164016, 0.584817244611059]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686, 0.59099437148217637]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686, 0.59099437148217637, 0.60694183864915574]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686, 0.59099437148217637, 0.60694183864915574, 0.61538461538461542]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686, 0.59099437148217637, 0.60694183864915574, 0.61538461538461542, 0.61163227016885557]\n",
      "[0.57825679475164016, 0.584817244611059, 0.5684803001876173, 0.56378986866791747, 0.57786116322701686, 0.59099437148217637, 0.60694183864915574, 0.61538461538461542, 0.61163227016885557, 0.59474671669793622]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57825679475164016,\n",
       " 0.584817244611059,\n",
       " 0.5684803001876173,\n",
       " 0.56378986866791747,\n",
       " 0.57786116322701686,\n",
       " 0.59099437148217637,\n",
       " 0.60694183864915574,\n",
       " 0.61538461538461542,\n",
       " 0.61163227016885557,\n",
       " 0.59474671669793622]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model1 with PV-DM and sum of context word vectors\n",
    "doc2vec_model1 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model1.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model2 with PV-DBOW and sum of context word vectors\n",
    "doc2vec_model2 = gensim.models.doc2vec.Doc2Vec(dm=0, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model2.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model3 with PV-DM and mean of context word vectors\n",
    "doc2vec_model3 = gensim.models.doc2vec.Doc2Vec(dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model3.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model4 with PV-DBOW and mean of context word vectors\n",
    "doc2vec_model4 = gensim.models.doc2vec.Doc2Vec(dm=0, dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model4.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.53% examples, 451598 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724888 effective words) took 1.6s, 459770 effective words/s\n",
      "CPU times: user 3.73 s, sys: 230 ms, total: 3.96 s\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724888"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model1.train(train_corpus, total_examples=doc2vec_model1.corpus_count, epochs=doc2vec_model1.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.98% examples, 579813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724799 effective words) took 1.2s, 586018 effective words/s\n",
      "CPU times: user 3.07 s, sys: 126 ms, total: 3.2 s\n",
      "Wall time: 1.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724799"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model2.train(train_corpus, total_examples=doc2vec_model2.corpus_count, epochs=doc2vec_model2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.78% examples, 262306 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.73% examples, 311389 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724835 effective words) took 2.2s, 326296 effective words/s\n",
      "CPU times: user 3.69 s, sys: 163 ms, total: 3.86 s\n",
      "Wall time: 2.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724835"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model3.train(train_corpus, total_examples=doc2vec_model3.corpus_count, epochs=doc2vec_model3.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.20% examples, 500867 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724340 effective words) took 1.4s, 527263 effective words/s\n",
      "CPU times: user 3.2 s, sys: 132 ms, total: 3.33 s\n",
      "Wall time: 1.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724340"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model4.train(train_corpus, total_examples=doc2vec_model4.corpus_count, epochs=doc2vec_model4.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.286438142989\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.269150921565\n",
      "Test Spearman: 0.262674319422\n",
      "Test MSE: 0.94450382038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.33818458,  3.32924404,  3.22118222, ...,  3.62762797,\n",
       "        3.44393652,  3.15340341])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model1, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.67400000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004, 0.67100000000000004]\n",
      "[0.67400000000000004, 0.67000000000000004, 0.65600000000000003, 0.64000000000000001, 0.66800000000000004, 0.68000000000000005, 0.67900000000000005, 0.66800000000000004, 0.67100000000000004, 0.69199999999999995]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.67400000000000004,\n",
       " 0.67000000000000004,\n",
       " 0.65600000000000003,\n",
       " 0.64000000000000001,\n",
       " 0.66800000000000004,\n",
       " 0.68000000000000005,\n",
       " 0.67900000000000005,\n",
       " 0.66800000000000004,\n",
       " 0.67100000000000004,\n",
       " 0.69199999999999995]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model1, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.5135895032802249]\n",
      "[0.5135895032802249, 0.53045923149015928]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584, 0.5684803001876173]\n",
      "[0.5135895032802249, 0.53045923149015928, 0.54878048780487809, 0.54409005628517826, 0.55816135084427765, 0.5684803001876173, 0.57879924953095685, 0.5412757973733584, 0.5684803001876173, 0.5478424015009381]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5135895032802249,\n",
       " 0.53045923149015928,\n",
       " 0.54878048780487809,\n",
       " 0.54409005628517826,\n",
       " 0.55816135084427765,\n",
       " 0.5684803001876173,\n",
       " 0.57879924953095685,\n",
       " 0.5412757973733584,\n",
       " 0.5684803001876173,\n",
       " 0.5478424015009381]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model1, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.39693084071\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.332600557652\n",
      "Test Spearman: 0.331574006828\n",
      "Test MSE: 0.905109955103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.57159891,  3.35031843,  3.5568133 , ...,  3.06880136,\n",
       "        3.49490334,  3.19048271])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model2, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.75]\n",
      "[0.75, 0.71699999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997, 0.71199999999999997]\n",
      "[0.75, 0.71699999999999997, 0.70499999999999996, 0.73999999999999999, 0.72299999999999998, 0.72099999999999997, 0.71599999999999997, 0.71099999999999997, 0.71199999999999997, 0.73499999999999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.75,\n",
       " 0.71699999999999997,\n",
       " 0.70499999999999996,\n",
       " 0.73999999999999999,\n",
       " 0.72299999999999998,\n",
       " 0.72099999999999997,\n",
       " 0.71599999999999997,\n",
       " 0.71099999999999997,\n",
       " 0.71199999999999997,\n",
       " 0.73499999999999999]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model2, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57638238050609181]\n",
      "[0.57638238050609181, 0.57357075913776945]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732, 0.56566604127579734]\n",
      "[0.57638238050609181, 0.57357075913776945, 0.59474671669793622, 0.55253283302063794, 0.57317073170731703, 0.55628517823639778, 0.59849906191369606, 0.56754221388367732, 0.56566604127579734, 0.59568480300187621]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57638238050609181,\n",
       " 0.57357075913776945,\n",
       " 0.59474671669793622,\n",
       " 0.55253283302063794,\n",
       " 0.57317073170731703,\n",
       " 0.55628517823639778,\n",
       " 0.59849906191369606,\n",
       " 0.56754221388367732,\n",
       " 0.56566604127579734,\n",
       " 0.59568480300187621]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model2, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.218506672244\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.268180823329\n",
      "Test Spearman: 0.267794536402\n",
      "Test MSE: 0.946454984438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.55381104,  3.65387629,  3.32172886, ...,  3.50422149,\n",
       "        3.73455072,  3.27572114])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model3, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.64700000000000002]\n",
      "[0.64700000000000002, 0.65500000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004, 0.66000000000000003]\n",
      "[0.64700000000000002, 0.65500000000000003, 0.623, 0.66400000000000003, 0.65100000000000002, 0.67300000000000004, 0.69399999999999995, 0.66500000000000004, 0.66000000000000003, 0.67800000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.64700000000000002,\n",
       " 0.65500000000000003,\n",
       " 0.623,\n",
       " 0.66400000000000003,\n",
       " 0.65100000000000002,\n",
       " 0.67300000000000004,\n",
       " 0.69399999999999995,\n",
       " 0.66500000000000004,\n",
       " 0.66000000000000003,\n",
       " 0.67800000000000005]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model3, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.50140581068416124]\n",
      "[0.50140581068416124, 0.53701968134957823]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765, 0.53752345215759845]\n",
      "[0.50140581068416124, 0.53701968134957823, 0.54690431519699811, 0.5478424015009381, 0.57035647279549717, 0.56378986866791747, 0.56754221388367732, 0.55816135084427765, 0.53752345215759845, 0.58724202626641653]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.50140581068416124,\n",
       " 0.53701968134957823,\n",
       " 0.54690431519699811,\n",
       " 0.5478424015009381,\n",
       " 0.57035647279549717,\n",
       " 0.56378986866791747,\n",
       " 0.56754221388367732,\n",
       " 0.55816135084427765,\n",
       " 0.53752345215759845,\n",
       " 0.58724202626641653]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model3, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.409921757155\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.341916171513\n",
      "Test Spearman: 0.340134184451\n",
      "Test MSE: 0.898672297686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.4285679 ,  3.71794175,  2.89571583, ...,  3.20624222,\n",
       "        3.11745406,  2.71925664])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model4, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.751]\n",
      "[0.751, 0.70699999999999996]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997, 0.72099999999999997]\n",
      "[0.751, 0.70699999999999996, 0.71999999999999997, 0.73299999999999998, 0.73899999999999999, 0.69999999999999996, 0.71599999999999997, 0.71399999999999997, 0.72099999999999997, 0.73099999999999998]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.751,\n",
       " 0.70699999999999996,\n",
       " 0.71999999999999997,\n",
       " 0.73299999999999998,\n",
       " 0.73899999999999999,\n",
       " 0.69999999999999996,\n",
       " 0.71599999999999997,\n",
       " 0.71399999999999997,\n",
       " 0.72099999999999997,\n",
       " 0.73099999999999998]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model4, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57919400187441428]\n",
      "[0.57919400187441428, 0.56419868791002814]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173, 0.575046904315197]\n",
      "[0.57919400187441428, 0.56419868791002814, 0.6031894934333959, 0.57410881801125702, 0.56285178236397748, 0.57129455909943716, 0.59474671669793622, 0.5684803001876173, 0.575046904315197, 0.61444652908067543]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57919400187441428,\n",
       " 0.56419868791002814,\n",
       " 0.6031894934333959,\n",
       " 0.57410881801125702,\n",
       " 0.56285178236397748,\n",
       " 0.57129455909943716,\n",
       " 0.59474671669793622,\n",
       " 0.5684803001876173,\n",
       " 0.575046904315197,\n",
       " 0.61444652908067543]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model4, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.398\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model1, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.422\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model2, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.378\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model3, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.404\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model4, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of sentence vectors obtained from averaging FastText word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 10781 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 45 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.fasttext:Total number of ngrams is 17006\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.95% examples, 28408 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.93% examples, 28707 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.62% examples, 30686 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.07% examples, 32848 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.52% examples, 29943 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.47% examples, 28568 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.43% examples, 28703 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.77% examples, 27083 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.68% examples, 27445 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.62% examples, 27188 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.23% examples, 28104 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.07% examples, 28544 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.03% examples, 28794 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.10% examples, 28609 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.02% examples, 27885 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.98% examples, 27894 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.13% examples, 27733 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.17% examples, 27662 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.03% examples, 27752 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1197800 raw words (652465 effective words) took 23.0s, 28370 effective words/s\n",
      "CPU times: user 39.4 s, sys: 414 ms, total: 39.8 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "lee_data = LineSentence(lee_train_file)\n",
    "fasttext_model = FastText(size=100, alpha=0.2, negative=10, max_vocab_size=30000000, seed=42, iter=20)\n",
    "fasttext_model.build_vocab(lee_data)\n",
    "% time fasttext_model.train(lee_data, total_examples=fasttext_model.corpus_count, epochs=fasttext_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving FastText object under ft1, separately None\n",
      "INFO:gensim.utils:not storing attribute syn0norm\n",
      "INFO:gensim.utils:not storing attribute syn0_ngrams_norm\n",
      "INFO:gensim.utils:not storing attribute syn0_vocab_norm\n",
      "INFO:gensim.utils:saved ft1\n"
     ]
    }
   ],
   "source": [
    "fasttext_model.save('ft1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading FastText object from ft1\n",
      "INFO:gensim.utils:loading wv recursively from ft1.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.utils:setting ignored attribute syn0_ngrams_norm to None\n",
      "INFO:gensim.utils:setting ignored attribute syn0_vocab_norm to None\n",
      "INFO:gensim.utils:loaded ft1\n"
     ]
    }
   ],
   "source": [
    "ft_loaded_model = FastText.load('ft1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim\n"
     ]
    }
   ],
   "source": [
    "% cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.488702283509\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.496503202758\n",
      "Test Spearman: 0.488234545185\n",
      "Test MSE: 0.768817127692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.28616724,  3.47784466,  3.59951735, ...,  3.53827728,\n",
       "        1.97959893,  2.69010568])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(ft_loaded_model, seed=42, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.83299999999999996]\n",
      "[0.83299999999999996, 0.83499999999999996]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005, 0.78700000000000003]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005, 0.78700000000000003, 0.79600000000000004]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005, 0.78700000000000003, 0.79600000000000004, 0.82099999999999995]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005, 0.78700000000000003, 0.79600000000000004, 0.82099999999999995, 0.79200000000000004]\n",
      "[0.83299999999999996, 0.83499999999999996, 0.78600000000000003, 0.79900000000000004, 0.80400000000000005, 0.78700000000000003, 0.79600000000000004, 0.82099999999999995, 0.79200000000000004, 0.82099999999999995]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.83299999999999996,\n",
       " 0.83499999999999996,\n",
       " 0.78600000000000003,\n",
       " 0.79900000000000004,\n",
       " 0.80400000000000005,\n",
       " 0.78700000000000003,\n",
       " 0.79600000000000004,\n",
       " 0.82099999999999995,\n",
       " 0.79200000000000004,\n",
       " 0.82099999999999995]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=ft_loaded_model, name='SUBJ', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.61855670103092786]\n",
      "[0.61855670103092786, 0.61105904404873479]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637, 0.60037523452157604]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637, 0.60037523452157604, 0.66041275797373356]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637, 0.60037523452157604, 0.66041275797373356, 0.62288930581613511]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637, 0.60037523452157604, 0.66041275797373356, 0.62288930581613511, 0.61538461538461542]\n",
      "[0.61855670103092786, 0.61105904404873479, 0.60600375234521575, 0.59568480300187621, 0.59099437148217637, 0.60037523452157604, 0.66041275797373356, 0.62288930581613511, 0.61538461538461542, 0.61819887429643527]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.61855670103092786,\n",
       " 0.61105904404873479,\n",
       " 0.60600375234521575,\n",
       " 0.59568480300187621,\n",
       " 0.59099437148217637,\n",
       " 0.60037523452157604,\n",
       " 0.66041275797373356,\n",
       " 0.62288930581613511,\n",
       " 0.61538461538461542,\n",
       " 0.61819887429643527]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=ft_loaded_model, name='MR', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.618\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(ft_loaded_model, evalcv=False, evaltest=True, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| S.No. | Model Name                                | Total Execution Time (in seconds) | Pearson/Spearman/MSE on SICK | Mean SUBJ | Mean MR | TREC |\n",
    "|-------|-------------------------------------------|----------------------------|------------------------------|-----------|---------|------|\n",
    "| 1.    | Gensim Sent2Vec                           | 27.5                       | 0.43/0.43/0.83               | 0.71      | 0.57    | 0.58 |\n",
    "| 2.    | Original Sent2Vec                         | 13.4                       | 0.41/0.42/0.84               | 0.78      | 0.58    | 0.60 |\n",
    "| 3.    | PV-DM with sum of context word vectors    | 1.59                       | 0.27/0.27/0.94               | 0.66      | 0.55    | 0.37 |\n",
    "| 4.    | PV-DM with mean of context word vectors   | 2.23                       | 0.28/0.28/0.93               | 0.67      | 0.55    | 0.38 |\n",
    "| 5.    | PV-DBOW with sum of context word vector   | 1.24                       | 0.36/0.35/0.88               | 0.73      | 0.57    | 0.42 |\n",
    "| 6.    | PV-DBOW with mean of context word vectors | 1.38                       | 0.34/0.34/0.89               | 0.72      | 0.57    | 0.41 |\n",
    "| 7.    | Mean of gensim fasttext word vectors      | 23.4                       | 0.49/0.48/0.76               | 0.80      | 0.61    | 0.61 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on sample of Toronto Book Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toronto_data = []\n",
    "lines = 0\n",
    "with open('./books_in_sentences/books_large_p1.txt') as f1, open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if np.random.random() > 0.5:\n",
    "            if lines >= 100000:\n",
    "                break\n",
    "            lines += 1\n",
    "            if line not in ['\\n', '\\r\\n']:\n",
    "                line = re.split('\\.|\\?|\\n', line.strip())\n",
    "                for sentence in line:\n",
    "                    if len(sentence) > 1:\n",
    "                        sentence = tokenize(sentence)\n",
    "                        toronto_data.append(list(sentence))\n",
    "                        f2.write(' '.join(toronto_data[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'its', u'a', u'place', u'where', u'your', u'parents', u'wouldnt', u'even', u'care', u'if', u'you', u'stayed', u'out', u'late', u'biking', u'with', u'your', u'friends'] \n",
      "\n",
      "[u'only', u'because', u'everyone', u'felt', u'so', u'safe', u'so', u'comfy'] \n",
      "\n",
      "[u'they', u'dont', u'know', u'the', u'half', u'of', u'it'] \n",
      "\n",
      "[u'but', u'i', u'do'] \n",
      "\n",
      "[u'i', u'know', u'it', u'all', u'and', u'starlings', u'is', u'not', u'the', u'place', u'where', u'you', u'want', u'to', u'be', u'after', u'dark'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample training data\n",
    "for sentence in toronto_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 1.00 M words\n",
      "INFO:gensim.models.sent2vec:Read 1.35 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 11565, tokens read: 1346199\n",
      "INFO:gensim.models.sent2vec:training model with 4 workers on 11565 vocabulary and 100 features\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 1.19% words, 71718 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 3.41% words, 103980 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 5.20% words, 107437 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 7.12% words, 108416 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 9.05% words, 110556 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 11.28% words, 113706 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 13.50% words, 115893 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 15.58% words, 118216 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 17.66% words, 117794 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 20.04% words, 121133 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 22.26% words, 121882 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 24.64% words, 123331 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 26.71% words, 123452 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 28.79% words, 123443 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 31.02% words, 124357 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 33.09% words, 124822 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 35.32% words, 125724 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 37.40% words, 125765 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 39.63% words, 126134 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 41.70% words, 126218 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 43.78% words, 126538 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 45.86% words, 126268 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 47.94% words, 126532 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 50.16% words, 127139 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 52.09% words, 126376 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 54.32% words, 126700 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 56.54% words, 127059 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 58.47% words, 126439 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 60.70% words, 127038 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 62.18% words, 125991 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 63.97% words, 125526 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 66.19% words, 125603 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 68.27% words, 125740 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 70.35% words, 125685 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 72.43% words, 125859 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 74.50% words, 125915 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 77.03% words, 126441 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 79.10% words, 126482 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 81.48% words, 126842 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 83.40% words, 126800 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 85.48% words, 126957 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 87.11% words, 126391 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 89.34% words, 126751 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 91.27% words, 126446 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 93.20% words, 126143 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 95.28% words, 126269 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 97.50% words, 126356 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 99.43% words, 126170 words/s\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.sent2vec:training on 6730995 raw words (6507040 effective words) took 51.3s, 126801 effective words/s\n",
      "CPU times: user 1min 32s, sys: 24 s, total: 1min 56s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model on part of the Toronto Book Corpus (100,000 sentences)\n",
    "% time sent2vec_toronto_model = Sent2Vec(toronto_data, size=100, epochs=5, seed=42, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Sent2Vec object under s2v2, separately None\n",
      "INFO:gensim.utils:storing np array 'wi' to s2v2.wi.npy\n",
      "INFO:gensim.utils:saved s2v2\n"
     ]
    }
   ],
   "source": [
    "sent2vec_toronto_model.save('s2v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.427122900262\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.480262181097\n",
      "Test Spearman: 0.487383241191\n",
      "Test MSE: 0.783650957419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.19838024,  3.28760001,  3.33666651, ...,  3.66994808,\n",
       "        3.31433722,  3.50956535])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(sent2vec_toronto_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(sent2vec_toronto_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.59231490159325206]\n",
      "[0.59231490159325206, 0.57638238050609181]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197, 0.59193245778611636]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197, 0.59193245778611636, 0.59005628517823638]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197, 0.59193245778611636, 0.59005628517823638, 0.58161350844277671]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197, 0.59193245778611636, 0.59005628517823638, 0.58161350844277671, 0.60131332082551592]\n",
      "[0.59231490159325206, 0.57638238050609181, 0.58630393996247654, 0.58630393996247654, 0.575046904315197, 0.59193245778611636, 0.59005628517823638, 0.58161350844277671, 0.60131332082551592, 0.55628517823639778]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.59231490159325206,\n",
       " 0.57638238050609181,\n",
       " 0.58630393996247654,\n",
       " 0.58630393996247654,\n",
       " 0.575046904315197,\n",
       " 0.59193245778611636,\n",
       " 0.59005628517823638,\n",
       " 0.58161350844277671,\n",
       " 0.60131332082551592,\n",
       " 0.55628517823639778]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=sent2vec_toronto_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.70699999999999996]\n",
      "[0.70699999999999996, 0.69899999999999995]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004, 0.69099999999999995]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004, 0.69099999999999995, 0.67500000000000004]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004, 0.69099999999999995, 0.67500000000000004, 0.68799999999999994]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004, 0.69099999999999995, 0.67500000000000004, 0.68799999999999994, 0.67400000000000004]\n",
      "[0.70699999999999996, 0.69899999999999995, 0.69299999999999995, 0.69499999999999995, 0.67500000000000004, 0.69099999999999995, 0.67500000000000004, 0.68799999999999994, 0.67400000000000004, 0.70499999999999996]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.70699999999999996,\n",
       " 0.69899999999999995,\n",
       " 0.69299999999999995,\n",
       " 0.69499999999999995,\n",
       " 0.67500000000000004,\n",
       " 0.69099999999999995,\n",
       " 0.67500000000000004,\n",
       " 0.68799999999999994,\n",
       " 0.67400000000000004,\n",
       " 0.70499999999999996]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=sent2vec_toronto_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/sent2vec\n",
      "Read 1M words\n",
      "Number of words:  12938\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 138014  lr: 0.000000  loss: 2.647149  eta: 0h0m m   lr: 0.199982  loss: 7.583591  eta: 5h36m 4.887782  eta: 0h2m   eta: 0h1m h0m 0.160521  loss: 3.154479  eta: 0h0m 3.058576  eta: 0h0m %  words/sec/thread: 109013  lr: 0.123338  loss: 2.994709  eta: 0h0m 42.0%  words/sec/thread: 112304  lr: 0.116090  loss: 2.945761  eta: 0h0m   loss: 2.941653  eta: 0h0m 0h0m %  words/sec/thread: 119588  lr: 0.094700  loss: 2.858512  eta: 0h0m   words/sec/thread: 122925  lr: 0.083742  loss: 2.813961  eta: 0h0m 60.0%  words/sec/thread: 123896  lr: 0.079910  loss: 2.790588  eta: 0h0m 128617  lr: 0.060927  loss: 2.755890  eta: 0h0m 0h0m 0m 0m 131905  lr: 0.040259  loss: 2.701730  eta: 0h0m 81.4%  words/sec/thread: 132186  lr: 0.037159  loss: 2.698457  eta: 0h0m 92.0%  words/sec/thread: 136248  lr: 0.015957  loss: 2.660486  eta: 0h0m \n",
      "\n",
      "real\t0m32.302s\n",
      "user\t0m54.028s\n",
      "sys\t0m4.119s\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "% cd sent2vec\n",
    "! time ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 5 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 4 -t 0.0001 -dropout_k 2 -bucket 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim\n",
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n",
      "Dev Pearson: 0.467496919577\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.519848031764\n",
      "Test Spearman: 0.509933660616\n",
      "Test MSE: 0.743931587436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.88400938,  3.2521763 ,  3.12047375, ...,  2.98194377,\n",
       "        2.9174632 ,  3.46705309])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% cd ..\n",
    "eval_sick.evaluate(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.82199999999999995]\n",
      "[0.82199999999999995, 0.82099999999999995]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995, 0.81000000000000005]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995, 0.81000000000000005, 0.78200000000000003]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995, 0.81000000000000005, 0.78200000000000003, 0.82999999999999996]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995, 0.81000000000000005, 0.78200000000000003, 0.82999999999999996, 0.81699999999999995]\n",
      "[0.82199999999999995, 0.82099999999999995, 0.81100000000000005, 0.79500000000000004, 0.81499999999999995, 0.81000000000000005, 0.78200000000000003, 0.82999999999999996, 0.81699999999999995, 0.82499999999999996]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.82199999999999995,\n",
       " 0.82099999999999995,\n",
       " 0.81100000000000005,\n",
       " 0.79500000000000004,\n",
       " 0.81499999999999995,\n",
       " 0.81000000000000005,\n",
       " 0.78200000000000003,\n",
       " 0.82999999999999996,\n",
       " 0.81699999999999995,\n",
       " 0.82499999999999996]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.60356138706654172]\n",
      "[0.60356138706654172, 0.61761949390815374]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482, 0.61538461538461542]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482, 0.61538461538461542, 0.6163227016885553]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482, 0.61538461538461542, 0.6163227016885553, 0.62945590994371481]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482, 0.61538461538461542, 0.6163227016885553, 0.62945590994371481, 0.62288930581613511]\n",
      "[0.60356138706654172, 0.61761949390815374, 0.63414634146341464, 0.60694183864915574, 0.62851782363977482, 0.61538461538461542, 0.6163227016885553, 0.62945590994371481, 0.62288930581613511, 0.6369606003752345]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60356138706654172,\n",
       " 0.61761949390815374,\n",
       " 0.63414634146341464,\n",
       " 0.60694183864915574,\n",
       " 0.62851782363977482,\n",
       " 0.61538461538461542,\n",
       " 0.6163227016885553,\n",
       " 0.62945590994371481,\n",
       " 0.62288930581613511,\n",
       " 0.6369606003752345]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.556\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_toronto_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100000:\n",
    "                break\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_toronto_corpus('./books_in_sentences/books_large_p1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #10000, processed 113217 words (786978/s), 7839 word types, 10000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #20000, processed 256086 words (1046949/s), 14477 word types, 20000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #30000, processed 377626 words (913805/s), 17118 word types, 30000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #40000, processed 481481 words (966669/s), 19727 word types, 40000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #50000, processed 594311 words (931086/s), 21187 word types, 50000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #60000, processed 755296 words (1243627/s), 24414 word types, 60000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #70000, processed 987300 words (1505903/s), 27989 word types, 70000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #80000, processed 1146885 words (1073779/s), 30225 word types, 80000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #90000, processed 1270547 words (973977/s), 31144 word types, 90000 tags\n",
      "INFO:gensim.models.doc2vec:collected 32782 word types and 100000 unique tags from a corpus of 100000 examples and 1400798 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 13214 unique words (40% of original 32782, drops 19568)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 1365715 word corpus (97% of original 1400798, drops 35083)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 32782 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 53 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 1037883 word corpus (76.0% of prior 1365715)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13214 words and 100 dimensions: 57178200 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec model with PV-DBOW and sum of context word vectors\n",
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(dm=0, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "doc2vec_model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:training model with 3 workers on 13214 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.64% examples, 129682 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.38% examples, 138924 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.16% examples, 139614 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.98% examples, 149130 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.72% examples, 167098 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.53% examples, 166746 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.40% examples, 166633 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.22% examples, 167030 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.06% examples, 164868 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.81% examples, 164362 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.47% examples, 167043 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.24% examples, 168179 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.03% examples, 167642 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.87% examples, 168309 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.70% examples, 167190 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.55% examples, 166484 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.25% examples, 167576 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.78% examples, 168214 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.55% examples, 166992 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.42% examples, 166855 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.20% examples, 166898 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.05% examples, 166411 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.83% examples, 166841 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.60% examples, 169827 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.42% examples, 169656 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.25% examples, 169416 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.05% examples, 169532 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.91% examples, 169081 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.71% examples, 169350 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.30% examples, 168608 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.97% examples, 169689 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.79% examples, 169416 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.64% examples, 169582 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.41% examples, 169297 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.28% examples, 168966 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.06% examples, 169388 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.82% examples, 171155 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.66% examples, 170885 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.51% examples, 170924 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.16% examples, 170263 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.93% examples, 169385 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.80% examples, 169671 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.54% examples, 170747 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.20% examples, 170290 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.87% examples, 169631 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.60% examples, 169134 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.40% examples, 169029 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.22% examples, 168480 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.84% examples, 167996 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.47% examples, 168509 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.22% examples, 168634 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.06% examples, 168619 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.89% examples, 168739 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.73% examples, 168399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.57% examples, 168215 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.19% examples, 168011 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.76% examples, 168515 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.41% examples, 167841 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.05% examples, 167121 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.81% examples, 166993 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.62% examples, 166832 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.24% examples, 166013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.87% examples, 165666 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.48% examples, 166046 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.09% examples, 165695 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.70% examples, 164991 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.55% examples, 165033 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.26% examples, 164869 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.06% examples, 164641 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.86% examples, 164936 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.52% examples, 165554 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.16% examples, 165313 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.86% examples, 164965 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.60% examples, 164739 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.36% examples, 164626 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.09% examples, 164235 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.71% examples, 163997 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.47% examples, 164768 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.26% examples, 164964 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.06% examples, 164950 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.90% examples, 165028 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.63% examples, 164618 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.39% examples, 164260 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 63.16% examples, 164573 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.86% examples, 165334 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.62% examples, 165141 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.58% examples, 165449 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.42% examples, 165536 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.24% examples, 165277 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.96% examples, 165198 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.68% examples, 166000 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.45% examples, 165950 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.15% examples, 165664 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.84% examples, 165380 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.44% examples, 164953 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.24% examples, 164688 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.06% examples, 164954 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.82% examples, 165745 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.65% examples, 165715 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.51% examples, 165768 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.37% examples, 165907 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.22% examples, 165778 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.06% examples, 166030 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.81% examples, 166787 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.64% examples, 166734 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.30% examples, 166385 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.03% examples, 166304 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.92% examples, 166217 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.61% examples, 166052 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.37% examples, 166513 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.10% examples, 166786 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.96% examples, 166813 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.62% examples, 166424 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.31% examples, 166198 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.12% examples, 166028 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.82% examples, 165940 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.63% examples, 166623 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.41% examples, 166617 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.21% examples, 166516 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.84% examples, 166335 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.66% examples, 166190 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.52% examples, 166118 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.26% examples, 166364 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.89% examples, 166690 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.61% examples, 166531 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.47% examples, 166509 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.30% examples, 166535 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.19% examples, 166476 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.03% examples, 166677 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.52% examples, 166755 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.07% examples, 166498 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.80% examples, 166327 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 28015960 raw words (22759350 effective words) took 136.8s, 166424 effective words/s\n",
      "CPU times: user 2min 48s, sys: 55 s, total: 3min 43s\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22759350"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time doc2vec_model.train(train_corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_sick.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n",
      "  lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.129711484713\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.153188257519\n",
      "Test Spearman: 0.133704859493\n",
      "Test MSE: 0.99598130755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.44692591,  3.56224901,  3.71145853, ...,  3.49521685,\n",
       "        3.63893017,  3.43376413])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sick.evaluate(doc2vec_model, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.67600000000000005]\n",
      "[0.67600000000000005, 0.68300000000000005]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003, 0.65700000000000003]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003, 0.65700000000000003, 0.66600000000000004]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003, 0.65700000000000003, 0.66600000000000004, 0.66800000000000004]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003, 0.65700000000000003, 0.66600000000000004, 0.66800000000000004, 0.68600000000000005]\n",
      "[0.67600000000000005, 0.68300000000000005, 0.69799999999999995, 0.66300000000000003, 0.65400000000000003, 0.65700000000000003, 0.66600000000000004, 0.66800000000000004, 0.68600000000000005, 0.68100000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.67600000000000005,\n",
       " 0.68300000000000005,\n",
       " 0.69799999999999995,\n",
       " 0.66300000000000003,\n",
       " 0.65400000000000003,\n",
       " 0.65700000000000003,\n",
       " 0.66600000000000004,\n",
       " 0.66800000000000004,\n",
       " 0.68600000000000005,\n",
       " 0.68100000000000005]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.55482661668228683]\n",
      "[0.55482661668228683, 0.549203373945642]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813, 0.57973733583489684]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813, 0.57973733583489684, 0.55159474671669795]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813, 0.57973733583489684, 0.55159474671669795, 0.57317073170731703]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813, 0.57973733583489684, 0.55159474671669795, 0.57317073170731703, 0.57129455909943716]\n",
      "[0.55482661668228683, 0.549203373945642, 0.56566604127579734, 0.55534709193245779, 0.54596622889305813, 0.57973733583489684, 0.55159474671669795, 0.57317073170731703, 0.57129455909943716, 0.56566604127579734]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.55482661668228683,\n",
       " 0.549203373945642,\n",
       " 0.56566604127579734,\n",
       " 0.55534709193245779,\n",
       " 0.54596622889305813,\n",
       " 0.57973733583489684,\n",
       " 0.55159474671669795,\n",
       " 0.57317073170731703,\n",
       " 0.57129455909943716,\n",
       " 0.56566604127579734]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_classification.eval_nested_kfold(model=doc2vec_model, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "eval_trec.evaluate(doc2vec_model, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: It is evident that more data = better results for sent2vec as the above model (trained for 5 epochs) achieves similar results to the model trained on the much smaller Lee corpus (trained for 20 epochs)\n",
    "\n",
    "| S.No. | Model             | Total Execution Time (in seconds) | Pearson/Spearman/MSE on SICK | MR   | SUBJ | TREC |\n",
    "|-------|-------------------|-----------------------------------|------------------------------|------|------|------|\n",
    "| 1.    | Gensim Sent2Vec   | 75                                | 0.48/0.48/0.78               | 0.58 | 0.69 | 0.51 |\n",
    "| 2.    | Original Sent2Vec | 32.3                              | 0.51/0.50/0.74               | 0.62 | 0.81 | 0.55 |\n",
    "| 3.    | Doc2Vec DBOW (sum)| 136                               | 0.15/0.13/0.99               | 0.56 | 0.67 | 0.29 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
