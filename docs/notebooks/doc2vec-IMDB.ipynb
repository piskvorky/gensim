{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim doc2vec & IMDB sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: section on introduction & motivation\n",
    "\n",
    "TODO: prerequisites + dependencies (statsmodels, patsy, ?)\n",
    "\n",
    "### Requirements\n",
    "Following are the dependencies for this tutorial:\n",
    "    - testfixtures\n",
    "    - statsmodels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total running time:  41.018378\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text\n",
    "\n",
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "    # Concat and normalize test/train data\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    alldata = u''\n",
    "\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "\n",
    "        for txt in txt_files:\n",
    "            with smart_open.smart_open(txt, \"rb\") as t:\n",
    "                t_clean = t.read().decode(\"utf-8\")\n",
    "\n",
    "                for c in control_chars:\n",
    "                    t_clean = t_clean.replace(c, ' ')\n",
    "\n",
    "                temp += t_clean\n",
    "\n",
    "            temp += \"\\n\"\n",
    "\n",
    "        temp_norm = normalize_text(temp)\n",
    "\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            n.write(temp_norm.encode(\"utf-8\"))\n",
    "\n",
    "        alldata += temp_norm\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "end = time.clock()\n",
    "print (\"total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is small enough to be read into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "Parameter choices below vary:\n",
    "\n",
    "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods for evaluating error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) – with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-06-06 15:19:50.208091\n",
      "*0.408320 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 131.9s 33.6s\n",
      "*0.341600 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 131.9s 48.3s\n",
      "*0.239960 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 35.3s 45.9s\n",
      "*0.193200 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 35.3s 48.3s\n",
      "*0.268640 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 48.6s 48.5s\n",
      "*0.208000 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 48.6s 47.4s\n",
      "*0.216160 : 1 passes : dbow+dmm 0.0s 168.9s\n",
      "*0.176000 : 1 passes : dbow+dmm_inferred 0.0s 176.4s\n",
      "*0.237280 : 1 passes : dbow+dmc 0.0s 169.3s\n",
      "*0.194400 : 1 passes : dbow+dmc_inferred 0.0s 183.9s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.346760 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 133.4s 42.2s\n",
      "*0.145280 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 29.0s 42.8s\n",
      "*0.210920 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 38.8s 42.2s\n",
      "*0.139120 : 2 passes : dbow+dmm 0.0s 173.2s\n",
      "*0.147120 : 2 passes : dbow+dmc 0.0s 191.8s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.314920 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 112.3s 37.6s\n",
      "*0.126720 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 28.4s 42.6s\n",
      "*0.191920 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 37.9s 42.2s\n",
      "*0.121640 : 3 passes : dbow+dmm 0.0s 190.8s\n",
      "*0.127040 : 3 passes : dbow+dmc 0.0s 188.1s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.282080 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 104.9s 36.3s\n",
      "*0.115520 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 27.6s 49.9s\n",
      "*0.181280 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 40.7s 42.2s\n",
      "*0.114760 : 4 passes : dbow+dmm 0.0s 188.6s\n",
      "*0.116040 : 4 passes : dbow+dmc 0.0s 192.5s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.257560 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 102.5s 35.8s\n",
      "*0.265200 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 102.5s 48.6s\n",
      "*0.110880 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 27.0s 46.5s\n",
      "*0.117600 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 27.0s 50.5s\n",
      "*0.171240 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 39.1s 43.7s\n",
      "*0.207200 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 39.1s 47.5s\n",
      "*0.108920 : 5 passes : dbow+dmm 0.0s 203.4s\n",
      "*0.114800 : 5 passes : dbow+dmm_inferred 0.0s 213.4s\n",
      "*0.111520 : 5 passes : dbow+dmc 0.0s 189.5s\n",
      "*0.132000 : 5 passes : dbow+dmc_inferred 0.0s 202.6s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.240440 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 117.6s 39.2s\n",
      "*0.107600 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 32.3s 52.1s\n",
      "*0.166800 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 46.4s 40.8s\n",
      "*0.108160 : 6 passes : dbow+dmm 0.0s 197.8s\n",
      "*0.109920 : 6 passes : dbow+dmc 0.0s 189.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.225280 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 102.8s 36.0s\n",
      "*0.105560 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 31.0s 47.0s\n",
      "*0.164320 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 38.6s 43.7s\n",
      "*0.104760 : 7 passes : dbow+dmm 0.0s 187.1s\n",
      "*0.107600 : 7 passes : dbow+dmc 0.0s 182.9s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.214280 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 99.2s 41.1s\n",
      "*0.102400 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 28.6s 47.3s\n",
      "*0.161000 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 36.4s 40.9s\n",
      "*0.102720 : 8 passes : dbow+dmm 0.0s 188.2s\n",
      "*0.104280 : 8 passes : dbow+dmc 0.0s 187.3s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.206840 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 96.9s 41.4s\n",
      " 0.102920 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 27.1s 46.4s\n",
      "*0.158600 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 40.3s 40.7s\n",
      "*0.101880 : 9 passes : dbow+dmm 0.0s 188.1s\n",
      "*0.103960 : 9 passes : dbow+dmc 0.0s 192.2s\n",
      "completed pass 9 at alpha 0.015400\n",
      "*0.198960 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 116.0s 43.0s\n",
      "*0.194000 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 116.0s 54.2s\n",
      "*0.102120 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 27.8s 47.1s\n",
      "*0.100000 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 27.8s 50.4s\n",
      "*0.156640 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 38.3s 41.9s\n",
      "*0.178400 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 38.3s 46.8s\n",
      " 0.102520 : 10 passes : dbow+dmm 0.0s 192.5s\n",
      "*0.104000 : 10 passes : dbow+dmm_inferred 0.0s 207.3s\n",
      "*0.103560 : 10 passes : dbow+dmc 0.0s 191.0s\n",
      "*0.115200 : 10 passes : dbow+dmc_inferred 0.0s 203.5s\n",
      "completed pass 10 at alpha 0.014200\n",
      "*0.192000 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 97.3s 42.7s\n",
      " 0.102840 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.8s 45.1s\n",
      " 0.156680 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 36.9s 41.1s\n",
      "*0.101600 : 11 passes : dbow+dmm 0.0s 187.8s\n",
      " 0.103880 : 11 passes : dbow+dmc 0.0s 187.9s\n",
      "completed pass 11 at alpha 0.013000\n",
      "*0.190440 : 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 99.1s 44.5s\n",
      " 0.103640 : 12 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 34.7s 45.9s\n",
      "*0.154640 : 12 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 37.3s 41.8s\n",
      " 0.103400 : 12 passes : dbow+dmm 0.0s 190.1s\n",
      " 0.103640 : 12 passes : dbow+dmc 0.0s 190.6s\n",
      "completed pass 12 at alpha 0.011800\n",
      "*0.186840 : 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 99.1s 41.0s\n",
      " 0.102560 : 13 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.7s 44.5s\n",
      "*0.153880 : 13 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.9s 40.0s\n",
      " 0.103760 : 13 passes : dbow+dmm 0.0s 182.8s\n",
      " 0.103680 : 13 passes : dbow+dmc 0.0s 174.8s\n",
      "completed pass 13 at alpha 0.010600\n",
      "*0.184600 : 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 92.0s 38.6s\n",
      " 0.103080 : 14 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.7s 44.5s\n",
      "*0.153760 : 14 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.8s 39.0s\n",
      " 0.103120 : 14 passes : dbow+dmm 0.0s 177.6s\n",
      " 0.103960 : 14 passes : dbow+dmc 0.0s 176.0s\n",
      "completed pass 14 at alpha 0.009400\n",
      "*0.182720 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 91.7s 38.7s\n",
      "*0.179600 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 91.7s 50.8s\n",
      " 0.103280 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.7s 43.5s\n",
      " 0.104400 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 26.7s 47.8s\n",
      "*0.153720 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 36.0s 39.0s\n",
      " 0.187200 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 36.0s 43.7s\n",
      " 0.103520 : 15 passes : dbow+dmm 0.0s 174.9s\n",
      " 0.105600 : 15 passes : dbow+dmm_inferred 0.0s 183.2s\n",
      " 0.103680 : 15 passes : dbow+dmc 0.0s 175.9s\n",
      "*0.106000 : 15 passes : dbow+dmc_inferred 0.0s 189.9s\n",
      "completed pass 15 at alpha 0.008200\n",
      "*0.181040 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 91.6s 41.2s\n",
      " 0.103240 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.7s 45.3s\n",
      "*0.153600 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 36.1s 40.6s\n",
      " 0.103960 : 16 passes : dbow+dmm 0.0s 175.9s\n",
      "*0.103400 : 16 passes : dbow+dmc 0.0s 175.9s\n",
      "completed pass 16 at alpha 0.007000\n",
      "*0.180080 : 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 92.1s 40.3s\n",
      " 0.102760 : 17 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.3s 44.9s\n",
      "*0.152880 : 17 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.4s 39.0s\n",
      " 0.103200 : 17 passes : dbow+dmm 0.0s 182.5s\n",
      "*0.103280 : 17 passes : dbow+dmc 0.0s 178.0s\n",
      "completed pass 17 at alpha 0.005800\n",
      "*0.178720 : 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 91.1s 39.0s\n",
      "*0.101640 : 18 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.4s 44.3s\n",
      "*0.152280 : 18 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.6s 39.5s\n",
      " 0.102360 : 18 passes : dbow+dmm 0.0s 183.8s\n",
      " 0.103320 : 18 passes : dbow+dmc 0.0s 179.0s\n",
      "completed pass 18 at alpha 0.004600\n",
      "*0.178600 : 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 91.1s 38.9s\n",
      " 0.102320 : 19 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.3s 45.7s\n",
      "*0.151920 : 19 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.5s 40.7s\n",
      " 0.102240 : 19 passes : dbow+dmm 0.0s 181.7s\n",
      "*0.103000 : 19 passes : dbow+dmc 0.0s 181.7s\n",
      "completed pass 19 at alpha 0.003400\n",
      "*0.177360 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 90.9s 40.0s\n",
      " 0.190800 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 90.9s 52.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.102520 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 26.4s 45.2s\n",
      " 0.108800 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 26.4s 48.7s\n",
      "*0.151680 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 35.5s 40.8s\n",
      " 0.182400 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 35.5s 45.3s\n",
      " 0.102320 : 20 passes : dbow+dmm 0.0s 183.5s\n",
      " 0.113200 : 20 passes : dbow+dmm_inferred 0.0s 192.3s\n",
      "*0.102800 : 20 passes : dbow+dmc 0.0s 183.3s\n",
      " 0.111200 : 20 passes : dbow+dmc_inferred 0.0s 196.1s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2017-06-06 19:46:10.508929\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100000 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred\n",
      "0.101600 dbow+dmm\n",
      "0.101640 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "0.102800 dbow+dmc\n",
      "0.104000 dbow+dmm_inferred\n",
      "0.106000 dbow+dmc_inferred\n",
      "0.151680 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n",
      "0.177360 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "0.178400 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred\n",
      "0.179600 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred\n"
     ]
    }
   ],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 47495...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4):\n",
      " [(47495, 0.8063223361968994), (28683, 0.4661555588245392), (10030, 0.3962923586368561)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4):\n",
      " [(47495, 0.9660482406616211), (17469, 0.5925078392028809), (52349, 0.5742233991622925)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4):\n",
      " [(47495, 0.8801028728485107), (60782, 0.5431949496269226), (42472, 0.5375599265098572)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated – just 3 steps starting at a high alpha – and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (43375): «the film \" chaos \" takes its name from gleick's 1988 pop science explanation of chaos theory . what does the book or anything related to the content of the book have to do with the plot of the movie \" chaos \" ? nothing . the film makers seem to have skimmed the book ( obviously without understanding a thing about it ) looking for a \" theme \" to united the series of mundane action sequences that overlie the flimsy string of events that acts in place of a plot in the film . in this respect , the movie \" choas \" resembles the canadian effort \" cube , \" in which prime numbers function as a device to mystify the audience so that the ridiculousness of the plot will not be noticed : in \" cube \" a bunch of prime numbers are tossed in so that viewers will attribute their lack of understanding to lack of knowledge about primes : the same approach is taken in \" chaos \" : disconnected extracts from gleick's books are thrown in make the doings of the bad guy in the film seem fiendishly clever . this , of course , is an insultingly condescending treatment of the audience , and any literate viewer of \" chaos \" who can stand to sit through the entire film will end up bewildered . how could a film so bad be made ? rewritten as a novel , the story in \" chaos \" would probably not even make it past a literary agent's secretary's desk . how could ( at least ) hundreds of thousands ( and probably millions ) of dollars have been thrown away on what can only be considered a waste of time for everyone except those who took home money from the film ? regarding what's in the movie , every performance is phoned in . save for technical glitches , it would be astonishing if more than one take was used for any one scene . the story is uniformly senseless : the last time i saw a story to disconnected it was the production of a literal eight-year-old . among other massive shortcomings are the following : the bad guy leaves hints for the police to follow . he has no reason whatsoever for leaving such hints . police officers do not carry or use radios . dupes of the bad guy have no reason to act in concert with the bad guy . let me strongly recommend that no one watch this film . if there is any other movie you like ( or even simply do not hate ) watch that instead .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4):\n",
      "\n",
      "MOST (48890, 0.5806792378425598): «asmali konak has arguably become one of the best tv series to come out of turkey . with its unique cinematography and visual approach to filming , the series has gained a wide following base with rating records continuously broken . personally i do not agree with singers becoming actors ( hence , ozcan deniz - the lead actor ) but i guess the figures speak for themselves . in relation to the movie , it was disgusting to see how much someone can destroy such a plotline . years in the making , this movie was able to oversee every descent story that existed within the series . not only that , the cultural mistakes were unacceptable , with an idiotic scene involving the family members dancing ( greek style ) and breaking plates , which does not exists anywhere within the turkish culture . some argue the movie should be taken as a stand alone movie not as a continuation of the tv series but this theory has one major fall , the way the movie was marketed was that it will be picking up where the series left off and will conclude the series once and for all . so with that note in mind , me and everyone i know , would have asked for a refund and accepted to stand outside the theatre to warn other victims .»\n",
      "\n",
      "MEDIAN (93452, 0.22335509955883026): «this is the second film ( dead men walking ) set in a prison by theasylum . the mythos behind plot is very good , russian mafia has this demon do there dirty work and the rainbow array of inmates have to defend their bars & mortar . jennifer lee ( see interview ) wiggins stars as a prison guard who has a inmate , who maybe a demon . the monster suit is awesome and frightening , and a different look that almost smacks of a toy franchise , hey if full moon and todd mcfarlane can make action figures for any character . . why not the beast from bray road wolfette , shapeshifter with medallion accessory , or the rhett giles everyman hero with removable appendages .»\n",
      "\n",
      "LEAST (57989, -0.22353392839431763): «saw this movie on re-run just once , when i was about 13 , in 1980 . it completely matched my teenaged fantasies of sweet , gentle , interesting  and let's face it  hot  \" older \" guys . just ordered it from cd universe about a month ago , and have given it about four whirls in the two weeks since . as somebody mentioned  i'm haunted by it . as somebody else mentioned  i think it's part of a midlife crisis as well ! being 39 and realizing how much has changed since those simpler '70s times when girls of 13 actually did take buses and go to malls together and had a lot more freedom away from the confines of modern suburbia makes me sad for my daughter  who is nearly 13 herself . thirteen back then was in many ways a lot more grown up . the film is definitely '70s but not in a super-dated cheesy way , in fact the outfits denise miller as jessie wears could be current now ! you know what they say , everything that goes around . . . although the short-short jogging shorts worn by rex with the to-the-knees sweat socks probably won't make a comeback . the subject matter is handled in a very sensitive way and the characters are treated with a lot of respect . it's not the most chatty movie going  i often wished for more to be said between jessie and michael that would cement why he was also attracted to her . but the acting is solid , the movie is sweet and atmospheric , and the fringe characters give great performances . mary beth manning as jessie's friend caroline is a total hoot  i think we all had friends like her . maia danziger as the relentless flirt with michael gives a wiggy , stoned-out performance that just makes you laugh  because we also all knew girls that acted like that . denise miller knocked her performance out of the ballpark with a very down-to-earth quality likely credited to her uknown status and being new to the industry . and i think not a little of the credit for the film's theatre-grade quality comes from the very capable , brilliant hands of the story's authors , carole and the late bruce hart , who also wrote for sesame street . they really cared about the message of the movie , which was not an overt in-your-face thing , while at the same time understanding how eager many girls are to grow up at that age . one thing that made me love the film then as much as now is not taking the cliché , easy , tied-with-a-bow but sort of let-down ending . in fact it's probably the end that has caused so many women to return to viewing the movie in their later years . re-watching sooner or later has me absolutely sick with nostalgia for those simpler times , and has triggered a ridiculous and sudden obsession with catching up with rex smith  whom while i enjoyed his albums sooner or later and forever when i was young , i never plastered his posters on my walls as i did some of my other faves . in the past week , i've put his music on my ipod , read fan sites , found interviews ( and marveled in just how brilliant he really is  the man has a fascinating way of thinking ) , watched clips on youtube  what am i , 13 ? i guess that's the biggest appeal of this movie . remembering what it was like to be 13 and the whole world was ahead of you .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'gymnast' (36 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)</th><th>Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)</th></tr><tr><td>[('scientist', 0.530441164970398),<br>\n",
       "('psychotherapist', 0.527083694934845),<br>\n",
       "('parapsychologist', 0.5239906907081604),<br>\n",
       "('cringer', 0.5199892520904541),<br>\n",
       "('samir', 0.5048707127571106),<br>\n",
       "('reporter', 0.49532145261764526),<br>\n",
       "('swimmer', 0.4937909245491028),<br>\n",
       "('thrill-seeker', 0.4905340373516083),<br>\n",
       "('chiara', 0.48281964659690857),<br>\n",
       "('psychiatrist', 0.4788440763950348),<br>\n",
       "('nerd', 0.4779984951019287),<br>\n",
       "('surgeon', 0.47712844610214233),<br>\n",
       "('jock', 0.4741038382053375),<br>\n",
       "('geek', 0.4714686870574951),<br>\n",
       "('mumu', 0.47104766964912415),<br>\n",
       "('painter', 0.4689804017543793),<br>\n",
       "('cheater', 0.4655175805091858),<br>\n",
       "('hypnotist', 0.4645438492298126),<br>\n",
       "('whizz', 0.46407681703567505),<br>\n",
       "('cryptozoologist', 0.4627385437488556)]</td><td>[('bang-bang', 0.4289792478084564),<br>\n",
       "('master', 0.41190674901008606),<br>\n",
       "('greenleaf', 0.38207903504371643),<br>\n",
       "('122', 0.3811250925064087),<br>\n",
       "('fingernails', 0.3794997036457062),<br>\n",
       "('cardboard-cutout', 0.3740081787109375),<br>\n",
       "(\"album'\", 0.3706256151199341),<br>\n",
       "('sex-starved', 0.3696949779987335),<br>\n",
       "('creme-de-la-creme', 0.36426788568496704),<br>\n",
       "('destroyed', 0.3638569116592407),<br>\n",
       "('imminent', 0.3612757921218872),<br>\n",
       "('cruisers', 0.3568859398365021),<br>\n",
       "(\"emo's\", 0.35605981945991516),<br>\n",
       "('lavransdatter', 0.3534432649612427),<br>\n",
       "(\"'video'\", 0.3508487641811371),<br>\n",
       "('garris', 0.3507363796234131),<br>\n",
       "('romanzo', 0.3495352268218994),<br>\n",
       "('tombes', 0.3494585454463959),<br>\n",
       "('story-writers', 0.3461073637008667),<br>\n",
       "('georgette', 0.34602558612823486)]</td><td>[('ex-marine', 0.5273298621177673),<br>\n",
       "('koichi', 0.5020822882652283),<br>\n",
       "('dorkish', 0.49750325083732605),<br>\n",
       "('fenyö', 0.4765225946903229),<br>\n",
       "('castleville', 0.46756264567375183),<br>\n",
       "('smoorenburg', 0.46484801173210144),<br>\n",
       "('chimp', 0.46456438302993774),<br>\n",
       "('swimmer', 0.46236276626586914),<br>\n",
       "('falcone', 0.4614230990409851),<br>\n",
       "('yak', 0.45991501212120056),<br>\n",
       "('gms', 0.4542686939239502),<br>\n",
       "('iván', 0.4503802955150604),<br>\n",
       "('spidy', 0.4494086503982544),<br>\n",
       "('arnie', 0.44659116864204407),<br>\n",
       "('hobo', 0.4465593695640564),<br>\n",
       "('evelyne', 0.4455353617668152),<br>\n",
       "('pandey', 0.4452363848686218),<br>\n",
       "('hector', 0.4442984461784363),<br>\n",
       "('baboon', 0.44382452964782715),<br>\n",
       "('miao', 0.4437481164932251)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors – they remain at their random initialized values – unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4): 31.50% correct (3154 of 10012)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4): 0.00% correct (0 of 10012)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4): 32.24% correct (3228 of 10012)\n"
     ]
    }
   ],
   "source": [
    "# assuming something like\n",
    "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
    "# is in local directory\n",
    "# note: this takes many minutes\n",
    "for model in word_models:\n",
    "    sections = model.accuracy('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies – at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This cell left intentionally erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mix the Google dataset (if locally available) into the word tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_g100b = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_g100b.compact_name = 'w2v_g100b'\n",
    "word_models.append(w2v_g100b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get copious logging output from above steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To auto-reload python code while developing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
