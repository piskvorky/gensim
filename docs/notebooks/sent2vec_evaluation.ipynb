{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains evaluation of Gensim sent2vec and its comparison with the [original c++ implementation of sent2vec](https://github.com/epfml/sent2vec), Gensim's Doc2Vec and Gensim's FastText. The evaluation scripts used in this notebook are based on the code provided by [Kiros et al., 2015](https://github.com/ryankiros/skip-thoughts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional installations required to run the notebook\n",
    "! pip install scipy\n",
    "! pip install sklearn\n",
    "! pip install keras\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download SICK data\n",
    "# Unzip the following files and place their contents in a folder named SICK2014\n",
    "! wget http://alt.qcri.org/semeval2014/task1/data/uploads/sick_train.zip\n",
    "! wget http://alt.qcri.org/semeval2014/task1/data/uploads/sick_trial.zip\n",
    "! wget http://alt.qcri.org/semeval2014/task1/data/uploads/sick_test_annotated.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download SUBJ and MR data\n",
    "# Unzip the following files and place their contents in a folder named classification_data\n",
    "! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "! wget https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download TREC data\n",
    "# Unzip the following files and place their contents in a folder named trec_data\n",
    "! wget http://cogcomp.org/Data/QA/QC/train_5500.label\n",
    "! wget http://cogcomp.org/Data/QA/QC/TREC_10.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.sent2vec import Sent2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.utils import tokenize as gensim_tokenize\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import smart_open\n",
    "import logging\n",
    "import sys\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import os.path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import subprocess\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from scipy.sparse import hstack\n",
    "from numpy.random import RandomState\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Evaluation Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive-Bayes features\n",
    "# Derived from https://github.com/mesnilgr/nbsvm\n",
    "def tokenize(sentence, grams):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    for gram in grams:\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            tokens += [\"_*_\".join(words[i:i+gram])]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_dict(X, grams):\n",
    "    dic = Counter()\n",
    "    for sentence in X:\n",
    "        dic.update(tokenize(sentence, grams))\n",
    "    return dic\n",
    "\n",
    "\n",
    "def compute_ratio(poscounts, negcounts, alpha=1):\n",
    "    alltokens = list(set(poscounts.keys() + negcounts.keys()))\n",
    "    dic = dict((t, i) for i, t in enumerate(alltokens))\n",
    "    d = len(dic)\n",
    "    p, q = np.ones(d) * alpha , np.ones(d) * alpha\n",
    "    for t in alltokens:\n",
    "        p[dic[t]] += poscounts[t]\n",
    "        q[dic[t]] += negcounts[t]\n",
    "    p /= abs(p).sum()\n",
    "    q /= abs(q).sum()\n",
    "    r = np.log(p/q)\n",
    "    return dic, r\n",
    "\n",
    "\n",
    "def process_text(text, dic, r, grams):\n",
    "    \"\"\"\n",
    "    Return sparse feature matrix\n",
    "    \"\"\"\n",
    "    X = lil_matrix((len(text), len(dic)))\n",
    "    for i, l in enumerate(text):\n",
    "        tokens = tokenize(l, grams)\n",
    "        indexes = []\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                indexes += [dic[t]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        indexes = list(set(indexes))\n",
    "        indexes.sort()\n",
    "        for j in indexes:\n",
    "            X[i,j] = r[j]\n",
    "    return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ft_sent_vec(model, sentence):\n",
    "    sent_vec = np.zeros(model.vector_size)\n",
    "    cnt = 0\n",
    "    for word in sentence:\n",
    "        if word in model.wv.vocab:\n",
    "            sent_vec += model[word]\n",
    "            cnt += 1\n",
    "    if cnt > 0:\n",
    "        sent_vec *= (1.0 / cnt)\n",
    "    return sent_vec\n",
    "\n",
    "\n",
    "def original_sent2vec(sentences):\n",
    "    with smart_open.smart_open('./input.txt', 'w') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    with smart_open.smart_open('./output.txt', 'w') as f1, smart_open.smart_open('./input.txt') as f2:\n",
    "        p = subprocess.Popen('./sent2vec/fasttext print-sentence-vectors ./sent2vec/my_model.bin', shell=True, stdin=f2, stdout=f1, stderr=f1)\n",
    "        p.communicate()\n",
    "    with smart_open.smart_open('./output.txt') as f:\n",
    "        input_ = []\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            input_.append([float(j) for j in line])\n",
    "    return np.array(input_)\n",
    "\n",
    "\n",
    "def sent2vec_vectors(model, examples):\n",
    "    ans = []\n",
    "    for example in examples:\n",
    "        res = model[example.split(' ')]\n",
    "        ans.append(res)\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code for SICK data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sick(model=None, model_name='sent2vec', evaltest=True, loc='./SICK2014', seed=42):\n",
    "    \"\"\"\n",
    "    Run experiment\n",
    "    \"\"\"\n",
    "    print 'Preparing data...'\n",
    "    train, dev, test, scores = load_sick_data(loc)\n",
    "    train[0], train[1], scores[0] = shuffle(train[0], train[1], scores[0], random_state=seed)\n",
    "    \n",
    "    print 'Computing training sentence vectors...'\n",
    "    if model_name == 'sent2vec':\n",
    "        trainA = sent2vec_vectors(model, train[0])\n",
    "        trainB = sent2vec_vectors(model, train[1])\n",
    "    elif model_name == 'doc2vec':\n",
    "        trainA = np.array([model.infer_vector(example.split(' ')) for example in train[0]])\n",
    "        trainB = np.array([model.infer_vector(example.split(' ')) for example in train[1]])\n",
    "    elif model_name == 'original_sent2vec':\n",
    "        trainA = original_sent2vec(train[0])\n",
    "        trainB = original_sent2vec(train[1])\n",
    "    else:\n",
    "        trainA = np.array([ft_sent_vec(model, example.split(' ')) for example in train[0]])\n",
    "        trainB = np.array([ft_sent_vec(model, example.split(' ')) for example in train[1]])\n",
    "    \n",
    "    print 'Computing development sentence vectors...'\n",
    "    if model_name == 'sent2vec':\n",
    "        devA = sent2vec_vectors(model, dev[0])\n",
    "        devB = sent2vec_vectors(model, dev[1])\n",
    "    elif model_name == 'doc2vec':\n",
    "        devA = np.array([model.infer_vector(example.split(' ')) for example in dev[0]])\n",
    "        devB = np.array([model.infer_vector(example.split(' ')) for example in dev[1]])\n",
    "    elif model_name == 'original_sent2vec':\n",
    "        devA = original_sent2vec(dev[0])\n",
    "        devB = original_sent2vec(dev[1])\n",
    "    else:\n",
    "        devA = np.array([ft_sent_vec(model, example.split(' ')) for example in dev[0]])\n",
    "        devB = np.array([ft_sent_vec(model, example.split(' ')) for example in dev[1]])\n",
    "\n",
    "    print 'Computing feature combinations...'\n",
    "    trainF = np.c_[np.abs(trainA - trainB), trainA * trainB]\n",
    "    devF = np.c_[np.abs(devA - devB), devA * devB]\n",
    "\n",
    "    print 'Encoding labels...'\n",
    "    trainY = encode_labels(scores[0])\n",
    "    devY = encode_labels(scores[1])\n",
    "\n",
    "    print 'Compiling model...'\n",
    "    lrmodel = prepare_model(ninputs=trainF.shape[1])\n",
    "\n",
    "    print 'Training...'\n",
    "    bestlrmodel = train_model(lrmodel, trainF, trainY, devF, devY, scores[1])\n",
    "\n",
    "    if evaltest:\n",
    "        print 'Computing test sentence vectors...'\n",
    "        if model_name == 'sent2vec':\n",
    "            testA = sent2vec_vectors(model, test[0])\n",
    "            testB = sent2vec_vectors(model, test[1])\n",
    "        elif model_name == 'doc2vec':\n",
    "            testA = np.array([model.infer_vector(example.split(' ')) for example in test[0]])\n",
    "            testB = np.array([model.infer_vector(example.split(' ')) for example in test[1]])\n",
    "        elif model_name == 'original_sent2vec':\n",
    "            testA = original_sent2vec(test[0])\n",
    "            testB = original_sent2vec(test[1])\n",
    "        else:\n",
    "            testA = np.array([ft_sent_vec(model, example.split(' ')) for example in test[0]])\n",
    "            testB = np.array([ft_sent_vec(model, example.split(' ')) for example in test[1]])\n",
    "\n",
    "        print 'Computing feature combinations...'\n",
    "        testF = np.c_[np.abs(testA - testB), testA * testB]\n",
    "\n",
    "        print 'Evaluating...'\n",
    "        r = np.arange(1,6)\n",
    "        yhat = np.dot(bestlrmodel.predict_proba(testF, verbose=0), r)\n",
    "        pr = pearsonr(yhat, scores[2])[0]\n",
    "        sr = spearmanr(yhat, scores[2])[0]\n",
    "        se = mse(yhat, scores[2])\n",
    "        print 'Test Pearson: ' + str(pr)\n",
    "        print 'Test Spearman: ' + str(sr)\n",
    "        print 'Test MSE: ' + str(se)\n",
    "\n",
    "        return yhat\n",
    "\n",
    "\n",
    "def prepare_model(ninputs=9600, nclass=5):\n",
    "    \"\"\"\n",
    "    Set up and compile the model architecture (Logistic regression)\n",
    "    \"\"\"\n",
    "    lrmodel = Sequential()\n",
    "    lrmodel.add(Dense(input_dim=ninputs, output_dim=nclass))\n",
    "    lrmodel.add(Activation('softmax'))\n",
    "    lrmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return lrmodel\n",
    "\n",
    "\n",
    "def train_model(lrmodel, X, Y, devX, devY, devscores):\n",
    "    \"\"\"\n",
    "    Train model, using pearsonr on dev for early stopping\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    best = -1.0\n",
    "    r = np.arange(1,6)\n",
    "    prev_val_loss = 1000000007\n",
    "    \n",
    "    while not done:\n",
    "        # Every 100 epochs, check Pearson on development set\n",
    "        train_history = lrmodel.fit(X, Y, verbose=0, shuffle=False, validation_data=(devX, devY))\n",
    "        val_loss = round(np.min(train_history.history['val_loss']), 4)\n",
    "        yhat = np.dot(lrmodel.predict_proba(devX, verbose=0), r)\n",
    "        score = pearsonr(yhat, devscores)[0]\n",
    "        if score > best and val_loss < prev_val_loss:\n",
    "            # print score\n",
    "            best = score\n",
    "            bestlrmodel = prepare_model(ninputs=X.shape[1])\n",
    "            bestlrmodel.set_weights(lrmodel.get_weights())\n",
    "            prev_val_loss = val_loss\n",
    "        else:\n",
    "            done = True\n",
    "\n",
    "    yhat = np.dot(bestlrmodel.predict_proba(devX, verbose=0), r)\n",
    "    score = pearsonr(yhat, devscores)[0]\n",
    "    print 'Dev Pearson: ' + str(score)\n",
    "    return bestlrmodel\n",
    "    \n",
    "\n",
    "def encode_labels(labels, nclass=5):\n",
    "    \"\"\"\n",
    "    Label encoding from Tree LSTM paper (Tai, Socher, Manning)\n",
    "    \"\"\"\n",
    "    Y = np.zeros((len(labels), nclass)).astype('float32')\n",
    "    for j, y in enumerate(labels):\n",
    "        for i in range(nclass):\n",
    "            if i+1 == np.floor(y) + 1:\n",
    "                Y[j,i] = y - np.floor(y)\n",
    "            if i+1 == np.floor(y):\n",
    "                Y[j,i] = np.floor(y) - y + 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def load_sick_data(loc='./data/'):\n",
    "    \"\"\"\n",
    "    Load the SICK semantic-relatedness dataset\n",
    "    \"\"\"\n",
    "    trainA, trainB, devA, devB, testA, testB = [],[],[],[],[],[]\n",
    "    trainS, devS, testS = [],[],[]\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(loc, 'SICK_train.txt'), 'rb') as f:\n",
    "        for line in f:\n",
    "            text = line.strip().split('\\t')\n",
    "            trainA.append(text[1])\n",
    "            trainB.append(text[2])\n",
    "            trainS.append(text[3])\n",
    "    with smart_open.smart_open(os.path.join(loc, 'SICK_trial.txt'), 'rb') as f:\n",
    "        for line in f:\n",
    "            text = line.strip().split('\\t')\n",
    "            devA.append(text[1])\n",
    "            devB.append(text[2])\n",
    "            devS.append(text[3])\n",
    "    with smart_open.smart_open(os.path.join(loc, 'SICK_test_annotated.txt'), 'rb') as f:\n",
    "        for line in f:\n",
    "            text = line.strip().split('\\t')\n",
    "            testA.append(text[1])\n",
    "            testB.append(text[2])\n",
    "            testS.append(text[3])\n",
    "\n",
    "    trainS = [float(s) for s in trainS[1:]]\n",
    "    devS = [float(s) for s in devS[1:]]\n",
    "    testS = [float(s) for s in testS[1:]]\n",
    "\n",
    "    return [trainA[1:], trainB[1:]], [devA[1:], devB[1:]], [testA[1:], testB[1:]], [trainS, devS, testS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code for CLASSIFICATION (SUBJ/TREC) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_classification_data(name, model=None, model_name='sent2vec', loc='./classification_data/', seed=42):\n",
    "    \"\"\"\n",
    "    Load one of MR or SUBJ\n",
    "    \"\"\"\n",
    "    z = {}\n",
    "    if name == 'MR':\n",
    "        pos, neg = load_rt(loc=loc)\n",
    "    elif name == 'SUBJ':\n",
    "        pos, neg = load_subj(loc=loc)\n",
    "\n",
    "    labels = compute_labels(pos, neg)\n",
    "    text, labels = shuffle_data(pos+neg, labels, seed=seed)\n",
    "    text = [texts.encode('utf-8') for texts in text]\n",
    "    z['text'] = text\n",
    "    z['labels'] = labels\n",
    "    print 'Computing sentence vectors...'\n",
    "    if model_name == 'sent2vec':\n",
    "        features = sent2vec_vectors(model, text)\n",
    "    elif model_name == 'doc2vec':\n",
    "        features = np.array([model.infer_vector(example.split(' ')) for example in text])\n",
    "    elif model_name == 'original_sent2vec':\n",
    "        features = original_sent2vec(text)\n",
    "    else:\n",
    "        features = np.array([ft_sent_vec(model, example.split(' ')) for example in text])\n",
    "    return z, features\n",
    "\n",
    "\n",
    "def load_rt(loc='./classification_data/'):\n",
    "    \"\"\"\n",
    "    Load the MR dataset\n",
    "    \"\"\"\n",
    "    pos, neg = [], []\n",
    "    with smart_open.smart_open(os.path.join(loc, 'rt-polarity.pos'), 'rb') as f:\n",
    "        for line in f:\n",
    "            pos.append(line.decode('latin-1').strip())\n",
    "    with smart_open.smart_open(os.path.join(loc, 'rt-polarity.neg'), 'rb') as f:\n",
    "        for line in f:\n",
    "            neg.append(line.decode('latin-1').strip())\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def load_subj(loc='./classification_data/'):\n",
    "    \"\"\"\n",
    "    Load the SUBJ dataset\n",
    "    \"\"\"\n",
    "    pos, neg = [], []\n",
    "    with smart_open.smart_open(os.path.join(loc, 'plot.tok.gt9.5000'), 'rb') as f:\n",
    "        for line in f:\n",
    "            pos.append(line.decode('latin-1').strip())\n",
    "    with smart_open.smart_open(os.path.join(loc, 'quote.tok.gt9.5000'), 'rb') as f:\n",
    "        for line in f:\n",
    "            neg.append(line.decode('latin-1').strip())\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def compute_labels(pos, neg):\n",
    "    \"\"\"\n",
    "    Construct list of labels\n",
    "    \"\"\"\n",
    "    labels = np.zeros(len(pos) + len(neg))\n",
    "    labels[:len(pos)] = 1.0\n",
    "    labels[len(pos):] = 0.0\n",
    "    return labels\n",
    "\n",
    "\n",
    "def shuffle_data(X, L, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle the data\n",
    "    \"\"\"\n",
    "    prng = RandomState(seed)\n",
    "    inds = np.arange(len(X))\n",
    "    prng.shuffle(inds)\n",
    "    X = [X[i] for i in inds]\n",
    "    L = L[inds]\n",
    "    return (X, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_nested_kfold(name, model=None, model_name='sent2vec', loc='./classification_data/', k=10, seed=42, use_nb=False):\n",
    "    \"\"\"\n",
    "    Evaluate features with nested K-fold cross validation\n",
    "    Outer loop: Held-out evaluation\n",
    "    Inner loop: Hyperparameter tuning\n",
    "\n",
    "    Datasets can be found at http://nlp.stanford.edu/~sidaw/home/projects:nbsvm\n",
    "    Options for name are 'MR' and 'SUBJ'\n",
    "    \"\"\"\n",
    "    # Load the dataset and extract features\n",
    "    z, features = load_classification_data(name, model, loc=loc, seed=seed, model_name=model_name)\n",
    "\n",
    "    scan = [2**t for t in range(0,9,1)]\n",
    "    npts = len(z['text'])\n",
    "    kf = KFold(npts, n_folds=k, random_state=seed)\n",
    "    scores = []\n",
    "    for train, test in kf:\n",
    "\n",
    "        # Split data\n",
    "        X_train = features[train]\n",
    "        y_train = z['labels'][train]\n",
    "        X_test = features[test]\n",
    "        y_test = z['labels'][test]\n",
    "\n",
    "        Xraw = [z['text'][i] for i in train]\n",
    "        Xraw_test = [z['text'][i] for i in test]\n",
    "\n",
    "        scanscores = []\n",
    "        for s in scan:\n",
    "\n",
    "            # Inner KFold\n",
    "            innerkf = KFold(len(X_train), n_folds=k, random_state=seed+1)\n",
    "            innerscores = []\n",
    "            for innertrain, innertest in innerkf:\n",
    "        \n",
    "                # Split data\n",
    "                X_innertrain = X_train[innertrain]\n",
    "                y_innertrain = y_train[innertrain]\n",
    "                X_innertest = X_train[innertest]\n",
    "                y_innertest = y_train[innertest]\n",
    "\n",
    "                Xraw_innertrain = [Xraw[i] for i in innertrain]\n",
    "                Xraw_innertest = [Xraw[i] for i in innertest]\n",
    "\n",
    "                # NB (if applicable)\n",
    "                if use_nb:\n",
    "                    NBtrain, NBtest = compute_nb(Xraw_innertrain, y_innertrain, Xraw_innertest)\n",
    "                    X_innertrain = hstack((X_innertrain, NBtrain))\n",
    "                    X_innertest = hstack((X_innertest, NBtest))\n",
    "\n",
    "                # Train classifier\n",
    "                clf = LogisticRegression(C=s)\n",
    "                clf.fit(X_innertrain, y_innertrain)\n",
    "                acc = clf.score(X_innertest, y_innertest)\n",
    "                innerscores.append(acc)\n",
    "                # print (s, acc)\n",
    "\n",
    "            # Append mean score\n",
    "            scanscores.append(np.mean(innerscores))\n",
    "\n",
    "        # Get the index of the best score\n",
    "        s_ind = np.argmax(scanscores)\n",
    "        s = scan[s_ind]\n",
    "        # print scanscores\n",
    "        # print s\n",
    " \n",
    "        # NB (if applicable)\n",
    "        if use_nb:\n",
    "            NBtrain, NBtest = compute_nb(Xraw, y_train, Xraw_test)\n",
    "            X_train = hstack((X_train, NBtrain))\n",
    "            X_test = hstack((X_test, NBtest))\n",
    "       \n",
    "        # Train classifier\n",
    "        clf = LogisticRegression(C=s)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        acc = clf.score(X_test, y_test)\n",
    "        scores.append(acc)\n",
    "        print scores\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_nb(X, y, Z):\n",
    "    \"\"\"\n",
    "    Compute NB features\n",
    "    \"\"\"\n",
    "    labels = [int(t) for t in y]\n",
    "    ptrain = [X[i] for i in range(len(labels)) if labels[i] == 0]\n",
    "    ntrain = [X[i] for i in range(len(labels)) if labels[i] == 1]\n",
    "    poscounts = build_dict(ptrain, [1,2])\n",
    "    negcounts = build_dict(ntrain, [1,2])\n",
    "    dic, r = compute_ratio(poscounts, negcounts)\n",
    "    trainX = process_text(X, dic, r, [1,2])\n",
    "    devX = process_text(Z, dic, r, [1,2])\n",
    "    return trainX, devX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code for TREC data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_trec(model=None, model_name='sent2vec', k=10, seed=1234, evalcv=True, evaltest=True, loc='./trec_data/'):\n",
    "    \"\"\"\n",
    "    Run experiment\n",
    "    k: number of CV folds\n",
    "    test: whether to evaluate on test set\n",
    "    \"\"\"\n",
    "    print 'Preparing data...'\n",
    "    traintext, testtext = load_trec_data(loc)\n",
    "    train, train_labels = prepare_data(traintext)\n",
    "    test, test_labels = prepare_data(testtext)\n",
    "    train_labels = prepare_labels(train_labels)\n",
    "    test_labels = prepare_labels(test_labels)\n",
    "    train, train_labels = shuffle(train, train_labels, random_state=seed)\n",
    "\n",
    "    print 'Computing training sentence vectors...'\n",
    "    if model_name == 'sent2vec':\n",
    "        trainF = sent2vec_vectors(model, train)\n",
    "    elif model_name == 'doc2vec':\n",
    "        trainF = [model.infer_vector(example.split(' ')) for example in train]\n",
    "    elif model_name == 'original_sent2vec':\n",
    "        trainF = original_sent2vec(train)\n",
    "    else:\n",
    "        trainF = [ft_sent_vec(model, example.split(' ')) for example in train]\n",
    "    \n",
    "    if evalcv:\n",
    "        print 'Running cross-validation...'\n",
    "        interval = [2**t for t in range(0,9,1)]     # coarse-grained\n",
    "        C = eval_kfold(trainF, train_labels, k=k, scan=interval, seed=seed)\n",
    "\n",
    "    if evaltest:\n",
    "        if not evalcv:\n",
    "            C = 128     # Best parameter found from CV\n",
    "\n",
    "        print 'Computing testing sentence vectors...'\n",
    "        if model_name == 'sent2vec':\n",
    "            testF = sent2vec_vectors(model, test)\n",
    "        elif model_name == 'doc2vec':\n",
    "            testF = [model.infer_vector(example.split(' ')) for example in test]\n",
    "        elif model_name == 'original_sent2vec':\n",
    "            testF = original_sent2vec(test)\n",
    "        else:\n",
    "            testF = [ft_sent_vec(model, example.split(' ')) for example in test]\n",
    "\n",
    "        print 'Evaluating...'\n",
    "        clf = LogisticRegression(C=C)\n",
    "        clf.fit(trainF, train_labels)\n",
    "        clf.predict(testF)\n",
    "        print 'Test accuracy: ' + str(clf.score(testF, test_labels))\n",
    "\n",
    "\n",
    "def load_trec_data(loc='./trec_data/'):\n",
    "    \"\"\"\n",
    "    Load the TREC question-type dataset\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    with smart_open.smart_open(os.path.join(loc, 'train_5500.label'), 'rb') as f:\n",
    "        for line in f:\n",
    "            train.append(line.strip())\n",
    "    with smart_open.smart_open(os.path.join(loc, 'TREC_10.label'), 'rb') as f:\n",
    "        for line in f:\n",
    "            test.append(line.strip())\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def prepare_data(text):\n",
    "    \"\"\"\n",
    "    Prepare data\n",
    "    \"\"\"\n",
    "    labels = [t.split()[0] for t in text]\n",
    "    labels = [l.split(':')[0] for l in labels]\n",
    "    X = [t.split()[1:] for t in text]\n",
    "    X = [' '.join(t) for t in X]\n",
    "    return X, labels\n",
    "\n",
    "\n",
    "def prepare_labels(labels):\n",
    "    \"\"\"\n",
    "    Process labels to numerical values\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    count = 0\n",
    "    setlabels = set(labels)\n",
    "    for w in setlabels:\n",
    "        d[w] = count\n",
    "        count += 1\n",
    "    idxlabels = np.array([d[w] for w in labels])\n",
    "    return idxlabels\n",
    "\n",
    "\n",
    "def eval_kfold(features, labels, k=10, scan=[2**t for t in range(0,9,1)], seed=1234):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation\n",
    "    \"\"\"\n",
    "    npts = len(features)\n",
    "    kf = KFold(npts, n_folds=k, random_state=seed)\n",
    "    scores = []\n",
    "\n",
    "    for s in scan:\n",
    "\n",
    "        scanscores = []\n",
    "\n",
    "        for train, test in kf:\n",
    "\n",
    "            # Split data\n",
    "            X_train = features[train]\n",
    "            y_train = labels[train]\n",
    "            X_test = features[test]\n",
    "            y_test = labels[test]\n",
    "\n",
    "            # Train classifier\n",
    "            clf = LogisticRegression(C=s)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            scanscores.append(score)\n",
    "            print (s, score)\n",
    "\n",
    "        # Append mean score\n",
    "        scores.append(np.mean(scanscores))\n",
    "        print scores\n",
    "\n",
    "    # Get the index of the best score\n",
    "    s_ind = np.argmax(scores)\n",
    "    s = scan[s_ind]\n",
    "    print (s_ind, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Lee Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll be training our model using the Lee Background Corpus included in gensim. This corpus contains 314 documents selected from the Australian Broadcasting Corporation’s news mail service, which provides text e-mails of headline stories and covers a number of broad topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hundreds', u'of', u'people', u'have', u'been', u'forced', u'to', u'vacate', u'their', u'homes', u'in', u'the', u'Southern', u'Highlands', u'of', u'New', u'South', u'Wales', u'as', u'strong', u'winds', u'today', u'pushed', u'a', u'huge', u'bushfire', u'towards', u'the', u'town', u'of', u'Hill', u'Top'] \n",
      "\n",
      "[u'A', u'new', u'blaze', u'near', u'Goulburn', u'south', u'west', u'of', u'Sydney', u'has', u'forced', u'the', u'closure', u'of', u'the', u'Hume', u'Highway'] \n",
      "\n",
      "[u'At', u'about', u'pm', u'AEDT', u'a', u'marked', u'deterioration', u'in', u'the', u'weather', u'as', u'a', u'storm', u'cell', u'moved', u'east', u'across', u'the', u'Blue', u'Mountains', u'forced', u'authorities', u'to', u'make', u'a', u'decision', u'to', u'evacuate', u'people', u'from', u'homes', u'in', u'outlying', u'streets', u'at', u'Hill', u'Top', u'in', u'the', u'New', u'South', u'Wales', u'southern', u'highlands'] \n",
      "\n",
      "[u'An', u'estimated', u'residents', u'have', u'left', u'their', u'homes', u'for', u'nearby', u'Mittagong'] \n",
      "\n",
      "[u'The', u'New', u'South', u'Wales', u'Rural', u'Fire', u'Service', u'says', u'the', u'weather', u'conditions', u'which', u'caused', u'the', u'fire', u'to', u'burn', u'in', u'a', u'finger', u'formation', u'have', u'now', u'eased', u'and', u'about', u'fire', u'units', u'in', u'and', u'around', u'Hill', u'Top', u'are', u'optimistic', u'of', u'defending', u'all', u'properties'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = data_dir + 'lee_background.cor'\n",
    "\n",
    "\n",
    "# Prepare training data for Sent2Vec\n",
    "lee_data = []\n",
    "with smart_open.smart_open(lee_train_file) as f1, smart_open.smart_open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if line not in ['\\n', '\\r\\n']:\n",
    "            line = re.split('\\.|\\?|\\n', line.strip())\n",
    "            for sentence in line:\n",
    "                if len(sentence) > 1:\n",
    "                    sentence = gensim_tokenize(sentence)\n",
    "                    lee_data.append(list(sentence))\n",
    "                    f2.write(' '.join(lee_data[-1]) + '\\n')\n",
    "\n",
    "\n",
    "# Prepare training data for Doc2Vec\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "lee_doc2vec_data = list(read_corpus(lee_train_file))\n",
    "\n",
    "\n",
    "# Prepare training data for FastText\n",
    "lee_fasttext_data = LineSentence(lee_train_file)\n",
    "\n",
    "\n",
    "# Print sample training data\n",
    "for sentence in lee_data[:5]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 0.06 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 1531, tokens read: 60302\n",
      "INFO:gensim.models.sent2vec:training model with 4 workers on 1531 vocabulary and 100 features\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 9.11% words, 85690 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 18.22% words, 85582 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 28.98% words, 90985 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 40.57% words, 93338 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 52.99% words, 98055 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 64.59% words, 99523 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 74.53% words, 99278 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 85.30% words, 99112 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 97.72% words, 100813 words/s\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.sent2vec:training on 1206040 raw words (983720 effective words) took 9.6s, 102524 effective words/s\n",
      "CPU times: user 23.6 s, sys: 5.94 s, total: 29.5 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "# Train new Gensim sent2vec model\n",
    "% time sent2vec_model = Sent2Vec(lee_data, size=100, epochs=20, seed=42, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/docs/notebooks/sent2vec\n",
      "Read 0M words\n",
      "Number of words:  1837\n",
      "Number of labels: 0\n",
      "Progress: 100.0%  words/sec/thread: 91638  lr: 0.000000  loss: 3.132677  eta: 0h0m 4m   eta: 0h0m   eta: 0h0m   lr: 0.144960  loss: 3.627340  eta: 0h0m thread: 44594  lr: 0.136616  loss: 3.590207  eta: 0h0m m m m 0h0m \n",
      "\n",
      "real\t0m13.698s\n",
      "user\t0m16.583s\n",
      "sys\t0m2.638s\n",
      "/Users/prerna135/Documents/GitHub/gensim/docs/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Train new original c++ sent2vec model\n",
    "#! git clone https://github.com/epfml/sent2vec.git\n",
    "% cd sent2vec\n",
    "#! make\n",
    "! time ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 20 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 4 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
    "% cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 217 µs, sys: 776 µs, total: 993 µs\n",
      "Wall time: 1 ms\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "CPU times: user 441 ms, sys: 229 ms, total: 670 ms\n",
      "Wall time: 646 ms\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.53% examples, 408700 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724836 effective words) took 1.7s, 434730 effective words/s\n",
      "CPU times: user 3.88 s, sys: 212 ms, total: 4.09 s\n",
      "Wall time: 1.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724836"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec model1 with PV-DM and sum of context word vectors\n",
    "% time doc2vec_model1 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "% time doc2vec_model1.build_vocab(lee_doc2vec_data)\n",
    "% time doc2vec_model1.train(lee_doc2vec_data, total_examples=doc2vec_model1.corpus_count, epochs=doc2vec_model1.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 µs, sys: 34 µs, total: 158 µs\n",
      "Wall time: 146 µs\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "CPU times: user 420 ms, sys: 18 ms, total: 438 ms\n",
      "Wall time: 436 ms\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.53% examples, 524652 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724572 effective words) took 1.3s, 540925 effective words/s\n",
      "CPU times: user 3.2 s, sys: 123 ms, total: 3.33 s\n",
      "Wall time: 1.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec model2 with PV-DBOW and sum of context word vectors\n",
    "% time doc2vec_model2 = gensim.models.doc2vec.Doc2Vec(dm=0, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "% time doc2vec_model2.build_vocab(lee_doc2vec_data)\n",
    "% time doc2vec_model2.train(lee_doc2vec_data, total_examples=doc2vec_model2.corpus_count, epochs=doc2vec_model2.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 µs, sys: 30 µs, total: 129 µs\n",
      "Wall time: 117 µs\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "CPU times: user 134 ms, sys: 32.3 ms, total: 166 ms\n",
      "Wall time: 143 ms\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.33% examples, 384879 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724840 effective words) took 1.7s, 423900 effective words/s\n",
      "CPU times: user 3.76 s, sys: 196 ms, total: 3.96 s\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724840"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec model3 with PV-DM and mean of context word vectors\n",
    "% time doc2vec_model3 = gensim.models.doc2vec.Doc2Vec(dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "% time doc2vec_model3.build_vocab(lee_doc2vec_data)\n",
    "% time doc2vec_model3.train(lee_doc2vec_data, total_examples=doc2vec_model3.corpus_count, epochs=doc2vec_model3.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 µs, sys: 20 µs, total: 142 µs\n",
      "Wall time: 137 µs\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1750 unique words (25% of original 6981, drops 5231)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 49335 word corpus (84% of original 58152, drops 8817)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 6981 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 51 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 35935 word corpus (72.8% of prior 49335)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1750 words and 100 dimensions: 2395000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "CPU times: user 161 ms, sys: 8.04 ms, total: 169 ms\n",
      "Wall time: 168 ms\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1750 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.18% examples, 462858 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1163040 raw words (724538 effective words) took 1.5s, 497919 effective words/s\n",
      "CPU times: user 3.3 s, sys: 122 ms, total: 3.43 s\n",
      "Wall time: 1.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "724538"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec model4 with PV-DBOW and mean of context word vectors\n",
    "% time doc2vec_model4 = gensim.models.doc2vec.Doc2Vec(dm=0, dm_mean=1, size=100, min_count=5, iter=20, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "% time doc2vec_model4.build_vocab(lee_doc2vec_data)\n",
    "% time doc2vec_model4.train(lee_doc2vec_data, total_examples=doc2vec_model4.corpus_count, epochs=doc2vec_model4.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 µs, sys: 269 µs, total: 387 µs\n",
      "Wall time: 368 µs\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 1762 unique words (16% of original 10781, drops 9019)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 46084 word corpus (76% of original 59890, drops 13806)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 10781 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 45 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 32610 word corpus (70.8% of prior 46084)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.fasttext:Total number of ngrams is 17006\n",
      "CPU times: user 1.19 s, sys: 474 ms, total: 1.66 s\n",
      "Wall time: 1.58 s\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.03% examples, 58330 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.33% examples, 66043 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.57% examples, 71806 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.62% examples, 74693 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.78% examples, 76453 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.78% examples, 73769 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.22% examples, 73244 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.95% examples, 76407 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 1197800 raw words (651801 effective words) took 8.5s, 76521 effective words/s\n",
      "CPU times: user 17.8 s, sys: 182 ms, total: 18 s\n",
      "Wall time: 8.64 s\n"
     ]
    }
   ],
   "source": [
    "# Train new Gensim fasttext model\n",
    "% time fasttext_model = FastText(size=100, alpha=0.2, negative=10, max_vocab_size=30000000, seed=42, iter=20)\n",
    "% time fasttext_model.build_vocab(lee_fasttext_data)\n",
    "% time fasttext_model.train(lee_fasttext_data, total_examples=fasttext_model.corpus_count, epochs=fasttext_model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised similarity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised evaluation of the the learnt sentence embeddings is performed using the sentence cosine similarity, on the [SICK 2014](http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools) datasets. These similarity scores are compared to the gold-standard human judgements using [Pearson’s correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) scores. The SICK dataset consists of about 10,000 sentence pairs along with relatedness scores of the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.454888083808\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.442726701635\n",
      "Test Spearman: 0.429214793005\n",
      "Test MSE: 0.819983275569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.33202561,  3.22572068,  3.42379462, ...,  3.22189722,\n",
       "        2.97011595,  3.45620743])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Gensim sent2vec model\n",
    "evaluate_sick(sent2vec_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.410262365496\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.385389086275\n",
      "Test Spearman: 0.389124024191\n",
      "Test MSE: 0.869150134381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.03246912,  3.10153695,  3.2572212 , ...,  3.32569634,\n",
       "        2.58646177,  2.93490493])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate original c++ sent2vec model\n",
    "evaluate_sick(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.328894630838\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.278608836989\n",
      "Test Spearman: 0.27301301549\n",
      "Test MSE: 0.938821396858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.52826811,  3.7083939 ,  3.47524076, ...,  2.88458871,\n",
       "        3.31107186,  3.86412868])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate doc2vec model1\n",
    "evaluate_sick(doc2vec_model1, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.450398849521\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.359457176208\n",
      "Test Spearman: 0.357782710799\n",
      "Test MSE: 0.886162110107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.49265406,  3.57416919,  3.04496671, ...,  3.05261419,\n",
       "        3.1936516 ,  2.89496976])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate doc2vec model2\n",
    "evaluate_sick(doc2vec_model2, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.300688964786\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.256824201286\n",
      "Test Spearman: 0.253144870099\n",
      "Test MSE: 0.951138264852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.24743648,  3.67006003,  3.3777536 , ...,  3.50714775,\n",
       "        3.37920226,  3.3091968 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate doc2vec model3\n",
    "evaluate_sick(doc2vec_model3, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.437143868676\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.343684457858\n",
      "Test Spearman: 0.342749636539\n",
      "Test MSE: 0.897495330406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.59765554,  3.72423426,  2.97780104, ...,  3.32320554,\n",
       "        3.41562496,  2.53789302])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate doc2vec model4\n",
    "evaluate_sick(doc2vec_model4, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.501943522556\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.514681550184\n",
      "Test Spearman: 0.506972908332\n",
      "Test MSE: 0.749812329121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.15966516,  3.21333559,  3.42929444, ...,  3.35751852,\n",
       "        2.77122039,  2.49526048])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate fasttext model\n",
    "evaluate_sick(fasttext_model, seed=42, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Downstream Supervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence embeddings are evaluated for various supervised classification tasks. We evaluate classification of movie review sentiment (MR) (Pang & Lee, 2005), subjectivity classification (SUBJ)(Pang & Lee, 2004) and question type classification (TREC) (Voorhees, 2002). Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic regression classifier. Accuracy scores are obtained using 10-fold cross-validation for the [MR and SUBJ](https://www.cs.cornell.edu/people/pabo/movie-review-data/) datasets. For those datasets nested cross-validation is used to tune the L2 penalty. For the [TREC dataset](http://cogcomp.cs.illinois.edu/Data/QA/QC/), the accuracy is computed on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gensim Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.69999999999999996]\n",
      "[0.69999999999999996, 0.72399999999999998]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997, 0.72699999999999998]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997, 0.72699999999999998, 0.71399999999999997]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997, 0.72699999999999998, 0.71399999999999997, 0.72199999999999998]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997, 0.72699999999999998, 0.71399999999999997, 0.72199999999999998, 0.71899999999999997]\n",
      "[0.69999999999999996, 0.72399999999999998, 0.69899999999999995, 0.73299999999999998, 0.71999999999999997, 0.72699999999999998, 0.71399999999999997, 0.72199999999999998, 0.71899999999999997, 0.73899999999999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69999999999999996,\n",
       " 0.72399999999999998,\n",
       " 0.69899999999999995,\n",
       " 0.73299999999999998,\n",
       " 0.71999999999999997,\n",
       " 0.72699999999999998,\n",
       " 0.71399999999999997,\n",
       " 0.72199999999999998,\n",
       " 0.71899999999999997,\n",
       " 0.73899999999999999]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=sent2vec_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.56607310215557638]\n",
      "[0.56607310215557638, 0.56513589503280226]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796, 0.56003752345215763]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796, 0.56003752345215763, 0.59756097560975607]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796, 0.56003752345215763, 0.59756097560975607, 0.60225140712945591]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796, 0.56003752345215763, 0.59756097560975607, 0.60225140712945591, 0.58536585365853655]\n",
      "[0.56607310215557638, 0.56513589503280226, 0.58348968105065668, 0.55534709193245779, 0.55065666041275796, 0.56003752345215763, 0.59756097560975607, 0.60225140712945591, 0.58536585365853655, 0.575046904315197]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.56607310215557638,\n",
       " 0.56513589503280226,\n",
       " 0.58348968105065668,\n",
       " 0.55534709193245779,\n",
       " 0.55065666041275796,\n",
       " 0.56003752345215763,\n",
       " 0.59756097560975607,\n",
       " 0.60225140712945591,\n",
       " 0.58536585365853655,\n",
       " 0.575046904315197]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=sent2vec_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.584\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(model=sent2vec_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Original C++ Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.78700000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004, 0.76300000000000001]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004, 0.76300000000000001, 0.79700000000000004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78700000000000003,\n",
       " 0.80600000000000005,\n",
       " 0.78200000000000003,\n",
       " 0.77800000000000002,\n",
       " 0.77500000000000002,\n",
       " 0.76700000000000002,\n",
       " 0.78300000000000003,\n",
       " 0.79600000000000004,\n",
       " 0.76300000000000001,\n",
       " 0.79700000000000004]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57357075913776945]\n",
      "[0.57357075913776945, 0.57263355201499533]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668, 0.59380863039399623]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527, 0.60412757973733588]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56660412757973733, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527, 0.60412757973733588, 0.59380863039399623]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57357075913776945,\n",
       " 0.57263355201499533,\n",
       " 0.56378986866791747,\n",
       " 0.56660412757973733,\n",
       " 0.58348968105065668,\n",
       " 0.59380863039399623,\n",
       " 0.60037523452157604,\n",
       " 0.61819887429643527,\n",
       " 0.60412757973733588,\n",
       " 0.59380863039399623]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.592\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.66900000000000004]\n",
      "[0.66900000000000004, 0.67200000000000004]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005, 0.67900000000000005]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005, 0.67900000000000005, 0.67700000000000005]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005, 0.67900000000000005, 0.67700000000000005, 0.67200000000000004]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005, 0.67900000000000005, 0.67700000000000005, 0.67200000000000004, 0.67300000000000004]\n",
      "[0.66900000000000004, 0.67200000000000004, 0.67800000000000005, 0.67500000000000004, 0.67800000000000005, 0.67900000000000005, 0.67700000000000005, 0.67200000000000004, 0.67300000000000004, 0.68000000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.66900000000000004,\n",
       " 0.67200000000000004,\n",
       " 0.67800000000000005,\n",
       " 0.67500000000000004,\n",
       " 0.67800000000000005,\n",
       " 0.67900000000000005,\n",
       " 0.67700000000000005,\n",
       " 0.67200000000000004,\n",
       " 0.67300000000000004,\n",
       " 0.68000000000000005]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model1, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.55201499531396436]\n",
      "[0.55201499531396436, 0.55295220243673848]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748, 0.57129455909943716]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748, 0.57129455909943716, 0.575046904315197]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748, 0.57129455909943716, 0.575046904315197, 0.55816135084427765]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748, 0.57129455909943716, 0.575046904315197, 0.55816135084427765, 0.55534709193245779]\n",
      "[0.55201499531396436, 0.55295220243673848, 0.55816135084427765, 0.5412757973733584, 0.56285178236397748, 0.57129455909943716, 0.575046904315197, 0.55816135084427765, 0.55534709193245779, 0.57410881801125702]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.55201499531396436,\n",
       " 0.55295220243673848,\n",
       " 0.55816135084427765,\n",
       " 0.5412757973733584,\n",
       " 0.56285178236397748,\n",
       " 0.57129455909943716,\n",
       " 0.575046904315197,\n",
       " 0.55816135084427765,\n",
       " 0.55534709193245779,\n",
       " 0.57410881801125702]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model1, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.38\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(doc2vec_model1, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.746]\n",
      "[0.746, 0.72399999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998, 0.69499999999999995]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998, 0.69499999999999995, 0.72499999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998, 0.69499999999999995, 0.72499999999999998, 0.72699999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998, 0.69499999999999995, 0.72499999999999998, 0.72699999999999998, 0.72999999999999998]\n",
      "[0.746, 0.72399999999999998, 0.72499999999999998, 0.73199999999999998, 0.72799999999999998, 0.69499999999999995, 0.72499999999999998, 0.72699999999999998, 0.72999999999999998, 0.755]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.746,\n",
       " 0.72399999999999998,\n",
       " 0.72499999999999998,\n",
       " 0.73199999999999998,\n",
       " 0.72799999999999998,\n",
       " 0.69499999999999995,\n",
       " 0.72499999999999998,\n",
       " 0.72699999999999998,\n",
       " 0.72999999999999998,\n",
       " 0.755]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model2, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.5670103092783505]\n",
      "[0.5670103092783505, 0.56513589503280226]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762, 0.5684803001876173]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762, 0.5684803001876173, 0.60225140712945591]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762, 0.5684803001876173, 0.60225140712945591, 0.57035647279549717]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762, 0.5684803001876173, 0.60225140712945591, 0.57035647279549717, 0.56097560975609762]\n",
      "[0.5670103092783505, 0.56513589503280226, 0.59849906191369606, 0.56378986866791747, 0.56097560975609762, 0.5684803001876173, 0.60225140712945591, 0.57035647279549717, 0.56097560975609762, 0.59287054409005624]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5670103092783505,\n",
       " 0.56513589503280226,\n",
       " 0.59849906191369606,\n",
       " 0.56378986866791747,\n",
       " 0.56097560975609762,\n",
       " 0.5684803001876173,\n",
       " 0.60225140712945591,\n",
       " 0.57035647279549717,\n",
       " 0.56097560975609762,\n",
       " 0.59287054409005624]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model2, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.406\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(doc2vec_model2, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.68799999999999994]\n",
      "[0.68799999999999994, 0.65800000000000003]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004, 0.67000000000000004]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004, 0.67000000000000004, 0.69099999999999995]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004, 0.67000000000000004, 0.69099999999999995, 0.66800000000000004]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004, 0.67000000000000004, 0.69099999999999995, 0.66800000000000004, 0.67000000000000004]\n",
      "[0.68799999999999994, 0.65800000000000003, 0.65600000000000003, 0.67900000000000005, 0.66500000000000004, 0.67000000000000004, 0.69099999999999995, 0.66800000000000004, 0.67000000000000004, 0.67400000000000004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.68799999999999994,\n",
       " 0.65800000000000003,\n",
       " 0.65600000000000003,\n",
       " 0.67900000000000005,\n",
       " 0.66500000000000004,\n",
       " 0.67000000000000004,\n",
       " 0.69099999999999995,\n",
       " 0.66800000000000004,\n",
       " 0.67000000000000004,\n",
       " 0.67400000000000004]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model3, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.53139643861293351]\n",
      "[0.53139643861293351, 0.53139643861293351]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845, 0.55347091932457781]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845, 0.55347091932457781, 0.59287054409005624]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845, 0.55347091932457781, 0.59287054409005624, 0.53283302063789872]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845, 0.55347091932457781, 0.59287054409005624, 0.53283302063789872, 0.5544090056285178]\n",
      "[0.53139643861293351, 0.53139643861293351, 0.56003752345215763, 0.53846153846153844, 0.53752345215759845, 0.55347091932457781, 0.59287054409005624, 0.53283302063789872, 0.5544090056285178, 0.56378986866791747]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.53139643861293351,\n",
       " 0.53139643861293351,\n",
       " 0.56003752345215763,\n",
       " 0.53846153846153844,\n",
       " 0.53752345215759845,\n",
       " 0.55347091932457781,\n",
       " 0.59287054409005624,\n",
       " 0.53283302063789872,\n",
       " 0.5544090056285178,\n",
       " 0.56378986866791747]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model3, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.404\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(doc2vec_model3, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.748]\n",
      "[0.748, 0.72199999999999998]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998, 0.73099999999999998]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998, 0.73099999999999998, 0.73599999999999999]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998, 0.73099999999999998, 0.73599999999999999, 0.71399999999999997]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998, 0.73099999999999998, 0.73599999999999999, 0.71399999999999997, 0.71999999999999997]\n",
      "[0.748, 0.72199999999999998, 0.70799999999999996, 0.73299999999999998, 0.73099999999999998, 0.73099999999999998, 0.73599999999999999, 0.71399999999999997, 0.71999999999999997, 0.746]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.748,\n",
       " 0.72199999999999998,\n",
       " 0.70799999999999996,\n",
       " 0.73299999999999998,\n",
       " 0.73099999999999998,\n",
       " 0.73099999999999998,\n",
       " 0.73599999999999999,\n",
       " 0.71399999999999997,\n",
       " 0.71999999999999997,\n",
       " 0.746]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model4, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.56888472352389874]\n",
      "[0.56888472352389874, 0.55576382380506095]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718, 0.57410881801125702]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718, 0.57410881801125702, 0.59474671669793622]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718, 0.57410881801125702, 0.59474671669793622, 0.54971857410881797]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718, 0.57410881801125702, 0.59474671669793622, 0.54971857410881797, 0.56941838649155718]\n",
      "[0.56888472352389874, 0.55576382380506095, 0.58911819887429639, 0.56566604127579734, 0.56941838649155718, 0.57410881801125702, 0.59474671669793622, 0.54971857410881797, 0.56941838649155718, 0.60694183864915574]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.56888472352389874,\n",
       " 0.55576382380506095,\n",
       " 0.58911819887429639,\n",
       " 0.56566604127579734,\n",
       " 0.56941838649155718,\n",
       " 0.57410881801125702,\n",
       " 0.59474671669793622,\n",
       " 0.54971857410881797,\n",
       " 0.56941838649155718,\n",
       " 0.60694183864915574]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model4, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.424\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(doc2vec_model4, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of sentence vectors obtained from averaging FastText word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.82599999999999996]\n",
      "[0.82599999999999996, 0.82099999999999995]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005, 0.79100000000000004]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005, 0.79100000000000004, 0.79500000000000004]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005, 0.79100000000000004, 0.79500000000000004, 0.82599999999999996]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005, 0.79100000000000004, 0.79500000000000004, 0.82599999999999996, 0.79300000000000004]\n",
      "[0.82599999999999996, 0.82099999999999995, 0.78700000000000003, 0.81699999999999995, 0.80400000000000005, 0.79100000000000004, 0.79500000000000004, 0.82599999999999996, 0.79300000000000004, 0.81000000000000005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.82599999999999996,\n",
       " 0.82099999999999995,\n",
       " 0.78700000000000003,\n",
       " 0.81699999999999995,\n",
       " 0.80400000000000005,\n",
       " 0.79100000000000004,\n",
       " 0.79500000000000004,\n",
       " 0.82599999999999996,\n",
       " 0.79300000000000004,\n",
       " 0.81000000000000005]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=fasttext_model, name='SUBJ', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.5895032802249297]\n",
      "[0.5895032802249297, 0.60168697282099348]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197, 0.60694183864915574]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197, 0.60694183864915574, 0.62757973733583494]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197, 0.60694183864915574, 0.62757973733583494, 0.62570356472795496]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197, 0.60694183864915574, 0.62757973733583494, 0.62570356472795496, 0.62288930581613511]\n",
      "[0.5895032802249297, 0.60168697282099348, 0.57692307692307687, 0.58536585365853655, 0.575046904315197, 0.60694183864915574, 0.62757973733583494, 0.62570356472795496, 0.62288930581613511, 0.61726078799249529]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5895032802249297,\n",
       " 0.60168697282099348,\n",
       " 0.57692307692307687,\n",
       " 0.58536585365853655,\n",
       " 0.575046904315197,\n",
       " 0.60694183864915574,\n",
       " 0.62757973733583494,\n",
       " 0.62570356472795496,\n",
       " 0.62288930581613511,\n",
       " 0.61726078799249529]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=fasttext_model, name='MR', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.632\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(fasttext_model, evalcv=False, evaltest=True, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| S.No. | Model Name                                | Training Time (in seconds) | Pearson/Spearman/MSE on SICK | Mean SUBJ | Mean MR | TREC |\n",
    "|-------|-------------------------------------------|----------------------------|------------------------------|-----------|---------|------|\n",
    "| 1.    | Gensim Sent2Vec                           | 9.6                        | 0.44/0.44/0.81               | 0.71      | 0.57    | 0.58 |\n",
    "| 2.    | Original Sent2Vec                         | 13.6(!)                    | 0.38/0.38/0.86               | 0.78      | 0.58    | 0.59 |\n",
    "| 3.    | PV-DM with sum of context word vectors    | 1.7                        | 0.27/0.27/0.93               | 0.67      | 0.56    | 0.38 |\n",
    "| 4.    | PV-DM with mean of context word vectors   | 1.3                        | 0.25/0.25/0.95               | 0.67      | 0.54    | 0.40 |\n",
    "| 5.    | PV-DBOW with sum of context word vector   | 1.7                        | 0.34/0.34/0.89               | 0.72      | 0.57    | 0.40 |\n",
    "| 6.    | PV-DBOW with mean of context word vectors | 1.5                        | 0.34/0.34/0.89               | 0.72      | 0.57    | 0.42 |\n",
    "| 7.    | Mean of gensim fasttext word vectors      | 8.5                        | 0.49/0.48/0.76               | 0.80      | 0.61    | 0.61 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(!)NOTE: For original sent2vec the time mentioned in the table denotes total execution time instead of just the training time i.e. the time taken to build the vocabulary, other preprocessing etc is also considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation on sample of Toronto Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Toronto Book Corpus has all sentences in 11,038 books. Only 7,087 out of 11,038 books in BookCorpus are unique. Among them 2089 books have one duplicate, 733 books have two and 95 books have more than two duplicates.\n",
    "\n",
    "In this notebook, we use a sample of 100,000 randomly selected sentences from the Toronto Corpus.\n",
    "\n",
    "As this is a private corpus, it can only be downloaded on request. See [this](http://yknzhu.wixsite.com/mbweb) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'half', u'ling', u'book', u'one', u'in', u'the', u'fall', u'of', u'igneeria', u'series', u'kaylee', u'soderburg', u'copyright', u'kaylee', u'soderburg', u'all', u'rights', u'reserved'] \n",
      "\n",
      "[u'isbn', u'isbn', u'for', u'my', u'family', u'who', u'encouraged', u'me', u'to', u'never', u'stop', u'fighting', u'for', u'my', u'dreams', u'chapter', u'summer', u'vacations', u'supposed', u'to', u'be', u'fun', u'right'] \n",
      "\n",
      "[u'starlings', u'new', u'york', u'is', u'not', u'the', u'place', u'youd', u'expect', u'much', u'to', u'happen'] \n",
      "\n",
      "[u'its', u'a', u'place', u'where', u'your', u'parents', u'wouldnt', u'even', u'care', u'if', u'you', u'stayed', u'out', u'late', u'biking', u'with', u'your', u'friends'] \n",
      "\n",
      "[u'they', u'dont', u'know', u'the', u'half', u'of', u'it'] \n",
      "\n",
      "[u'the', u'only', u'reason', u'why', u'no', u'one', u'knows', u'this', u'is', u'because', u'jason', u'emily', u'seth', u'and', u'i', u'have', u'kept', u'it', u'that', u'way'] \n",
      "\n",
      "[u'i', u'walked', u'along', u'the', u'empty', u'road', u'alone', u'occasionally', u'waving', u'to', u'passing', u'kids', u'on', u'bikes'] \n",
      "\n",
      "[u'my', u'backpack', u'was', u'slung', u'over', u'my', u'shoulder', u'filled', u'with', u'my', u'writing', u'books', u'and', u'sketchpads'] \n",
      "\n",
      "[u'i', u'kept', u'my', u'eyes', u'on', u'the', u'shadowed', u'road', u'watching', u'my', u'every', u'step'] \n",
      "\n",
      "[u'the', u'sun', u'was', u'starting', u'to', u'set', u'painting', u'the', u'sky', u'in', u'brilliant', u'oranges', u'and', u'reds'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data for sent2vec\n",
    "toronto_data_file = './books_in_sentences/books_large_p1.txt'\n",
    "toronto_data = []\n",
    "lines = 0\n",
    "with smart_open.smart_open(toronto_data_file) as f1, smart_open.smart_open(\"./input.txt\",'w') as f2:\n",
    "    for line in f1:\n",
    "        if np.random.random() > 0.5:\n",
    "            if lines >= 100000:\n",
    "                break\n",
    "            lines += 1\n",
    "            if line not in ['\\n', '\\r\\n']:\n",
    "                line = re.split('\\.|\\?|\\n', line.strip())\n",
    "                for sentence in line:\n",
    "                    if len(sentence) > 1:\n",
    "                        sentence = gensim_tokenize(sentence)\n",
    "                        toronto_data.append(list(sentence))\n",
    "                        f2.write(' '.join(toronto_data[-1]) + '\\n')\n",
    "\n",
    "\n",
    "# Prepare training data for doc2vec\n",
    "def read_toronto_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100000:\n",
    "                break\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "toronto_doc2vec_data = list(read_toronto_corpus(toronto_data_file))\n",
    "\n",
    "\n",
    "# Print sample training data\n",
    "for sentence in toronto_data[:10]:\n",
    "    print sentence,'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.sent2vec:Creating dictionary...\n",
      "INFO:gensim.models.sent2vec:Read 1.00 M words\n",
      "INFO:gensim.models.sent2vec:Read 1.35 M words\n",
      "INFO:gensim.models.sent2vec:Dictionary created, dictionary size: 11587, tokens read: 1352333\n",
      "INFO:gensim.models.sent2vec:training model with 4 workers on 11587 vocabulary and 100 features\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 1.18% words, 73365 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 3.10% words, 95459 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 5.02% words, 103381 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 6.79% words, 104080 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 8.57% words, 104455 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 10.49% words, 107485 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 12.41% words, 109406 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 14.62% words, 112886 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 16.54% words, 113359 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 18.02% words, 111503 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 19.94% words, 112714 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 21.86% words, 113533 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 23.78% words, 113478 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 26.15% words, 116240 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 28.06% words, 115615 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 30.13% words, 116314 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 31.91% words, 116226 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 34.12% words, 117397 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 36.34% words, 118129 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 38.41% words, 118129 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 40.48% words, 118340 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 42.40% words, 118607 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 44.61% words, 119667 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 46.24% words, 118978 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 48.30% words, 119035 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 50.22% words, 119036 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 52.14% words, 119229 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 54.36% words, 119775 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 56.43% words, 119913 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 58.35% words, 119896 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 60.42% words, 120177 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 62.49% words, 120420 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 64.70% words, 121070 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 66.63% words, 120601 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 68.40% words, 119899 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 70.17% words, 119792 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 71.80% words, 119231 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 73.57% words, 118901 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 75.34% words, 118842 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 77.26% words, 118809 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 79.33% words, 119020 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 81.10% words, 118960 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 83.32% words, 119305 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 85.24% words, 119334 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 87.16% words, 119154 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 89.08% words, 119203 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 91.00% words, 119136 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 93.22% words, 119558 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 95.14% words, 119467 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 97.06% words, 119561 words/s\n",
      "INFO:gensim.models.sent2vec:PROGRESS: at 99.13% words, 119779 words/s\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.sent2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.sent2vec:training on 6761665 raw words (6534290 effective words) took 54.4s, 120190 effective words/s\n",
      "CPU times: user 1min 45s, sys: 23.6 s, total: 2min 9s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "# Train new sent2vec model\n",
    "% time sent2vec_toronto_model = Sent2Vec(toronto_data, size=100, epochs=5, seed=42, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/Documents/GitHub/gensim/docs/notebooks/sent2vec\n",
      "Read 1M words\n",
      "Number of words:  12999\n",
      "Number of labels: 0\n",
      "Progress: 8.0%  words/sec/thread: 47948  lr: 0.183938  loss: 3.334899  eta: 0h0m /bin/sh: line 1:  4272 Segmentation fault: 11  ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 5 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 4 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
      "\n",
      "real\t0m8.875s\n",
      "user\t0m13.991s\n",
      "sys\t0m1.651s\n",
      "/Users/prerna135/Documents/GitHub/gensim/docs/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Train model using original c++ implementation of sent2vec\n",
    "% cd sent2vec\n",
    "! time ./fasttext sent2vec -input ../input.txt -output my_model -minCount 5 -dim 100 -epoch 5 -lr 0.2 -wordNgrams 2 -loss ns -neg 10 -thread 4 -t 0.0001 -dropoutK 2 -bucket 2000000\n",
    "% cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 211 µs, sys: 1.03 ms, total: 1.25 ms\n",
      "Wall time: 2.02 ms\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #10000, processed 113217 words (760493/s), 7839 word types, 10000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #20000, processed 256086 words (845753/s), 14477 word types, 20000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #30000, processed 377626 words (865909/s), 17118 word types, 30000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #40000, processed 481481 words (764731/s), 19727 word types, 40000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #50000, processed 594311 words (702955/s), 21187 word types, 50000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #60000, processed 755296 words (972531/s), 24414 word types, 60000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #70000, processed 987300 words (1289103/s), 27989 word types, 70000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #80000, processed 1146885 words (1169736/s), 30225 word types, 80000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #90000, processed 1270547 words (1117354/s), 31144 word types, 90000 tags\n",
      "INFO:gensim.models.doc2vec:collected 32782 word types and 100000 unique tags from a corpus of 100000 examples and 1400798 words\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 13214 unique words (40% of original 32782, drops 19568)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 1365715 word corpus (97% of original 1400798, drops 35083)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 32782 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 53 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 1037883 word corpus (76.0% of prior 1365715)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 13214 words and 100 dimensions: 57178200 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "CPU times: user 5.26 s, sys: 745 ms, total: 6.01 s\n",
      "Wall time: 5.94 s\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 13214 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.42% examples, 122315 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.39% examples, 138009 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.48% examples, 137617 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.27% examples, 143034 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.65% examples, 149013 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.21% examples, 150412 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.30% examples, 149473 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.20% examples, 148930 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.18% examples, 150339 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.09% examples, 147614 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.23% examples, 149824 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.47% examples, 157018 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.58% examples, 157561 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.94% examples, 157972 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.14% examples, 158803 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.24% examples, 157852 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.35% examples, 157786 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.48% examples, 161511 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.46% examples, 162992 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.60% examples, 162959 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.73% examples, 162868 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.12% examples, 162740 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.58% examples, 162419 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.79% examples, 163637 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.60% examples, 166191 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.87% examples, 166048 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.96% examples, 165591 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.09% examples, 165703 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.32% examples, 165147 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.72% examples, 165898 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.51% examples, 167993 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.63% examples, 167907 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 7003990 raw words (5690060 effective words) took 33.8s, 168149 effective words/s\n",
      "CPU times: user 45 s, sys: 13.5 s, total: 58.5 s\n",
      "Wall time: 33.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5690060"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train new Doc2Vec model with PV-DBOW and sum of context word vectors\n",
    "% time doc2vec_model = gensim.models.doc2vec.Doc2Vec(dm=0, size=100, min_count=5, iter=5, alpha=0.2, max_vocab_size=30000000, negative=10, seed=42)\n",
    "% time doc2vec_model.build_vocab(toronto_doc2vec_data)\n",
    "% time doc2vec_model.train(toronto_doc2vec_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 µs, sys: 424 µs, total: 596 µs\n",
      "Wall time: 590 µs\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 124196 words, keeping 10460 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 241930 words, keeping 14721 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 382938 words, keeping 18492 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 581109 words, keeping 23462 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 707569 words, keeping 25275 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 823532 words, keeping 27145 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 951042 words, keeping 29090 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 1048822 words, keeping 30036 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 1177548 words, keeping 32605 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 1301417 words, keeping 34493 word types\n",
      "INFO:gensim.models.word2vec:collected 34950 word types from a corpus of 1352333 raw words and 104604 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 12997 unique words (37% of original 34950, drops 21953)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 1313908 word corpus (97% of original 1352333, drops 38425)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 34950 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 53 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 981546 word corpus (74.7% of prior 1313908)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 12997 words and 100 dimensions: 16896100 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.fasttext:Total number of ngrams is 77699\n",
      "CPU times: user 6.13 s, sys: 1.48 s, total: 7.61 s\n",
      "Wall time: 7.56 s\n",
      "INFO:gensim.models.word2vec:training model with 3 workers on 12997 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=10 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.56% examples, 71944 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.45% examples, 76428 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.25% examples, 75549 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.67% examples, 81782 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.76% examples, 80691 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.58% examples, 81247 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.62% examples, 81213 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.71% examples, 82593 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.72% examples, 80976 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.47% examples, 81653 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.41% examples, 82117 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.51% examples, 82786 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.73% examples, 83855 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.40% examples, 83896 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.76% examples, 84479 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.00% examples, 84020 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.96% examples, 84345 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.17% examples, 84963 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.18% examples, 84733 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.54% examples, 85040 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.32% examples, 85381 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.60% examples, 85238 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.63% examples, 85426 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.98% examples, 85667 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.47% examples, 85854 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.51% examples, 85946 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.32% examples, 85966 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.45% examples, 86048 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.38% examples, 86115 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.91% examples, 86154 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.50% examples, 86150 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.47% examples, 85985 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.81% examples, 86489 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.92% examples, 86445 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.99% examples, 86729 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.95% examples, 86618 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.63% examples, 86659 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.27% examples, 86463 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.21% examples, 86267 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.22% examples, 86282 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.44% examples, 86245 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.18% examples, 86284 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.46% examples, 86334 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.54% examples, 86552 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.49% examples, 86492 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.28% examples, 86666 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.27% examples, 86580 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.19% examples, 86616 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.30% examples, 86638 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.43% examples, 86645 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.13% examples, 86702 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.77% examples, 86712 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 6761665 raw words (4906423 effective words) took 56.4s, 87021 effective words/s\n",
      "CPU times: user 1min 47s, sys: 1.75 s, total: 1min 48s\n",
      "Wall time: 57.1 s\n"
     ]
    }
   ],
   "source": [
    "# Train new Gensim fasttext model\n",
    "% time fasttext_model = FastText(size=100, alpha=0.2, negative=10, max_vocab_size=30000000, seed=42, iter=5)\n",
    "% time fasttext_model.build_vocab(toronto_data)\n",
    "% time fasttext_model.train(toronto_data, total_examples=fasttext_model.corpus_count, epochs=fasttext_model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.468258209783\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.498787005964\n",
      "Test Spearman: 0.506137348117\n",
      "Test MSE: 0.764512114702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.2211674 ,  3.68008844,  3.37953488, ...,  3.27733084,\n",
       "        2.89488411,  3.11638982])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate gensim sent2vec model\n",
    "evaluate_sick(sent2vec_toronto_model, seed=42, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Dev Pearson: 0.40930191142\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.388923255965\n",
      "Test Spearman: 0.391691012721\n",
      "Test MSE: 0.866768122911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.12643148,  3.12836795,  3.27785104, ...,  3.37349816,\n",
       "        2.57517373,  2.94589332])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate original sent2vec model\n",
    "evaluate_sick(seed=42, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.502551939299\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.497833663676\n",
      "Test Spearman: 0.477028702016\n",
      "Test MSE: 0.768101925819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.84552883,  3.52230607,  3.06499181, ...,  2.86501964,\n",
       "        2.76126113,  2.84552446])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate gensim doc2vec model\n",
    "evaluate_sick(doc2vec_model, seed=42, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing development sentence vectors...\n",
      "Computing feature combinations...\n",
      "Encoding labels...\n",
      "Compiling model...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prerna135/anaconda/envs/gensim_env/lib/python2.7/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, input_dim=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Pearson: 0.557180612207\n",
      "Computing test sentence vectors...\n",
      "Computing feature combinations...\n",
      "Evaluating...\n",
      "Test Pearson: 0.569093999422\n",
      "Test Spearman: 0.559321843143\n",
      "Test MSE: 0.689002154196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.73126831,  2.70995621,  2.80035074, ...,  2.78172965,\n",
       "        2.28476275,  3.02997125])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate gensim fasttext model\n",
    "evaluate_sick(fasttext_model, seed=42, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Supervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gensim Sent2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.71399999999999997]\n",
      "[0.71399999999999997, 0.71699999999999997]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995, 0.72499999999999998]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995, 0.72499999999999998, 0.69499999999999995]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995, 0.72499999999999998, 0.69499999999999995, 0.71699999999999997]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995, 0.72499999999999998, 0.69499999999999995, 0.71699999999999997, 0.69299999999999995]\n",
      "[0.71399999999999997, 0.71699999999999997, 0.69199999999999995, 0.69899999999999995, 0.69599999999999995, 0.72499999999999998, 0.69499999999999995, 0.71699999999999997, 0.69299999999999995, 0.70699999999999996]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.71399999999999997,\n",
       " 0.71699999999999997,\n",
       " 0.69199999999999995,\n",
       " 0.69899999999999995,\n",
       " 0.69599999999999995,\n",
       " 0.72499999999999998,\n",
       " 0.69499999999999995,\n",
       " 0.71699999999999997,\n",
       " 0.69299999999999995,\n",
       " 0.70699999999999996]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=sent2vec_toronto_model, name='SUBJ', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.59793814432989689]\n",
      "[0.59793814432989689, 0.57638238050609181]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746, 0.59193245778611636]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746, 0.59193245778611636, 0.59099437148217637]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746, 0.59193245778611636, 0.59099437148217637, 0.57129455909943716]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746, 0.59193245778611636, 0.59099437148217637, 0.57129455909943716, 0.61726078799249529]\n",
      "[0.59793814432989689, 0.57638238050609181, 0.59849906191369606, 0.59380863039399623, 0.56472795497185746, 0.59193245778611636, 0.59099437148217637, 0.57129455909943716, 0.61726078799249529, 0.60131332082551592]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.59793814432989689,\n",
       " 0.57638238050609181,\n",
       " 0.59849906191369606,\n",
       " 0.59380863039399623,\n",
       " 0.56472795497185746,\n",
       " 0.59193245778611636,\n",
       " 0.59099437148217637,\n",
       " 0.57129455909943716,\n",
       " 0.61726078799249529,\n",
       " 0.60131332082551592]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=sent2vec_toronto_model, name='MR', use_nb=False, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.508\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(sent2vec_toronto_model, evalcv=False, evaltest=True, model_name='sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Original C++ Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.78700000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004, 0.76300000000000001]\n",
      "[0.78700000000000003, 0.80600000000000005, 0.78200000000000003, 0.77800000000000002, 0.77500000000000002, 0.76700000000000002, 0.78300000000000003, 0.79600000000000004, 0.76300000000000001, 0.79700000000000004]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78700000000000003,\n",
       " 0.80600000000000005,\n",
       " 0.78200000000000003,\n",
       " 0.77800000000000002,\n",
       " 0.77500000000000002,\n",
       " 0.76700000000000002,\n",
       " 0.78300000000000003,\n",
       " 0.79600000000000004,\n",
       " 0.76300000000000001,\n",
       " 0.79700000000000004]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(name='SUBJ', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.57357075913776945]\n",
      "[0.57357075913776945, 0.57263355201499533]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668, 0.59380863039399623]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527, 0.60412757973733588]\n",
      "[0.57357075913776945, 0.57263355201499533, 0.56378986866791747, 0.56566604127579734, 0.58348968105065668, 0.59380863039399623, 0.60037523452157604, 0.61819887429643527, 0.60412757973733588, 0.59380863039399623]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57357075913776945,\n",
       " 0.57263355201499533,\n",
       " 0.56378986866791747,\n",
       " 0.56566604127579734,\n",
       " 0.58348968105065668,\n",
       " 0.59380863039399623,\n",
       " 0.60037523452157604,\n",
       " 0.61819887429643527,\n",
       " 0.60412757973733588,\n",
       " 0.59380863039399623]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(name='MR', use_nb=False, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.592\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(evalcv=False, evaltest=True, model_name='original_sent2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gensim Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.75600000000000001]\n",
      "[0.75600000000000001, 0.75900000000000001]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002, 0.74199999999999999]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002, 0.74199999999999999, 0.745]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002, 0.74199999999999999, 0.745, 0.77300000000000002]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002, 0.74199999999999999, 0.745, 0.77300000000000002, 0.746]\n",
      "[0.75600000000000001, 0.75900000000000001, 0.75, 0.75, 0.76900000000000002, 0.74199999999999999, 0.745, 0.77300000000000002, 0.746, 0.78200000000000003]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.75600000000000001,\n",
       " 0.75900000000000001,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.76900000000000002,\n",
       " 0.74199999999999999,\n",
       " 0.745,\n",
       " 0.77300000000000002,\n",
       " 0.746,\n",
       " 0.78200000000000003]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model, name='SUBJ', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.60168697282099348]\n",
      "[0.60168697282099348, 0.6204311152764761]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604, 0.64165103189493433]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604, 0.64165103189493433, 0.6163227016885553]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604, 0.64165103189493433, 0.6163227016885553, 0.62101313320825513]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604, 0.64165103189493433, 0.6163227016885553, 0.62101313320825513, 0.61444652908067543]\n",
      "[0.60168697282099348, 0.6204311152764761, 0.61069418386491559, 0.6303939962476548, 0.60037523452157604, 0.64165103189493433, 0.6163227016885553, 0.62101313320825513, 0.61444652908067543, 0.61350844277673544]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60168697282099348,\n",
       " 0.6204311152764761,\n",
       " 0.61069418386491559,\n",
       " 0.6303939962476548,\n",
       " 0.60037523452157604,\n",
       " 0.64165103189493433,\n",
       " 0.6163227016885553,\n",
       " 0.62101313320825513,\n",
       " 0.61444652908067543,\n",
       " 0.61350844277673544]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=doc2vec_model, name='MR', use_nb=False, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.464\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(doc2vec_model, evalcv=False, evaltest=True, model_name='doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Gensim FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.81000000000000005]\n",
      "[0.81000000000000005, 0.78400000000000003]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003, 0.76900000000000002]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003, 0.76900000000000002, 0.77000000000000002]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003, 0.76900000000000002, 0.77000000000000002, 0.80100000000000005]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003, 0.76900000000000002, 0.77000000000000002, 0.80100000000000005, 0.77400000000000002]\n",
      "[0.81000000000000005, 0.78400000000000003, 0.76700000000000002, 0.79200000000000004, 0.78900000000000003, 0.76900000000000002, 0.77000000000000002, 0.80100000000000005, 0.77400000000000002, 0.81200000000000006]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.81000000000000005,\n",
       " 0.78400000000000003,\n",
       " 0.76700000000000002,\n",
       " 0.79200000000000004,\n",
       " 0.78900000000000003,\n",
       " 0.76900000000000002,\n",
       " 0.77000000000000002,\n",
       " 0.80100000000000005,\n",
       " 0.77400000000000002,\n",
       " 0.81200000000000006]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=fasttext_model, name='SUBJ', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence vectors...\n",
      "[0.60543580131208996]\n",
      "[0.60543580131208996, 0.61199625117150891]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636, 0.64915572232645402]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636, 0.64915572232645402, 0.64165103189493433]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636, 0.64915572232645402, 0.64165103189493433, 0.63789868667917449]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636, 0.64915572232645402, 0.64165103189493433, 0.63789868667917449, 0.63508442776735463]\n",
      "[0.60543580131208996, 0.61199625117150891, 0.63508442776735463, 0.62476547842401498, 0.59193245778611636, 0.64915572232645402, 0.64165103189493433, 0.63789868667917449, 0.63508442776735463, 0.62476547842401498]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60543580131208996,\n",
       " 0.61199625117150891,\n",
       " 0.63508442776735463,\n",
       " 0.62476547842401498,\n",
       " 0.59193245778611636,\n",
       " 0.64915572232645402,\n",
       " 0.64165103189493433,\n",
       " 0.63789868667917449,\n",
       " 0.63508442776735463,\n",
       " 0.62476547842401498]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nested_kfold(model=fasttext_model, name='MR', use_nb=False, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Computing training sentence vectors...\n",
      "Computing testing sentence vectors...\n",
      "Evaluating...\n",
      "Test accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "evaluate_trec(fasttext_model, evalcv=False, evaltest=True, model_name='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that more data = better results for sent2vec, as the above model (trained for 5 epochs) achieves similar results to the model trained on the much smaller Lee corpus (trained for 20 epochs)\n",
    "\n",
    "| S.No. | Model             | Training Time (in seconds)        | Pearson/Spearman/MSE on SICK | MR   | SUBJ | TREC |\n",
    "|-------|-------------------|-----------------------------------|------------------------------|------|------|------|\n",
    "| 1.    | Gensim Sent2Vec   | 54.4                              | 0.49/0.50/0.76               | 0.59 | 0.70 | 0.50 |\n",
    "| 2.    | Original Sent2Vec | 8.87(!)                           | 0.38/0.39/0.86               | 0.58 | 0.78 | 0.59 |\n",
    "| 3.    | Doc2Vec DBOW (sum)| 33.8                              | 0.49/0.47/0.76               | 0.61 | 0.75 | 0.46 |\n",
    "| 4.    | FastText (average)| 56.4                              | 0.56/0.55/0.68               | 0.62 | 0.78 | 0.58 |\n",
    "\n",
    "(!)NOTE: For original sent2vec the time mentioned in the table denotes total execution time instead of just the training time i.e. the time taken to build the vocabulary, other preprocessing etc is also considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
