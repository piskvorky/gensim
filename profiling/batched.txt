Timer unit: 1e-06 s

Total time: 9.66989 s
File: /home/olavur/RaRe/w2v_batch_sentences/gensim/gensim/models/word2vec_inner.pyx
Function: train_batch_sg at line 351

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   351                                           def train_batch_sg(model, sentences, alpha, _work):
   352         1            2      2.0      0.0      cdef int hs = model.hs
   353         1            2      2.0      0.0      cdef int negative = model.negative
   354         1            1      1.0      0.0      cdef int sample = (model.sample != 0)
   355                                           
   356         1            1      1.0      0.0      cdef REAL_t *syn0 = <REAL_t *>(np.PyArray_DATA(model.syn0))
   357         1            1      1.0      0.0      cdef REAL_t *word_locks = <REAL_t *>(np.PyArray_DATA(model.syn0_lockf))
   358                                               cdef REAL_t *work
   359         1            1      1.0      0.0      cdef REAL_t _alpha = alpha
   360         1            1      1.0      0.0      cdef int size = model.layer1_size
   361                                           
   362                                               cdef int codelens[MAX_SENTENCE_LEN]
   363                                               cdef np.uint32_t indexes[MAX_SENTENCE_LEN]
   364                                               cdef np.uint32_t reduced_windows[MAX_SENTENCE_LEN]
   365                                               cdef int sentence_len[MAX_NUM_SENTENCES]
   366         1            1      1.0      0.0      cdef int window = model.window
   367                                           
   368                                               cdef int i, j, k, m
   369         1            0      0.0      0.0      cdef long result = 0
   370         1            1      1.0      0.0      cdef int num_sentences = 0
   371         1            1      1.0      0.0      cdef int sent_idx = 0
   372         1            0      0.0      0.0      cdef int idx = 0
   373                                           
   374                                               # For hierarchical softmax
   375                                               cdef REAL_t *syn1
   376                                               cdef np.uint32_t *points[MAX_SENTENCE_LEN]
   377                                               cdef np.uint8_t *codes[MAX_SENTENCE_LEN]
   378                                           
   379                                               # For negative sampling
   380                                               cdef REAL_t *syn1neg
   381                                               cdef np.uint32_t *cum_table
   382                                               cdef unsigned long long cum_table_len
   383                                               # for sampling (negative and frequent-word downsampling)
   384                                               cdef unsigned long long next_random
   385                                           
   386         1            0      0.0      0.0      if hs:
   387         1            1      1.0      0.0          syn1 = <REAL_t *>(np.PyArray_DATA(model.syn1))
   388                                           
   389         1            1      1.0      0.0      if negative:
   390                                                   syn1neg = <REAL_t *>(np.PyArray_DATA(model.syn1neg))
   391                                                   cum_table = <np.uint32_t *>(np.PyArray_DATA(model.cum_table))
   392                                                   cum_table_len = len(model.cum_table)
   393         1            1      1.0      0.0      if negative or sample:
   394                                                   next_random = (2**24) * model.random.randint(0, 2**24) + model.random.randint(0, 2**24)
   395                                           
   396                                               # convert Python structures to primitive types, so we can release the GIL
   397         1            1      1.0      0.0      work = <REAL_t *>np.PyArray_DATA(_work)
   398                                           
   399         1            1      1.0      0.0      vlookup = model.vocab
   400      5338         3240      0.6      0.0      for sent_idx, sent in enumerate(sentences):
   401      5337         3345      0.6      0.0          i = 0
   402     93538        52269      0.6      0.5          for token in sent:
   403     99992        70275      0.7      0.7              word = vlookup[token] if token in vlookup else None
   404     99992        50088      0.5      0.5              if word is None:
   405     11791         6547      0.6      0.1                  continue  # leaving i unchanged/shortening sentence
   406     88201        44746      0.5      0.5              if sample and word.sample_int < random_int32(&next_random):
   407                                                           continue
   408     88201        50990      0.6      0.5              indexes[idx + i] = word.index
   409     88201        44749      0.5      0.5              if hs:
   410     88201        52049      0.6      0.5                  codelens[idx + i] = <int>len(word.code)
   411     88201        47031      0.5      0.5                  codes[idx + i] = <np.uint8_t *>np.PyArray_DATA(word.code)
   412     88201        50448      0.6      0.5                  points[idx + i] = <np.uint32_t *>np.PyArray_DATA(word.point)
   413     88201        44224      0.5      0.5              result += 1
   414     88201        44149      0.5      0.5              i += 1
   415     88201        43912      0.5      0.5              if i == MAX_SENTENCE_LEN:
   416                                                           break  # TODO: log warning, tally overflow?
   417                                           
   418      5337         2699      0.5      0.0          sentence_len[sent_idx] = i
   419                                                   # single randint() call avoids a big thread-sync slowdown
   420     93538        91428      1.0      0.9          for i, item in enumerate(model.random.randint(0, window, sentence_len[sent_idx])):
   421     88201        68955      0.8      0.7              reduced_windows[idx + i] = item
   422                                           
   423      5337         3033      0.6      0.0          idx += len(sent)
   424      5337         3531      0.7      0.0          num_sentences += 1
   425                                           
   426                                               # release GIL & train on the sentences
   427         2            3      1.5      0.0      with nogil:
   428         1            2      2.0      0.0          for sent_idx in range(num_sentences):
   429      5337         3372      0.6      0.0              m = 0 if sent_idx == 0 else sentence_len[sent_idx - 1]
   430      5337         3457      0.6      0.0              for i in range(sentence_len[sent_idx]):
   431     88201        61926      0.7      0.6                  j = i - window + reduced_windows[m + i]
   432     88201        55674      0.6      0.6                  if j < 0:
   433     17316        10970      0.6      0.1                      j = 0
   434     88201        60958      0.7      0.6                  k = i + window + 1 - reduced_windows[m + i]
   435     88201        57073      0.6      0.6                  if k > sentence_len[sent_idx]:
   436     16852        10375      0.6      0.1                      k = sentence_len[sent_idx]
   437     88201        60433      0.7      0.6                  for j in range(j, k):
   438    590350       393176      0.7      4.1                      if j == i:
   439     88201        56467      0.6      0.6                          continue
   440    502149       326425      0.7      3.4                      if hs:
   441    502149      7473863     14.9     77.3                          fast_sentence_sg_hs(points[m + i], codes[m + i], codelens[m + i], syn0, syn1, size, indexes[m + j], _alpha, work, word
_locks)
   442    502149       317992      0.6      3.3                      if negative:
   443                                                                   next_random = fast_sentence_sg_neg(negative, cum_table, cum_table_len, syn0, syn1neg, size, indexes[m + i], indexes[m 
+ j], _alpha, work, next_random, word_locks)
   444                                           
   445         1            3      3.0      0.0      return result
